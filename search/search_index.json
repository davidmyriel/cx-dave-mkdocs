{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#get-started-quickly","title":"Get started quickly","text":""},{"location":"#find-your-quickstart","title":"Find your quickstart","text":"OpenTelemetry AWS GCP Azure"},{"location":"#product-features","title":"Product Features","text":"Feature Group Description Data Transformation Monitoring &amp; Insights Data Visualization Custom Dashboards Real User Monitoring Application Performance Monitoring Alerts Security"},{"location":"changelog/","title":"What's New in Coralogix","text":""},{"location":"changelog/#january-2024","title":"January 2024","text":""},{"location":"changelog/#take-control-with-our-new-role-based-access","title":"Take Control with Our New Role Based Access","text":"<p>Whether you\u2019re a growing business or enterprise, you need to manage your users\u2019 permissions in a clear and effective way. With Coralogix\u2019s new RBAC, you can easily control user access. Create a brand new user role choosing from over 300 different permissions, or select one of our seven predefined system roles\u2014all customizable.</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#suppress-your-alerts-when-needed","title":"Suppress Your Alerts When Needed","text":"<p>Use our new Alert Suppression Rules to mute alerts you don\u2019t need during scheduled maintenance, testing, auto-scaling events or outside working hours. Simply select your specific parameters to suppress what, when and which alerts go off in your system.  </p> <p>View Documentation&gt;</p>"},{"location":"changelog/#more-rum-anyone-core-web-vitals-now-available-to-optimize-user-experience","title":"More RUM Anyone? Core Web Vitals Now Available To Optimize User Experience","text":"<p>In addition to our updated RUM Browser SDK and error tracking alerts, we\u2019ve added Core Web Vitals, so you can easily keep an eye on your web performance and overall user experience. </p> <p>View Documentation&gt;</p>"},{"location":"changelog/#easily-import-and-export-custom-dashboard","title":"Easily Import and Export Custom Dashboard","text":"<p>Effortlessly share your custom dashboards across teams within your organization by importing and exporting them, eliminating the need to recreate dashboards and minimizing overhead.</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#view-metrics-and-what-they-monitor","title":"View Metrics And What They Monitor","text":"<p>Adding metrics to your Custom Dashboard? You can now use free text to search for a metric of your choice and see what that metric monitors. Simply hover over any metric to view its system-generated metadata labels and values.</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#december-2023","title":"December 2023","text":""},{"location":"changelog/#rum-user-sessions","title":"RUM User Sessions","text":"<p>Track every step of your users\u2019 interaction with your website or application. Analyze and correlate real-time data for rapid response to critical issues. Gain insight into user flow for improved conversions and high user satisfaction.</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#genai-query-assistant","title":"GenAI Query Assistant","text":"<p>Our new Query Assistant is an AI-powered search feature that lets you use natural language for querying data. Just type in what you are looking for and the Query Assistant will translate your request into Coralogix\u2019s DataPrime query language. You can modify your query, either in natural language or by changing the generated DataPrime query itself.</p> <p>Learn More&gt;</p>"},{"location":"changelog/#incidents-screen","title":"Incidents Screen","text":"<p>Gain more insights into your triggered alerts with our improved Incidents feature. With a secondary set of notifications, you\u2019ll get more contextual information to drill down around any issues that come your way. Group by tags by region, environment, and more.</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#metric-alert-less-than-usual-condition","title":"Metric Alert \u2013 Less Than Usual Condition","text":"<p>Our AI-leveraged alerts give you full visibility of what is happening in your services, setting the most accurate threshold for your data. The alert system allows for prediction modeling methods and variability to enhance your system\u2019s overall performance.</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#apdex-score","title":"Apdex Score","text":"<p>Check out our latest widget added to our Service Catalog, the Apdex Score, or Application Performance Index. Measure and quantify user requests with a configured response time threshold that\u2019s true to customer satisfaction, from tolerated to frustrated categorization.</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#opentelemetry-lambda-auto-instrumentation","title":"OpenTelemetry Lambda Auto Instrumentation","text":"<p>Coralogix now supports the latest version of OpenTelemetry Lambda auto instrumentation wrappers, together with our AWS Lambda Telemetry Exporter. Continue to get complete telemetry and generate traces for Lambda.  </p> <p>View Documentation&gt;</p>"},{"location":"changelog/#october-2023","title":"October 2023","text":""},{"location":"changelog/#error-template-view","title":"Error Template View","text":"<p>Simplify error management with advanced templating of similar errors into definable issues. Effectively reduce unnecessary noise with drill-down based on customizable filters such as error type, username, URL, session ID, data source and more. </p> <p>View Documentation&gt;</p>"},{"location":"changelog/#gcp-traces","title":"GCP Traces","text":"<p>Expand your APM coverage by sending your Google Cloud traces seamlessly to Coralogix, being able to search, analyze and visualize your applications\u2019 performance and health.</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#aws-kinesis-data-firehose-metrics","title":"AWS Kinesis Data Firehose Metrics","text":"<p>Monitor all your AWS services with real-time streaming data delivered from AWS Kinesis Data Firehose directly to Coralogix. With our simple integration, you can get started in just a few steps.</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#azure-resource-manager-arm-integration","title":"Azure Resource Manager (ARM) Integration","text":"<p>We\u2019ve added more to our ARM integration so you can create custom Coralogix templates to send events from not only Event Hub but also Blob storage, Queue storage and Diagnostic storage.</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#september-2023","title":"September 2023","text":""},{"location":"changelog/#new-quota-manager","title":"New Quota Manager","text":"<p>For extensive org-level data management, the Quota Manager now presents data consumption as both Coralogix Units and GB sent; with group by options: Pillar (Logs, Metrics, Traces) and Priority (Blocked, Low, Medium, High).</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#extensions-improvements","title":"Extensions Improvements","text":"<p>Our evolving extensions page simplifies data transfer to Coralogix. We\u2019ve added new integration flows for GCP Spans, Azure ARM, Azure Metrics, and Slack. Plus, you can now preview flow alerts in Coralogix extensions.</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#dashboard-improvements","title":"Dashboard Improvements","text":"<p>Exciting dashboard updates include interlinked filters, stacked bar charts, and a new colour scheme for graphs. Plus, enjoy design flexibility with the new Markdown Widget and query archived data within custom dashboards.</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#rum-onboarding","title":"RUM Onboarding","text":"<p>Enable Real User Monitoring quickly with the RUM Integration Package. It automates RUM Browser SDK setup and source map uploads, capturing and sending network requests and errors to Coralogix once configured.</p> <p>View Documentation&gt;</p>"},{"location":"changelog/#robust-cspm","title":"Robust CSPM","text":"<p>We\u2019ve improved our CSPM (Cloud Security Posture Management) offering by extending GCP support, offering enhanced clarity and guidance on issue resolution across AWS and GCP in around 600 scenarios.</p> <p>View Documentation &gt;</p>"},{"location":"config-tools/config-tools/","title":"Configuration Tools","text":""},{"location":"config-tools/helm-kubernetes/","title":"Open Source Integrations","text":"<p>Coralogix Open Source Integrations repository is Coralogix's way to ship our best practices when it comes to interaction with our platform, as well as collaborating with our users. Currently we support:</p> <p>Logging integrations, Fluentd and Fluentbit,</p> <p>Metrics integrations, Prometheus,</p> <p>Tracing integrations, OpenTelemetry.</p> <p>Please see #Getting Started for more information about the existing integrations.</p>"},{"location":"config-tools/helm-kubernetes/#getting-started","title":"Getting Started","text":"<p>This repository contains directories for each integration type, logs and metrics, and open-telemetry that can send all.</p> <p>Inside each integration type there are multiple ways to install our integrations, using helm, installing kubernetes manifests directly, using a docker image or installing a service.</p>"},{"location":"config-tools/helm-kubernetes/#helmkubernetes-integrations-prerequisite","title":"Helm/Kubernetes integrations prerequisite","text":"<p>All K8s integrations, both helm and manifests, require a <code>secret</code> called <code>coralogix-keys</code> with the relevant <code>private key</code> under a secret key called <code>PRIVATE_KEY</code>, inside the <code>same namespace</code> that the integration is installed in.</p> <ul> <li>The <code>private key</code> appears under 'Data Flow' --&gt; 'API Keys' in Coralogix UI:</li> </ul> <pre><code>kubectl create secret generic coralogix-keys \\\n  -n &lt;the-namespace-of-the-integrations&gt; \\\n  --from-literal=PRIVATE_KEY=&lt;private-key&gt;\n</code></pre> <p>for more information regarding the coralogix private key please visit here</p> <p>The created secret should look like this:</p> <pre><code>apiVersion: v1\ndata:\n  PRIVATE_KEY: &lt;encrypted-private-key&gt;\nkind: Secret\nmetadata:\n  name: coralogix-keys\n  namespace: &lt;the-integration-namespace&gt;\ntype: Opaque \n</code></pre>"},{"location":"config-tools/helm-kubernetes/#installation","title":"Installation","text":""},{"location":"config-tools/helm-kubernetes/#helm","title":"Helm","text":"<p>In the 'logs' integrations inside the 'k8s-helm' there are two supported <code>helm charts</code>, one using the <code>Coralogix</code> output plugin, and another one using the <code>http</code> output plugin. We recommend using the <code>http</code> chart, since it's an open source plugin, and therefore it is more community driven.</p> <p>Under each integration there is an 'image' directory which our GitHub Actions workflows use in order to build the image and publish it to DockerHub.</p> <p>Our Helm charts repository can be added to the local repos list with the following command. It will create a repository named <code>coralogix-charts-virtual</code>. If you wish to change it to anything else, be sure to adapt your commands in the other segments referring to this repository.</p> <pre><code>helm repo add coralogix-charts-virtual https://cgx.jfrog.io/artifactory/coralogix-charts-virtual\n</code></pre> <p>In order to get the updated helm charts from the added repository, please run:</p> <pre><code>helm repo update\n</code></pre> <p>For installation of each integration, please go inside each intergation's directory: - Fluentd-HTTP chart - Fluent-bit-HTTP chart - Prometheus operator chart - OpenTelementry-Agent chart</p>"},{"location":"config-tools/helm-kubernetes/#kubernetes","title":"Kubernetes","text":"<p>Our k8s manifests integration allow you to install without the use of Helm, specifically for those times were using helm is impossible.</p> <p>For installation of each integration, please go inside each intergation's directory: - Fluentd-HTTP - Fluent-bit-HTTP</p>"},{"location":"config-tools/helm-kubernetes/#these-integrations-were-checked-on-kubernetes-120","title":"These integrations were checked on Kubernetes 1.20+.","text":""},{"location":"config-tools/terraform-provider/","title":"Coralogix Terraform Provider","text":"<p>The Coralogix provider is used to interact with the resources supported by Coralogix. The provider needs to be configured with the proper credentials before it can be used.</p>"},{"location":"config-tools/terraform-provider/#example-usage","title":"Example Usage","text":"<pre><code>terraform {\n  required_providers {\n    coralogix = {\n      version = \"&lt;your desired version&gt;\"\n      source  = \"coralogix/coralogix\"\n    }\n  }\n}\n\nprovider \"coralogix\" {\n  api_key = \"&lt;add your api key here or add env variable CORALOGIX_API_KEY&gt;\"\n  env     = \"&lt;add the environment you want to work at or add env variable CORALOGIX_ENV&gt;\"\n}\n\nresource \"coralogix_rules_group\" \"my_first_rules_group\" {\n  name = \"my first rules_group\"\n}\n</code></pre>"},{"location":"config-tools/terraform-provider/#authentication","title":"Authentication","text":"<p>For authentication, the Coralogix provider uses an api-key (under API Keys in the UI - Alerts, Rules and Tags API Key). The api-key and the desired environment for using the Coralogix provider can be set in two ways:</p> <ol> <li>Explicitly:</li> </ol> <pre><code>provider \"coralogix\" {\n  api_key = \"&lt;add your api key&gt;\"\n  env     = \"&lt;add the environment you want to work at&gt;\"\n}\n</code></pre> <ol> <li>Implicitly through environment variables:</li> </ol> <pre><code>$ export CORALOGIX_API_KEY=\"&lt;add your api key&gt;\"\n</code></pre> <pre><code>$ export CORALOGIX_ENV=\"&lt;add the environment you want to work at&gt;\" \n</code></pre>"},{"location":"config-tools/terraform-provider/#private-domains","title":"Private Domains","text":"<p>For private domain the <code>domain</code> field or the environment variables <code>CORALOGIX_DOMAIN</code> have to be defined (instead of <code>env</code> or <code>CORALOGIX_ENV</code>).</p> <pre><code>provider \"coralogix\" {\n  api_key = \"&lt;add your api key&gt;\"\n  domain = \"cx.coralogix.com\"\n}\n</code></pre> <p>OR</p> <pre><code>$ export CORALOGIX_DOMAIN=\"&lt;add the environment you want to work at&gt;\" \n</code></pre>"},{"location":"config-tools/terraform-provider/#region-domain-table","title":"region-domain table:","text":"Region Domain APAC1 app.coralogix.in APAC2 coralogixsg.com EUROPE1 coralogix.com EUROPE2 eu2.coralogix.com USA1 coralogix.us USA2 cx498.coralogix.com"},{"location":"config-tools/terraform-provider/#argument-reference","title":"Argument Reference","text":"<ul> <li><code>api_key</code> (String, Sensitive) A key for using coralogix APIs (Auto Generated), appropriate for the defined   environment. environment variable 'CORALOGIX_API_KEY' can be defined instead.</li> <li><code>domain</code> (String) The Coralogix domain. Conflict With 'env'. environment variable 'CORALOGIX_DOMAIN' can be defined   instead.</li> <li><code>env</code> (String) The Coralogix API environment. can be one of [\"USA1\" \"USA2\" \"APAC1\" \"APAC2\" \"EUROPE1\" \"EUROPE2\"]. environment   variable 'CORALOGIX_ENV' can be defined instead.</li> </ul>"},{"location":"features/features/","title":"Coralogix Features","text":""},{"location":"features/administration/teams/","title":"Teams","text":""},{"location":"features/administration/teams/#overview","title":"Overview","text":"<p>A Coralogix Team is a platform environment with its unique URL, settings, and Send-Your-Data API key. Teams are used to group users together based on their project, department, or any other relevant criteria. Teams provide a convenient way to collectively manage permissions and settings for a group of users. You can assign teams to specific logs, alerts, or dashboards, ensuring that the right people can access the relevant information. Users may view data only of those teams of which they are members.</p> <p>Teams can consist of users from different Groups, allowing you to create flexible and dynamic access controls. For example, you can have a Development Team with users from the Engineering, Operations, and Support groups. This allows you to grant permissions and manage access granularly, aligning with your organization\u2019s structure and requirements.</p>"},{"location":"features/administration/teams/#create-a-team","title":"Create a Team","text":"<p>If you haven\u2019t already done so, sign up for a free Coralogix account. You will be prompted to create a new team. Input a team name and click CREATE TEAM.</p> <p>If you already have an account, click CREATE NEW TEAM in your login screen.</p> <p></p> <p>Alternatively, click + CREATE NEW TEAM in the upper right-hand corner of your Coralogix dashboard.</p> <p></p>"},{"location":"features/administration/teams/#manage-existing-team-members","title":"Manage Existing Team Members","text":"<p>STEP 1. Access your settings in the upper-right hand corner of your Coralogix dashboard.</p> <p>STEP 2. In the left-hand sidebar, click Team Members. A list of existing team members will appear.</p> <p>STEP 3. Search existing team members. You may filter your search according to member roles.</p> <p></p> <p>STEP 4. Administrators (admins) may add team members, remove them, or change their permissions by clicking on the drop-down menu right of the user\u2019s name.</p> <p>Notes:</p> <ul> <li>Users can be assigned to more than one team.</li> <li>Users may view data only of those teams of which they are members.</li> <li>Upon logging in, users may select the team within which they would like to work.</li> <li>Each team has its own unique Send-Your-Data API key.</li> </ul>"},{"location":"features/administration/teams/#sso-login","title":"SSO Login","text":"<p>For instructions on how to set up a single sign-on (SSO) with your IDP, follow our tutorial here.</p> <p>Notes:</p> <ul> <li>If your admin configures a SAML SSO, no password is necessary for you to sign in to Coralogix.</li> <li>Only the admin is authorized to change the password if an SSO is enabled.</li> </ul>"},{"location":"features/administration/teams/#session-length-management","title":"Session Length Management","text":"<p>Team administrators can define the duration of idle sessions for all users in the team. Enabling this option will end all current login sessions and require users to log in again. Find out more here.</p>"},{"location":"features/administration/teams/#role-based-access-control","title":"Role-Based Access Control","text":"<p>Role-based access control (RBAC) allows account administrators to grant some or all team members specific application and subsystem data scope permissions for logs and traces, as well as action permissions.</p> <ul> <li>RBAC for logs. Find out more here.</li> <li>RBAC for traces. Find out more here.</li> </ul>"},{"location":"features/administration/teams/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us via our in-app chat or by sending us an email at support@coralogix.com.</p>"},{"location":"features/apm/page1/","title":"Overview","text":"<p>Coralogix offers application performance monitoring (APM) for modern, cloud-native environments. Our new features decorate all pillars of observability with additional information that extends beyond system availability, service performance, and response times.</p> <p>With this expanded visibility into service performance, you can effectively monitor latency and rapidly find the component responsible for issues like performance degradation or an increase in errors. APM allows you to contextualize and pinpoint the root cause of a problem and respond immediately before the user is affected.</p>"},{"location":"features/apm/page1/#concepts","title":"Concepts","text":"<p>Spans and traces form the basis of application performance monitoring in Coralogix APM.</p> <p>Using this telemetry data, Coralogix allows you to observe application resource consumption and infrastructure resource consumption using two new observability layers.</p> POD application resource consumption Key factors that impact response times and throughput of applications HOST infrastructure resource consumption Usage of IT resources, systems, and processes"},{"location":"features/apm/page1/#features","title":"Features","text":"<p>Pod &amp; Host</p> <p>Our APM provides you with all logs relevant to a particular span context, granting a full picture of the services that power your applications.</p> <p>Use our newest layers of observability \u2013 POD and HOST \u2013 to:</p> <ul> <li>Instantly view all of your pod and host metrics, including resource consumption and associated network information</li> <li>Compare metrics within a specific pod and across pods from a specific service</li> <li>Compare all of the pods associated with a specific service</li> <li>Correlate between Kubernetes spans, logs, and metrics for a specific pod and/or host</li> <li>Troubleshoot log span errors</li> <li>Annotate deployment tags based on span context</li> </ul> <p> </p>"},{"location":"features/rum/page2/","title":"Page 2","text":""},{"location":"features/rum/page2/#another-heading","title":"Another heading","text":"<p>Some more example text</p>"},{"location":"getting-started/","title":"Getting Started with Coralogix","text":"<p>Coralogix is a cloud-based, SaaS analytics and monitoring platform that combines  logs, metrics, and traces  to gain full observability into your system using one tool. The platform ingests data from any digital source and transforms it using our  core features, allowing you to fully understand your system, analyze that data efficiently, and respond to incidents before they become problems.</p> <p>To get started with Coralogix, sign up for a free account and import your system\u2019s telemetry. Once your data is ingested by our platform, you can use our core features to obtain full observability in your Coralogix dashboard.</p> <p></p>"},{"location":"getting-started/#sign-up-for-a-free-account","title":"Sign Up for a Free Account","text":"<p>Coralogix offers free account setup. If you don\u2019t already have an account, you can sign up here. You will be prompted to create a new team.</p> <p></p> <p>If your organization already has an account, you may have different signup options depending on the permissions set by your organization administrator.</p>"},{"location":"getting-started/#send-data-to-coralogix","title":"Send Data to Coralogix","text":"<p>Coralogix supports logs, metrics, and traces from many different sources using any of the following  integrations. All integrations require:</p> <ul> <li>Your Coralogix  Send-Your-Data API key</li> <li>An  endpoint  associated with your Coralogix account  domain</li> <li>Application and subsystem names  to organize the data in your Coralogix account</li> </ul>"},{"location":"getting-started/#integration-packages","title":"Integration Packages","text":"<p>The  easiest method  for sending us your data is using our two-step, out-of-the box  integration packages.</p> <p></p>"},{"location":"getting-started/#integrations-interactive","title":"Integrations (Interactive)","text":"<p>For those integrations which are yet to be packaged, select a shipper for which to send us your data from our full list of  integrations.</p> <ul> <li>Cloud-Based Integrations. We offer a wide range of cloud-based shippers, including AWS, Azure, and GCP integrations.</li> <li>Telemetry Shippers. Choose from our many shippers, including integrations using  OpenTelemetry,  Prometheus,  Fluentd  and  Fluent Bit.</li> <li>Push &amp; Pull Integrations. Choose from our list of push and pull integrations, including  Cloudflare,  Nagios, and  Okta.</li> <li>Use-Cases.  Select a use-case integration on the basis of the particular logs, metrics, or traces you\u2019d like to send us.</li> </ul> <p></p>"},{"location":"getting-started/#coralogix-apis","title":"Coralogix APIs","text":"<p>Optimize Coralogix\u2019s observability monitoring and unlock its most powerful features by using our wide range of  APIs. Use them to send data to Coralogix, build visualizations, manage your data, and query it.</p> <ul> <li>Data Ingestion APIs.  Data is ingested seamlessly and reliably into the Coralogix platform using a wide range of APIs.</li> <li>Data Management APIs.  Configure the Coralogix platform, customize your user interface, and optimize it for your observability requirements.</li> <li>Data Query APIs. Use these APIs to access and query your data.</li> </ul> <p>Our APIs can be configured using our Helm charts, Terraform modules, or the  Coralogix Operator.</p>"},{"location":"getting-started/#get-familiar-with-coralogix-features","title":"Get Familiar with Coralogix Features","text":"<p>Once you have started sending us your data, Coralogix offers a rich bank of  extensions  to enrich your data with a set of predefined items \u2013 alerts, parsing rules, dashboards, saved views, actions, and more. Take our  features tour  to better understand these concepts and kick-start your observability monitoring process.</p> <p></p>"},{"location":"getting-started/#coralogix-academy","title":"Coralogix Academy","text":"<p>Dive into Coralogix and discover the platform\u2019s vast capabilities tailored for both new and experienced users. Our newest  Coralogix Academy Course  will introduce you to Coralogix\u2019s main features and functionalities, so that you\u2019re ready to utilize the platform effectively. Through straightforward lesson plans, interactive sessions and practical exercises, you\u2019ll gain a complete understanding of how to turn Coralogix into an asset for all your observability operations.</p>"},{"location":"getting-started/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up. Feel free to reach out to us via our in-app chat or by sending us an email at support@coralogix.com</p>"},{"location":"getting-started/page1/","title":"Key Features","text":"<p>The Coralogix monitoring platform ingests data from any digital source and transforms it using our core features, unleashed with our out-of-the-box extension packages. Take advantage of these evolving features to fully understand your system, analyze changes in its behavior, and respond to incidents before they become problems.</p> <p></p>"},{"location":"getting-started/page1/#core-features","title":"Core Features","text":""},{"location":"getting-started/page1/#data-collection-and-centralized-storage","title":"Data Collection and Centralized Storage","text":"<p>Coralogix provides integrations with popular logging frameworks and libraries, enabling you to easily transfer your data to Coralogix for centralized storage and analysis. With our unique Streama\u00a9 technology, you can analyze your data without needing to index it, thereby avoiding costly storage expenses.</p>"},{"location":"getting-started/page1/#data-transformation-and-parsing","title":"Data transformation and parsing","text":"<p>Coralogix automatically parses and structures your telemetry data. - Parse your data to make it easier to search, filter, and analyze. Get started with our Parsing Rules Cheat Sheet. - Enrich your data with business, operations, or security information, including IP-based geographical information, that may not be available at runtime. AWS customers can enrich their logs with tags from their AWS EC2 instances to connect their business and operation metadata, and gain greater insight into their data. - Record new metric time series to produce leaner and more quickly queried metrics.</p>"},{"location":"getting-started/page1/#data-query","title":"Data query","text":"<p>Extract specific information or insights from your collected logs, metrics, and traces using Lucene, SQL, or our advanced DataPrime query language. We make it possible for you to directly query your archived data at a speed 5x faster than Athena.</p>"},{"location":"getting-started/page1/#alerting","title":"Alerting","text":"<p>By querying logs and metrics, you can set up alerts and notifications based on specific conditions or thresholds. These alerts can notify you when predefined events occur or when metrics deviate from normal behavior.</p>"},{"location":"getting-started/page1/#monitoring","title":"Monitoring","text":"<p>Monitor your data using our advanced Explore screen, where you can drill down into your logs and traces. Engage with our Events2Metrics functionality to generate metrics from your spans and logs to optimize storage without sacrificing important data. What\u2019s more, we offer application performance monitoring (APM) which decorates our standard observability options with additional information.</p> <p></p>"},{"location":"getting-started/page1/#visualizations","title":"Visualizations","text":"<p>Create unlimited, personalized custom dashboards catered to your specific observability needs, or take advantage of our pre-built dashboards to help you analyze and visualize log data. We also offer Grafana and Kubernetes dashboards.</p> <p></p>"},{"location":"getting-started/page1/#compliance-and-security","title":"Compliance and security","text":"<p>Coralogix prioritizes data security and compliance. It offers encryption at rest and in transit, access controls, and adheres to industry-standard security practices. The platform helps businesses meet compliance requirements, such as GDPR and HIPAA, by providing features like data anonymization and audit logs.</p> <p>Our features, along with unparalleled customer support and excellent cost optimization, make us one of the few observability providers that can help you grow, optimize, and save money at the same time. To get started, click here.</p>"},{"location":"getting-started/page2/","title":"Coralogix Endpoints","text":"<p>Coralogix offers a regional endpoint for sending data from all observability sources into the Coralogix platform.</p> <p>This document provides generally available endpoints. AWS PrivateLink endpoints can be found here.</p> <p>Notes: The tables below provide the most up-to-date endpoints provided by Coralogix. Other endpoints utilized in Coralogix integrations remain available. Find out more about your Coralogix domain here.</p>"},{"location":"getting-started/page2/#endpoints","title":"Endpoints","text":""},{"location":"getting-started/page2/#opentelemetry","title":"OpenTelemetry","text":"Coralogix Domain Coralogix AWS Region Endpoint coralogix.com eu-west-1 [EU1 \u2013 Ireland] ingress.coralogix.com:443 coralogix.in ap-south1 [AP1 \u2013 India] ingress.coralogix.in:443 coralogix.us us-east2 [US1 \u2013 Ohio] ingress.coralogix.us:443 eu2.coralogix.com eu-north-1 [EU2 \u2013 Stockholm] ingress.eu2.coralogix.com:443 coralogixsg.com ap-southeast-1 [AP2 \u2013 Singapore] ingress.coralogixsg.com:443 cx498.coralogix.com us-west-2 [US2 \u2013 Oregon] ingress.cx498-aws-us-west-2.coralogix.com:443"},{"location":"getting-started/page2/#coralogix-logs","title":"Coralogix Logs","text":"Coralogix Domain Coralogix AWS Region Endpoint coralogix.com eu-west-1 [EU1 \u2013 Ireland] https://ingress.coralogix.com/logs/v1/singles coralogix.in ap-south1 [AP1 \u2013 India] https://ingress.coralogix.in/logs/v1/singles coralogix.us us-east2 [US1 \u2013 Ohio] https://ingress.coralogix.us/logs/v1/singles eu2.coralogix.com eu-north-1 [EU2 \u2013 Stockholm] https://ingress.eu2.coralogix.com/logs/v1/singles coralogixsg.com ap-southeast-1 [AP2 \u2013 Singapore] https://ingress.coralogixsg.com/logs/v1/singles cx498.coralogix.com us-west-2 [US2 \u2013 Oregon] https://ingress.cx498-aws-us-west-2.coralogix.com/logs/v1/singles"},{"location":"getting-started/page2/#coralogix-rest-api-bulk","title":"Coralogix REST API Bulk","text":"Coralogix Domain Coralogix AWS Region Endpoint coralogix.com eu-west-1 [EU1 \u2013 Ireland] https://ingress.coralogix.com/logs/v1/bulk coralogix.in ap-south1 [AP1 \u2013 India] https://ingress.coralogix.in/logs/v1/bulk coralogix.us us-east2 [US1 \u2013 Ohio] https://ingress.coralogix.us/logs/v1/bulk eu2.coralogix.com eu-north-1 [EU2 \u2013 Stockholm] https://ingress.eu2.coralogix.com/logs/v1/bulk coralogixsg.com ap-southeast-1 [AP2 \u2013 Singapore] https://ingress.coralogixsg.com/logs/v1/bulk cx498.coralogix.com us-west-2 [US2 \u2013 Oregon] https://ingress.cx498.coralogix.com/logs/v1/bulk"},{"location":"getting-started/page2/#coralogix-rest-api-singles","title":"Coralogix REST API Singles","text":"Coralogix Domain Coralogix AWS Region Endpoint coralogix.com eu-west-1 [EU1 \u2013 Ireland] https://ingress.coralogix.com/logs/v1/singles coralogix.in ap-south1 [AP1 \u2013 India] https://ingress.coralogix.in/logs/v1/singles coralogix.us us-east2 [US1 \u2013 Ohio] https://ingress.coralogix.us/logs/v1/singles eu2.coralogix.com eu-north-1 [EU2 \u2013 Stockholm] https://ingress.eu2.coralogix.com/logs/v1/singles coralogixsg.com ap-southeast-1 [AP2 \u2013 Singapore] https://ingress.coralogixsg.com/logs/v1/singles cx498.coralogix.com us-west-2 [US2 \u2013 Oregon] https://ingress.cx498.coralogix.com/logs/v1/singles"},{"location":"getting-started/page2/#prometheus-remotewrite","title":"Prometheus RemoteWrite","text":"Coralogix Domain Coralogix AWS Region Endpoint coralogix.com eu-west-1 [EU1 \u2013 Ireland] https://ingress.coralogix.com/prometheus/v1 coralogix.in ap-south1 [AP1 \u2013 India] https://ingress.coralogix.in/prometheus/v1 coralogix.us us-east2 [US1 \u2013 Ohio] https://ingress.coralogix.us/prometheus/v1 eu2.coralogix.com eu-north-1 [EU2 \u2013 Stockholm] https://ingress.eu2.coralogix.com/prometheus/v1 coralogixsg.com ap-southeast-1 [AP2 \u2013 Singapore] https://ingress.coralogixsg.com/prometheus/v1 cx498.coralogix.com us-west-2 [US2 \u2013 Oregon] https://ingress.cx498-aws-us-west-2.coralogix.com/prometheus/v1"},{"location":"getting-started/page2/#tls-tcp-syslog","title":"TLS / TCP Syslog","text":"Coralogix Domain Coralogix AWS Region Endpoint coralogix.com eu-west-1 [EU1 \u2013 Ireland] syslog.coralogix.com:6514 coralogix.in ap-south1 [AP1 \u2013 India] syslog.coralogix.in:6514 coralogix.us us-east2 [US1 \u2013 Ohio] syslog.coralogix.us:6514 eu2.coralogix.com eu-north-1 [EU2 \u2013 Stockholm] syslog.eu2.coralogix.com:6514 coralogixsg.com ap-southeast-1 [AP2 \u2013 Singapore] syslog.coralogixsg.com:6514 cx498.coralogix.com us-west-2 [US2 \u2013 Oregon] syslog.cx498.coralogix.com:6514"},{"location":"getting-started/page2/#management","title":"Management","text":"Coralogix Domain Coralogix AWS Region Endpoint coralogix.com eu-west-1 [EU1 \u2013 Ireland] https://ng-api-http.coralogix.com/api/v1/dataprime/query coralogix.in ap-south1 AP1"},{"location":"integrations/","title":"Integrations","text":""},{"location":"integrations/integrations/","title":"Integrations","text":""},{"location":"integrations/gcp/gcp0/","title":"GCP - Getting Started","text":""},{"location":"integrations/gcp/gcp0/#overview","title":"Overview","text":"<p>Coralogix offers a number of basic integrations with Google Cloud Platform.  However, a prerequisite for each is that you first Configure a Service Account. Once you have created a Service Account and its corresponding API key, access your integration-specific instructions in the table below:</p>"},{"location":"integrations/gcp/gcp0/#configure-a-service-account","title":"Configure a Service Account","text":"<p>This should be done before any GCP integration with Coralogix. Please make sure that you have Super admin permissions. Also, you should have an existing project within Google Cloud. Your new service account will be created there.</p> <p>On the Google Cloud side, basic Service Account configuration is done in two steps: </p> <p>Step 1. You first need to create a new Service Account to authenticate with Coralogix. </p> <p>Step 2. For this account, you will create an API key. This key will be used for the Coralogix integration. </p>"},{"location":"integrations/gcp/gcp0/#step-1-create-a-service-account","title":"Step 1: Create a Service Account","text":"<p>1. Sign in to Google Cloud Console and choose the project where you want to create the service account.</p> <p>2. Go to IAM &amp; Admin in the menu and scroll down to Service Accounts.</p> <p>3. On the Service accounts list, click + CREATE SERVICE ACCOUNT.</p> <p>4. Input your service account details: name, account ID, and description. Click CREATE AND CONTINUE.</p> <p>5. Based on the type of integration you are setting up, you may need to assign specific roles to the service account.</p> <ul> <li>GCP Logs: To collect logs, select the <code>Pub/Sub Subscriber</code>role.</li> <li>GCP Traces: To collect traces, select\u00a0<code>BigQuery Job User</code>\u00a0and\u00a0<code>BigQuery Data Viewer</code>\u00a0.</li> <li>GCP Metrics: To collect metrics, select\u00a0<code>Compute Viewer</code>,\u00a0<code>Monitoring Viewer</code>, and\u00a0<code>Cloud Asset Viewer.</code></li> <li>Google Workspace Users: You don\u2019t need to assign roles for this service account.</li> <li>Google Alerts Center: You don\u2019t need to assign roles for this service account.</li> </ul> <p>At the end, click CONTINUE and DONE in the step below. Your service account is now ready for use.</p> <p>6.  Find the new service account on the list. Note down the OAuth 2 Client ID. You will need it to finish this tutorial.</p> <p></p>"},{"location":"integrations/gcp/gcp0/#step-2-create-a-private-key","title":"Step 2: Create a Private Key","text":"<p>1. Click the three dots in the rightmost Actions column and choose Manage keys.</p> <p></p> <p>2. Click Add Key and choose the JSON Key type. Download it and store locally. You will need to upload it to your Coralogix integration.</p>"},{"location":"integrations/gcp/gcp0/#next-steps","title":"Next Steps","text":"<p>Now that you have created a Service Account and an API key, consult our integration-specific tutorials. For these, you will need to use the Coralogix platform.</p>"},{"location":"integrations/gcp/gcp0/#support","title":"Support","text":"<p>Need help? </p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up. </p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email to\u00a0support@coralogix.com.</p>"},{"location":"integrations/gcp/gcp1/","title":"Google Workspace Users","text":""},{"location":"integrations/gcp/gcp1/#overview","title":"Overview","text":"<p>This integration will let you collect all user details from your Google Workspace Admin Console along with their metadata. This integration normalizes user identifiers from relevant product logs into a <code>cx_security.user</code> key. This will let you enrich user data on d with additional context, which can be leveraged through many platform features.</p>"},{"location":"integrations/gcp/gcp1/#benefits","title":"Benefits","text":"<ul> <li>Monitor User Activities: By ingesting user metadata, you can track and oversee user activities across products. Monitor actions such as user logins, network browsing activities, email communications.</li> <li>Investigate Incidents: Since each user entry is enriched with additional metadata, security administrators can easily get full user context within Coralogix.</li> <li>Maintain Compliance: Additional user metadata will help you meet compliance requirements. You can generate a user activity report for auditing purposes, which is often used to maintain proof of compliance efforts.</li> <li>Detect Anomalies: By analyzing user metadata along with other logs, you can establish a baseline for user behavior. For example, as you import additional context, you can set Alert Rules based on metadata, such as user department or role. </li> <li>Visualize Results: Track your organizational activity via Custom Dashboards. As you ingest user metadata, you can apply this information to your existing dashboards across all products or create new dashboards with added context. </li> </ul>"},{"location":"integrations/gcp/gcp1/#prerequisites","title":"Prerequisites","text":"<ul> <li>Super admin permissions in Google Cloud.</li> <li>An existing project within your Google Cloud.</li> <li>Configure a Service Account and API Key to facilitate automated intermediation.</li> <li>Domain Wide Delegation, to authorise your Service Account to read user data and send it to Coralogix.</li> </ul>"},{"location":"integrations/gcp/gcp1/#setup-instructions","title":"Setup Instructions","text":"<p>1. From your Coralogix toolbar, navigate to Data Flow &gt; Integrations.</p> <p>2. From the Integrations section, select Google Workspace.</p> <p>3. Click + ADD NEW.</p> <p>4. If you haven\u2019t already done so, click GO TO GCP ACCOUNT and create a key file. Then, click NEXT.</p> <p>5. Click SELECT FILE and upload the key file you previously created.</p> <p>6. A confirmation will appear when the file is uploaded successfully. Click NEXT.</p> <p>7. Fill in the settings: </p> <ul> <li>Integration Name: Enter a name for your integration. This field is automatically populated, but can be changed if you want.</li> <li>Organization Name: Enter the organization name in Google Cloud where the service account was created. You can find this by navigating to IAM &amp; Admin &gt; Settings on Google Cloud.</li> <li>Organization ID: Enter the organization ID of Google Cloud where the service account was created. You can find this by going to IAM &amp; Admin &gt; Settings on Google Cloud.</li> <li>Impersonated Email: Enter a valid email address to be impersonated when connecting to Google Workspace using the service account created above.</li> </ul> <p>8. Click COMPLETE and finish the setup. </p>  \ud83d\udca1 Please wait a few minutes before the integration takes effect and user data is available.   <p>9. Verify that the integration pipeline occurred. Go to Data Flow &gt; Data Enrichment. Under Custom Enrichment verify that the following two files were created: <code>users_normalization</code> and <code>users_metadata</code></p> <p>10. Check that user data was properly ingested. Go to the Explore screen, and run these DataPrime queries to verify that the enrichment files were uploaded with the user details: </p> <pre><code>source users_normalization\n\nsource users_metadata\n</code></pre> <p>If you are not getting a response, please wait few minutes and try again.</p>"},{"location":"integrations/gcp/gcp1/#work-with-enriched-user-data","title":"Work with Enriched User Data","text":"<p>Once you have activated this integration, you will want to start working with your ingested user data. For this, you will need to work with two auto generated files in DataPrime.</p>"},{"location":"integrations/gcp/gcp1/#look-up-users-in-the-users_normalization-file","title":"Look up users in the users_normalization file","text":"<p>Before enrichment, all you see is the user\u2019s email address. After this integration, the UI will reveal additional identity metadata. At least, you will see the user\u2019s display name, which is automatically enriched via <code>users_normalization</code>.</p> <p>Specifically, the <code>users_normalization</code> file lists enrichment details across users. In the example above, a log key containing the user\u2019s email address <code>cx_security.email</code> is augmented with the user\u2019s display name and email address in brackets under <code>cx_security.email_enriched</code>.</p> <p>This basic enrichment update is automatic. To enrich other keys containing the email address during log ingestion, select the key in Data Flow &gt; Data Enrichment  &gt; Custom Enrichments  &gt; <code>users_normalization</code>. To view the contents of this file, run the DataPrime query:</p> <pre><code>source users_normalization\n</code></pre>"},{"location":"integrations/gcp/gcp1/#enrich-metadata-via-the-users_metadata-file","title":"Enrich metadata via the users_metadata file","text":"<p>The <code>users_metadata</code> file augments the user with additional metadata, such as department, title and manager\u2019s email. To view all user metadata and further filter results by user, run the DataPrime query:</p> <pre><code>source users_metadata\n</code></pre> <p>To look up the individual user, find the lookup value in the <code>users_normalization</code> file above. To enrich the individual user, run the DataPrime command:</p> <pre><code>cx_security.email_enriched\n</code></pre>"},{"location":"integrations/gcp/gcp1/#support","title":"Support","text":"<p>Need help? </p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up. </p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email to\u00a0support@coralogix.com.</p>"},{"location":"integrations/gcp/gcp2/","title":"Google Alerts Center","text":""},{"location":"integrations/gcp/gcp2/#overview","title":"Overview","text":"<p>Google Workspace Admin Alert Center offers real-time security alerts and insights that help you protect your organization from the latest threats, including phishing, malware, and other suspicious activity.</p> <p>You can receive these alerts directly to Coralogix, enabling you to further expand your org\u2019s security administration. The following tutorial will show you how to directly integrate Alert Center with Coralogix.</p>"},{"location":"integrations/gcp/gcp2/#benefits","title":"Benefits","text":"<p>Centralized view of all security incidents. Broad context to improve the investigation with other incidents/activities done in other products/systems. Dashboard visualization of overall security issues, across products.</p>"},{"location":"integrations/gcp/gcp2/#prerequisites","title":"Prerequisites","text":"<ul> <li>Super admin permissions in Google Cloud.</li> <li>An existing project within Google Cloud.</li> <li>Configure a Service Account and API Key to facilitate automated intermediation.</li> <li>Domain Wide Delegation, to authorise your Service Account to read user data and send it to Coralogix.</li> </ul>"},{"location":"integrations/gcp/gcp2/#setup-instructions","title":"Setup Instructions","text":"<p>1. From your Coralogix toolbar, navigate to Data Flow &gt; Integrations.</p> <p>2. From the Integrations section, select Google Alerts Center.</p> <p>3. Click + ADD NEW.</p> <p>4. If you haven\u2019t already done so, click GO TO GCP ACCOUNT and create a key file.  Then, click NEXT.</p> <p>5. Click SELECT FILE and upload the key file you previously created.</p> <p>6. A confirmation will appear when the file is uploaded successfully. Click NEXT.</p> <p>7. Fill in the settings: </p> <ul> <li>Integration Name: Enter a name for your integration. This field is automatically populated, but can be changed if you want.</li> <li>Organization Name: Enter the name of the organization on Google Workspaces to be monitored. You can find this by navigating to IAM &amp; Admin &gt; Settings on Google Cloud.</li> <li>Organization ID: Enter the ID of the organization to be monitored. You can find this by going to IAM &amp; Admin &gt; Settings on Google Cloud.</li> <li>Impersonated Email: Enter a valid email address to be impersonated when connecting to Google Workspace using the service account created above.</li> </ul> <p>8. Click COMPLETE and finish the setup. </p>  \ud83d\udca1 Please wait a few minutes before the integration takes effect and your data is available on the platform."},{"location":"integrations/gcp/gcp2/#support","title":"Support","text":"<p>Need help? </p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up. </p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email to\u00a0support@coralogix.com.</p>"},{"location":"integrations/open-telemetry/open-telemetry1/","title":"Open Telemetry 1","text":""},{"location":"integrations/prometheus/prometheus1/","title":"Prometheus","text":""},{"location":"newoutput/adding-archive-data-to-custom-dashboard-widgets/","title":"Add Monitoring Data to Custom Dashboard Widgets","text":"<p>Coralogix supports adding monitoring data (formerly known as archive data) to your custom dashboard widgets. This enables you to see data from the Monitoring (medium priority) tier in your custom dashboards in addition to the regular Frequent Search (high priority) data.</p>"},{"location":"newoutput/adding-archive-data-to-custom-dashboard-widgets/#what-is-monitoring-data","title":"What is Monitoring Data?","text":"<p>When using Coralogix for log analytics, the platform typically ingests log, metrics, and traces from various sources such as servers, applications, or cloud services.</p> <p>Using our\u00a0Streama\u00a9 technology, this data is processed, analyzed, and visualized within the Coralogix platform. However, not all system capabilities are available for compliance data.</p> <p>To optimize costs and performance, Coralogix provides an option to archive log data in Amazon S3. This means that based on specific criteria (TCO policies) or after a set period of time (retention period), certain logs and spans are available only in the lower-cost, scalable, and durable storage solution. While in the archive, the data is still accessible, but has limitations on real-time processing when compared to actively indexed, high-priority data.</p>"},{"location":"newoutput/adding-archive-data-to-custom-dashboard-widgets/#how-can-i-use-monitoring-data-in-custom-dashboards","title":"How Can I Use Monitoring Data in Custom Dashboards?","text":"<p>Including monitoring data in your custom dashboard allows you to have a more comprehensive and detailed view of your data, combining both recent real-time information and additional context. Specific reasons for using monitoring data might vary depending on your organization's requirements, compliance policies, and analytical needs.</p> <p>Take a look at these use-cases to get a feel for the many ways that monitoring data in Custom Dashboards can serve you.</p>"},{"location":"newoutput/adding-archive-data-to-custom-dashboard-widgets/#use-case-1-multiple-subsystem-locations","title":"Use-Case 1: Multiple Subsystem Locations","text":"<p>A company has a case where some of their subsystems are under a policy that is saved in OpenSearch servers, and some of their subsystems are saved in the archive due to TCO policies. A DevOps engineer wants to create a widget that collates the subsystems from all the different locations.</p> <p>The DevOps engineer uses an Archive Query to create a visualization that shows the subsystems from all sources, regardless of whether they are saved in the OpenSearch servers or in the archive.</p>"},{"location":"newoutput/adding-archive-data-to-custom-dashboard-widgets/#use-case-2-data-whose-retention-period-has-passed","title":"Use-Case 2: Data Whose Retention Period Has Passed","text":"<p>A DevOps engineer wants to query a specific status from today and compare it to the same time period in the previous month (for example, to compare a 15 minute period on the 1st of the month at 10am to the same 15 minutes on the 1st of the previous month at 10am). The data from the previous month is no longer within the retention period, and therefore is only available in the archive.</p> <p>The DevOps engineer creates a visualization that shows the period of time this month and compares it to the same period of time last month from the archive data.</p>"},{"location":"newoutput/adding-archive-data-to-custom-dashboard-widgets/#configuration","title":"Configuration","text":"<p>STEP 1. From your Coralogix toolbar, navigate to Dashboards &gt; Custom Dashboards.</p> <p>STEP 2. Select the dashboard you want to view with monitoring data or create a new dashboard.</p> <p>STEP 3. If you haven\u2019t done so already, set the definitions for each widget (Data Table,\u00a0Line Chart,\u00a0Gauge,\u00a0Pie Chart,\u00a0Vertical Bar Chart, and\u00a0Horizontal Bar Chart) in the right-hand sidebar.</p> <p>Notes:</p> <ul> <li>Source options supporting monitoring data include logs and spans.</li> </ul> <p>STEP 4. In your custom dashboard, select a widget to which you want to add monitoring data and click the Add Monitoring Data icon on the right-hand side of the widget\u2019s title bar. Alternately, select Monitoring in the Load data from section in the right-hand toolbar.</p> <p></p> <p>Once monitoring data is added to a widget, making changes to the query will no longer trigger automatic changes in the widget. Instead, you must manually refresh the data when you want to see more recent data.</p> <p>STEP 5. To manually refresh data, click the Partial Data icon. Then click REFRESH DATA in the popup that appears.</p> <p></p> <p>If you make changes to the dashboard filters, variables, or timeframe, you must manually refresh the data in order to see the filters applied.</p> <p>STEP 6. To return to viewing OpenSearch data only, click the Switch to Frequent Search Data icon. Alternately, click FREQUENT SEARCH in the right-hand toolbar. This reverts the dashboard to its original state where you are locally querying your Frequent Search data.</p> <p></p>"},{"location":"newoutput/adding-archive-data-to-custom-dashboard-widgets/#additional-resources","title":"Additional Resources","text":"DocumentationCustom DashboardsLine ChartsData TablesGaugesPie ChartsVertical Bar ChartsHorizontal Bar Charts"},{"location":"newoutput/adding-archive-data-to-custom-dashboard-widgets/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aggregation-function/","title":"Aggregation Function","text":"<p>In the past, you were only able to aggregate and group by a certain\u00a0field. Today, in the log screen graph, we have added a new feature that allows you to aggregate on metrics data, AVG, MIN, SUM and MAX.</p> <p>To do so, under the log screen on top you will see a graph, click on the blue Gear Icon.</p> <p></p> <p>You will get the different options we support so far.</p> <p>The option Count applies to the logs you are seeing in your screen. You can count them and/or group them by any field that is in the drop down menu.</p> <p></p> <p>If you chose any of the options AVG, MIN, MAX, and SUM, you will need to have Log2Metrics configured, Prometheus logs, or both.</p> <p>In case you chose any of these options and you do not have Log2Metrics or Prometheus logs you will get the screen below:</p> <p></p> <p>If you have configured Log2Metrics or you have Prometheus logs, the AVG, MIN, MAX, and SUM would work like the example below. We aggregate on the AVG of a metric field called https_resp_took_ms:</p> <p></p> <p>As before, you can also add a Group By to this graph and also Compare To.</p> <p></p> <p>Remember that Group By fields are supposed to come from L2M or Prometheus.</p>"},{"location":"newoutput/alcide-kaudit/","title":"Alcide kAudit","text":"<p>Stream Alcide kAudit findings to Coralogix, enabling you to view, analyze and monitor your logs using cutting-edge tools.</p>"},{"location":"newoutput/alcide-kaudit/#overview","title":"Overview","text":"<p>Coralogix complements Alcide kAudit by creating one pane of glass through which DevSecOps as well as other teams like engineering and CS can view logs from different parts of the infrastructure in a consolidated way. Beyond visualizing the data, Coralogix gives all these teams powerful analysis tools and helps identify correlations between applications and components events using ML-techniques.</p> <p>Coralogix can also put logs in the context of the application\u2019s lifecycle in the CI/CD process \u2013 allowing you to assess the impact of every change to your infrastructure. The end result is faster problem identification and time to resolution.</p> <p>kAudit can send two log categories to Coralogix:</p> <ul> <li> <p>Policy Findings: The K8s audit log entries that show violations of the policy which are configured by the Sec and DevOps team.</p> </li> <li> <p>Detections: Suspicious behavior with potential security risks to the K8s cluster which is automatically detected by kAudit.</p> </li> </ul>"},{"location":"newoutput/alcide-kaudit/#configuration","title":"Configuration","text":"<p>Alcide kAudit findings are streamed to Coralogix, enabling you to view, analyze and monitor your data.</p> <p>Exporting kAudit findings is available via the kaudit-integrationConfigMap.</p> <p>URL: https://api./logs/v1/bulk. Input your Coralogix domain. Private Key: Input your Coralogix Send-Your-Data API key Reference: https://coralogixstg.wpengine.com/integrations/coralogix-rest-api/ <pre><code>body fields:\napplicationName: application\nsubsystemName: subsystem\nlogEntries: each log has timestamp, severity, category\n</code></pre> <p>Note: the ConfigMap should already exist where kAudit has been deployed. Edit kaudit-integration- ConfigMap and add the integrations that you wish. <p>Example configuration:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: kaudit-integration-&lt;your cluster name&gt;\nnamespace: alcide-kaudit\nlabels:\napp: kaudit\napp-name: kaudit # kAudit instance\ndata:\naudit-integration: |\n\n- type: detections\ntarget:\ntarget-type: http-api\nhttp-api-uri: 'https://api.&lt;domain&gt;/logs/v1/bulk'\nhttp-api-token: 'Private-key'\nstopped: true\n- type: selections\ntarget:\ntarget-type: http-api\nhttp-api-uri: 'https://api.&lt;domain&gt;/logs/v1/bulk'\nrate-limit: 10\ndata-filter:\nentity-no-match: ^system:|^admin$\nrules-match: ^exec|unsafe$\nreport: details\n</code></pre> <p>UI:</p> <p>In Alcide, select the Integrations tab and go to the Detections Integrations Configuration section, which is used to configure integrations for threat intel logs.</p> <p>Select HTTP API as your target.</p> <p>In the URL box, enter https://api./logs/v1/bulk. <p>Under Entities Types, select the types that you want to forward threat intel about.</p> <p>Under Detection Categories, select the categories you wish to forward.</p> <p>Under Detection Confidence, select your desired levels of confidence. Coralogix recommends selecting at least high and medium.</p> <p>Optionally, you can create whitelist and blacklist filters on entities using the Entities Matching and Entities Not Matching boxes.</p> <p>Then, go to the Selected Audit Entries Integration Configuration section, located underneath the previous section. This section is used to configure integrations for audit logs.</p> <p>Select HTTP API as your target.</p> <p>In the URL box, enter https://api./logs/v1/bulk."},{"location":"newoutput/alcide-kaudit/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/alert-notifications-outbound-webhooks/","title":"Configure Alert Notifications for Outbound Webhooks","text":"<p>Outbound Webhooks, or alert webhooks, offer a streamlined way to receive immediate notifications for critical events in the form of alerts, facilitating prompt responses to incidents.</p> <p>This tutorial guides you through configuring your outgoing webhook notification settings and associating each webhook with multiple alerts.</p>"},{"location":"newoutput/alert-notifications-outbound-webhooks/#configure-alert-notifications","title":"Configure Alert Notifications","text":"<p>STEP 1. Configure the alert notification settings.</p> <ul> <li> <p>Notify Every. Sets the alert cadence. After an alert is triggered and a notification is sent to your outbound webhook, the alert will continue to work. Messages will be suppressed for the duration of the suppression period.</p> </li> <li> <p>Notify When Resolved. Activate to receive an automatic update once an alert has ceased.</p> </li> </ul> <p></p> <p>STEP 2. Select one or more alerts to integrate into the webhook.</p> <p>STEP 3. Click DONE to complete the configuration process.</p>"},{"location":"newoutput/alert-notifications-outbound-webhooks/#view-edit-alert-notifications","title":"View &amp; Edit Alert Notifications","text":"<p>STEP 1. Click on each webhook (e.g., Send Log) to view the specific webhooks you\u2019ve set up for this notification channel and the number of associated alerts for each.</p> <p></p> <p>STEP 2. Click on the number of Associated Alerts to view details for each alert integrated within a specific webhook.</p> <p>STEP 3. Click on each row to view all the settings for that alert in read-only mode. Edit or detach the alert from the webhook in the ACTIONS column.</p> <p></p> <p>STEP 4. To attach additional alerts to the specific webhook, click ADD. You will be rerouted to the Alert Notifications Settings page.</p> <p>STEP 5. When the process is complete, click CLOSE.</p>"},{"location":"newoutput/alert-notifications-outbound-webhooks/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/alert-suppression-rules/","title":"Alert Suppression Rules","text":"<p>Use Alert Suppression Rules to eliminate unnecessary alerts during scheduled maintenance, testing, auto-scaling events, or outside working hours.</p> <p></p>"},{"location":"newoutput/alert-suppression-rules/#overview","title":"Overview","text":"<p>Coralogix Alert Suppression Rules allow you to automatically mute alerts according to your specific parameters. You can set what to suppress (what group-by keys), when to suppress (specific times and dates, recurring times and dates, one-time suppressions), and which alerts to suppress during those times.</p> <p>Use our preset configurations to set up your suppression rules with minimal fuss:</p> <ul> <li> <p>Scheduled Maintenance. Easily plan and execute maintenance tasks by suppressing alerts during scheduled maintenance windows, preventing unnecessary interruptions.</p> </li> <li> <p>Whitelisting. Designate specific conditions or sources as exempt from alerts, ensuring critical events are still monitored while minimizing noise from expected or benign activities.</p> </li> <li> <p>Working Hours. Automatically suppresses alerts during non-working hours, ensuring your team stays focused during downtime and receives notifications only when it matters.</p> </li> <li> <p>System Upgrade. Seamlessly manages system upgrades by suppressing alerts temporarily, allowing uninterrupted maintenance without unnecessary notifications.</p> </li> <li> <p>Blank Rule. Create a custom suppression rule tailored to your unique needs, allowing you to fine-tune alert management based on your specific operational requirements.</p> </li> </ul> <p></p>"},{"location":"newoutput/alert-suppression-rules/#create-a-suppression-rule","title":"Create a Suppression Rule","text":""},{"location":"newoutput/alert-suppression-rules/#getting-started","title":"Getting Started","text":"<p>STEP 1. In the Coralogix toolbar, navigate to Alerts &gt; Suppression Rules.</p> <p>STEP 2. Click NEW RULE in the top right-hand corner.</p> <p>STEP 3. Enter a name, description, and label for the new rule. The label serves as a metadata label attached to your suppression rule. Use it to manage, filter, and organize your suppression rules.</p> <p></p>"},{"location":"newoutput/alert-suppression-rules/#select-which-keys-to-suppress","title":"Select Which Keys to Suppress","text":"<p>STEP 4. Select key:value pairs for which the alert should be suppressed or choose to suppress all alert activity during a particular time frame.</p> <ul> <li> <p>Select one or more group-by keys and enter values for each key that should be suppressed. These values are defined in your alert settings and are aggregated into a histogram. An alert is triggered whenever the condition threshold is met for a specific aggregated value within the specified timeframe.</p> </li> <li> <p>Any specified values will be suppressed if you have multiple values for a single key.</p> </li> <li> <p>If you choose multiple keys, all of the keys must appear in the alert to trigger the suppression.</p> </li> <li> <p>If you select Suppress all selected alert activity, the selected alerts will be completely suppressed during the selected timeframe regardless of triggered values, including alerts set to notify immediately and alerts without a group-by key set.</p> </li> </ul> <p></p>"},{"location":"newoutput/alert-suppression-rules/#select-suppression-timing-frequency","title":"Select Suppression Timing &amp; Frequency","text":"<p>STEP 5. Choose when to suppress or when to trigger alerts.</p> <p></p> <p>STEP 6. Select between one-time and recurring suppression.</p> <ul> <li> <p>Select the frequency with which the alert should be suppressed or triggered if you have defined it as recurring. This can be on a daily, weekly, or monthly basis.</p> </li> <li> <p>[Optional] When recurring is selected, set a termination date.</p> </li> <li> <p>If Always is selected, the rule will always run for recurring rules. Selected alerts will always be suppressed.</p> </li> </ul> <p>STEP 7. Select the start/end date and time for the rule or its duration.</p>"},{"location":"newoutput/alert-suppression-rules/#select-which-alerts-to-suppress","title":"Select Which Alerts to Suppress","text":"<p>STEP 8. Choose which alerts to suppress.</p> <ul> <li> <p>Select whether to suppress specific alerts by Name or all alerts carrying a certain Label. The Label option refers to a particular metadata label attached to one or more alters when defining alert settings.</p> </li> <li> <p>If you select Apply to all existing and future system alerts, all alerts will be suppressed, including alerts created in the future, for as long as the rule applies.</p> </li> </ul> <p></p> <p>STEP 9. Click SAVE CHANGES.</p>"},{"location":"newoutput/alert-suppression-rules/#manage-suppression-rules","title":"Manage Suppression Rules","text":""},{"location":"newoutput/alert-suppression-rules/#suppression-rules-overview","title":"Suppression Rules Overview","text":"<p>The Suppression Rules Overview lets you see when suppressions are scheduled in the upcoming future and when they were scheduled in the past. Filter rules by timeframe, label, status, and free search.</p> <p></p>"},{"location":"newoutput/alert-suppression-rules/#edit-clone-delete-recent-rules","title":"Edit / Clone / Delete Recent Rules","text":"<p>The Suppression Rules Management tabs list the currently active and expired past rules. For each rule, you can see the rule name, its labels, the suppressed values, the alerts that were suppressed during the suppression time, the start and end time, the rule type, and whether or not it is enabled.</p> <p>To edit, clone, or delete a rule:</p> <p>STEP 1. Click on the three dots at the right-hand side of the rule.</p> <p></p> <p>STEP 2. Select the action you want to perform:</p> <ul> <li> <p>Editing. Click EDIT and make changes to the rule, then click SAVE CHANGES.</p> </li> <li> <p>Clone. Click CLONE and make any changes you want to the rule, then click SAVE CHANGES.</p> </li> <li> <p>Deleting. Click DELETE and confirm you want to delete the rule.</p> </li> </ul>"},{"location":"newoutput/alert-suppression-rules/#additional-resources","title":"Additional Resources","text":"DocumentationGetting Started with Coralogix Alerts"},{"location":"newoutput/alert-suppression-rules/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/alert-suppression-rules-api/","title":"Alert Suppression Rules API","text":""},{"location":"newoutput/alert-suppression-rules-api/#overview","title":"Overview","text":"<p>This document outlines the Alert Suppression Rules API. Alert Suppression Rules allow you to automatically mute alerts according to your specific parameters. You can set what to suppress (what group-by keys), when to suppress (specific times and dates, recurring times and dates, one-time suppressions), and which alerts to suppress during those times.</p>"},{"location":"newoutput/alert-suppression-rules-api/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, please make sure you have the following:</p> <ul> <li> <p>API Key for Alerts, Rules &amp; Tags to successfully authenticate.</p> </li> <li> <p>Management API Endpoint that corresponds with your Coralogix subscription.</p> </li> <li> <p>Administrator permissions to manage your services.</p> </li> </ul>"},{"location":"newoutput/alert-suppression-rules-api/#authentication","title":"Authentication","text":"<p>Coralogix API uses API keys to authenticate requests. You can view and\u00a0manage your API keys\u00a0from the Data Flow tab in Coralogix. You need to use this API key in the Authorization request header to successfully connect.</p> <p>Example:</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\"\n</code></pre> <p>Then, use one of our designated\u00a0Management Endpoints\u00a0to structure your header.</p> <pre><code>-d @ ng-api-grpc.coralogix.com:443\n</code></pre> <p>For the Alert Suppression Rules API, the service name will be\u00a0<code>AlertSchedulerRuleService</code>.</p> <pre><code>com.coralogixapis.alerting.alert_scheduler_rule_protobuf.v1.AlertSchedulerRuleService/\n</code></pre> <p>The complete request header should look like this:</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogix.com:443 com.coralogixapis.alerting.alert_scheduler_rule_protobuf.v1.AlertSchedulerRuleService/\n</code></pre>"},{"location":"newoutput/alert-suppression-rules-api/#api-endpoints","title":"API Endpoints","text":""},{"location":"newoutput/alert-suppression-rules-api/#alertschedulerruleservice","title":"AlertSchedulerRuleService","text":"Method Name Description GetAlertSchedulerRule Retrieves an alert suppression rule based on the provided rule ID. CreateAlertSchedulerRule Creates a new alert suppression rule according to the provided parameters. UpdateAlertSchedulerRule Updates an existing alert suppression rule with new settings and configurations. DeleteAlertSchedulerRule Deletes a specified alert suppression rule, removing it from the system. GetBulkAlertSchedulerRule Retrieves multiple alert suppression rules in bulk, potentially filtered by various criteria. CreateBulkAlertSchedulerRule Creates multiple alert suppression rules in bulk, based on the provided list of rule creation requests. UpdateBulkAlertSchedulerRule Updates multiple existing alert suppression rules with new settings and configurations in bulk."},{"location":"newoutput/alert-suppression-rules-api/#getalertschedulerrule","title":"GetAlertSchedulerRule","text":""},{"location":"newoutput/alert-suppression-rules-api/#get-alert-scheduler-rule-request","title":"Get Alert Scheduler Rule Request","text":"Field Type Label Description alert_scheduler_rule_id string The rule ID."},{"location":"newoutput/alert-suppression-rules-api/#getalertschedulerruleresponse","title":"GetAlertSchedulerRuleResponse","text":"Field Type Label Description alert_scheduler_rule AlertSchedulerRule Metadata from the alert rule."},{"location":"newoutput/alert-suppression-rules-api/#createalertschedulerrule","title":"CreateAlertSchedulerRule","text":""},{"location":"newoutput/alert-suppression-rules-api/#createalertschedulerrulerequest","title":"CreateAlertSchedulerRuleRequest","text":"Field Type Label Description alert_scheduler_rule AlertSchedulerRule Metadata from the alert rule"},{"location":"newoutput/alert-suppression-rules-api/#createalertschedulerruleresponse","title":"CreateAlertSchedulerRuleResponse","text":"Field Type Label Description alert_scheduler_rule AlertSchedulerRule Metadata from the alert rule"},{"location":"newoutput/alert-suppression-rules-api/#updatealertschedulerrule","title":"UpdateAlertSchedulerRule","text":""},{"location":"newoutput/alert-suppression-rules-api/#updatealertschedulerrulerequest","title":"UpdateAlertSchedulerRuleRequest","text":"Field Type Label Description alert_scheduler_rule AlertSchedulerRule Metadata from the alert rule"},{"location":"newoutput/alert-suppression-rules-api/#updatealertschedulerruleresponse","title":"UpdateAlertSchedulerRuleResponse","text":"Field Type Label Description alert_scheduler_rule AlertSchedulerRule Metadata from the alert rule"},{"location":"newoutput/alert-suppression-rules-api/#deletealertschedulerrule","title":"DeleteAlertSchedulerRule","text":""},{"location":"newoutput/alert-suppression-rules-api/#deletealertschedulerrulerequest","title":"DeleteAlertSchedulerRuleRequest","text":"Field Type Label Description alert_scheduler_rule_id string The rule ID."},{"location":"newoutput/alert-suppression-rules-api/#deletealertschedulerruleresponse","title":"DeleteAlertSchedulerRuleResponse","text":"Field Type Label Description"},{"location":"newoutput/alert-suppression-rules-api/#getbulkalertschedulerrule","title":"GetBulkAlertSchedulerRule","text":""},{"location":"newoutput/alert-suppression-rules-api/#getbulkalertschedulerrulerequest","title":"GetBulkAlertSchedulerRuleRequest","text":"Field Type Label Description active_timeframe ActiveTimeframe enabled bool optional alert_scheduler_rules_ids FilterByAlertSchedulerRuleIds next_page_token string optional"},{"location":"newoutput/alert-suppression-rules-api/#getbulkalertschedulerruleresponse","title":"GetBulkAlertSchedulerRuleResponse","text":"Field Type Label Description alert_scheduler_rules AlertSchedulerRuleWithActiveTimeframe repeated next_page_token string"},{"location":"newoutput/alert-suppression-rules-api/#createbulkalertschedulerrule","title":"CreateBulkAlertSchedulerRule","text":""},{"location":"newoutput/alert-suppression-rules-api/#createbulkalertschedulerrulerequest","title":"CreateBulkAlertSchedulerRuleRequest","text":"Field Type Label Description create_alert_scheduler_rule_requests CreateAlertSchedulerRuleRequest repeated"},{"location":"newoutput/alert-suppression-rules-api/#createbulkalertschedulerruleresponse","title":"CreateBulkAlertSchedulerRuleResponse","text":"Field Type Label Description create_suppression_responses CreateAlertSchedulerRuleResponse repeated"},{"location":"newoutput/alert-suppression-rules-api/#updatebulkalertschedulerrule","title":"UpdateBulkAlertSchedulerRule","text":""},{"location":"newoutput/alert-suppression-rules-api/#updatebulkalertschedulerrulerequest","title":"UpdateBulkAlertSchedulerRuleRequest","text":"Field Type Label Description update_alert_scheduler_rule_requests UpdateAlertSchedulerRuleRequest repeated"},{"location":"newoutput/alert-suppression-rules-api/#updatebulkalertschedulerruleresponse","title":"UpdateBulkAlertSchedulerRuleResponse","text":"Field Type Label Description update_suppression_responses UpdateAlertSchedulerRuleResponse repeated"},{"location":"newoutput/alert-suppression-rules-api/#deletebulkalertschedulerrule","title":"DeleteBulkAlertSchedulerRule","text":""},{"location":"newoutput/alert-suppression-rules-api/#deletebulkalertschedulerrulerequest","title":"DeleteBulkAlertSchedulerRuleRequest","text":"Field Type Label Description delete_alert_scheduler_rule_requests DeleteAlertSchedulerRuleRequest repeated"},{"location":"newoutput/alert-suppression-rules-api/#deletebulkalertschedulerruleresponse","title":"DeleteBulkAlertSchedulerRuleResponse","text":"Field Type Label Description"},{"location":"newoutput/alert-suppression-rules-api/#alertschedulerrule","title":"AlertSchedulerRule","text":"Field Type Label Description unique_identifier string optional Rule unique_identifier: The rule id. id string optional Rule id: The rule version id. name string Rule name. description string optional Rule description. meta_labels MetaLabel repeated Rule meta_labels: Rule tags over the system. filter Filter Rule filter: The rule filter definition over alert-events. schedule Schedule Rule schedule: The schedule time definition, how often the rule will be active. enabled bool Rule enabled: The rule activation mode. created_at string optional Rule created_at: The date-time when the rule was created. updated_at string optional Rule updated_at: The date-time when the rule was updated."},{"location":"newoutput/alert-suppression-rules-api/#alertschedulerrulewithactivetimeframe","title":"AlertSchedulerRuleWithActiveTimeframe","text":"Field Type Label Description alert_scheduler_rule AlertSchedulerRule next_active_timeframes ActiveTimeframe repeated"},{"location":"newoutput/alert-suppression-rules-api/#activetimeframe","title":"ActiveTimeframe","text":"Field Type Label Description start_time string Timeframe start time: The point in the time(date-time) when the rule will start to be active. end_time string Timeframe end time: The point in the time(date-time) when the rule will finish to be active. timezone string Timeframe timezone: The rule will be active according to a specific timezone."},{"location":"newoutput/alert-suppression-rules-api/#duration","title":"Duration","text":"Field Type Label Description for_over int32 Duration for_over: The duration interval. frequency DurationFrequency Duration frequency: The duration frequency types (minute hour or day)."},{"location":"newoutput/alert-suppression-rules-api/#timeframe","title":"Timeframe","text":"Field Type Label Description start_time string Timeframe start time: The point in the time(date-time) when the rule will start to be active. end_time string Timeframe end time: The point in the time(date-time) when the rule will finish to be active. duration Duration Timeframe duration: The duration interval of the rule activation. timezone string Timeframe timezone: The rule will be active according to a specific timezone."},{"location":"newoutput/alert-suppression-rules-api/#durationfrequency","title":"DurationFrequency","text":"Name Number Description DURATION_FREQUENCY_UNSPEC <p>IFIED | 0 | | | DURATION_FREQUENCY_MINUTE | 1 | | | DURATION_FREQUENCY_HOUR | 2 | | | DURATION_FREQUENCY_DAY | 3 | |</p>"},{"location":"newoutput/alert-suppression-rules-api/#recurring","title":"Recurring","text":"Field Type Label Description always Recurring.Always dynamic Recurring.Dynamic"},{"location":"newoutput/alert-suppression-rules-api/#recurringalways","title":"Recurring.Always","text":"Field Type Label Description"},{"location":"newoutput/alert-suppression-rules-api/#recurringdynamic","title":"Recurring.Dynamic","text":"Field Type Label Description repeat_every int32 Recurring Dynamic repeat_every: The rule will be activated in a recurring mode according to the interval. daily Daily weekly Weekly monthly Monthly timeframe Timeframe Recurring Dynamic timeframe: The rule will be activated in a recurring mode according to the specific timeframe. termination_date string optional Recurring Dynamic termination_date: The rule will be terminated according to termination_date."},{"location":"newoutput/alert-suppression-rules-api/#schedule","title":"Schedule","text":"Field Type Label Description schedule_operation ScheduleOperation Rule schedule_operation: The rule operation mode (mute/active). one_time OneTime Schedule one_time: The scheduling definition will activate the rule for a specific timeframe. recurring Recurring Schedule recurring: The scheduling definition will activate the rule for a recurring timeframe."},{"location":"newoutput/alert-suppression-rules-api/#daily","title":"Daily","text":"Field Type Label Description"},{"location":"newoutput/alert-suppression-rules-api/#weekly","title":"Weekly","text":"Field Type Label Description days_of_week int32 repeated Dynamic Weekly days_of_week: The rule will be activated at weekly intervals on specific days in a week."},{"location":"newoutput/alert-suppression-rules-api/#monthly","title":"Monthly","text":"Field Type Label Description days_of_month int32 repeated Dynamic Monthly days_of_month: The rule will be activated at monthly intervals on specific days in a month."},{"location":"newoutput/alert-suppression-rules-api/#onetime","title":"OneTime","text":"Field Type Label Description timeframe Timeframe"},{"location":"newoutput/alert-suppression-rules-api/#scheduleoperation","title":"ScheduleOperation","text":"Name Number Description SCHEDULE_OPERATION_UNSPECIFIED 0 SCHEDULE_OPERATION_MUTE 1 SCHEDULE_OPERATION_ACTIVATE 2"},{"location":"newoutput/alert-suppression-rules-api/#filterbyalertschedulerruleids","title":"FilterByAlertSchedulerRuleIds","text":"Field Type Label Description alert_scheduler_ids AlertSchedulerRuleIds alert_scheduler_version_ids AlertSchedulerRuleVersionIds"},{"location":"newoutput/alert-suppression-rules-api/#alertschedulerruleids","title":"AlertSchedulerRuleIds","text":"Field Type Label Description alert_scheduler_rule_ids string repeated"},{"location":"newoutput/alert-suppression-rules-api/#alertschedulerruleversionids","title":"AlertSchedulerRuleVersionIds","text":"Field Type Label Description alert_scheduler_rule_version_ids string repeated"},{"location":"newoutput/alert-suppression-rules-api/#filter","title":"Filter","text":"Field Type Label Description what_expression string Filter what_expression: dataprime expression that filter the alerts group by values. alert_meta_labels MetaLabels Filter alert_meta_labels: filter alerts by meta labels tagging. alert_unique_ids AlertUniqueIds Filter alert_unique_ids: filter specific alerts (when alert_unique_ids is empty meaning it wil filter all alerts)."},{"location":"newoutput/alert-suppression-rules-api/#alertuniqueids","title":"AlertUniqueIds","text":"Field Type Label Description value string repeated"},{"location":"newoutput/alert-suppression-rules-api/#metalabels","title":"MetaLabels","text":"Field Type Label Description value MetaLabel repeated"},{"location":"newoutput/alert-suppression-rules-api/#metalabel","title":"MetaLabel","text":"Field Type Label Description id string optional MetaLabel id: The meta label id key string MetaLabel key: The meta label key value string optional MetaLabel value: The meta label value"},{"location":"newoutput/alert-webhook-with-google-chat/","title":"Alert Webhook with GCP Chat","text":"<p>Configuring a Google chat webhook integration can easily be done with the custom webhook integration. Choose the WebHook integration and fill in your destination chat URL, you can check the documentation from Google here to see how to retrieve the URL.</p> <p>Next, define your webhook body. Note that Google chat API expects a flat JSON structure with one key \"text\" as the webhook body. It can still of course contain all the relevant information you are interested in from your log itself, by tagging the keys using '$' as explained above. Here is an example for you to test:</p> <pre><code>{\"text\": \"Hi team! This is the Coralogix team, your webhook structure needs to be flat with one key in the JSON in order to fit Google chats. Use the Coralogix keys tagged with '$' to signify what you would like to send. Here is an example: alert_id=$ALERT_ID, name= $ALERT_NAME, description = $ALERT_DESCRIPTION, application = $APPLICATION_NAME  ,subsystem= $SUBSYSTEM_NAME, Alert Log = $LOG_TEXT  ------- You may see the above table containing all the different options you may use to structure your custom messages. Enjoy!\"}\n</code></pre> <p>For more Google chat API options such as using formatted text in messages, including links in messages, @mention specific/all users you can visit here.</p> <p>When you are done configuring your desired webhook, In your alert, go to the 'Notification settings\" section and choose your newly defined webhook.\u00a0</p> <p>** If you don't see your new integration under your alert definition, try to refresh your browser</p>"},{"location":"newoutput/alert-webhook-with-slack/","title":"Slack Outbound Webhooks","text":"<p>Enhance your observability workflows by sending real-time event notifications and log data to Slack. With this outbound webhook, you can easily integrate Coralogix with Slack, automate responses to critical events, and improve your organization's incident management and alerting processes.</p>"},{"location":"newoutput/alert-webhook-with-slack/#prerequisites","title":"Prerequisites","text":"<p>Access your Slack webhook URL.</p> <p>STEP 1. While logged into Slack, click here.</p> <p>STEP 2. Choose the room name. Click Add incoming webhook integration.</p> <p>STEP 3. Copy the webhook URL.</p>"},{"location":"newoutput/alert-webhook-with-slack/#create-a-slack-webhook","title":"Create a Slack Webhook","text":"<p>STEP 1. From the Coralogix toolbar, navigate to DATA FLOW &gt; EXTENSIONS.</p> <p>STEP 2. In the Outbound Webhooks section, click SLACK WEBHOOK.</p> <p></p> <p>STEP 3. Click + ADD NEW.</p> <p></p> <p>STEP 4. Enter the following details for your webhook:</p> <ul> <li> <p>Webhook Name. A memorable name for your webhook that will enable you to quickly identify this webhook later when attaching it to one of your alerts.</p> </li> <li> <p>URL. Paste the Slack webhook URL you previously copied (prerequisite step).</p> </li> </ul> <p>STEP 5. Select the types of events for which you would like to receive notifications. The Notify About section allows you to select any of four general notifications and send them to the Slack channel webhook you are creating.</p> <p>The four general notification types are:</p> <ul> <li> <p>Error and critical logs</p> </li> <li> <p>Flow anomalies</p> </li> <li> <p>Spike anomalies</p> </li> <li> <p>Data usage</p> </li> </ul> <p>STEP 6. Click SAVE &amp; TEST.</p> <p>The system sends a test message to the channel you specified in the Slack webhook URL and to check that your configuration is valid. If the test message is received successfully, a confirmation message is displayed.</p> <p>STEP 7. Once the configuration is confirmed and the webhook is in place, you can choose the alert or alerts in which this webhook will be used once the alert is triggered.</p> <p></p> <p>STEP 8. Configure your alert notifications.</p>"},{"location":"newoutput/alert-webhook-with-slack/#additional-resources","title":"Additional Resources","text":"DocumentationConfigure Alert Notifications for Outbound Webhooks"},{"location":"newoutput/alert-webhook-with-slack/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/alert-webhooks/","title":"Generic Outbound Webhooks (Alert Webhooks)","text":"<p>Enhance your observability workflows by sending real-time event notifications and log data to any endpoint that accepts HTTP requests. With this generic outbound webhook, you can easily integrate Coralogix with different endpoints, automate responses to critical events, and improve your organization's incident management and alerting processes.</p>"},{"location":"newoutput/alert-webhooks/#create-a-webhook","title":"Create a Webhook","text":"<p>STEP 1. From the Coralogix toolbar, navigate to Data Flow &gt; Outbound Webhooks.</p> <p>STEP 2. In the Outbound Webhooks section, click GENERIC WEBHOOK.</p> <p></p> <p>STEP 3. Click + ADD NEW.</p> <p></p> <p>STEP 4. Enter a webhook name and the URL to which you want to send an event notification.</p> <p>The UUID field is auto-populated.</p> <p>STEP 5. Select an HTTP method for the webhook (GET, POST, or PUT).</p> <p>STEP 6. Click NEXT.</p> <p></p> <p>STEP 7. [Optional] Edit the message to customize the header and body of the messages that will be sent when the webhook is triggered.</p>"},{"location":"newoutput/alert-webhooks/#placeholders","title":"Placeholders","text":"<p>Here is a list of all available placeholders you may use and a description of each one.</p> <p>Alert Event Information</p> PlaceholderDescription$ALERT_NAMEName of the alert$ALERT_ACTIONAlert action, whether triggered or resolved$ALERT_URLURL used to access the alert in Coralogix$ALERT_IDAlert IDThis changes every time a significant alert parameter, such as query or condition, is changed.$ALERT_DESCRIPTIONDescription added in the alert$ALERT_UNIQUE_IDENTIFIERPersists even when significant alert parameters are changed$ALERT_THRESHOLDThreshold that was defined in the alert$ALERT_TIMEWINDOW_MINUTESThe time frame in minutes for which the alert is defined$ALERT_GROUPBY_LABELSThe group by labels defined in the alert$ALERT_GROUP_BY_VALUESThe values for the group by labels defined in the alert$EVENT_TIMESTAMP_ISOThe event timestamp in ISO format$EVENT_SEVERITYThe significance chosen for the alert: Info, Warning, Error, or Critical.$EVENT_SEVERITY_LOWERCASEActs like $EVENT_SEVERITY, but uses lowercase\u00a0letters$OPSGENIE_PRIORITYOpsGenie severity mapped from this event\u2019s severity (INFO - P5, WARNING - P3, ERROR - P2, CRITICAL - P1)$META_LABELSMeta labels are the\u00a0<code>Labels</code>\u00a0that you attach to an alert when defining it. If you want your outbound webhooks to contain these labels, add them to your template when defining the custom webhook.Labels of the alert as one string of key-value pairs, comma-separated.Example:\"firstKey:firstValue, justThis, anotherKey:anotherValue\"$META_LABELS_JSONMeta labels are the\u00a0<code>Labels</code>\u00a0that you attach to an alert when defining it. If you want your outbound webhooks to contain these labels, add them in your template when defining the custom webhook.Labels of the alert presented as a JSON-formatted stringExample:\"{\\\"firstKey\\\":\\\"firstValue\\\",\\\"justThis\\\":null,\\\"anotherKey\\\":\\\"anotherValue\\\"}\"$META_LABELS_LISTMeta labels are the\u00a0<code>Labels</code>\u00a0that you attach to an alert when defining it. If you want your outbound webhooks to contain these labels, add them in your template when defining the custom webhook.Alert label definedThe set of labels is presented as an array of elements.Example:[\"firstKey:firstValue\",\"justThis\",\"anotherKey:anotherValue\"]$EVENT_TIMESTAMP_MSThe time in milliseconds when the alert was triggered$EVENT_TIMESTAMPThe time when the alert was triggered as a string with the date and time$GROUP_BY_FIELD_1Provides the first group-by field that triggers an alert.$GROUP_BY_FIELD_2Provides the second group-by field that triggers an alert.$GROUP_BY_FIELD_#Provides the X group-by field that triggers an alert. May be higher than 2 in some cases.$GROUP_BY_VALUE_1Provides the first group-by value for the field that triggers an alert.When grouping by a given Group By field in your alert settings, you must group the metric by this field to allow the data to propagate to the $GROUP_BY_VALUE_1.$GROUP_BY_VALUE_2Provides the second group-by value for the field that triggers an alert.When grouping by a given Group By field in your alert settings, you must group the metric by this field to allow the data to propagate to the $GROUP_BY_VALUE_2.$GROUP_BY_VALUE_#Provides the X group-by value that triggers an alert. May be higher than 2 in some cases.When grouping by a given Group By field in your alert settings, you must group the metric by this field to allow the data to propagate to the $GROUP_BY_VALUE_X.$HIT_COUNTHit count presents the hit count of logs that triggered the alert$RELATIVE_HIT_COUNTFor ratio and time relative alerts, relative hit count presents the hit count of the second query logs$QUERY_TEXTPresents the alert's query$RELATIVE_QUERY_TEXTFor Ratio and Time Relative alerts, relative query text presents the alert's second query$DEFINED_RATIO_THRESHOLDFor Ratio and Time Relative alerts, the defined ratio threshold presents the ratio threshold defined in the alert$ACTUAL_RATIOFor Ratio and Time Relative alerts, the actual ratio presents the resulted ratio for the alert$METRIC_KEYFor Metric Lucene-based alerts, the metric key is the field on which you create the metric alert.This alert type is deprecated and exists only for existing customers who previously defined this type of alert.$METRIC_OPERATORFor Metric Lucene-based alerts, the metric operator is the arithmetic function that is being applied when checking the alertThis alert type is deprecated and exists only for existing customers who previously defined this type of alert.$TIMEFRAMEFor Metric alerts, the timeframe over which the metric alert is checked$TIMEFRAME_OVER_THRESHOLDFor Metric alerts, contains all of the following elements:\u2022 The percentage of time over the threshold.\u2022 Average of the values crossing the threshold.\u2022 Max of the values crossing the threshold.\u2022 Min of the values crossing the threshold.(Irrelevant for sum and count arithmetic operators.)$METRIC_CRITERIAFor Metric alerts, the condition that is checked in the alert (\u2018over\u2019 or \u2018under\u2019)$SERVICEThe service for which the span was triggered$SPANSThe number of spans$DURATIONDuration of the triggered span <p>Ratio / Time Relative Alerts</p> PlaceholderDescription$RATIO_QUERY_ONEQuery one alias$RATIO_QUERY_TWOQuery two aliases$RATIO_TIMEFRAMEThe timeframe over which the alert triggers <p>Flow Alerts</p> PlaceholderDescription$FLOW_ALERT_RELATED_ALERTSThe data about the alerts that trigger this flow <p>Unique Count Alerts</p> PlaceholderDescription$UNIQUE_COUNT_VALUES_LISTThe unique values for the triggered alert <p>New Value Alerts</p> PlaceholderDescription$NEW_VALUE_TRACKED_KEYThe key defined to track new values from <p>Log Information</p> PlaceholderDescription$LOG_URLLink to the alert logs$APPLICATION_NAMEThe application name of the presented example log$SUBSYSTEM_NAMEThe subsystem name of the presented example log$LOG_TEXTThe entire log payload, whether it is a textual log or JSON formatted log$JSON_KEYIn case the logs are JSON formatted, you may include any key (JSON field) from the log itself$JSON_KEY.numericIf the chosen field possesses a number value and you wish to include it in its numeric form (use it in the custom webhook body without wrapping quotes), use it with the suffix .numeric. E.g. $status_code.numeric$COMPUTER_NAMEThe computer name (if it exists) of the presented example log$CATEGORYThe category (if it exists) of the presented example log$IP_ADDRESSThe IP address (if it exists) of the presented example log$THREAD_IDThe thread ID (if it exists) of the presented example log <p>General Information</p> PlaceholderDescription$TEAM_NAMEThe Coralogix account name from which the alert originates$CORALOGIX_ICON_URLThe Coralogix icon$COMPANY_IDThe company ID$DEDUP_KEYThe key Coralogix uses to dedup when sending to different integrations <p>STEP 8. Click TEST CONFIG.</p> <p>The system sends an HTTP call with the specified parameters to check that your configuration is valid. If the HTTP call is received successfully, a confirmation message is displayed.</p> <p></p> <p>STEP 9. Once the configuration is validated, configure your alert notifications.</p>"},{"location":"newoutput/alert-webhooks/#additional-resources","title":"Additional Resources","text":"DocumentationConfigure Alert Notifications for Outbound Webhooks"},{"location":"newoutput/alert-webhooks/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/alert-webhooks-2/","title":"Generic Outbound Webhook","text":"<p>Enhance your observability workflows by sending real-time event notifications and log data to any endpoint that accepts HTTP requests. With this generic outbound webhook, you can easily integrate Coralogix with different endpoints, automate responses to critical events, and improve your organization's incident management and alerting processes.</p>"},{"location":"newoutput/alert-webhooks-2/#create-a-webhook","title":"Create a Webhook","text":"<p>STEP 1. From the Coralogix toolbar, navigate to DATA FLOW &gt; EXTENSIONS.</p> <p>STEP 2. In the Outbound Webhooks section, click GENERIC WEBHOOK.</p> <p></p> <p>STEP 3. Click + ADD NEW.</p> <p></p> <p>STEP 4. Enter a webhook name and the URL to which you want to send an event notification.</p> <p>The UUID field is auto-populated.</p> <p>STEP 5. Select an HTTP method for the webhook (GET, POST, or PUT).</p> <p>STEP 6. Click NEXT.</p> <p></p> <p>STEP 7. [Optional] Edit the message to customize the header and body of the messages that will be sent when the webhook is triggered.</p> <p>Here is a list of all available placeholders you may use and a description of each one.</p> <p>[table id=39/]</p> <p>STEP 8. Click TEST CONFIG.</p> <p>The system sends an HTTP call with the specified parameters to check that your configuration is valid. If the HTTP call is received successfully, a confirmation message is displayed.</p> <p></p> <p>STEP 9. Once the configuration is validated, configure your alert notifications.</p>"},{"location":"newoutput/alert-webhooks-2/#additional-resources","title":"Additional Resources","text":"DocumentationConfigure Alert Notifications for Outbound Webhooks"},{"location":"newoutput/alert-webhooks-2/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/alerts-map/","title":"Alerts Map","text":"<p>Alerts Map presents users with a visual representation of each alert status in real-time. Grouping all of your alerts in a scalable, information-dense manner, this feature ensures optimal system monitoring.</p> <p>Use Alerts Map to visualize the following:</p> <ul> <li> <p>Standard Alerts (for all alert conditions except <code>notify immediately</code>)</p> </li> <li> <p>Ratio Alerts</p> </li> <li> <p>Metric Alerts</p> </li> <li> <p>Time Relative Alerts</p> </li> <li> <p>Flow Alerts</p> </li> </ul>"},{"location":"newoutput/alerts-map/#accessing-alerts-map","title":"Accessing Alerts Map","text":"<p>To access the Alerts Map feature, navigate to Alerts in your navigation pane &gt; Alerts Map.  </p> <p></p>"},{"location":"newoutput/alerts-map/#alerts-map-visualization","title":"Alerts Map Visualization","text":""},{"location":"newoutput/alerts-map/#view-triggered-alerts","title":"View Triggered Alerts","text":"<p>Alerts Map is divided into different visualizations based on the field or fields a user has grouped by in his / her alerts. Red hexagons represent values per key that have resulted in a triggered alert. Green hexagons represent all other conditions.</p> <p></p> <ul> <li> <p>Hexagon color is updated at intervals defined by <code>value:time-range</code> in the alert definition itself:</p> <ul> <li> <p>Every 2 min: When the less than\\more than condition is under 30min</p> </li> <li> <p>Every 6 min: When the less than\\more than condition is between 30min to 12hrs</p> </li> <li> <p>Every 12 min: When the less than\\more than condition is bigger than 12hr</p> </li> </ul> </li> <li> <p>To view additional information about a triggered alert, hover over a hexagon of interest. You will see a list of all the alerts that were triggered for a specific value and how many of them have been resolved.</p> </li> </ul> <p></p>"},{"location":"newoutput/alerts-map/#filter","title":"Filter","text":"<p>Filter Alters Map in the left-hand sidebar. Filter by Alert status, Alert name, and Alert Severity.</p> <p></p>"},{"location":"newoutput/alerts-map/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/alerts-rules-tags-api-key/","title":"Alerts, Rules, & Tags API Key","text":"<p>Retrieve and generate your Alert, Rules, and Tags API key.</p>"},{"location":"newoutput/alerts-rules-tags-api-key/#permissions","title":"Permissions","text":"<p>To view, retrieve, or generate your Alert, Rules, and Tags API key, the following permissions are required.</p>"},{"location":"newoutput/alerts-rules-tags-api-key/#view-retrieve","title":"View &amp; Retrieve","text":"<pre><code>user-legacy-other-api-keys:ReadConfig\u00a0\n</code></pre>"},{"location":"newoutput/alerts-rules-tags-api-key/#generate","title":"Generate","text":"<pre><code>user-legacy-other-api-keys:Manage\n</code></pre>"},{"location":"newoutput/alerts-rules-tags-api-key/#accessing-your-api-key","title":"Accessing Your API Key","text":"<p>STEP 1. To access your Alert, Rules, and Tags API key, navigate to Data Flow &gt; API Keys from your Coralogix toolbar.</p> <p></p> <p>STEP 2. View or copy your existing Alerts, Rules, &amp; Tags API key, or generate a new one.</p>"},{"location":"newoutput/alerts-rules-tags-api-key/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/amazon-eks-fargate-logs/","title":"AWS EKS Fargate Logs","text":"<p>Seamleslly collect and send your\u00a0<code>EKS Fargate</code>\u00a0cluster logs straight to\u00a0Coralogix.</p> <p>Amazon EKS on Fargate offers a built-in log router based on Fluent Bit. Using the side-car pattern, all logs captured by the container will be sent to Fluent Bit and Coralogix.</p>"},{"location":"newoutput/amazon-eks-fargate-logs/#general","title":"General","text":"<p>Private Key\u00a0\u2013 Your Send Your Data - API Key is a unique token that represents your company</p> <p>Application Name\u00a0\u2013 The name of your main application or the environment of the application</p> <p>SubSystem Name\u00a0\u2013 Your application probably has multiple subsystems. For example: Backend servers, Middleware, Frontend servers, etc. in order to help you examine the data you need, inserting the subsystem parameter is vital.</p>"},{"location":"newoutput/amazon-eks-fargate-logs/#configuration","title":"Configuration","text":"<p>To enable the built-in log router AWS provides, create a kubernetes manifest file named coralogix-logger.yaml with the FluentBit configMap named <code>aws-logging</code> in the <code>aws-observability</code> namespace.</p> <pre><code>kind: Namespace\napiVersion: v1\nmetadata:\n  name: aws-observability\n  labels:\n    aws-observability: enabled\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: aws-logging\n  namespace: aws-observability\ndata:\n  filters.conf: |\n    [FILTER]\n        Name parser\n        Match *\n        Key_name log\n        Parser crio\n\n    [FILTER]\n        Name             kubernetes\n        Match            kube.*\n        Merge_Log           On\n        Buffer_Size         0\n        Kube_Meta_Cache_TTL 300s\n        Keep_Log Off\n        Merge_Log_Key log_obj\n        K8S-Logging.Parser On\n        K8S-Logging.Exclude On\n        Annotations Off\n  output.conf: |\n    [OUTPUT]\n        Name  kinesis_firehose\n        Match *\n        region &lt;AWS_Region&gt;\n        delivery_stream &lt;Delivery_Stream_Name&gt;\n  parsers.conf: |\n    [PARSER]\n        Name crio\n        Format Regex\n        Regex ^(?&lt;time&gt;[^ ]+) (?&lt;stream&gt;stdout|stderr) (?&lt;logtag&gt;P|F) (?&lt;log&gt;.*)$\n        Time_Key    time\n        Time_Format %Y-%m-%dT%H:%M:%S.%L%z\n        Time_Keep true\n</code></pre> <p>Note: change the  to your kinesis delivery stream name and  to the delivery stream region. <p>Then apply the manifest file to your EKS cluster</p> <pre><code>kubectl apply -f coralogix-logger.yaml\n</code></pre> <p>Now we need to add to the pod execution role a policy that allows pods to send record to firehose.</p> <p>Go to AWS EKS -&gt; Clusters -&gt; 'Your-cluster' -&gt; Compute -&gt; 'Your-Fargate-Profile' -&gt; Pod execution role</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"firehose:PutRecord\",\n                \"firehose:PutRecordBatch\"\n            ],\n            \"Resource\": [\n                \"&lt;firehose_ARN&gt;\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Note: Each fargate profile has a different execution role so apply this step to all your profiles.</p> <p>To apply all these changes the pods have to restart.</p> <p>That's it all logs will be sent to your firehose delivery stream.</p> <p>Note: don't forget to follow our guide on Configuring firehose to make sure you have your firehose configured correctly.</p>"},{"location":"newoutput/amazon-eventbridge/","title":"AWS Eventbridge","text":"<p>Amazon EventBridge is a serverless event bus service that makes it easy to collect and send data from across your applications and services to any destination. Use EventBridge to seamlessly deliver real-time data from your application to Coralogix for monitoring and analysis.</p>"},{"location":"newoutput/amazon-eventbridge/#requirements","title":"Requirements","text":"<ul> <li> <p>AWS Account\u00a0</p> </li> <li> <p>Amazon EventBridge Bus created</p> </li> </ul>"},{"location":"newoutput/amazon-eventbridge/#instructions","title":"Instructions","text":"<p>Go to EventBridge -&gt; API Destinations, and create a new API Destination.  </p> <p> </p> <p>For API destination endpoint, select the URL below associated with your Coralogix domain.</p> Coralogix DomainURLcoralogixstg.wpengine.comhttps://aws-events.coralogixstg.wpengine.com/aws/eventcoralogix.ushttps://aws-events.coralogix.us/aws/eventeu2.coralogixstg.wpengine.comhttps://aws-events.eu2.coralogixstg.wpengine.com/aws/eventcoralogix.inhttps://aws-events.app.coralogix.in/aws/eventcoralogixsg.comhttps://aws-events.coralogixsg.com/aws/event <p>We will need to create a new connection:</p> <p></p> <p>If you would like to specify the application name and the subsystem name.</p> <p>Click on Invocation Http Parameters and add 2 parameters as shown in the screen shot below. The values can be anything you would like to identify the logs with.</p> <p></p> <p>For Authorization type you need to select API Key. API Key Name will be x-amz-event-bridge-access-key and you will use Coralogix \u201cSend your Data\u201d API Key\u00a0 (You can get it by going to the Data Flow menu in Coralogix and navigating to API Keys).</p> <p>We now need to create a rule to route the events into the API Destination.</p> <ul> <li> <p>On the Buses section, select Rules</p> </li> <li> <p>Click on Create rule</p> </li> <li> <p>Give the rule a name, select the correct Event bus. Click Next.</p> </li> <li> <p>For Event pattern (source), scroll down and select the desired source</p> <ul> <li>For example GuardDuty Findings</li> </ul> </li> </ul> <p></p> <ul> <li>For target, select EventBridge API destination</li> </ul> <p></p> <p>Every event that goes to the selected bus will be sent to Coralogix.</p>"},{"location":"newoutput/amazon-eventbridge/#notes","title":"Notes","text":"<p>A \u201cresources\u201d key is present in the AWS Eventbridge and Cloudtrail integrations. If you are integrating both EventBridge and CloudTrail logs, create a parsing rule to rename the mentioned field to avoid mapping conflicts.</p>"},{"location":"newoutput/amazon-eventbridge/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/amazon-kinesis-data-firehose-metrics/","title":"AWS Kinesis Data Firehose - Metrics","text":"<p>Amazon Kinesis Data Firehose delivers real-time streaming data to destinations like Amazon Simple Storage Service (Amazon S3), Amazon Redshift, or Amazon OpenSearch Service (successor to Amazon Elasticsearch Service), and now supports delivering streaming data to Coralogix. There is no limit on the number of delivery streams, so it can be used for getting data from multiple AWS services.</p> <p>Coralogix is an AWS Partner Network (APN) Advanced Technology Partner with AWS\u00a0 Competencies in DevOps. The platform enables you to easily explore and analyze logs, metrics, and traces to gain deeper insights into the state of your applications and AWS infrastructure. You can analyze all your AWS service metrics to uncover trends in your AWS services.</p> <p>Using Coralogix with Amazon Kinesis Data Firehose offers a few significant benefits compared with other solutions:</p> <ul> <li> <p>It keeps monitoring simple.</p> </li> <li> <p>It integrates flawlessly.</p> </li> <li> <p>It's flexible with minimum maintenance.</p> </li> <li> <p>Scale, scale, scale.</p> </li> </ul>"},{"location":"newoutput/amazon-kinesis-data-firehose-metrics/#prerequisites","title":"Prerequisites","text":"<ul> <li>Metrics bucket configured in your Coralogix dashboard [In your Coralogix toolbar, navigate to Data Flow &gt; Setup Archive.]</li> </ul>"},{"location":"newoutput/amazon-kinesis-data-firehose-metrics/#configuration","title":"Configuration","text":"<p>STEP 1. Go to the Kinesis Data Firehose console and choose 'Create delivery stream'.</p> <p>STEP 2. Under 'Choose source and destination'.</p> <ul> <li> <p>Source: Direct PUT</p> </li> <li> <p>Destination: Coralogix\u00a0</p> </li> <li> <p>Delivery stream name: Fill in the desired stream name.</p> </li> </ul> <p>STEP 3. Scroll down to 'Destination settings'.</p> <ul> <li> <p>HTTP endpoint URL: Choose the Firehose endpoint associated with your Coralogix domain.</p> </li> <li> <p>Private key: Enter your Coralogix Send Your Data - API Key.</p> </li> <li> <p>Content encoding: Select GZIP.</p> </li> <li> <p>Retry duration: Choose 300 seconds.</p> </li> </ul> <p>STEP 4. Scroll down to 'Parameters':</p> <p>By default, your delivery stream arn and name will be used as 'applicationName' and 'subsystemName'. To override the associated 'applicationName' or 'subsystemName', add a new parameter with the desired value.</p> <ul> <li> <p>Key: 'applicationName' , value - 'new-app-name'</p> </li> <li> <p>Key: 'subsystemName' , value - 'new-subsystem-name'</p> </li> </ul> <p>The source of the data in firehose determines the 'integrationType' parameter value:</p> <ul> <li>Key: \u2018integrationType' , value: \u2018CloudWatch_Metrics_OpenTelemetry070'</li> </ul> <p>STEP 5. Scroll down to 'Backup settings':</p> <ul> <li> <p>Source record backup in Amazon S3: We suggest selecting\u00a0'Failed data only'.</p> </li> <li> <p>S3 backup bucket: Choose an existing bucket or create a new one.</p> </li> <li> <p>Buffer hints, compression, encryption: Leave these fields as is.</p> </li> </ul> <p>STEP 6. Review your settings and choose 'Create delivery stream'.</p> <p>Metrics subscribed to your delivery stream will be immediately sent and available for analysis within Coralogix.</p>"},{"location":"newoutput/amazon-kinesis-data-firehose-metrics/#data-source-configuration","title":"Data Source Configuration","text":""},{"location":"newoutput/amazon-kinesis-data-firehose-metrics/#cloudwatch-metrics","title":"Cloudwatch Metrics","text":"<p>To start sending your metrics to coralogix you first need to create a metric stream.</p> <p>STEP 1. Go to the Cloudwatch console and choose 'Streams' under the 'Metrics' side menu.</p> <p>STEP 2. Click on 'Create metric stream'.</p> <p>STEP 3. Under 'Metrics to be streamed':</p> <ul> <li>Choose what metrics to send</li> </ul> <p>STEP 4. Scroll down to 'Configuration':</p> <ul> <li> <p>For 'Select configuration option' choose 'Select an existing Firehose owned by your account'</p> </li> <li> <p>For 'Select your Kinesis Data Firehose stream' choose the delivery stream created above</p> </li> </ul> <p>STEP 5. Scroll down to 'Change output format'</p> <ul> <li>Make sure that 'OpenTelemetry 0.7' is selected</li> </ul> <p>STEP 6. Scroll down to 'Custom metric stream name' and pick a name for the metrics stream.</p> <p>STEP 7. Scroll down and click on 'Create metric stream'.</p> <p>After a few minutes, the metrics will start streaming to coralogix and you will see them on the Grafana dashboard.</p>"},{"location":"newoutput/amazon-kinesis-data-firehose-metrics/#transformation-lambda-optional","title":"Transformation Lambda [optional]","text":"<p>CloudWatch Metric Streams Lambda transformation function can be used as a Kinesis Firehose transformation function to enrich the metrics from CloudWatch Metric Streams with AWS resource tags.</p> <p>This installation is optional and you can install the transformation Lambda if you\u2019d like to take advantage of having AWS resource tags as labels in your metrics data. Find out more here.</p> <p>Take the following steps to install the transformation Lambda.</p> <p>STEP 1. Create a new AWS Lambda function in your designated region with the following parameters:</p> <ul> <li> <p>Runtime:\u00a0Custom runtime on Amazon Linux 2</p> </li> <li> <p>Handler:\u00a0bootstrap</p> </li> <li> <p>Architecture:\u00a0arm64</p> </li> </ul> <p>STEP 2. Download the function ZIP\u00a0file from the releases in the repository. Unless instructed otherwise, use the latest release. Upload the function.zip as the code source for Lambda.</p> <p>STEP 3. Make sure to set the memory. We recommend starting with\u00a0<code>128 MB</code>. Depending on the number of metrics you export and the speed of Lambda processing, you may consider increasing it.</p> <p>STEP 4. Adjust the role of the Lambda function as described in\u00a0the necessary permissions.</p> <p>STEP 5. Optionally, add environment variables to configure the Lambda, as described in the configuration.</p> <p>STEP 6. The Lambda function is ready to be used in\u00a0Kinesis Data Firehose Data Transformation. Please note the function ARN and provide it in the relevant section of the Kinesis Data Firehose configuration.</p> <p>In order to prevent a delay in the delivery of your data, and depending on the size of your setup, we recommend adjusting your Lambda\u00a0buffer hint\u00a0and Kinesis Data Firehose\u00a0buffer size\u00a0configuration accordingly. For optimal experience, we recommend setting the Lambda buffer hint to\u00a0<code>0.2 MB</code>\u00a0and the Kinesis Data Firehose buffer size to\u00a0<code>1 MB</code>.\u00a0Note that this may cause more frequent Lambda runs, which may result in higher costs.</p> <p>You can check the staleness of your data in your Kinesis Data Firehose delivery stream in the Monitoring tab by looking at the \u2018Delivery to HTTP endpoint data freshness\u2019. If you see the staleness value grow, this may indicate the Lambda function runs are too slow. In such a case, you may increase the Lambda\u2019s memory and set the buffering configuration as described above.</p>"},{"location":"newoutput/amazon-kinesis-data-firehose-metrics/#destination-errors","title":"Destination Errors","text":"<p>View these common destinations errors and their possible solutions.</p> MessageSolutionThe delivery timed out before a response was received and will be retried. If this error persists, contact the AWS Firehose service team.None needed - no data lossDelivery to the endpoint was unsuccessful. See Troubleshooting HTTP Endpoints in the Firehose documentation for more information. Response received with status code 502.Coralogix returned HTTP 502 error code, firehose will resend the data. None needed - no data loss"},{"location":"newoutput/amazon-kinesis-data-firehose-metrics/#limitations","title":"Limitations","text":"<p>CloudWatch Metric Streams does not send metrics that are older than 2 hours. This means that some CloudWatch metrics are calculated at the end of a day and reported with the beginning timestamp of the same day. This includes S3 daily storage metrics and some billing metrics.</p> <p>Should you need these metrics, we recommend using Cloudwatch Exporter using Prometheus alongside our new CloudWatch integration designed to retrieve those metrics. For updated information, contact Coralogix Support.</p>"},{"location":"newoutput/amazon-kinesis-data-firehose-metrics/#additional-resources","title":"Additional Resources","text":"DocumentationGetting Started with Coralogix"},{"location":"newoutput/amazon-kinesis-data-firehose-metrics/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/amazon-s3-data-collection-options/","title":"Amazon S3: Data Collection Options","text":"<p>Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. You can record the actions that are taken by users, roles, or AWS services on Amazon S3 resources and maintain log records for auditing and compliance purposes.</p> <p>Use any of our customized log collection options to allow Coralogix to ingest the logs stored in your Amazon S3 bucket and process them for further analysis and monitoring.</p>"},{"location":"newoutput/amazon-s3-data-collection-options/#overview","title":"Overview","text":"<p>Coralogix provides multiple methods in which you can collect logs from Amazon S3. Send us your CloudTrail data from your Amazon S3 bucket using an AWS Lambda function, with one of two event-driven design patterns:</p> <ul> <li> <p>Invoke the Lambda directly through an S3 event</p> </li> <li> <p>Send the S3 event to a Simple Notification Service (SNS) queue, which in turn triggers the Lambda</p> </li> </ul>"},{"location":"newoutput/amazon-s3-data-collection-options/#customized-log-collection-options","title":"Customized Log Collection Options","text":"<p>Use any of our customized log collection options to allow Coralogix to ingest the logs stored in your Amazon S3 bucket and process them for further analysis and monitoring.</p> <ul> <li> <p>Automated Integration Packages (Recommended). For both S3 event and SNS design patterns, deploy the Coralogix Lambda function using our two-step, automated integration packages.</p> </li> <li> <p>Serverless Application Repository (SAR). Deploy our Coralogix Lambda function via our AWS serverless application repository. Use this for S3 event or SNS design patterns.</p> </li> <li> <p>Terraform. For both S3 event and SNS design patterns, install and manage the Amazon S3 integration with AWS services as modules in your infrastructure code using our AWS S3 Log Collection Terraform module.</p> </li> </ul>"},{"location":"newoutput/amazon-s3-data-collection-options/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/amazon-web-services-aws-ecs-ec2-opentelemetry-instrumentation/","title":"AWS ECS-EC2 OpenTelemetry Instrumentation","text":"<p>This tutorial demonstrates how to deploy OpenTelemetry to ECS to facilitate the collection of logs, metrics, and traces, and send them to Coralogix.</p>"},{"location":"newoutput/amazon-web-services-aws-ecs-ec2-opentelemetry-instrumentation/#image","title":"Image","text":"<p>This implementation utilizes the wrapper image <code>coralogixrepo/otel-coralogix-ecs-ec2</code>, based on the official OpenTelemetry contrib image.</p> <p>Notes:</p> <ul> <li> <p>The wrapper image is used to dynamically apply the OpenTelemetry configuration at runtime from an environment variable.</p> </li> <li> <p>The image configuration utilizes the\u00a0otlp receiver\u00a0for both\u00a0HTTP (on 4318)\u00a0and\u00a0GRPC (on 4317). Data can be sent using either endpoint.</p> </li> </ul>"},{"location":"newoutput/amazon-web-services-aws-ecs-ec2-opentelemetry-instrumentation/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Coralogix account</p> </li> <li> <p>AWS credentials configured</p> </li> <li> <p>ecs-cli</p> </li> <li> <p>aws-cli</p> </li> <li> <p>Coralogix Otel ECS Agent\u00a0CloudFormation template</p> </li> </ul>"},{"location":"newoutput/amazon-web-services-aws-ecs-ec2-opentelemetry-instrumentation/#configuration","title":"Configuration","text":""},{"location":"newoutput/amazon-web-services-aws-ecs-ec2-opentelemetry-instrumentation/#opentelemetry","title":"OpenTelemetry","text":"<p>The OpenTelemetry configuration for the agent is stored in a Base64-encoded environment variable and is applied at runtime. This allows you to dynamically pass any configuration values you choose as a parameter to CloudFormation.</p> <p>Use either of the following configuration examples or combine them to send Coralogix your logs, metrics, and traces.</p> <ul> <li> <p>logs</p> </li> <li> <p>metrics and traces</p> </li> </ul> <p>Notes:</p> <ul> <li> <p>The configuration examples work directly with the\u00a0<code>coralogixrepo/otel-coralogix-ecs-wrapper</code>\u00a0docker image for ECS.</p> </li> <li> <p>The Coralogix Exporter enables the use of enrichments such as dynamic\u00a0<code>application</code>\u00a0or\u00a0<code>subsystem</code>\u00a0name, defined using\u00a0<code>application_name_attributes</code>\u00a0and\u00a0<code>subsystem_name_attributes</code>, respectively.</p> </li> </ul>"},{"location":"newoutput/amazon-web-services-aws-ecs-ec2-opentelemetry-instrumentation/#ecs-cluster","title":"ECS Cluster","text":"<p>Deploy a new ECS cluster. If you already have an existing ECS Cluster, skip this step.</p> <p>STEP 1. Deploy a new ECS cluster:</p> <pre><code>ecs-cli up --region &lt;region&gt; --keypair &lt;your-key-pair&gt; --cluster &lt;cluster-name&gt; --size &lt;no. of instances&gt; --capability-iam \n</code></pre> <p>Note: The\u00a0<code>--keypair</code>\u00a0flag and STEP 2 are not mandatory. However, if no Key Pair is supplied, you will be unable to connect to any of the EC2 instances in the cluster via SSH.</p> <p>STEP 2. Create a key pair using the command below:</p> <pre><code>aws ec2 create-key-pair --key-name MyKeyPair --query 'KeyMaterial' --output text &gt; MyKeyPair.pem\n</code></pre> <p>STEP 3. Control default values.</p> <p>The\u00a0<code>ecs-cli up</code>\u00a0command will leverage CloudFormation to create an ECS Cluster. Default values will be used to create and configure a VPC and Subnets. These values can also be controlled using:</p> <pre><code>ecs-cli up --help\n</code></pre>"},{"location":"newoutput/amazon-web-services-aws-ecs-ec2-opentelemetry-instrumentation/#ecs-task-definition-service","title":"ECS Task Definition &amp; Service","text":"<p>Deploy a Task Definition, used by ECS to create an ECS Service to run OpenTelemetry.</p> <p>STEP 1. Deploy this CloudFormation template, ensuring that all the necessary parameters are provided. You are required to input:</p> <ul> <li> <p>Coralogix Send Your Data - API Key</p> </li> <li> <p>Coralogix region associated with your account domain</p> </li> <li> <p>Coralogix application and subsystem names</p> </li> </ul> <p>STEP 2. Once the template is deployed, verify that the container is running:</p> <pre><code>ecs-cli ps --region &lt;region&gt; -c &lt;cluster name&gt;\n</code></pre>"},{"location":"newoutput/amazon-web-services-aws-ecs-ec2-opentelemetry-instrumentation/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/amazon-web-services-aws-s3-log-collection-via-sns-trigger/","title":"AWS S3 Log Collection via SNS Trigger","text":"<p>Like our S3 Log collection integration, this integration will process logs from you S3 buckets, but is triggered by an SNS notification. For easy setup, use our app in the AWS Serverless Application Repository.</p>"},{"location":"newoutput/amazon-web-services-aws-s3-log-collection-via-sns-trigger/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Ready-made SNS Topic</p> </li> <li> <p>Ready-made S3 bucket with configured Event Notifications to above SNS Topic</p> </li> <li> <p>AWS permissions to create Lambdas and IAM roles</p> </li> </ul>"},{"location":"newoutput/amazon-web-services-aws-s3-log-collection-via-sns-trigger/#installation","title":"Installation","text":"<p>STEP 1. Navigate to the application page and search for Coralogix-S3-via-SNS.</p> <p>STEP 2. Fill in the required parameters.</p> <p>STEP 3. Click Deploy.</p>"},{"location":"newoutput/amazon-web-services-aws-s3-log-collection-via-sns-trigger/#parameters-descriptions","title":"Parameters &amp; Descriptions","text":"Variable Description Application Name Stack name of the application created via AWS CloudFormation. If your log is JSON format, use its dynamic value. Example: $.level1.level2.value NotificationEmail Failure notification email address ApplicationName Application name as it appears in your Coralogix UI BlockingPattern If you wish to block some of the logs, adding a substring will act as a selector. Default is empty to send all logs. BufferSize Buffer size for logs in the Lambda function CoralogixRegion Coralogix region associated with your Coralogix domain CustomDomain Coralogix custom domain. Leave empty if you do not use a custom domain. Debug Coralogix logger debug mode FunctionArchitecture Function supports x86_64 or arm64 FunctionMemorySize Max memory for the function itself FunctionTimeout Maximum time in seconds the function may be allowed to run NewlinePattern Pattern for lines splitting. Default is\u00a0(?:\\r\\n PrivateKey Coralogix Send-Your-Data API Key S3BucketName Name of the S3 bucket to watch SNSTopicArn ARN of SNS topic to subscribe SamplingRate Sets the sampling rate The rate is set to 1 by default, meaning that it collects every log message from the S3 bucket. Increase it to change the sampling rate [i.e. increase it to 2 to ship 1 of every 2 logs, etc]. SubsystemName Subsystem name as it appears in your Coralogix UI. If your log is JSON format, can use its dynamic value, for example: $.level1.level2.value. <p>Notes:</p> <ul> <li>To perform filtering based on prefix or suffix of the S3 object, filters need to be placed on the S3 bucket\u2019s Event Notification. Documentation for this process can be found here.</li> </ul>"},{"location":"newoutput/amazon-web-services-aws-s3-log-collection-via-sns-trigger/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/apdex-score/","title":"Apdex Score","text":"<p>Take advantage of our Apdex Score widget to measure and quantify user satisfaction in your Coralogix Service Catalog.</p>"},{"location":"newoutput/apdex-score/#overview","title":"Overview","text":"<p>Apdex, or Application Performance Index, is a standardized metric used to measure and quantify user satisfaction with the response time of software applications. It provides a numerical score on a scale of 0 to 1, where a score of 0-0.74 is considered poor, 0.75-0.84 is fair, 0.85-0.94 is good, and 0.95-1 is excellent.</p> <p>Apdex takes into account the response time threshold, categorizing user interactions as satisfied, tolerated, or frustrating based on a predefined performance threshold. This metric is particularly valuable for organizations and developers in assessing and optimizing application performance, as it offers a concise and standardized way to communicate and benchmark user satisfaction with application responsiveness.</p>"},{"location":"newoutput/apdex-score/#how-it-works","title":"How It Works","text":"<p>An Apdex score is calculated using a simple formula that involves defining a response time threshold and then categorizing \u201cSatisfied\u201d user requests based on those thresholds. The formula is as follows:</p> <p></p> <p>In this formula, request satisfaction is categorized as follows:</p> <ul> <li> <p>Satisfied requests: The response time is less than or equal to T (Threshold).</p> </li> <li> <p>Tolerated requests: The response time is greater than T and less than or equal to 4T.</p> </li> <li> <p>Frustrated requests: The response time is greater than 4T or the request returns a server-side error (not shown in the formula).</p> </li> <li> <p>Total requests: The total number of requests.</p> </li> </ul> <p>In your Service Catalog, configure the response time threshold based on what you consider to be satisfactory for users. The tolerated and frustrated requests are calculated accordingly. For example, if you define response times less than or equal to 2 seconds as satisfactory, this threshold will be used in the Apdex calculation, and a response time of 0-2 seconds will be considered satisfactory, 2-8 seconds (less than or equal to 4T) will be tolerated, and 8 seconds and above will be frustrated. The formula then generates a score between 0 and 1, where a score closer to 1 indicates better user satisfaction with application performance.</p> <p></p> <p>In this example, the threshold is 0.5 seconds. This gives the following response times:</p> <ul> <li> <p>0 - 0.5 seconds: Satisfied</p> </li> <li> <p>0.51 - 2 seconds: Tolerated</p> </li> <li> <p>2+ seconds: Frustrated</p> </li> </ul>"},{"location":"newoutput/apdex-score/#configure-the-response-time-threshold","title":"Configure the Response Time Threshold","text":"<p>STEP 1. In your Coralogix toolbar, navigate to APM &gt; Service Catalog.</p> <p>STEP 2. Select the timeframe for which you want to view information.</p> <p>STEP 3. Select a service to view the service drill-down.</p> <p>STEP 4. Click Threshold on the Apdex widget.</p> <p></p> <p>Note: The default threshold is 0.5 seconds. Unless you change it, that is the data that you will see.</p> <p>STEP 5. Select the threshold and unit for the Apdex score.</p> <p></p> <p>The Latency graph shows your selected threshold along with the P99, P95, P75 and Average latencies, enabling you to see how your selected threshold compares with the current latency of your service.</p> <p>This graph offers a clear context to evaluate the appropriateness of your threshold. For instance, setting a threshold of 5 seconds for services with a p99 latency of 1 second could be overly tolerant, indicating a need for a more stringent threshold.</p> <p>STEP 6. Click SAVE CHANGES.</p> <p>Going forward the Apdex score will calculated by the threshold you\u2019ve defined.</p>"},{"location":"newoutput/apdex-score/#additional-resources","title":"Additional Resources","text":"DocumentationAPMService CatalogBlogsApdex Score: Calculation, Pros/Cons &amp; 5 Ways to Improve Yours - Coralogix"},{"location":"newoutput/apdex-score/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/apm/","title":"Introduction to APM","text":"<p>Coralogix offers\u00a0Application Performance Monitoring (APM) for modern, cloud-native environments. Our new features decorate all pillars of\u00a0observability\u00a0with additional information that extends beyond system availability, service performance, and response times.</p> <p>With this expanded visibility into service performance, you can effectively monitor latency and rapidly find the component responsible for issues like performance degradation or increased errors. APM allows you to contextualize and pinpoint the root cause of a problem and respond immediately before the user is affected.</p>"},{"location":"newoutput/apm/#concepts","title":"Concepts","text":"<p>Spans and traces form the basis of application performance monitoring in Coralogix APM.</p> <p>Using this telemetry data, Coralogix allows you to observe application resource consumption and infrastructure resource consumption using two new observability layers.</p> <code>POD</code>application resource consumptionKey factors that impact response times and throughput of applications<code>HOST</code>infrastructure resource consumptionUsage of IT resources, systems, and processes"},{"location":"newoutput/apm/#features","title":"Features","text":""},{"location":"newoutput/apm/#pod-host","title":"Pod &amp; Host","text":"<p>Our APM provides you with all logs relevant to a particular span context, granting a full picture of the services that power your applications.</p> <p>Use our newest layers of observability \u2013\u00a0<code>POD</code>\u00a0and\u00a0<code>HOST</code>\u00a0\u2013 to:</p> <ul> <li> <p>Instantly view all of your pod and host metrics, including resource consumption and associated network information</p> </li> <li> <p>Compare metrics within a specific pod and across pods from a specific service</p> </li> <li> <p>Compare all of the pods associated with a specific service</p> </li> <li> <p>Correlate between Kubernetes spans, logs, and metrics for a specific pod and/or host</p> </li> <li> <p>Troubleshoot log span errors</p> </li> <li> <p>Annotate deployment tags based on span context</p> </li> </ul> <p>Find out more here.</p> <p></p> <p></p>"},{"location":"newoutput/apm/#service-map","title":"Service Map","text":"<p>Our Service Map feature provides a full visualization of your system architecture, breaking down your application into all its constituent\u00a0services and drawing the observed dependencies between these services in real time on the basis of your distributed tracing.</p> <p></p> <p>Use Service Map to:</p> <ul> <li> <p>Enjoy a visual representation of your entire system</p> </li> <li> <p>Gain a better understanding of the health of your system by viewing data flows</p> </li> <li> <p>Identify bottlenecks and troubleshoot in real time</p> </li> </ul> <p>Find out more here.</p>"},{"location":"newoutput/apm/#service-catalog","title":"Service Catalog","text":"<p>Our Service Catalog feature provides a list of all the services that you have in your system, showing you the health of each service. You can choose the timeframe for which you want to see the services, from a list of options. The service catalog shows the service type, the number of requests sent by the service, the error rate, and the P95 latency (that is, the frequency with which there was a delay in the request).</p> <p>The service catalog also includes a search bar that lets you search either by service or by any of the other parameters shown.</p> <p></p> <p>Use the Service Catalog to:</p> <ul> <li> <p>Get a deeper understanding of the health of your services using the service drilldown.</p> </li> <li> <p>See how many and what resources are being used by your services.</p> </li> <li> <p>Identify problems and troubleshoot in real time.</p> </li> </ul> <p>Find out more here.</p>"},{"location":"newoutput/apm/#severless-monitoring","title":"Severless Monitoring","text":"<p>Our Serverless Monitoring feature provides customers using the Coralogix AWS Lambda Telemetry Exporter with the ability to better control and understand your lambda servers on both macro and granular levels.</p> <p></p> <p>For each lambda function, view its triggers, the AWS account to which the function belongs, region, runtime, invocations, errors, timeouts, last invocation, average latency, and whether the function is out of memory.</p> <p>Find out more here.</p>"},{"location":"newoutput/apm/#getting-started","title":"Getting Started","text":"<p>Get started using our APM Onboarding Tutorial.</p>"},{"location":"newoutput/apm/#additional-resources","title":"Additional Resources","text":"BlogOne Click Visibility: Coralogix Expands APM Capabilities to Kubernetes"},{"location":"newoutput/apm/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/apm-amazon-ec2/","title":"APM using AWS EC2","text":"<p>Using Amazon Kinesis Data Firehose, you can now send metrics to Coralogix from Amazon Elastic Compute Cloud (Amazon EC2) and view them on your Coralogix dashboard using our application performance monitoring features.</p> <p></p>"},{"location":"newoutput/apm-amazon-ec2/#prerequisites","title":"Prerequisites","text":"<p>1. Sign up for a Coralogix account. Set up your account on the Coralogix domain corresponding to the region within which you would like your data stored.</p> <p>2. Access your Coralogix Send Your Data - API Key.</p> <p>3. Configure a metrics bucket in your Coralogix dashboard. Data Flow &gt; Setup Archive</p> <p>4. Create an active AWS account with relevant permissions</p>"},{"location":"newoutput/apm-amazon-ec2/#installation-configuration","title":"Installation &amp; Configuration","text":"<p>Configure Amazon Kinesis Data Firehose to send your telemetry data to Coralogix. We strongly recommend sending Coralogix your logs, metrics, and tracing, so that you can enjoy the highest quality APM.</p> <p>STEP 1: Configure Amazon Kinesis Data Firehose to send your metrics to Coralogix.</p> <p>STEP 2: Set up OpenTelemetry Collector to correlate your logs and traces.</p> <p>Add the resource detection processor.</p> <pre><code>processors:\n      resourcedetection/ec2:\n        detectors: [\"ec2\", \"env\"]\n        ec2:\n          # A list of regex's to match tag keys to add as resource attributes\n          tags:\n            - ^ec2.tag.name$\n            - ^ec2.tag.subsystem$\n            - ^aws.*$\n\n</code></pre> <p>STEP 3: Add AWS tags to spans.</p> <p>Add the <code>resourcedetection/ec2</code> processor to the list of span processors.</p> <pre><code>traces:\n    processors:\n      - memory_limiter\n      - batch\n      - resourcedetection/ec2\n\n</code></pre> <p>STEP 4: Add AWS tags to logs.</p> <p>Add the <code>resourcedetection/ec2</code> processor to the list of logs processors.</p> <pre><code>logs:\n  processors:\n    - resourcedetection/ec2\n\n</code></pre> <ul> <li> <p>Use <code>cloud.provider</code> and <code>cloud.platform</code> to validate that the process is running on AWS using the EC2 platform.</p> </li> <li> <p>Use <code>host.id</code> and <code>host.name</code> to correlate the logs with metrics and spans.</p> </li> </ul>"},{"location":"newoutput/apm-amazon-ec2/#apm-on-your-coralogix-dashboard","title":"APM on Your Coralogix Dashboard","text":"<p>Access our APM features on your Coralogix dashboard</p> <p>STEP 1: In your Coralogix dashboard, click on Explore &gt; Tracing. Click on the trace of interest.</p> <p></p> <p>STEP 2: Click on the <code>HOST</code> feature to view the AWS metrics with a <code>host.id</code> associated with a specific EC2 instance. View your metrics in either SPANS VIEW or DEPENDENCIES VIEW.</p> <p></p>"},{"location":"newoutput/apm-amazon-ec2/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/apm-kubernetes/","title":"APM using OpenTelemetry Collector with Kubernetes","text":"<p>Coralogix now offers features of application performance monitoring (APM) for modern, cloud-native environments for those customers using OpenTelemetry collector with a Kubernetes processor. Our new features decorate all pillars of observability with additional information that extends beyond system availability, service performance, and response times.</p> <p></p>"},{"location":"newoutput/apm-kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Kubernetes installed with the command-line tool kubectl</p> </li> <li> <p>Helm installed and configured</p> </li> <li> <p>Metrics bucket set up</p> </li> </ul>"},{"location":"newoutput/apm-kubernetes/#installation-configuration","title":"Installation &amp; Configuration","text":"<p>Install and configure OpenTelemetry for Kubernetes to send your telemetry data to Coralogix. We strongly recommend configuring Otel to send Coralogix your logs, metrics, and traces, so that you can enjoy the highest quality APM and correlate your data.</p>"},{"location":"newoutput/apm-kubernetes/#apm-on-your-coralogix-dashboard","title":"APM on Your Coralogix Dashboard","text":"<p>Access our APM features on your Coralogix dashboard</p> <p>STEP 1. In your Coralogix dashboard, click on the Explore tab &gt; Tracing. Then click on the trace of interest.</p> <p></p> <p>STEP 2. Click on <code>POD</code> and <code>HOST</code> features, in addition to <code>RELATED LOGS</code> and <code>SPAN LOGS</code>. View them in either SPANS VIEW or DEPENDENCIES VIEW.</p> <p></p>"},{"location":"newoutput/apm-kubernetes/#validation","title":"Validation","text":"<p>Validate your configuration by taking the following steps. If any of the queries below return empty or invalid results, review the data being sent.</p>"},{"location":"newoutput/apm-kubernetes/#k8s-resource-attributes-and-trace-spans","title":"K8s Resource Attributes and Trace Spans","text":"<p>To match Prometheus or OpenTelemetry metrics with trace spans, it is necessary to ship valid K8s Resource Attributes from OpenTelemetry.</p> <p>Here is an example of K8s Resource Attributes associated with a span from the Coralogix Tracing UI.</p> <p></p> <p>Our APM backend will check for both the k8s.pod.name and k8s.node.name attributes and correlate them with metrics and logs. If these values are not set or are missing, the APM process will not function correctly.</p>"},{"location":"newoutput/apm-kubernetes/#host-metrics-prometheus","title":"Host Metrics (Prometheus)","text":"<p>There are specific metrics that are required when correlating data for APM. When using Prometheus, metrics from the Kube State Metrics (KSM) and the Node Exporter are required. Note that both are included when installing the Coralogix Prometheus Operator.</p>"},{"location":"newoutput/apm-kubernetes/#validate-the-kube-state-metrics-ksm","title":"Validate the Kube State Metrics (KSM)","text":"<p>To verify KSM metrics are being received, run a query via the Coralogix Grafana console to validate that KSM metric labels match the k8s.node.name attribute described in our OpenTelemetry span:</p> <pre><code>kube_node_info{node=\"&lt;your node name&gt;\"}\n</code></pre> <p></p>"},{"location":"newoutput/apm-kubernetes/#validate-node-exporter-metrics","title":"Validate Node Exporter Metrics","text":"<p>The PromQL query from the previous step will return the internal_ip label. Use this value to verify that you are receiving node exporter metrics for the relevant node or host:</p> <pre><code>count(node_cpu_seconds_total{instance=~\"&lt;internal_ip&gt;.+\"}) by (instance)\n</code></pre> <p></p>"},{"location":"newoutput/apm-kubernetes/#validate-pod-metrics","title":"Validate Pod Metrics","text":"<p>To verify that pod metrics are being received from KSM, run the following query in Grafana:</p> <pre><code>kube_pod_info{pod=~\"&lt;partialpodname&gt;.*\"}\n</code></pre> <p></p> <p>Use the following queries to confirm that you are receiving pod metrics in the correct format:</p> <pre><code>kube_pod_info{pod=~\"&lt;partialpodname&gt;.*\"}\n</code></pre> <p></p> <pre><code>k8s_pod_cpu_time_total{k8s_pod_name=~\"&lt;partialpodname&gt;.*\"}\n</code></pre> <p></p>"},{"location":"newoutput/apm-kubernetes/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/apm-kubernetes-open-telemetry-opentelemetry/","title":"APM using OpenTelemetry as a Unified Shipper with Kubernetes","text":"<p>When we refer to OpenTelemetry as a Unified Shipper, we are describing architecture where by OpenTelemetry is leveraged to collect all the required data (logs, metrics &amp; traces) to facilitate\u00a0application performance monitoring (APM)\u00a0functionality.</p>"},{"location":"newoutput/apm-kubernetes-open-telemetry-opentelemetry/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Sign up for a Coralogix account. Set up your account on the Coralogix domain corresponding to the region within which you would like your data stored.  </p> </li> <li> <p>Access your Coralogix Send Your Data - API Key.  </p> </li> <li> <p>Install Kubernetes. This should include installation of the command-line tool kubectl, designed to operate on your Kubernetes cluster.  </p> </li> <li> <p>Install and configure Helm. We suggest you use this guide to familiarize yourself with the basics of using Helm to manage packages on your Kubernetes cluster.  </p> </li> </ol>"},{"location":"newoutput/apm-kubernetes-open-telemetry-opentelemetry/#installation-configuration","title":"Installation &amp; Configuration","text":"<p>As mentioned above, we will be using Helm to install the Coralogix OpenTelemetry Collector which will be used to collect all the required data (logs, metrics &amp; traces).</p> <ol> <li>Configure and Install OpenTelemetry using the\u00a0Coralogix OpenTelemetry Helm Chart.  </li> </ol> <p>The following\u00a0<code>values.yaml</code>\u00a0override file can be used as a starting point. Additional configuration values can be added as required:</p> <pre><code>global:\n  traces:\n    endpoint: \"ingress.coralogix.&lt;domain&gt;:443\"\n  metrics:\n    endpoint: \"ingress.coralogix.&lt;domain&gt;:443\"\n  logs:\n    endpoint: \"ingress.coralogix.&lt;domain&gt;:443\"\n\nopentelemetry-collector:\n  mode: \"daemonset\"\n  presets:\n    logsCollection:\n      enabled: true\n    kubernetesAttributes:\n      enabled: true\n    hostMetrics:\n      enabled: true\n    kubeletMetrics:\n      enabled: true\n\n</code></pre> <p>Once installed, logs, metrics and any available trace data will be sent to Coralogix and will be visible via the Console.</p>"},{"location":"newoutput/apm-kubernetes-open-telemetry-opentelemetry/#validation-troubleshooting","title":"Validation &amp; Troubleshooting","text":"<p>Once OpenTelemetry is installed and sending data, there are some steps we can take to verify that the we are receiving all the data required for APM to function.  </p>"},{"location":"newoutput/apm-kubernetes-open-telemetry-opentelemetry/#k8s-resource-attributes","title":"K8s Resource Attributes","text":"<p>In order to correlate OpenTelemetry metrics with trace spans, we need to have valid values in the k8s Resource Attributes being shipped from OpenTelemetry. Here is a screenshot of expected K8s Resource Attributes associated with a Span from the Coralogix Tracing UI.</p> <p></p>"},{"location":"newoutput/apm-kubernetes-open-telemetry-opentelemetry/#otel-metrics","title":"Otel Metrics","text":"<ul> <li> <p>Check that the OpenTelemetry K8s node metrics are being received correctly.  </p> <p>In the Coralogix Grafana view, execute the following query to validate that both k8s_node_cpu_utilization and the associated label k8s_node_name are present and can be queried.</p> </li> </ul> <pre><code>sum(k8s_node_cpu_utilization_1) by (k8s_node_name)\n</code></pre> <p></p> <ul> <li> <p>Check that the OpenTelemetry system metrics are being received correctly.  </p> <p>In the Coralogix Grafana view, execute the following query to validate that both system_memory_usage_By and the associated label host_name are present and can be queried.</p> </li> </ul> <pre><code>sum(system_memory_usage_By) by (host_name)\n</code></pre> <p></p> <p>These are some steps that can be taken to verify that Coralogix has everything it requires for APM. If any of the queries above return empty or invalid results, there may be an issue with the data being sent.</p>"},{"location":"newoutput/apm-kubernetes-open-telemetry-opentelemetry/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us via our in-app chat or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/apm-onboarding-tutorial/","title":"APM Onboarding Tutorial","text":"<p>This guide provides step-by-step instructions on configuring and using Coralogix Application Performance Monitoring (APM). Follow these steps to effectively onboard your services, install collectors, correlate resources, and leverage advanced features.</p>"},{"location":"newoutput/apm-onboarding-tutorial/#apm-basic-setup","title":"APM Basic Setup","text":"<p>To set up APM, instrument your service and install the Otel Collector.</p>"},{"location":"newoutput/apm-onboarding-tutorial/#service-instrumentation","title":"Service Instrumentation","text":"<p>To get started with Coralogix APM, generate spans from your services by instrumenting them. Choose the relevant documentation for your service\u2019s programming language:</p> <ul> <li> <p>Node.js</p> </li> <li> <p>Golang</p> </li> <li> <p>PHP</p> </li> <li> <p>Java</p> </li> <li> <p>Python</p> </li> </ul>"},{"location":"newoutput/apm-onboarding-tutorial/#otel-collector-installation","title":"Otel Collector Installation","text":"<p>Install the OpenTelemetry Collector. The Collector is an intermediary component responsible for aggregating and transmitting data, playing a key role in the overall APM workflow. Once installed, the Coralogix Events2Metrics feature automatically converts your spans and logs to metrics to be displayed in your UI. Select from any of the following configuration options:</p> <ul> <li> <p>Kubernetes Complete Observability using OpenTelemetry</p> </li> <li> <p>Docker</p> </li> <li> <p>EC2</p> </li> <li> <p>ECS-Fargate</p> </li> <li> <p>ECS-EC2</p> </li> </ul> <p>APM should run once your service is instrumented and the Collector is installed.</p>"},{"location":"newoutput/apm-onboarding-tutorial/#resource-correlation","title":"Resource Correlation","text":"<p>Investigate service-resource correlation within your Kubernetes environment in your APM Resources tab or Kubernetes Dashboard. This correlation relies on span tags and metric labels.</p> <p>Span tags are metadata attached to requests moving through a distributed system, facilitating traceability and correlation. Each microservice adds or propagates span tags, creating a trace that visualizes the request's path, aiding in identifying performance bottlenecks and dependencies.</p> <p>Metric labels are key-value pairs associated with metrics collected from Kubernetes resources. These labels offer additional context to metrics, such as CPU usage or network traffic, making organizing and analyzing data based on service names, pod names, or environments easier.</p>"},{"location":"newoutput/apm-onboarding-tutorial/#kubernetes-opentelemetry-extension","title":"Kubernetes OpenTelemetry Extension","text":"<p>Coralogix recommends the\u00a0Kubernetes Complete Observability integration package\u00a0for comprehensive Kubernetes and application observability. Deploying the Kubernetes OpenTelemetry extension enables telemetry data collection and full service-resource correlation in your APM Resources tab or Kubernetes Dashboard, without any additional steps.</p>"},{"location":"newoutput/apm-onboarding-tutorial/#kubernetes-resource-correlation-without-the-otel-extension","title":"Kubernetes Resource Correlation Without the Otel Extension","text":"<p>If you are not using the\u00a0Kubernetes Complete Observability integration package, take the following steps:</p> <ol> <li> <p>Ensure your span tags include the <code>K8s.deployment.name</code> label. In case of using StatefulSets or DaemonSets, replace <code>K8s.deployment.name</code> with <code>K8s.DaemonSets.name</code> or <code>K8s.StatefulSets.name</code>, respectively.</p> </li> <li> <p>In cases where metrics or metric labels are missing, you will be prompted to deploy our Kubernetes Complete Observability integration package or reload the missing labels and tags.</p> </li> </ol> <p></p>"},{"location":"newoutput/apm-onboarding-tutorial/#ecs-resource-correlation","title":"ECS Resource Correlation","text":"<p>Correlate ECS-EC2 or ECS Fargate performance metrics with your service as follows:</p> <ul> <li> <p>ECS-EC2</p> </li> <li> <p>ECS Fargate</p> </li> </ul> <p>Service-resource correlation within your Kubernetes environment is presented in your APM Resources tab.</p>"},{"location":"newoutput/apm-onboarding-tutorial/#correlation-using-prometheus","title":"Correlation Using Prometheus","text":"<p>If you are using Prometheus to send us telemetry, ensure that kube state metrics and node exporter metrics are sent by Prometheus Operator to set up metric labels for Kubernetes resource correlation in your Kubernetes Dashboard. You may validate the metrics by following these instructions.</p>"},{"location":"newoutput/apm-onboarding-tutorial/#correlate-logs-with-services","title":"Correlate Logs with Services","text":"<p>Investigate the logs produced by your service by enriching your logs with a subsystem name matching the service name. Alternatively, set up the correlation by following these instructions.</p>"},{"location":"newoutput/apm-onboarding-tutorial/#add-your-slos","title":"Add Your SLOs","text":"<p>Set up your service level objectives by following these instructions.</p>"},{"location":"newoutput/apm-onboarding-tutorial/#advanced","title":"Advanced","text":""},{"location":"newoutput/apm-onboarding-tutorial/#instrument-serverless-functions","title":"Instrument Serverless Functions","text":"<p>Auto-instrument AWS Lambda by following these instructions.</p>"},{"location":"newoutput/apm-onboarding-tutorial/#tail-sampling","title":"Tail Sampling","text":"<p>Learn about Tail Sampling with OpenTelemetry using Kubernetes and Docker.</p>"},{"location":"newoutput/apm-onboarding-tutorial/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogix.com.</p>"},{"location":"newoutput/application-and-subsystem-names/","title":"Application and Subsystem Names","text":"<p>In order for us to help you make sense of your logs, metrics, and traces, we require that you organize your data using two metadata fields: <code>application</code>\u00a0and\u00a0<code>subsystem</code>\u00a0name.</p> <p>These values will ultimately give value to your data in a manner that is most meaningful for you on your Coralogix Dashboard.</p> <p>Completely customizable, you can use them to catalogue and filter your logs and metrics, grant different user permissions and access to different employees, create log templates and template branches, and track anomalies in your data most efficiently.</p>"},{"location":"newoutput/application-and-subsystem-names/#application-name","title":"Application Name","text":"<p>We recommend that your <code>application</code> name refer to the environment that produces and sends data to Coralogix. For instance, you may have an <code>application</code> name for production, development and / or staging environments. You can also use <code>application</code> name to refer to a certain namespace. Of course, you can map the <code>application</code> name differently based on your custom setup or if you have many Kubernetes clusters.</p>"},{"location":"newoutput/application-and-subsystem-names/#subsystem-name","title":"Subsystem Name","text":"<p>We recommend that your <code>subsystem</code> name refer to the service or applications that send your logs, metrics, and traces.</p>"},{"location":"newoutput/application-and-subsystem-names/#example","title":"Example","text":"<p>Input your chosen <code>application</code>\u00a0and\u00a0<code>subsystem</code>\u00a0names into any of the integrations supported by Coralogix.</p> <p>The following example shows mapping <code>application</code> name to\u00a0<code>production-$NAMESPACE</code>\u00a0for Telegraf Operator deployed in the production Kubernetes cluster.</p> <pre><code>    infra: |\n      [[outputs.opentelemetry]]\n        [outputs.opentelemetry.coralogix]\n          private_key = \"&lt;Send-Your-Data API key&gt;\"\n          application = \"production-$NAMESPACE\"\n          subsystem = \"$HOSTNAME\"\n\n</code></pre> <p>The following example shows mapping <code>application</code> name to <code>kubernetes.namespace_name</code> and <code>subsystem</code> to <code>kubernetes.container_name</code> for Fluent Bit.</p> <pre><code>---\n#fluentbit-override.yaml:\nfluent-bit:\n  endpoint: api.coralogixstg.wpengine.com\n  app_name: kubernetes.namespace_name\n  sub_system: kubernetes.container_name\n  logLevel: error\n\n</code></pre>"},{"location":"newoutput/application-and-subsystem-names/#application-and-subsystem-on-your-coralogix-dashboard","title":"Application and Subsystem on your Coralogix Dashboard","text":"<p>Use the <code>application</code>\u00a0and\u00a0<code>subsystem</code>\u00a0fields on your Coralogix dashboard to create alerts specific to these filters, filter across the UI, visualize in dashboard widgets, and more.</p> <p>When viewing your logs, metrics, and traces, you will see that <code>application</code> and <code>subsystem</code> fields act as filters, enabling the selection of values to be queried per filter.</p> <p></p>"},{"location":"newoutput/application-and-subsystem-names/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/archive-query/","title":"Archive Query","text":"<p>Archive Query enables you to directly query your logs and spans from your S3 archive using any text or a wide range of syntax queries. Query data irrespective of priority, daily quota, or its time frame.</p>"},{"location":"newoutput/archive-query/#overview","title":"Overview","text":"<p>Archive Query enables you to directly query your logs and spans from your S3 archive.</p> <p>Use this feature to:</p> <ul> <li> <p>Take advantage of various querying options. This includes text-based queries or specific query languages such as Lucene or DataPrime.</p> </li> <li> <p>Enjoy multiple data sources. Using our innovative DataPrime syntax language, query both logs and spans.</p> </li> <li> <p>Query data irrespective of priority and daily quota. Only blocked logs are not sent to the archive.</p> </li> <li> <p>Query data with unlimited time frames. There are no restrictions on how far back in time your data can go.</p> </li> <li> <p>Save on costs while maintaining interactive query times. Store more of your data in our Monitoring and Compliance priority levels. By prioritizing logs at the Monitoring priority level, for example, you can view and query them in LiveTail, receive real-time alerts, utilize parsing rules, Loggregation and Events2Metrics, and query them without indexing your data. All of this is available at 40% of the cost.</p> </li> </ul>"},{"location":"newoutput/archive-query/#prerequisites","title":"Prerequisites","text":"<ul> <li>Read/ Write permissions enabled in your AWS S3 bucket</li> </ul>"},{"location":"newoutput/archive-query/#archive-query-setup","title":"Archive Query Setup","text":"<p>STEP 1. In your Coralogix toolbar, navigate to Data Flow &gt; Archive Queries.</p> <p></p> <p>STEP 2. Click ARCHIVE QUERY.</p> <p></p> <p>STEP 3. Define a New Archive Query.</p> <p></p> <ul> <li> <p>Enter a Query Name and Description.</p> </li> <li> <p>Search Query. Enter a text search query, Lucene, or DataPrime syntax query to match a subset of logs from your S3 bucket.</p> </li> </ul> <p>Notes:</p> <ul> <li> <p>Only the data matching the query will be presented.</p> </li> <li> <p>Using our innovative DataPrime syntax language, query both logs and spans.</p> </li> </ul> <p>Query Examples</p> <p>1. A query to find logs with the field ClientIP_geoip.continent_name:\u201dEurope\u201d and the field ClientIP_geoip.country_name with values other than: Czechia, United Kingdom or Germany:ClientIP_geoip.continent_name:\u201dEurope\u201d NOT (ClientIP_geoip.country_name:\u201dCzechia\u201d OR ClientIP_geoip.country_name:\u201dUnited Kingdom\u201d OR ClientIP_geoip.country_name:\u201dGermany\u201d)  </p> <p>2. A query to find logs with words status and get:status get  </p> <p>3. A query to find only logs with HTTP method post:\u201chttp_method\u201d:\u201dpost\u201d </p> <ul> <li> <p>Source. Select CX-Data.</p> </li> <li> <p>Select the Timeframe, Applications, Subsystems, and Severity for the query.</p> </li> </ul> <p>STEP 4. Click RUN ARCHIVE QUERY. Once you have set up and run your query, a test will be run to validate your setup.</p>"},{"location":"newoutput/archive-query/#view-archive-query-results","title":"View Archive Query Results","text":"<p>View your query results in one of three formats: Logs Preview, Download TSV, or Clone.</p> <p></p>"},{"location":"newoutput/archive-query/#logs-preview","title":"Logs Preview","text":"<p>This option allows you to view your logs without ever indexing your data.</p> <p></p>"},{"location":"newoutput/archive-query/#download-tsv","title":"Download TSV","text":"<p>Download a TSV file to view query results.</p> <p></p>"},{"location":"newoutput/archive-query/#clone","title":"Clone","text":"<p>Duplicate your current query by clicking on the\u00a0Clone\u00a0button. \u00a0In the new duplicated query, click RUN ARCHIVE QUERY.</p> <p></p> <p>If you wish to share an archive query with another teammate, click on the chain-link icon in the query of choice. This will copy to your clipboard the link to that same archive query.</p> <p></p> <p>After some time, the archive query you created will expire so you can no longer view or download the data. Click Clone and duplicate the same query with the same criteria instead of recreating the query from scratch.</p>"},{"location":"newoutput/archive-query/#limitations","title":"Limitations","text":"<p>The limitations placed on queries are described below.</p> Limitation Description Bytes processed Up to 30% of daily ingested bytes Parquet files scanned Up to 500k files Clone results Up to results 1M results while running Archive Query Time out Up to 5 min of query execution"},{"location":"newoutput/archive-query/#warnings","title":"Warnings","text":"<p>Once a limit is reached, a warning message is displayed. Refine your query results to avoid reaching a limit.</p>"},{"location":"newoutput/archive-query/#refine-your-query-results","title":"Refine Your Query Results","text":"<p>Refine your query results using any of the following methods:</p> <ul> <li> <p>Apply more selective filters to your queries (for example, on application or subsystem).</p> </li> <li> <p>If using the Dataprime extract operator and subsequently filtering its results, create a parsing rule and filter on the parsed field instead.</p> </li> <li> <p>Avoid regular expressions or wildcards in filters.</p> </li> <li> <p>In Dataprime, switch from using the contains operator on strings to the free text search operator (~).</p> </li> </ul>"},{"location":"newoutput/archive-query/#additional-resources","title":"Additional Resources","text":"DocumentationArchive Query from the Explore Screen"},{"location":"newoutput/archive-query/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com</p>"},{"location":"newoutput/archive-query-from-logs-screen/","title":"Archive Query from the Explore Screen","text":"<p>Directly query logs and spans in your S3 archive from your Explore Screen. Using\u00a0text-based queries or specific query languages such as\u00a0DataPrime or Lucene, you may query your data irrespective of its priority or timeframe and regardless of your daily quota - all with the ease of familiar functionalities.</p>"},{"location":"newoutput/archive-query-from-logs-screen/#overview","title":"Overview","text":"<p>Archive Query allows you to query the data in your S3 archive directly from your Explore screen.</p> <p>Use this feature to:</p> <ul> <li> <p>Query your archive in the Coralogix UI using any text or Lucene or DataPrime syntax query, irrespective of log priority or daily quota.</p> </li> <li> <p>Enjoy various data sources. Using DataPrime, you may query logs or spans.</p> </li> <li> <p>Query data with unlimited time frames. There are no restrictions on how far back in time your data can go.</p> </li> <li> <p>View query results alongside live data streams and with the use of familiar functionalities.</p> </li> <li> <p>Store more of your data in our Monitoring and Compliance priority levels, while maintaining interactive query times and saving on costs.</p> </li> </ul>"},{"location":"newoutput/archive-query-from-logs-screen/#prerequisites","title":"Prerequisites","text":"<ul> <li>Read/ Write permissions enabled in your AWS S3 bucket</li> </ul>"},{"location":"newoutput/archive-query-from-logs-screen/#getting-started","title":"Getting Started","text":"<p>STEP 1. From your Coralogix toolbar, navigate to Explore &gt; Logs tab.</p> <p>STEP 2. Select Archive the upper left-hand corner of your Logs screen.</p> <p>STEP 3. Select CX-Data in the dropdown menu.</p> <p></p>"},{"location":"newoutput/archive-query-from-logs-screen/#explore-screen-functionalities","title":"Explore Screen Functionalities","text":""},{"location":"newoutput/archive-query-from-logs-screen/#query","title":"Query","text":"<p>Query using any text or Lucene or DataPrime syntax query.</p>"},{"location":"newoutput/archive-query-from-logs-screen/#filter","title":"Filter","text":"<p>Filter your logs using application, subsystem, and severity filters.</p>"},{"location":"newoutput/archive-query-from-logs-screen/#manage-settings","title":"Manage Settings","text":"<p>Manage your settings by clicking on SETTINGS in the upper right-hand corner of your screen.</p> <p>Select ACTIONS to trigger 3rd party services based on your search results and / or values under specific keys. Find out more here.</p>"},{"location":"newoutput/archive-query-from-logs-screen/#limitations","title":"Limitations","text":"<p>The following limitations on Explore Screen archive queries are described below.</p> Limitation Description Bytes processed Up to 30% of daily ingested bytes Parquet files scanned Up to 500k files Total log results Up to 2k results While aggregations are available across all the data, we only pull up to 2,000 raw logs to display in the main logs grid. Exported log results Up to 2k results Exporting the data in the logs grid from an archive query is limited to the top 20 pages (100 logs per page), so you can export a max of 2000 logs at a time. In order to export all archive query logs, create an archive query by navigating Data Flow &gt; Archive Queries. <p></p> <p>Notes:</p> <ul> <li> <p>You may incur slight delays when querying archived data, when compared with other Explore screen queries.</p> </li> <li> <p>It is possible to use the same query syntax as queries run on the Archive Query page.</p> </li> </ul>"},{"location":"newoutput/archive-query-from-logs-screen/#warnings","title":"Warnings","text":"<p>Once a limit is reached, a warning message is displayed. Refine your query results to avoid reaching a limit.</p>"},{"location":"newoutput/archive-query-from-logs-screen/#refine-your-query-results","title":"Refine Your Query Results","text":"<p>Refine your query results using any of the following methods:</p> <ul> <li> <p>Apply more selective filters to your queries (for example, on application or subsystem).</p> </li> <li> <p>If using the Dataprime extract operator and subsequently filtering its results, create a parsing rule and filter on the parsed field instead.</p> </li> <li> <p>Avoid regular expressions or wildcards in filters.</p> </li> <li> <p>In Dataprime, switch from using the contains operator on strings to the free text search operator (~).</p> </li> </ul>"},{"location":"newoutput/archive-query-from-logs-screen/#additional-resources","title":"Additional Resources","text":"DocumentationArchive Query"},{"location":"newoutput/archive-query-from-logs-screen/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/archive-query-sql/","title":"Archive Query with SQL & On the Fly Parsing","text":"<p>Coralogix's 'Archive query' feature allows you to query logs directly from your S3 archive using SQL query syntax without counting against your daily quota, even if the data was never indexed. This enables you to store more of your data in our monitoring and compliance priority levels and take advantage of Coralogix\u2019s real-time analysis and remote storage search capabilities. This means you can use a shorter retention period and still be able to query all your data in less than 1 minute.</p> <p>Archive queries run on the S3 archive you set in Coralogix and are available for all TCO logging levels per our unique TCO model. For example, prioritizing logs at the monitoring level still allows you to query them without ever indexing the data as well as view and query them in the LiveTail, receive real-time alerts and anomalies on top of them, leverage parsing rules, Loggregation, and Events2Metrics at 40% of the cost.</p> <p></p> <p>In order to use this feature, make sure you have set Read/Write permission to your AWS S3 archive bucket (read more about enabling the Archive feature here) </p> <p>If you don't have such permission you will see the following screen:</p> <p></p> <p>Click on the 'ARCHIVE QUERY' button and the following dialogue box will open:</p> <p></p> <p>Toggle to the SQL Archive query option and the following dialog box will be shown. For more information on the Lucene query check our Archive query tutorial.</p> <p></p> <p>In the top section, you will fill the query name and description.</p> <p>In the Search query section, enter your SQL query statement and use LOGS as the table reference. You may include Presto functions for performing on-the-fly parsing, aggregations, etc. to match a subset of logs from S3.</p> <p>The Presto functions allow you to run on the fly parsing on data that was already sent to Coralogix, including textual data that was not parsed before its ingestion by using Coralogix parsing rules. Therefore, the value this capability brings is tremendous since not always you are in control of the parsing created/not created to the data in your account, either because you are not the owner of this data, don't know how to parse the data, or simply forgot to do so. Nevertheless, you are able to parse and aggregate the data after hand.</p> <p>Note: we will not mount anything besides logs matching this query.</p> <p>Choose the time frame criteria for the query. (Note: The time range limit is up to 24 hours)</p> <p>After clicking on the \"ARCHIVE QUERY\" button and wait till it is processed, you will see your new query and the 3 options: Logs preview, Download TSV, clone. (Note: Reindex by using SQL queries will be available soon!!)</p> <p></p> <p>If you click on \"Logs preview\" you will be able to view your logs. In case we are \"dealing\" with logs you have placed under Medium or Low priority (according to TCO model) you are previewing the data without ever indexing it!</p> <p></p> <p>Clicking on \"Download TSV\" will show you the following screen with the files you will download</p> <p></p> <p>In case you want to create another Archive query similar to one query you already created you can easily duplicate it by clicking on the Clone button.</p> <p>If you wish to share an Archive query with another teammate click on your query and on the top click again on the chain-link icon. This will copy to your clipboard the link to that same Archive query.</p> <p></p> <p>Notes:</p> <ul> <li> <p>Once a query expires, it is impossible to view, download, or reindex the data.</p> </li> <li> <p>In this case, create another similar query.</p> </li> </ul>"},{"location":"newoutput/archive-query-sql/#query-examples","title":"Query examples","text":"<ol> <li> <p>This example matches logs with the field kubernetes.pod_id equal to \"540d39bc-32f1-4e66-bcaf-e2ee04cede53\" and returns the count of them grouped by application name.</p> <p><code>SELECT applicationName,count(*)  FROM logs WHERE json_extract_scalar(text,'$.kubernetes.pod_id') = '540d39bc-32f1-4e66-bcaf-e2ee04cede53' GROUP BY applicationname</code></p> <p></p> </li> <li> <p>This example matches text logs where subsystem name is \"presto_archive_query\" with regular expressions and extracts the values bytes_sent, method into separate columns.</p> <p><code>SELECT regexp_extract(text, 'bytes_sent=(\\d+)', 1) bytes_sent,regexp_extract(text, 'method=([A-Z]+)', 1) method FROM LOGS WHERE subsystemName='presto_archive_query'</code></p> <p></p> </li> </ol>"},{"location":"newoutput/archive-retention-policy/","title":"Archive Retention Policy","text":"<p>In addition to our TCO Optimizer, Coralogix now offers an extra layer of granularity for your data retention. Using our new ARCHIVE RETENTION feature, you can now control and modify the length of time your logs are archived.</p>"},{"location":"newoutput/archive-retention-policy/#overview","title":"Overview","text":"<p>When creating a new policy for your data retention in your TCO Quota Optimizer screen, you now have the option of defining a lifecycle policy - the length of archive retention for a specific group of logs, defined by application, subsystem and severity.</p> <p>Users may choose between 4 default retention periods: default, short, intermediate, or long. The names and values for the latter three are subjective and determined by the tag names set by the user in his/her s3 bucket configuration. While one user may define a \u201cshort\u201d retention period as 3 days, another may define this period as 15 days. Another user may choose to change the name of the \u201cshort\u201d retention period to \u201cminimum\u201d.</p> <p></p> <p>Note:</p> <ul> <li> <p>Only newly archived files are affected by these new settings.</p> </li> <li> <p>Files created before the new lifecycle policy have no retention tag and are not changed retroactively. The data retention policy that applied to them before the new lifecycle policy - default or defined - will continue to apply even after the new settings are put in place.</p> </li> </ul>"},{"location":"newoutput/archive-retention-policy/#configuration","title":"Configuration","text":"<p>STEP 1. Create an s3 bucket for configuration.</p> <p>STEP 2. Configure <code>GetObjectTagging</code> and <code>PutObjectTagging</code> permissions.</p> <ul> <li> <p>Search S3 in your AWS search bar and select this service.</p> </li> <li> <p>Find and click the bucket of choice for storing the archive.</p> </li> <li> <p>Navigate to the Permissions tab. Edit the Bucket policy.</p> </li> </ul> <p></p> <ul> <li>Paste the following code and update the name of your bucket:</li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Id\": \"MyPolicyID\",\n    \"Statement\": [\n        {\n            \"Sid\": \"MyStatementSid\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::625240141681:root\"\n            },\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:ListBucket\",\n                \"s3:PutObject\",\n                \"s3:PutObjectTagging\",\n                \"s3:GetObjectTagging\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::&lt;bucket name&gt;\",\n                \"arn:aws:s3:::&lt;bucket name&gt;/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <ul> <li>Click Save.</li> </ul> <p>STEP 3. Set up a Lifecycle configuration.</p> <ul> <li> <p>Navigate to Lifecycle configuration.</p> </li> <li> <p>Define Lifecycle rule name and at least one Lifecycle rule action.</p> </li> <li> <p>Select Limit the scope of this rule using one or more filters.</p> </li> <li> <p>Add Object tags by defining a Key/Value pair.</p> </li> <li> <p>Click Save.</p> </li> </ul> <p>Note:</p> <ul> <li>The value of your object tag must be identical in content and form (case-sensitivity) to the archive retention name in your Coralogix Archive Retention Settings.</li> </ul> <p></p>"},{"location":"newoutput/archive-retention-policy/#create-a-lifecycle-policy","title":"Create a Lifecycle Policy","text":"<p>The following section demonstrates one method of creating a lifecycle policy using AWS CLI. The example below sets a policy to remove archive files with the retention \u201cshort\u201d after 15 days.</p> <p>STEP 1. Define a policy in a local lifecycle.json file.</p> <pre><code>{\n    \"Rules\": [\n        {\n            \"Filter\": {\n                \"Tag\": {\n                  \"Key\": \"CORALOGIX_ARCHIVE_RETENTION\",\n                  \"Value\": \"Short\"\n                }\n            },\n            \"Status\": \"Enabled\",\n            \"Expiration\": {\n                \"Days\": 15\n            },\n            \"ID\": \"ExampleRuleShort\"\n        },\n        {\n            \"Filter\": {\n                \"Tag\": {\n                  \"Key\": \"CORALOGIX_ARCHIVE_RETENTION\",\n                  \"Value\": \"Intermediate\"\n                }\n            },\n            \"Status\": \"Enabled\",\n            \"Expiration\": {\n                \"Days\": 150\n            },\n            \"ID\": \"ExampleRuleIntermediate\"\n        },\n        {\n            \"Filter\": {\n                \"Tag\": {\n                  \"Key\": \"CORALOGIX_ARCHIVE_RETENTION\",\n                  \"Value\": \"Long\"\n                }\n            },\n            \"Status\": \"Enabled\",\n            \"Expiration\": {\n                \"Days\": 365\n            },\n            \"ID\": \"ExampleRuleLong\"\n        }\n    ]\n}\n\n</code></pre> <p>STEP 2. Apply the policy.</p> <pre><code>aws s3api put-bucket-lifecycle-configuration --bucket &lt;bucket-name&gt; --lifecycle-configurationfile://lifecycle.json\n\n</code></pre> <p>Note:</p> <ul> <li> <p>This command will completely override the current policy of the bucket, if one exists.</p> </li> <li> <p>If you wish to append your existing policy, proceed with this command first and only then update lifecycle.json accordingly (STEP 2 above).</p> </li> </ul> <pre><code>get-bucket-lifecycle-configuration\n\n</code></pre> <p>STEP 3. Verify that the policy has been applied.</p> <pre><code>aws s3api get-bucket-lifecycle-configuration --bucket &lt;bucket-name&gt;\n\n</code></pre>"},{"location":"newoutput/archive-retention-policy/#set-archive-retention-settings","title":"Set Archive Retention Settings","text":"<p>Once you configure your cx-data bucket in S3, set your Archive Retention settings.</p> <p>STEP 1. In your Coralogix navigation bar, click Data Flow &gt; select Setup Archive.</p> <p>STEP 2. In the Archive Retention section, name Retention Periods 2, 3, and 4. You may opt for names \u2018Short\u2019, \u2018Intermediate\u2019, and \u2018Long\u2019 as in the example below, or you may choose otherwise. The value for each period - the length of time data will be retained in a specific Retention Period - is managed by the s3 storage lifecycle defined in your AWS account.</p> <p>STEP 3. Click ACTIVATE. You will receive a popup message that reads: \u201cAn archive retention policy has been added to the  Bucket successfully.\u201d <p>Notes:</p> <ul> <li> <p>Only once you have activated your archive retention settings will they appear in your TCO Quota Optimizer.</p> </li> <li> <p>When modifying existing archive retention settings, the ACTIVATE button will be replaced with a SAVE button.</p> </li> </ul> <p></p> <p>STEP 4. View your changes by navigating to Data Flow &gt; TCO Quota Optimizer.</p>"},{"location":"newoutput/archive-retention-policy/#create-a-new-policy","title":"Create a New Policy","text":"<p>Once you create your Archive Retention settings, create a new data retention policy.</p> <p>STEP 1. In your Coralogix navigation bar, click Data Flow &gt; select TCO Quota Optimizer.</p> <p>STEP 2. Click +ADD NEW POLICY.</p> <p></p> <p>STEP 3. Input POLICY NAME.</p> <p>STEP 4. Define APPLICATIONS, SUBSYSTEMS and SEVERITY level.</p> <p>STEP 5. Define PRIORITY.</p> <p>Notes:</p> <ul> <li> <p>If data is marked as \u2018blocked\u2019, it will not be archived.</p> </li> <li> <p>Only data marked as priority \u2018High\u2019, \u2018Medium\u2019, or \u2018Low\u2019 is archived. Only this data is eligible for additional archive retention settings.</p> </li> </ul> <p>STEP 6. Define ARCHIVE RETENTION settings.</p> <p>Notes:</p> <ul> <li> <p>When you add an archive retention policy, logs meeting the established criteria (application, subsystem, and severity level) will be retained in your archive for the period of time associated with the policy.</p> </li> <li> <p>If you do not specify your retention policy, logs meeting the established criteria will be retained in your archive for the default period of time.</p> </li> </ul> <p></p>"},{"location":"newoutput/archive-retention-policy/#previously-archived-files","title":"Previously Archived Files","text":"<p>Files created before the new lifecycle policy have no retention tag and are not changed retroactively when you establish a new retention policy. The data retention policy that applied to them beforehand - whether default or defined - will continue to apply even after your new settings are put in place.</p> <pre><code>{\n    \"Rules\": [\n        {\n            \"Status\": \"Enabled\",\n            \"Expiration\": {\n                \"Days\": 365\n            },\n            \"ID\": \"ExampleRule\"\n        }\n    ]\n}\n\n</code></pre>"},{"location":"newoutput/archive-retention-policy/#additional-resources","title":"Additional Resources","text":"<ul> <li>s3 Bucket Configuration</li> </ul>"},{"location":"newoutput/archive-retention-policy/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/archive-setup-grpc-api/","title":"Archive Setup gRPC API","text":"<p>This tutorial will demonstrate how to use our Archive Setup API to:</p> <ul> <li> <p>View bucket definitions and set a target bucket (Get target / Set target)</p> </li> <li> <p>Define archive retentions (Get, Update, Activate)</p> </li> </ul>"},{"location":"newoutput/archive-setup-grpc-api/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Coralogix\u00a0Alerts, Rules &amp; Tags API Key (Access this in your navigation pane by clicking\u00a0Data Flow\u00a0&gt; API Keys)</p> </li> <li> <p>Management API endpoint</p> </li> </ul>"},{"location":"newoutput/archive-setup-grpc-api/#supported-api-calls","title":"Supported API Calls","text":"<p>The API supports the following gRPCs:</p>"},{"location":"newoutput/archive-setup-grpc-api/#examples","title":"Examples","text":"<p>rpc GetTarget(GetTargetRequest) returns (GetTargetResponse)</p> <p>GetTargetRequest</p> <pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.archive.v1.TargetService/GetTarget &lt;&lt;EOF\n{\n}\nEOF\n\n</code></pre> <p>GetTargetResponse</p> <pre><code>{\n  \"target\": {\n    \"archivingFormatId\": \"cx_data_v1\",\n    \"isActive\": true,\n    \"region\": \"eu-west-1\",\n    \"bucket\": \"example-bucket\",\n    \"enableTags\": false\n  }\n}\n\n</code></pre> Field Type Description archivingFormatId string Format of the archive log files isActive boolean Indicates whether archive is active region string AWS region in which your s3 bucket is located bucket string S3 bucket name enableTags boolean Indicates whether retention tagging feature in enabled <p>rpc SetTarget(SetTargetRequest) returns (SetTargetResponse)</p> <p>SetTargetRequest</p> <pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.archive.v1.TargetService/SetTarget &lt;&lt;EOF\n{\n    \"bucket\": \"example-bucket\",\n    \"is_active\": true\n}\nEOF\n\n</code></pre> Field Type Description bucket string S3 bucket name is_active boolean Activate / deactivate archive <p>SetTargetResponse</p> <pre><code>{\n  \"target\": {\n    \"archivingFormatId\": \"cx_data_v1\",\n    \"isActive\": true,\n    \"region\": \"eu-west-1\",\n    \"bucket\": \"example-bucket\",\n    \"enableTags\": false\n  }\n}\n\n</code></pre> Field Type Description archivingFormatId string Format of the archive log files isActive boolean Indicates whether archive is active region string AWS region in which your s3 bucket is located bucket string S3 bucket name enableTags boolean Indicates whether retention tagging feature in enabled <p>rpc GetRetentionsEnabled(GetRetentionsEnabledRequest) returns (GetRetentionsEnabledResponse)</p> <p>GetRetentionsEnabledRequest</p> <pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.archive.v1.RetentionsService/GetRetentionsEnabled &lt;&lt;EOF\n{\n}\nEOF\n\n</code></pre> <p>GetRetentionsEnabledResponse</p> <pre><code>{\n  \"enableTags\": false\n}\n\n</code></pre> Field Type Description enableTags boolean Indicates whether retention tagging feature in enabled <p>rpc GetRetentions(GetRetentionsRequest) returns (GetRetentionsResponse)</p> <pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.archive.v1.RetentionsService/GetRetentions &lt;&lt;EOF\n{\n}\nEOF\n\n</code></pre> <p>GetRetentionsRequest</p> <pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.archive.v1.RetentionsService/GetRetentions &lt;&lt;EOF\n{\n}\nEOF\n\n</code></pre> <p>GetRetentionsResponse</p> <pre><code>{\n  \"retentions\": [\n    {\n      \"id\": \"c157476b-642d-400d-9acf-40f3ab02721a\",\n      \"order\": 1,\n      \"name\": \"Default\",\n      \"editable\": false\n    },\n    {\n      \"id\": \"485a2668-e815-4ec1-9b22-0228f5a27dc8\",\n      \"order\": 2,\n      \"name\": \"short\",\n      \"editable\": true\n    },\n    {\n      \"id\": \"3dacd60d-f83e-40f2-bf36-e7d2657a98d1\",\n      \"order\": 3,\n      \"name\": \"Intermediate\",\n      \"editable\": true\n    },\n    {\n      \"id\": \"ba29d452-966b-4ca7-8234-dc2d9bbe2d5e\",\n      \"order\": 4,\n      \"name\": \"Long\",\n      \"editable\": true\n    }\n  ]\n}\n\n</code></pre> Field Type Description retentions array Array of retention elements retentions[].id string Retention ID to be used in TCO policy API retentions[].order number Has no significance. Currently only 4 retentions are supported retentions[].name string Retention name, which can be changed (except 'default') retentions[].editable boolean Whether retention is editable (All but 'default' are editable) <p>rpc UpdateRetentions(UpdateRetentionsRequest) returns (UpdateRetentionsResponse)</p> <p>UpdateRetentionsRequest</p> <pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.archive.v1.RetentionsService/UpdateRetentions &lt;&lt;EOF\n{\n  \"retention_update_elements\": [\n    {\n      \"id\": \"485a2668-e815-4ec1-9b22-0228f5a27dc8\",\n      \"name\": \"1_week\"\n    },\n    {\n      \"id\": \"e9ee9138-4533-463d-a93b-bb18692208a5\",\n      \"name\": \"1_month\"\n    }\n  ]\n}\nEOF\n\n</code></pre> Field Type Description retention_update_elements Array Array of retention elements retention_update_elements[].id string ID of the retention to update retention_update_elements[].name string Name of the retention to update <p>UpdateRetentionsResponse</p> <pre><code>{\n  \"retentions\": [\n    {\n      \"id\": \"485a2668-e815-4ec1-9b22-0228f5a27dc8\",\n      \"order\": 2,\n      \"name\": \"1_week\",\n      \"editable\": true\n    },\n    {\n      \"id\": \"3dacd60d-f83e-40f2-bf36-e7d2657a98d1\",\n      \"order\": 3,\n      \"name\": \"1_month\",\n      \"editable\": true\n    }\n  ]\n}\n\n</code></pre> <p>rpc ActivateRetentions(ActivateRetentionsRequest) returns (ActivateRetentionsResponse)</p> <p>ActivateRetentionsRequest</p> <pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.archive.v1.RetentionsService/ActivateRetentions &lt;&lt;EOF\n{\n}\nEOF\n\n</code></pre> <p>ActivateRetentionsResponse</p> <pre><code>{\n  \"activateRetentions\": true\n}\n\n</code></pre> Field Type Description activateRetentions boolean True, if activated <p>If missing tagging permissions on a bucket, response will be the following error:</p> <pre><code>ERROR:\n  Code: FailedPrecondition\n  Message: Failed to update archive retention policy. Please provide Coralogix with tagging permissions in your AWS dashboard and try again.\n\n</code></pre>"},{"location":"newoutput/archive-setup-grpc-api/#additional-resources","title":"Additional Resources","text":"DocumentationArchive Retention PolicyTCO OptimizerTCO Optimizer API"},{"location":"newoutput/archive-setup-grpc-api/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/argo-cd-version-tags/","title":"Argo CD Version Tags","text":"<p>Coralogix provides seamless integration with\u00a0<code>Argo CD</code>\u00a0so you can push tags to Coralogix automatically from your Argo CD pipelines.</p>"},{"location":"newoutput/argo-cd-version-tags/#prerequisites","title":"Prerequisites","text":"<ul> <li>Have\u00a0<code>Argo CD</code>\u00a0installed, for more information on how to install:\u00a0https://argo-cd.readthedocs.io/en/stable/getting_started/</li> <li>Have\u00a0<code>Argo CD Notifications</code> and <code>Triggers and templates</code>\u00a0installed, for more information on how to install:\u00a0https://argocd-notifications.readthedocs.io/en/stable/</li> <li>API Token - should be obtained from \u00a0<code>Data Flow --&gt; API Keys --&gt; Alerts, Rules and Tags API Key</code></li> </ul>"},{"location":"newoutput/argo-cd-version-tags/#configuration","title":"Configuration","text":"<p>Add\u00a0Coralogix\u00a0API Token to\u00a0argocd-notifications-secret\u00a0Secret:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: argocd-notifications-secret\ntype: Opaque\nstringData:\n  coralogix-api-token: &lt;YOUR-API-TOKEN&gt;\n</code></pre> <p>Add\u00a0<code>webhook</code>,\u00a0<code>template</code>\u00a0and\u00a0<code>trigger</code>\u00a0to\u00a0argocd-notifications-cm\u00a0ConfigMap:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-notifications-cm\ndata:\n  service.webhook.coralogix: |\n    url: https://webapi.coralogixstg.wpengine.com/api/v1/external/tags\n    headers:\n    - name: Authorization\n      value: Bearer $coralogix-api-token\n    - name: Content-Type\n      value: application/json\n  template.coralogix: |\n    webhook:\n      coralogix:\n        method: POST\n        body: |\n          {\n            \"name\": \"{{.app.status.sync.revision}}\",\n            \"application\": [\"{{.app.spec.project}}\"],\n            \"subsystem\": [\"{{.app.metadata.name}}\"],\n            \"iconUrl\": \"https://raw.githubusercontent.com/coralogix/integrations-docs/master/integrations/argocd/images/argocd.png\"\n          }\n  trigger.coralogix-on-success: |\n    - when: app.status.operationState.phase in ['Succeeded']\n      send: [coralogix]\n</code></pre> <p>There are 2 ways to publish a patch with the added annotation, the first by creating a yml and preforming the patch by specifing the patch file. Or by using a command with the patch written into it.</p> <ul> <li>The yml file:</li> </ul> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: &lt;deployed-app-name&gt;\n  annotations:\n    notifications.argoproj.io/subscribe.coralogix-on-success.coralogix: \"\"\n</code></pre> <ul> <li>The command:</li> </ul> <pre><code>kubectl patch app &lt;deployed-app-name&gt; -n argocd -p '{\"metadata\": {\"annotations\": {\"notifications.argoproj.io/subscribe.coralogix-on-success.coralogix\": \"\"}}}' --type merge\n</code></pre> <p>To check if the patch was successfully been created, in Argocd enter the deployed app in which the notifications were added under the annotations field you will find the Coralogix notification. Each time the application syncs in Argocd it'll automaticly send a tag to Coralogix dashboard.</p>"},{"location":"newoutput/async-traces-otellink/","title":"Exploring Async Trace Calls with OTEL Link","text":"<p>When navigating through Coralogix Distributed Tracing, encountering converging traces forming a queue is common. Exploring async trace calls using the OTEL Link allows you to establish connections between these traces, revealing interconnections and aiding in the identification of related traces.</p>"},{"location":"newoutput/async-traces-otellink/#overview","title":"Overview","text":"<p>In scenarios where multiple traces merge into a single flow, creating asynchronous interactions, the ability to observe and explore these interconnections becomes vital for troubleshooting span-related issues.</p> <p>Asynchronous trace calls, (also known as async trace calls), serve as a powerful mechanism for tracking the flow of execution in applications utilizing asynchronous programming patterns. These calls help capture the journey of a request or transaction as it moves through various asynchronous tasks, such as background jobs, microservices, or distributed systems. They record the timing, dependencies, and contextual information associated with each asynchronous operation, allowing you to visualize the entire path of a request, even when it's distributed across multiple components and services.</p>"},{"location":"newoutput/async-traces-otellink/#benefits","title":"Benefits","text":"<p>Tracking async trace calls offers several key benefits:</p> <ul> <li> <p>End-to-End Visibility. Trace the entire lifecycle of a request or transaction through asynchronous tasks, understanding the full journey and identifying bottlenecks.</p> </li> <li> <p>Root Cause Analysis. Quickly pinpoint the exact step or component where issues occur, facilitating efficient root cause analysis.</p> </li> <li> <p>Performance Optimization. Understand the timing and dependencies of asynchronous operations to optimize system performance.</p> </li> <li> <p>Proactive Issue Detection. Proactively detect anomalies or deviations from expected behavior before they impact users.</p> </li> <li> <p>Microservices and Distributed Systems. Gain a holistic view of system behavior in modern, distributed systems with a microservices architecture.</p> </li> <li> <p>User Experience Improvement. Ensure a smooth and responsive user experience by observing how different components contribute to overall performance.</p> </li> </ul>"},{"location":"newoutput/async-traces-otellink/#prerequisites","title":"Prerequisites","text":"<ul> <li>OpenTelemetry configured for traces</li> </ul>"},{"location":"newoutput/async-traces-otellink/#view-async-trace-calls","title":"View Async Trace Calls","text":"<p>STEP 1. On the Coralogix toolbar, click Explore &gt; Tracing.</p> <p></p> <p>STEP 2. Click on a span for which you want to see the Dependencies View.</p> <p></p> <p>In the dependencies view, you can see which links are asynchronous because they will have curved dotted lines connecting between them, rather than uninterrupted straight lines.</p> <p>STEP 3. Inside the dependencies view, click on a specific trace to view the trace details in the right-hand panel.</p> <p></p> <p>STEP 4. Click on the operation name of a specific trace to open the trace up for exploration in a new tab.</p> <p></p>"},{"location":"newoutput/async-traces-otellink/#legend","title":"Legend","text":"<p>Within the dependencies view, the following shows the different types of images and connections.</p> <ul> <li> <p>Dotted line between two steps. Asynchronous request.</p> </li> <li> <p>Full line. Synchronous request.</p> </li> <li> <p>Double lines around a trace. Asynchronous trace.</p> </li> <li> <p>Arrows. The direction in which a trace is headed.</p> </li> <li> <p>Rhombus shape. HTTP method (GET/POST etc.)</p> </li> <li> <p>Hexagon. Pubsub (<code>span.kind</code> consumer/producer)</p> </li> <li> <p>Square. Database (the span has a <code>db.statement</code> tag)</p> </li> <li> <p>Circle. Generic operation (none of the above)</p> </li> </ul>"},{"location":"newoutput/async-traces-otellink/#group-by-filter","title":"Group By Filter","text":"<p>At the bottom left of the dependencies view screen is the Group By filter. This controls how operations are grouped in the Dependencies View.</p> <p>Choose to group by different actions or services, such as <code>operation.name</code>, <code>service.name</code>, tags, process tags, etc.</p> <p>Select the aggregation method to be used for spans containing multiple converging traces. Choose to see either the max, min, average, or sum of the time it took to perform operations.</p>"},{"location":"newoutput/async-traces-otellink/#show-internal-spans","title":"Show Internal Spans","text":"<p>Next to the Group By filter is the Show Internal Spans toggle. This enables you to show or hide internal spans, which can be useful when there are a large number of internal spans which makes the map difficult to navigate.</p>"},{"location":"newoutput/async-traces-otellink/#limitations","title":"Limitations","text":"<p>Due to the endless nature of asynchronous calls, there is a limit of three asynchronous calls per trace level.</p>"},{"location":"newoutput/async-traces-otellink/#additional-resources","title":"Additional Resources","text":"DocumentationDistributed Tracing"},{"location":"newoutput/async-traces-otellink/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/auto-json-parsing/","title":"Auto JSON parsing","text":"<p>Coralogix provides\u00a0automated parsing of any valid JSON message you send, allowing you to sort, filter, group, and visualize any JSON parameter without having to adjust your logs to any specific format.\u00a0</p> <p>1) Any JSON format log sent will appear in its original format on the log grid and info panel (to open the info panel simply mark a row and press the space button):\u00a0</p> <p></p> <p>2) Click on the field name drop-down menu will show. Select Show Graph Per key option to show a graph of its top occurrences:</p> <p></p> <p>You can also add any key as a column to your logs view and as a filter criterion.</p> <p>*Note that if a new key that was already indexed (new type of log you are shipping or maybe a new key since you applied a parsing rule) is not shown via \u2018manage columns\u2019 you should refresh your browser to have it.</p> <p>3) Use the pin Graph to add the visualization to \u00a0your dashboard as a widget:\u00a0</p> <p></p> <p>Join Coralogix now and enjoy a whole new world of machine learning-powered insights.</p>"},{"location":"newoutput/automate-vpc-mirroring/","title":"Automate VPC Mirroring","text":"<p>After installing Coralogix Security Traffic Analyzer (STA) and choosing a mirroring strategy suitable for your organization\u2019s needs, the next step would be to set the mirroring configuration in AWS. However, the configuration of VPC Traffic Mirroring in AWS is tedious and cumbersome \u2013 it requires you to create a mirror session per network interface of every mirrored instance, and just to add an insult to injury, if that instance terminates for some reason and a new one replaces it, you\u2019ll have to re-create the mirroring configuration from scratch.</p>"},{"location":"newoutput/automate-vpc-mirroring/#aws-configuration","title":"AWS Configuration","text":"<p>Each deployed STA instance holds a mirroring filter indicator tag.</p> <p>Important Notes:</p> <ol> <li> <p>Not all instances can mirror traffic.     AWS's supported instances for mirroring:</p> <ul> <li> <p>C4, D2, G3, G3s, H1, I3, M4, P2, P3, R4, X1, X1e, A1, C5, C5d, C5n, I3en, M5, M5a, M5ad, M5d,\u00a0p3dn.24xlarge, R5, R5a, R5ad, R5d, T3, T3a, and z1d</p> </li> <li> <p>References:</p> <ul> <li> <p>https://aws.amazon.com/about-aws/whats-new/2021/02/amazon-vpc-traffic-mirroring-supported-select-non-nitro-instance-types/</p> </li> <li> <p>https://aws.amazon.com/blogs/aws/new-vpc-traffic-mirroring/</p> </li> </ul> </li> <li> <p>In case you are not using one of those instance types, we strongly advise you to read our article about Virtual Tap </p> </li> </ul> </li> <li> <p>Mirror filtering is not acting as a firewall or any traffic blocking tool, It's not dropping or excluding any original traffic. Mirror filtering only filters what traffic is mirrored to the STA.</p> </li> </ol> <p>To configure the mirroring strategy, follow these steps:</p> <ol> <li> <p>Head to the deployed STA and search for tag: <code>sta.coralogixstg.wpengine.com:mirror-filter-indicator-tagname</code>, and copy the value.     (template: <code>sta.&lt;BUCKET_NAME&gt;.coralogixstg.wpengine.com:mirror-filter-id</code>)     note: to use a different tag, please read the \"Advanced Configuration\" section  </p> <p> </p> </li> <li> <p>To find the relevant <code>tmf</code> (Traffic mirror filter), head to the <code>VPC -&gt; Mirror filters</code>, and locate the following name:\u00a0<code>STA - Mirror Filter - &lt;MIRROR_TYPE&gt;</code> </p> <p> </p> </li> <li> <p>Select the desired mirror filter depending on the selected strategy: <code>All/Moderate/Essential</code>, and copy the <code>Filter ID</code> value  </p> </li> <li> <p>Add tag to the mirrored instance:     Key: <code>sta.&lt;bucket-name&gt;.coralogixstg.wpengine.com:mirror-filter-id</code> - see section (1) above     Value: <code>&lt;SELECTED_FILTER_ID&gt;</code> </p> <p></p> </li> </ol> <p>Now your instance is configured for mirroring to STA.</p>"},{"location":"newoutput/automate-vpc-mirroring/#advanced-configuration","title":"Advanced Configuration","text":"<ol> <li> <p>You can create your own filtering rules, to fit your organization\u2019s desired strategy. If you decide to do so, add this <code>tmf</code> as a tag.</p> </li> <li> <p>You also can edit/add rules to our provided <code>tmfs</code></p> </li> <li> <p>Each tag can be renamed to whatever you prefer. The tag names are specified inside the <code>sta.conf</code> file, can be configured inside the STA and in the AWS S3 bucket - we suggest to do so using the bucket.</p> </li> </ol>"},{"location":"newoutput/automate-vpc-mirroring/#additional-information","title":"Additional Information","text":"<p>When several STA instances are paralleling their work, they need to handle the communication between mirrored instances. Using balancing algorithms the STAs scan the mirroring instances, and split the load between them.</p> <p>Pro Tip: You can use AWS \u201cResource Groups &amp; Tag Editor\u201d to quickly assign tags to multiple instances based on arbitrary criteria.</p>"},{"location":"newoutput/automate-vpc-mirroring/#mirroring-cost-optimization","title":"Mirroring Cost Optimization","text":"<p>By default, STA mirrors all cloud traffic relevant to the selected strategy.\u00a0 By mirroring all traffic you benefit from observability over the cloud\u2019s traffic, which helps to monitor, investigate, and detect every suspicious activity. However, it is not free.\u00a0 Cloud providers, such as AWS, bill you for every mirror, and by mirroring all traffic, the cost can be extremely high.</p> <p>It\u2019s important on one hand to reduce costs as much as possible, but on the other hand to not damage observability and STA\u2019s abilities. To do so, and stay with sufficient and productive observability, STA offers additional optimization to reduce cost while keeping the monitoring value. In addition to strategy selection, STA has an inner handling configuration for mirroring mode:</p> <ol> <li> <p><code>dynamic mirroring</code></p> </li> <li> <p><code>manual mirroring</code></p> </li> </ol>"},{"location":"newoutput/automate-vpc-mirroring/#dynamic-mirroring","title":"Dynamic Mirroring","text":"<p>Behind the scenes, the STA calculates the availability of each session, counts the traffic, and categorizes it by best practice principles and tight security measurements. On the fly, the STA decides if some session is currently not providing relevant visibility and if so, stops the mirroring for a period of time and this process is repeated.</p> <p>This is achieved due to understanding the concept of any malicious attack - it\u2019s never a one-action attack and is always spread over some timeframe.</p> <p>The mode can be defined via <code>sta.conf</code> file that can be changed locally in the STA or using the AWS S3 bucket's <code>sta.conf</code> file.</p>"},{"location":"newoutput/automate-vpc-mirroring/#configuration-schema","title":"Configuration schema","text":"<pre><code>{\n    \"automations\": {\n        \"vpc-mirroring-auto-handler\": {\n            \"mirror_handling_mode\": \"DYNAMIC\" | \"MANUAL\"\n        }\n    }\n}\n\n</code></pre>"},{"location":"newoutput/aws-cloudformation/","title":"AWS CloudFormation Logs","text":"<p>Send your logs to Coralogix using AWS CloudFormation, granting you observability into your CloudFormation events.</p> <p>The following tutorial demonstrates how to configure an AWS CloudFormation template using a Lambda function to send your telemetry data to Coralogix.</p>"},{"location":"newoutput/aws-cloudformation/#prerequisites","title":"Prerequisites","text":"<p>1. Sign up for a Coralogix account. Set up your account on the Coralogix domain corresponding to the region within which you would like your data stored.</p> <p>2. Access your Coralogix Send Your Data - API Key.</p> <p>3. Create an active AWS account with permissions to manage Lambda functions. Log in to your account as administrator.</p> <p>4. Save the script for the AWS CloudFormation template on your desktop or in an S3 bucket.</p>"},{"location":"newoutput/aws-cloudformation/#configuration","title":"Configuration","text":""},{"location":"newoutput/aws-cloudformation/#create-a-stack","title":"Create a Stack","text":"<p>Navigate to CloudFormation Page and click Create Stack (with new resources).</p> <p>Note! AWS CloudFormation may create IAM resources.</p> <p>2. Under the Stack option tab, select Template is ready. Upload a .yaml file from your S3 bucket or desktop, as in the example below.</p> <pre><code>Resources:\nLambdaFunction:\nType: 'AWS::Lambda::Function'\nProperties:\nFunctionName: AwsLambdaExample\nHandler: index.handler\nRuntime: nodejs14.x\nRole: !GetAtt LambdaFunctionRole.Arn\nMemorySize: 1024\nCode:\nZipFile: |\nexports.handler = async (event) =&gt; {\nreturn \"Hello World!\";\n}\nLambdaFunctionRole:\nType: AWS::IAM::Role\nProperties:\nAssumeRolePolicyDocument:\nVersion: '2012-10-17'\nStatement:\n- Effect: Allow\nPrincipal:\nService:\n- [lambda.amazonaws.com](&lt;http://lambda.amazonaws.com/&gt;)\nAction:\n- sts:AssumeRole\nPath: \"/\"\nPolicies:\n- PolicyName: AppendToLogsPolicy\nPolicyDocument:\nVersion: '2012-10-17'\nStatement:\n- Effect: Allow\nAction:\n- logs:CreateLogGroup\n- logs:CreateLogStream\n- logs:PutLogEvents\nResource: \"*\"\n\n</code></pre> <p>3. Click Next and define your stack name.</p> <p>4. No other changes are required. Click Next and submit the stack.</p> <p></p>"},{"location":"newoutput/aws-cloudformation/#api-destination","title":"API Destination","text":"<p>1. Navigate to Amazon EventBridge and click Create API destination in the API Destination section.</p> <p>2. Define the name.</p> <p>3. Define the API destination associated with your Coralogix domain.</p> Coralogix DomainEndpointcoralogixstg.wpengine.comhttps://aws-events.coralogixstg.wpengine.com/aws/eventcoralogix.ushttps://aws-events.coralogix.us/aws/eventeu2.coralogixstg.wpengine.comhttps://aws-events.eu2.coralogixstg.wpengine.com/aws/eventcoralogix.inhttps://aws-events.coralogix.in/aws/eventcoralogixsg.comhttps://aws-events.coralogix.in/aws/event <ul> <li> <p>HTTP Method: POST</p> </li> <li> <p>Invocation rate limit per second \u2013 300 sec</p> </li> <li> <p>Select API Key as your\u00a0Authorization type.</p> </li> <li> <p>Name your API key x-amz-event-bridge-access-key.</p> </li> <li> <p>The value of your API key will be your Coralogix Send Your Data - API Key.</p> </li> </ul> <p></p>"},{"location":"newoutput/aws-cloudformation/#create-a-rule-in-amazon-eventbridge","title":"Create a Rule in Amazon EventBridge","text":"<p>1. Name your Rule CoralogixRule. Leave all other settings as is. Click Next.</p> <p>2. Select All Events.</p> <p>Note! This may affect customer AWS billing.</p> <p>3. Click Next.</p> <p>4. In the Target, select EventBridge API destination and input the destination previously configured.</p> <p>5. Click Next to the create the rule.</p>"},{"location":"newoutput/aws-cloudformation/#validation","title":"Validation","text":"<p>View your logs, together with the Lambda execution, in your Coralogix dashboard.</p> <p></p> <p></p>"},{"location":"newoutput/aws-cloudformation/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-cloudtrail-data-collection-options/","title":"AWS CloudTrail: Data Collection Options","text":"<p>AWS CloudTrail is an AWS service that helps you enable operational and risk auditing, governance, and compliance of your AWS account. Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail.</p> <p>Use any of our customized log collection options to allow Coralogix to ingest the logs stored in your Amazon S3 bucket and process them for further analysis and monitoring.</p>"},{"location":"newoutput/aws-cloudtrail-data-collection-options/#overview","title":"Overview","text":"<p>Coralogix provides multiple methods in which you can collect logs from Amazon CloudTrail. Send us your CloudTrail data from your Amazon S3 bucket using an AWS Lambda function, with one of two event-driven design patterns:</p> <ul> <li>Invoke the Lambda directly through an S3 event</li> </ul> <p></p> <ul> <li>Send the S3 event to a Simple Notification Service (SNS) queue, which in turn triggers the Lambda</li> </ul> <p></p>"},{"location":"newoutput/aws-cloudtrail-data-collection-options/#customized-log-collection-options","title":"Customized Log Collection Options","text":"<p>Use any of our customized log collection options to allow Coralogix to ingest the logs stored in your Amazon S3 bucket and process them for further analysis and monitoring.</p> <ul> <li> <p>Automated Integration Packages (Recommended). For both S3 event and SNS design patterns, deploy the Coralogix Lambda function using our two-step, automated integration packages.</p> </li> <li> <p>Serverless Application Repository (SAR). Deploy our Coralogix Lambda function via our AWS serverless application repository. Use this for S3 event or SNS design patterns.</p> </li> <li> <p>Terraform. For both S3 event and SNS design patterns, install and manage the CloudTrail integration with AWS services as modules in your infrastructure code using our AWS CloudTrail Terraform module.</p> </li> </ul>"},{"location":"newoutput/aws-cloudtrail-data-collection-options/#best-practices","title":"Best Practices","text":"<p>For any customized data collection option, customers should add environment variable\u00a0<code>CORALOGIX_BUFFER_SIZE</code>\u00a0with value\u00a0<code>268435456</code>.</p>"},{"location":"newoutput/aws-cloudtrail-data-collection-options/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-cloudwatch-integration-options/","title":"AWS CloudWatch: Data Collection Options","text":"<p>Amazon CloudWatch collects and visualizes real-time logs, metrics, and event data in automated dashboards to streamline your infrastructure and application maintenance. Send these to Coralogix to enhance your data management, analysis, and monitoring capabilities.</p> <p>Coralogix provides multiple methods to collect logs and metrics from Amazon CloudWatch.</p>"},{"location":"newoutput/aws-cloudwatch-integration-options/#custom-coralogix-experience","title":"Custom Coralogix Experience","text":"<p>Use any of our customized cost-optimized options to allow Coralogix to ingest the logs stored in your S3 bucket and process them for further analysis and monitoring. Alternatively, take advantage of our customized performance-optimized AWS Kinesis Data Firehose integration options.</p> <ul> <li> <p>Automated Integration Packages (Recommended). Deploy the Coralogix Lambda function using our two-step automated integration packages.</p> </li> <li> <p>Serverless Application Repository (SAR). Deploy the Coralogix Lambda function via our AWS serverless application repository.</p> </li> <li> <p>AWS Kinesis Data Firehose. Select the Coralogix destination in your AWS Kinesis Data Firehose delivery stream to forward your logs and metrics to Coralogix.</p> </li> <li> <p>Terraform. Install and manage the CloudWatch integration with AWS services as modules in your infrastructure code using our AWS CloudWatch or AWS Kinesis with Lambda Function Terraform modules.</p> </li> </ul>"},{"location":"newoutput/aws-cloudwatch-integration-options/#open-source-tools","title":"Open Source Tools","text":"<p>To reduce costs, you have the option of sending us your CloudWatch data using our OpenTelemetry integration.</p>"},{"location":"newoutput/aws-cloudwatch-integration-options/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-coudtrail-log-collection-via-sns-trigger/","title":"AWS CoudTrail Log Collection via SNS Trigger","text":"<p>Coralogix provides a predefined Lambda function to easily forward your CloudTrail logs through SNS to the Coralogix platform. For easy setup, use our app in the AWS serverless application repository.</p>"},{"location":"newoutput/aws-coudtrail-log-collection-via-sns-trigger/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Active CloudTrail account</p> </li> <li> <p>Ready-made SNS topic with permissions <code>SNS:Publish</code> to the bucket</p> </li> <li> <p>Ready-made CloudTrail S3 bucket with configured event notifications to the above SNS topic</p> </li> <li> <p>AWS permissions to create Lambdas and IAM roles</p> </li> </ul>"},{"location":"newoutput/aws-coudtrail-log-collection-via-sns-trigger/#installation","title":"Installation","text":"<p>STEP 1. Navigate to the application page and search for Coralogix-CloudTrail-via-SNS.</p> <p>STEP 2. Fill in the required parameters.</p> <p>STEP 3. Click Deploy.</p>"},{"location":"newoutput/aws-coudtrail-log-collection-via-sns-trigger/#parameters","title":"Parameters","text":"ParameterDescriptionApplication NameStack name of the application created via AWS CloudFormationApplicationNameApplication name as it will be seen in your Coralogix UISubsystemNameSubsystem name as it will appear in your Coralogix UINotificationEmailA notification email will be sent to this address via SNS if the Lambda fails.Requires you have a working SNS with a validated domainS3BucketNameName of the S3 bucket with CloudTrail logs to watch.Must be in the same region as the stack that you createSNSTopicARNARN of the SNS topic.Must be in the same region as the S3 bucketCoralogixRegionRegion associated with your Coralogix domainFunctionArchitectureLambda function architecture. Possible options: x86_64, arm64FunctionMemorySizeMaximum allocated memory this Lambda may consume. Do not change default, which is set to 1024.FunctionTimeoutMaximum time (seconds) that the function may be allowed to run. Do not change default, which is set to 300.PrivateKeyCoralogix Send-Your-Data API Key <p>Notes:</p> <ul> <li>Do not change the\u00a0<code>**FunctionMemorySize**</code> and\u00a0<code>**FunctionTimeout**</code>\u00a0parameters.</li> </ul>"},{"location":"newoutput/aws-coudtrail-log-collection-via-sns-trigger/#additional-resources","title":"Additional Resources","text":"DocumentationAWS CloudTrail"},{"location":"newoutput/aws-coudtrail-log-collection-via-sns-trigger/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-ecs-fargate/","title":"AWS ECS Fargate","text":"<p>Seamlessly stream logs, metrics, and traces generated by AWS ECS Fargate containers to Coralogix for optimal monitoring, analysis, and visualization.</p>"},{"location":"newoutput/aws-ecs-fargate/#logs","title":"Logs","text":"<p>Send Coralogix your ECS Fargate logs using AWS FireLens via Fluent Bit, a lightweight data shipper for your AWS ECS Fargate workloads.</p>"},{"location":"newoutput/aws-ecs-fargate/#overview","title":"Overview","text":"<p>This integration requires that you deploy the fluentbit log_router into an existing AWS ECS Fargate task definition. The example below uses an AWS customized Fluent Bit image called aws-for-fluent-bit [init version]. A CloudFormation template may also be used.</p> <p>The aws-for-fluent-bit image, maintained by AWS here, enables loading the Fluent Bit configuration via S3 or local files, making it more convenient and dynamic than using a static configuration in your container image.</p> <p>The base_filters.conf file includes a set of filters to ensure proper ingestion by the Coralogix backend. This should be included as the first configuration file for your instance deployment. Ensure you upload this to an S3 bucket in your AWS account.</p> <p>You can load multiple configuration files from S3 to build your final configuration by setting custom environment variables within the task definition.</p>"},{"location":"newoutput/aws-ecs-fargate/#container-declaration-within-a-task-definition","title":"Container declaration within a task definition","text":"<p>The following is an example container declaration.</p> <pre><code>    \"containerDefinitions\": [\n        {\n            &lt;Existing Container Definitions&gt;\n        },\n        {\n            \"name\": \"log_router\",\n            \"image\": \"public.ecr.aws/aws-observability/aws-for-fluent-bit:init-2.31.12\",\n            \"cpu\": 0,\n            \"portMappings\": [],\n            \"essential\": false,\n            \"environment\": [\n                {\n                    \"name\": \"aws_fluent_bit_init_s3_1\",\n                    \"value\": \"arn:aws:s3:::&lt;Your S3 Bucket&gt;/base_filters.conf\"\n                },\n                {\n                    \"name\": \"aws_fluent_bit_init_s3_2\",\n                    \"value\": \"arn:aws:s3:::&lt;Your S3 Bucket&gt;/more_filters.conf\"\n                },\n                {\n                    \"name\": \"aws_fluent_bit_init_s3_3\",\n                    \"value\": \"arn:aws:s3:::&lt;Your S3 Bucket&gt;/custom_parser.conf\"\n                }\n            ],\n            \"mountPoints\": [],\n            \"volumesFrom\": [],\n            \"user\": \"0\",\n            \"firelensConfiguration\": {\n                \"type\": \"fluentbit\",\n                \"options\": {}\n            }\n        }\n    ]\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>Our\u00a0<code>aws_fluent_bit_init_s3_1</code>\u00a0environment variable points to the base_filters.conf file hosted in your S3 bucket.</p> </li> <li> <p>Add additional configuration files by increasing the _# suffix to reference additional files.</p> </li> <li> <p>Create your own custom image and include files locally if preferred. To do this, use environment variable\u00a0<code>aws_fluent_bit_init_file_1</code>\u00a0instead. You can use S3 and local files in the same deployment.</p> </li> <li> <p>Full details can be found in the AWS documentation\u00a0here.</p> </li> </ul>"},{"location":"newoutput/aws-ecs-fargate/#allow-container-access-to-your-s3-bucket","title":"Allow container access to your S3 bucket","text":"<p>To allow container access to the S3 object, provide the s3:GetObject and s3:GetBucketLocation action permissions to the task, as in the following example.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"&lt;Your specific bucket ARN&gt;\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\"\n            ],\n            \"Resource\": \"&lt;Your specific bucket ARN&gt;/*\"\n        }\n    ]\n}\n</code></pre> <p>Notes:</p> <ul> <li>The permission must be added to the Task Role.</li> </ul>"},{"location":"newoutput/aws-ecs-fargate/#container-adjustment","title":"Container adjustment","text":"<p>After adding the above container to your existing Task Definition, adjust the logConfiguration for the containers you wish to forward to Coralogix.</p> <p>Add the following \"logConfiguration\" section to each of your application containers at the root.</p> <pre><code>\"logConfiguration\": {\n                \"logDriver\": \"awsfirelens\",\n                \"options\": {\n                    \"Format\": \"json_lines\",\n                    \"Header\": \"authorization Bearer &lt;&lt;API_key&gt;&gt;\",\n                    \"Host\": \"ingress.&lt;Coralogix Domain&gt;\",\n                    \"Name\": \"http\",\n                    \"Port\": \"443\",\n                    \"Retry_Limit\": \"10\",\n                    \"TLS\": \"On\",\n                    \"URI\": \"/logs/v1/singles\",\n                    \"compress\": \"gzip\"\n                }\n            }\n\n</code></pre> <p>Notes:</p> <ul> <li>Input the \"logConfiguration\" section at the same level as \"name\", \"image\", etc.</li> </ul>"},{"location":"newoutput/aws-ecs-fargate/#metrics-traces","title":"Metrics &amp; Traces","text":"<p>Send your ECS Fargate metrics and traces using OpenTelemetry (Otel) Collector, offering a vendor-agnostic implementation of how to receive, process and export telemetry data.</p>"},{"location":"newoutput/aws-ecs-fargate/#overview_1","title":"Overview","text":"<p>This integration requires that you add the Otel Collector as a sidecar agent to your ECS Task Definitions. The example below uses an AWS customized OpenTelemetry image called AWS Distro for OpenTelemetry (ADOT). A CloudFormation template may also be used.</p> <p>The ADOT image, maintained by AWS\u00a0here, enables the loading of the OpenTelemetry configuration via Systems Manager Parameter Store, making configuration adjustment more convenient and dynamic when compared with a static configuration in your container image.</p> <p>The config.yaml file includes a standard configuration, ensuring proper ingestion by the Coralogix backend. Create this Parameter Store in the same region as your ECS cluster, as in the CloudFormation template provided. Once the Parameter Store has been created, add the container to your existing Task Definition.</p>"},{"location":"newoutput/aws-ecs-fargate/#container-declaration-within-a-task-definition_1","title":"Container declaration within a task definition","text":"<pre><code>    \"containerDefinitions\": [\n        {\n            &lt;Existing Container Definitions&gt;\n        },\n        {\n            \"name\": \"otel-collector\",\n            \"image\": \"public.ecr.aws/aws-observability/aws-otel-collector\",\n            \"cpu\": 0,\n            \"portMappings\": [\n                {\n                    \"name\": \"otel-collector-4317-tcp\",\n                    \"containerPort\": 4317,\n                    \"hostPort\": 4317,\n                    \"protocol\": \"tcp\",\n                    \"appProtocol\": \"grpc\"\n                },\n                {\n                    \"name\": \"otel-collector-4318-tcp\",\n                    \"containerPort\": 4318,\n                    \"hostPort\": 4318,\n                    \"protocol\": \"tcp\",\n                    \"appProtocol\": \"grpc\"\n                }\n            ],\n            \"essential\": false,\n            \"environment\": [\n                {\n                    \"name\": \"PRIVATE_KEY\",\n                    \"value\": \"&lt;Coralogix PrivateKey&gt;\"\n                },\n                {\n                    \"name\": \"CORALOGIX_DOMAIN\",\n                    \"value\": \"&lt;Coralogix Domain&gt;\"\n                }\n            ],\n            \"mountPoints\": [],\n            \"volumesFrom\": [],\n            \"secrets\": [\n                {\n                    \"name\": \"AOT_CONFIG_CONTENT\",\n                    \"valueFrom\": \"config.yaml\"\n                }\n            ],\n            \"logConfiguration\": {\n                \"logDriver\": \"awsfirelens\",\n                \"options\": {\n                    \"Format\": \"json_lines\",\n                    \"Header\": \"authorization Bearer &lt;API_key&gt;\",\n                    \"Host\": \"ingress.&lt;Coralogix Domain&gt;\",\n                    \"Name\": \"http\",\n                    \"Port\": \"443\",\n                    \"Retry_Limit\": \"10\",\n                    \"TLS\": \"On\",\n                    \"URI\": \"/logs/v1/singles\",\n                    \"compress\": \"gzip\"\n                }\n            }\n        }\n    ]\n\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>Input your Send-Your-Data API key as <code>API_key</code>.</p> </li> <li> <p>Input the domain associated with your Coralogix account.</p> </li> <li> <p>The \u201clogConfiguration\u201d section included in the example will forward Otel logs to the Coralogix platform, as documented in our Fluent Bit log processing configuration instructions\u00a0here.</p> </li> <li> <p>If you don't want to have logs submitted to the Coralogix platform, replace the logConfiguration with the logDriver configuration of preference.</p> </li> <li> <p>To submit to Cloudwatch, use the following configuration:</p> </li> </ul> <pre><code>\"logConfiguration\": {\n                \"logDriver\": \"awslogs\",\n                \"options\": {\n                    \"awslogs-create-group\": \"true\",\n                    \"awslogs-group\": \"&lt;Log Group Name&gt;\",\n                    \"awslogs-region\": \"&lt;Your Region&gt;\",\n                    \"awslogs-stream-prefix\": \"&lt;Stream Prefix&gt;\"\n                }\n            }\n\n\n</code></pre>"},{"location":"newoutput/aws-ecs-fargate/#access-the-systems-manager-parameter-store","title":"Access the Systems Manager Parameter Store","text":"<p>To allow container access to the Systems Manager Parameter Store, provide the ssm:GetParameters action permissions to the Task Execution Role, as in the following example.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ssm:GetParameters\"\n      ],\n      \"Resource\": [\n        \"arn:aws:ssm:region:aws_account_id:parameter/parameter_name\"\n      ]\n    }\n  ]\n}\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>The permission must be added to the Task Execution Role.</p> </li> <li> <p>After adding the above container to your existing Task Definition, your applications will submit their traces and metrics exports to\u00a0http://localhost:4318/v1/traces\u00a0and /v1/metrics. They will also collect container metrics from all containers in the Task Definition.</p> </li> </ul>"},{"location":"newoutput/aws-ecs-fargate/#additional-resources","title":"Additional Resources","text":"GitHubECS Fargate LogsECS Fargate Traces"},{"location":"newoutput/aws-ecs-fargate/#support","title":"Support","text":"<p>Need help?</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-eks-fargate/","title":"AWS EKS Fargate","text":"<p>Integrate Coralogix seamlessly with Amazon EKS on AWS Fargate to effortlessly collect, analyze, and visualize logs, metrics, and traces from your containerized applications, empowering you with comprehensive full-stack monitoring and insights.</p>"},{"location":"newoutput/aws-eks-fargate/#overview","title":"Overview","text":"<p>The Coralogix AWS EKS Fargate integration has two independent parts: metrics and traces via OpenTelemetry, and logs via the AWS log_router framework. Each integration can be deployed separately from the other.</p>"},{"location":"newoutput/aws-eks-fargate/#metrics-traces","title":"Metrics &amp; Traces","text":"<p>This integration leverages the OpenTelemetry Collector (Contrib) to collect your Fargate workload pod and container metrics. It does so by querying the Kubelet stats API running on each node. As Fargate hosts are managed by AWS, node metrics are not available.</p>"},{"location":"newoutput/aws-eks-fargate/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p><code>cx-eks-fargate-otel</code> namespace declared in your EKS cluster [Note: This namespace need not be hosted by a Fargate profile. If this is desired, create a profile.]</p> </li> <li> <p>A secret containing your Coralogix Send-Your-Data API key in the <code>cx-eks-fargate-otel</code> namespace</p> </li> </ul>"},{"location":"newoutput/aws-eks-fargate/#create-a-secret","title":"Create a Secret","text":"<p>STEP 1. Export your API key to a local variable.</p> <pre><code>export PRIVATE_KEY=&lt;Send-Your-Data API key&gt;\n\n</code></pre> <p>STEP 2. Set your namespace variable.</p> <pre><code>export NAMESPACE=cx-eks-fargate-otel\n\n</code></pre> <p>STEP 3. Create the secret using kubectl.</p> <pre><code>kubectl create secret generic coralogix-keys -n $NAMESPACE --from-literal=PRIVATE_KEY=$PRIVATE_KEY\n\n</code></pre> <p>STEP 4. Validate the newly-created secret.</p> <pre><code>kubectl get secret coralogix-keys -o yaml -n $NAMESPACE\n\n</code></pre>"},{"location":"newoutput/aws-eks-fargate/#create-a-serviceaccount","title":"Create a ServiceAccount","text":"<p>In order for the OpenTelemetry Collector to get full access to the Kubernetes API, it will need to bind to a ServiceAccount. Create the service account by running the following bash script. Set the CLUSTER_NAME and REGION accordingly, with the remainder unchanged.</p> <pre><code>##!/bin/bash\nCLUSTER_NAME=&lt;EKS Cluster Name&gt;\nREGION=&lt;EKS Cluster Region&gt;\nSERVICE_ACCOUNT_NAMESPACE=cx-eks-fargate-otel\nSERVICE_ACCOUNT_NAME=cx-otel-collector\nSERVICE_ACCOUNT_IAM_ROLE=EKS-Fargate-cx-OTEL-ServiceAccount-Role\nSERVICE_ACCOUNT_IAM_POLICY=arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\n\neksctl utils associate-iam-oidc-provider \\\\\n--cluster=$CLUSTER_NAME \\\\\n--approve\n\neksctl create iamserviceaccount \\\\\n--cluster=$CLUSTER_NAME \\\\\n--region=$REGION \\\\\n--name=$SERVICE_ACCOUNT_NAME \\\\\n--namespace=$SERVICE_ACCOUNT_NAMESPACE \\\\\n--role-name=$SERVICE_ACCOUNT_IAM_ROLE \\\\\n--attach-policy-arn=$SERVICE_ACCOUNT_IAM_POLICY \\\\\n--approve\n\n</code></pre>"},{"location":"newoutput/aws-eks-fargate/#configure-and-deploy-otel-collector-service","title":"Configure and Deploy OTEL Collector Service","text":"<p>The attached yaml manifest will deploy the following:</p> <ul> <li> <p>an OpenTelemetry Collector,</p> </li> <li> <p>a clusterIP service for submission of application traces and metrics, and</p> </li> <li> <p>cluster permissions required to query the Kubernetes API.</p> </li> </ul> <p>STEP 1. Set the container environment variables detailed at the top of the yaml file.</p> <p>STEP 2. Once you\u2019ve adjusted the manifests appropriately, deploy using the kubectl apply command.</p> <pre><code>kubectl apply -f cx-eks-fargate-otel.yaml\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>This manifest is all that is required to collect metrics from your EKS Fargate cluster and process application metrics and traces from gRPC sources.</p> </li> <li> <p>The OTLP gRPC endpoint is: <code>http://cx-otel-collector-service.cx-eks-fargate-otel.svc.cluster.local:4317</code>.</p> </li> </ul>"},{"location":"newoutput/aws-eks-fargate/#configure-and-deploy-the-self-monitoring-pod","title":"Configure and Deploy the Self Monitoring Pod","text":"<p>Since Fargate workloads are unable to directly communicate with their host due to networking restrictions, we cannot monitor the OTEL collector pod\u2019s performance directly. Instead, we have constructed a secondary manifest that\u2019ll deploy a second OTEL collector to collect just these missing pod metrics.</p> <p>This manifest also has some required environmental variables that need to be set, which detailed at the top.</p> <p>Again, after setting the environment variables, deploy using kubectl apply:</p> <pre><code>kubectl apply -f cx-eks-fargate-otel-self-monitoring.yaml\n\n</code></pre>"},{"location":"newoutput/aws-eks-fargate/#logs","title":"Logs","text":"<p>For EKS Fargate logs, leverage the AWS log_router built into the Fargate Kubelet to route your application logs to the Coralogix platform through a Kinesis Firehose.</p>"},{"location":"newoutput/aws-eks-fargate/#prerequisites_1","title":"Prerequisites","text":"<ul> <li> <p>AWS Kinesis Firehose configured [Note: By following our documentation, you will have configured Kinesis Firehose to use with your log_router. Though there are other exporters available for the log_router (fluent-bit), it is restricted to ElasticSearch, Firehose and Cloudwatch. We recommend Kinesis Firehose as it limits added cost, while allowing direct submission to the Coralogix Platform.]</p> </li> <li> <p>Proper permissions added to each of your Fargate profile pod execution roles</p> </li> <li> <p>Deployment of the log_router configuration</p> </li> </ul>"},{"location":"newoutput/aws-eks-fargate/#fargate-profile-permissions","title":"Fargate Profile Permissions","text":"<p>For the sidecar fluent-bit pod to submit messages to Kinesis Firehose, it requires put and put batch permissions. This is accomplished by adding the following permissions to every Fargate profile that you wish to monitor for application logs.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"firehose:PutRecord\",\n                \"firehose:PutRecordBatch\"\n            ],\n            \"Resource\": [\n                \"&lt;firehose_ARN&gt;\"\n            ]\n        }\n    ]\n}\n\n</code></pre>"},{"location":"newoutput/aws-eks-fargate/#log_router-deployment","title":"log_router Deployment","text":"<p>To deploy the log_router, you\u2019ll need a specific namespace with appropriate labels configured in your cluster. You won\u2019t be deploying any workloads into this namespace, so you don\u2019t need a Fargate Profile configured for it.</p> <p>Though the configuration will look like a standard fluent-bit configuration, only specific sections can be modified and only certain modules can be used. Below is our recommended Kubernetes manifest which will deploy the namespace and the fluent-bit ConfigMap.</p> <pre><code>---\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: aws-observability\n  labels:\n    aws-observability: enabled\n\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: aws-logging\n  namespace: aws-observability\ndata:\n  filters.conf: |\n    [FILTER]\n        Name parser\n        Match *\n        Key_name log\n        Parser crio\n\n    [FILTER]\n        Name             kubernetes\n        Match            kube.*\n        Merge_Log           On\n        Buffer_Size         0\n        Kube_Meta_Cache_TTL 300s\n        Keep_Log Off\n        Merge_Log_Key log_obj\n        K8S-Logging.Parser On\n        K8S-Logging.Exclude On\n        Annotations Off\n\n  output.conf: |\n    [OUTPUT]\n      Name  kinesis_firehose\n      Match *\n      region &lt;AWS Region&gt; (no quotes)\n      delivery_stream &lt;Data Firehose Delivery Stream Name&gt; (no quotes)\n\n  parsers.conf: |\n    [PARSER]\n        Name crio\n        Format Regex\n        Regex ^(?&lt;time&gt;[^ ]+) (?&lt;stream&gt;stdout|stderr) (?&lt;logtag&gt;P|F) (?&lt;log&gt;.*)$\n        Time_Key    time\n        Time_Format %Y-%m-%dT%H:%M:%S.%L%z\n        Time_Keep true\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>The log_router will only be attached to workloads started after the manifest has been applied. You will need to restart your pods in order for the log_router to start forwarding the logs to your Firehose. Find out more about the AWS log router here.</p> </li> <li> <p>You can add additional filters, but they are limited to the following types: <code>grep, parser, record_modifier, rewrite_tag, throttle, nest, modify, kubernetes</code>.</p> </li> <li> <p>If you wish to add Cloudwatch output, you are required to add additional permissions to your Fargate profile. Find out more here.</p> </li> </ul>"},{"location":"newoutput/aws-eks-fargate/#additional-resources","title":"Additional Resources","text":"DocumentationAWS Kinesis Data Firehose - Logs"},{"location":"newoutput/aws-eks-fargate/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-elastic-beanstalk/","title":"AWS Elastic Beanstalk","text":""},{"location":"newoutput/aws-elastic-beanstalk/#overview","title":"Overview","text":"<p>In the realm of cloud computing, AWS offers Elastic Beanstalk, a robust Platform-as-a-Service (PaaS) solution. Seamlessly aligning with this, Coralogix stands out by facilitating the effortless transportation of application traces and host metrics. This is achieved through adept utilization of the OpenTelemetry Collector and language-specific implementations of OpenTelemetry agents.</p> <p>This tutorial demonstrates how to instrument a Java application running on the Tomcat platform within an Elastic Beanstalk environment.</p>"},{"location":"newoutput/aws-elastic-beanstalk/#prerequisites","title":"Prerequisites","text":""},{"location":"newoutput/aws-elastic-beanstalk/#elastic-beanstalk-environment","title":"Elastic Beanstalk Environment","text":"<p>Create a Beanstalk environment for a supported platform of your choice.</p>"},{"location":"newoutput/aws-elastic-beanstalk/#sample-app","title":"Sample App","text":"<p>Download and unzip a sample Java application tomcat.zip.</p>"},{"location":"newoutput/aws-elastic-beanstalk/#instrument-java-app","title":"Instrument Java App","text":"<p>Elastic Beanstalk offers multiple ways to configure your environment's behavior and the resources that it contains. We recommend creating the configuration files in the <code>.ebextensions</code> folder when setting up the OpenTelemetry Collector and Java Agent configurations.</p> <p>STEP 1. Open the unzipped Java application folder in Visual Studio Code.</p> <p>STEP 2. Create a folder named <code>.ebextensions</code> at the root of the application source bundle and add the config and config.yaml files listed below.</p> <p>STEP 3. Create the <code>01_collector.config</code> file with the following content.</p> <p>Notes:</p> <ul> <li> <p>All configuration files must end with <code>.config</code>.</p> </li> <li> <p>The number prefix of a file indicates the execution sequence. <code>Script 01_</code> is executed first, then <code>02_</code> and <code>03_</code>. This script does two things:</p> <ul> <li> <p>Downloads an OTEL collector v0.89 into Amazon Linux 2 node (the default EC2 instance used by Beanstalk).</p> </li> <li> <p>Installs <code>otelcol-contrib</code>. The service will automatically start with a default <code>config.yaml</code> under the folder <code>/etc/otelcol-contrib</code>.</p> </li> </ul> </li> </ul> <pre><code>---\ncommands:\n  download_otel_contrib_collector:\n    command: sudo wget -O /opt/otelcol-contrib.rpm &lt;https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v0.89.0/otelcol-contrib_0.89.0_linux_amd64.rpm&gt;\n\n  install_otel_contrib_collector:\n    command: sudo rpm -U /opt/otelcol-contrib.rpm\n    ignoreErrors: true\n\n</code></pre> <p>STEP 4. Create a <code>config.yaml</code> file with the following content. Substitute <code>private_key</code> with your Send-Your-Data API key and <code>domain</code> with your Coralogix domain.</p> <pre><code>receivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4317\n      http:\n        endpoint: 0.0.0.0:4318\n  hostmetrics:\n    collection_interval: 30s\n    scrapers:\n      cpu:\n      disk:\n      load:\n      filesystem:\n      memory:\n      network:\n\nprocessors:\n  resource:\n    attributes:\n    - key: cx.application.name\n      value: otel-collector\n      action: upsert\n    - key: cx.subsystem.name\n      value: collector-java\n      action: upsert\n  resourcedetection:\n    detectors: [env, ec2]\n    timeout: 5s\n    override: true\n  tail_sampling:\n    decision_wait: 10s \n    expected_new_traces_per_sec: 1000\n    policies:\n      [\n        {\n            name: percent-sample-policy,\n            type: probabilistic,\n            probabilistic: {sampling_percentage: 100}\n        },\n        {\n          name: errors-policy,\n          type: status_code,\n          status_code: { status_codes: [ERROR] }\n        }\n      ]\n  batch:\n    timeout: 5s\n    send_batch_size: 256\n\nexporters:\n  coralogix:\n    timeout: \"30s\"\n    private_key: &lt;your-send-your-data-api-key&gt;\n    domain: &lt;your-cx-domain&gt;\n    application_name: elastic-java\n    subsystem_name: java-api\n    #application_name_attributes:\n    #  - \"cloud.region\"\n    #subsystem_name_attributes:\n    #  - \"host.name\"\n\nservice:\n  pipelines:\n    logs/otlp:\n      receivers: [otlp]\n      processors: [resourcedetection, resource, batch]\n      exporters: [coralogix]\n    traces/otlp:\n      receivers: [otlp]\n      processors: [resourcedetection, resource, tail_sampling, batch]\n      exporters: [coralogix]\n    metrics/otlp:\n      receivers: [otlp, hostmetrics]\n      processors: [resourcedetection, resource, batch]\n      exporters: [coralogix]\n\n</code></pre> <p>STEP 5. Create a file called <code>02_agent.config</code> with the following content.</p> <p>This script:</p> <ul> <li> <p>Replaces the <code>config.yaml</code> file in the <code>/etc/otelcol-contrib</code> with the file in the <code>.ebextensions</code> folder.</p> </li> <li> <p>Downloads the OTEL Java agent and places it in the <code>/opt</code> folder.</p> </li> <li> <p>Changes permission of the JAR file.</p> </li> <li> <p>Sets the JVM options.</p> </li> <li> <p>Sets the <code>otel.resource.attributes</code> and <code>otel.service.name</code> environment variables.</p> </li> </ul> <pre><code>---\ncontainer_commands: \n  copy_config_yaml: \n    command: \"yes | cp .ebextensions/config.yaml /etc/otelcol-contrib/config.yaml\"\n\ncommands:\n  download_opentelemetry_javaagent:\n    command: sudo wget -O /opt/otel-agent.jar &lt;https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar&gt;\n\n  give_permissions_all_to_otel: \n    command: sudo chmod a+rwx /opt/otel-agent.jar\n\noption_settings:\n  - namespace: aws:elasticbeanstalk:container:tomcat:jvmoptions\n    option_name: 'JVM Options'\n    value: -javaagent:/opt/otel-agent.jar \n\n  - namespace: aws:elasticbeanstalk:application:environment\n    option_name: otel.resource.attributes\n    value: application.name=elastic-java,api.name=java-api,cx.application.name=elastic-java,cx.subsystem.name=java-api\n\n  - namespace: aws:elasticbeanstalk:application:environment\n    option_name: otel.service.name\n    value: otel-collector\n\n</code></pre> <p>STEP 6. Create a file called <code>03_restart.config</code> with the following content.</p> <p>This script restarts the <code>otelcol-contrib</code> service using a new <code>config.yaml</code> file.</p> <pre><code>---\ncommands:\n  restart_otel_service: \n    command: sudo systemctl restart otelcol-contrib\n\n</code></pre> <p>STEP 7. When all the configurations required are completed, open a terminal and zip your application code at the root of the source bundle.</p> <p>This command will create a <code>tomcat-v1.zip</code> and exclude any MACOS specific files.</p> <p>Note: Do not use compress to zip from the Finder/Explore.</p> <pre><code>zip -r tomcat-v1.zip . -x '**/.*' -x '**/__MACOSX'\n\n</code></pre> <p>STEP 8. Deploy a new version of an application onto your Elastic Beanstalk environment.</p> <p>Notes:</p> <ul> <li> <p>The attached zip file contains all of the relevant changes under the Instrument Java App section.</p> </li> <li> <p>Download the zip file, extract, and specify the correct Coralogix <code>private_key</code> and the <code>domain</code> in the <code>config.yaml</code> file, zip it again, and deploy.</p> </li> </ul> <p>tomcat-with-post-deploy-script.zip</p>"},{"location":"newoutput/aws-elastic-beanstalk/#validation-testing","title":"Validation &amp; Testing","text":"<p>STEP 1. Log in to the AWS console and navigate to your Elastic Beanstalk environment. Click on Domain link to validate the Java application you deployed is running.</p> <p></p> <p>STEP 2. A Congratulations message should appear on the screen. Refresh the screen a few times to generate traces. Elastic Beanstalk runs a health check by periodically hitting this endpoint, so you will see additional traces.</p> <p></p> <p>STEP 3. Log in to your Coralogix account. Navigate to Explore &gt; Tracing to view the traces.</p> <p></p> <p>STEP 4. Deploy the Grafana dashboard for the host metrics to validate that the metrics are flowing into Coralogix. Go to the Grafana dashboard repo and copy the contents of the host_otel_metrics.json file.</p> <p>STEP 5. Navigate to Grafana &gt; Dashboard &gt; Manage. Click the Import button, paste the JSON file content, and then hit Load and Import. This will create a host metrics dashboard.</p> <p></p>"},{"location":"newoutput/aws-elastic-beanstalk/#troubleshooting","title":"Troubleshooting","text":"<p>STEP 1. Navigate to Elastic Beanstalk &gt; Environments &gt; Your-Environment-Name &gt; Configuration in your AWS console. From the main panel, navigate to the Updates, monitoring, and logging section. You should see the JVM options and the OTEL environment variables that were set in the <code>02_agent.config</code> script.</p> <p></p> <p>STEP 2. If the configuration looks good but you are not seeing the data, then SSH into an EC2 instance where OTEL Collector and Java agent are installed and validate that:</p> <ul> <li> <p>The Java agent is installed in the <code>/opt/otel-agent.jar</code></p> </li> <li> <p>The OTEL collector is installed in <code>/etc/otelcol-contrib</code></p> </li> <li> <p><code>config.yaml</code> is correctly copied into <code>/etc/otelcol-contrib</code> and the <code>private_key</code> and <code>domain</code> are set correctly.</p> </li> <li> <p>The <code>otelcol-contrib</code> service is running by issuing the following command:</p> </li> </ul> <pre><code>sudo systemctl status otelcol-contrib\n\n</code></pre> <ul> <li>If <code>otelcol-contrib</code> is not running, issue the following command to check the log to identify an issue and fix it, and then restart the service.</li> </ul> <pre><code>journalctl -n 100 | grep otelcol\n\n</code></pre>"},{"location":"newoutput/aws-elastic-beanstalk/#additional-resources","title":"Additional Resources","text":"<ul> <li> <p>OTel Java Instrumentation</p> </li> <li> <p>Beanstalk Tomcat Platform</p> </li> <li> <p>Beanstalk Customization with .ebextensions</p> </li> <li> <p>Commands for .ebextensions</p> </li> <li> <p>Option Settings for .ebextensions</p> </li> </ul>"},{"location":"newoutput/aws-elastic-beanstalk/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-eventbridge-outbound-webhook/","title":"AWS EventBridge Outbound Webhook","text":"<p>Amazon EventBridge serves as a serverless event bus service, facilitating the effortless collection and transmission of data from various applications and services to designated destinations. Employ the AWS EventBridge Outbound Webhook to establish a streamlined real-time mechanism, enabling Coralogix to transmit events seamlessly to AWS EventBridge.</p>"},{"location":"newoutput/aws-eventbridge-outbound-webhook/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>AWS account</p> </li> <li> <p>Amazon EventBridge event bus created</p> </li> </ul>"},{"location":"newoutput/aws-eventbridge-outbound-webhook/#create-a-webhook","title":"Create a Webhook","text":"<p>STEP 1. From your Coralogix toolbar, navigate to DATA FLOW &gt; OUTBOUND WEBHOOKS.</p> <p>STEP 2. Click AWS EventBridge.</p> <p></p> <p>STEP 3. Click + ADD NEW.</p> <p>STEP 4. Enter the webhook details and click Next.</p> <ul> <li> <p>Webhook Name. Enter a memorable name that will allow you to identify the webhook when attaching it to an alert easily.</p> </li> <li> <p>Event Bus ARN. Corresponds to the event bus, which will receive notifications. The policy attached must contain permission to publish.</p> </li> <li> <p>Detail Type. Free text to be included in the event.</p> </li> <li> <p>Source. Free text is used to identify the messages Coralogix sends.</p> </li> <li> <p>Role Name. Corresponds to the AWS IAM role that will be created in your account.</p> </li> </ul> <p>STEP 5. You will be rerouted to the AWS website for the integration to create and deploy a CloudFormation stack. This step makes a role and an associated permissions policy to allow Coralogix to use a set of temporary security credentials to access your AWS resources.</p> <p>Verify that all of the auto-pre-populated values are correct. Click Create Stack.</p> <p></p> <p>STEP 6. Revert to the Coralogix application and click CONTINUE.</p> <p></p> <p>STEP 6. [Optional] Edit the message to customize the header and body of the messages sent when the webhook is triggered. Click SAVE.</p> <p></p> <p>STEP 7. Select one or more alerts to join the webhook.</p> <p></p> <p>The Alert Notifications panel will appear.</p> <p></p> <p>STEP 8. The new webhook will appear in the Notifications section of the alert. Click +Add WEBHOOK and select the newly created webhook. Click SAVE CHANGES.</p> <p>STEP 9. Click DONE.</p> <p></p> <p>STEP 10. Modify or delete any of the webhook settings in the Actions column of the AWS EventBridge app. To add an alert to an existing webhook, click on the number under Associated Alerts.</p>"},{"location":"newoutput/aws-eventbridge-outbound-webhook/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to contact us\u00a0via our in-app chat\u00a0or by emailing support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-firehose/","title":"AWS Kinesis Data Firehose - Logs","text":"<p>Amazon Kinesis Data Firehose delivers real-time streaming data to destinations like Amazon Simple Storage Service (Amazon S3), Amazon Redshift, or Amazon OpenSearch Service (successor to Amazon Elasticsearch Service), and now supports delivering streaming data to Coralogix. There is no limit on the number of delivery streams, so it can be used for retrieving data from multiple AWS services.</p> <p>Coralogix is an AWS Partner Network (APN) Advanced Technology Partner with AWS\u00a0 Competencies in DevOps. The platform enables you to easily explore and analyze logs to gain deeper insights into the state of your applications and AWS infrastructure. Analyze all of your AWS service logs while storing only those you need. Generate metrics from aggregated logs to uncover and alert on trends in your AWS services.</p>"},{"location":"newoutput/aws-firehose/#overview","title":"Overview","text":"<p>Using Coralogix with Amazon Kinesis Data Firehose offers significant benefits when compared with other solutions.</p> <ul> <li> <p>It keeps monitoring simple.</p> </li> <li> <p>It integrates flawlessly.</p> </li> <li> <p>It's flexible with minimum maintenance.</p> </li> <li> <p>Scale, scale, scale.</p> </li> <li> <p>Real-time push monitoring involves pushing events instead of pulling.</p> </li> </ul>"},{"location":"newoutput/aws-firehose/#configuration","title":"Configuration","text":"<p>STEP 1. Navigate to the Kinesis Data Firehose console and choose 'Create delivery stream'.</p> <p>STEP 2. Under 'Choose source and destination':</p> <ul> <li> <p>Source: Choose Direct PUT</p> </li> <li> <p>Destination: Choose Coralogix\u00a0</p> </li> </ul> <p></p> <ul> <li>Delivery stream name: Fill in the desired stream name</li> </ul> <p></p> <p>STEP 3. Scroll down to 'Destination settings':</p> <ul> <li> <p>HTTP endpoint URL: Choose a HTTP endpoint URL based on your Coralogix region.</p> </li> <li> <p>Private key: Enter your Coralogix Send Your Data - API Key.</p> </li> <li> <p>Content encoding: Select GZIP.</p> </li> <li> <p>Retry duration: Choose 300 seconds.</p> </li> </ul> <p></p> <p>STEP 4. Scroll down to 'Parameters'. This section allows you to add and configure additional parameters surrounding the Coralogix platform.</p> <p></p> <p>The following parameters are available:</p> ParameterDescriptionapplicationNameA comma-separated list of application name sourcesapplicationNameDefaultDeprecatedsubsystemNameA comma-separated list of subsystem name sourcessubsystemNameDefaultDeprecatedintegrationTypeData structure:- CloudWatch_JSON: data from cloudWatch log groups- WAF- CloudWatch_CloudTrail- EksFargate- Default- RawText: use for VPC flow logsdynamicMetadataDeprecated <p>A name source can be a literal string (something), a quoted string (\"something\"), or a variable reference (${name}). Sources in a list are evaluated in order, variables without a value are skipped. For example, ${applicationName}, MyApp for a Default integration will use applicationName field if available, otherwise it will default to MyApp.</p> <p>Available variables depend on the integration type:</p> <ul> <li> <p>Default</p> <ul> <li> <p>applicationName \u2014 a value of the applicationName field of the JSON log record</p> </li> <li> <p>subsystemName \u2014 a value of the subsystemName field of the JSON log record</p> </li> </ul> </li> <li> <p>CloudWatch_JSON, CloudWatch_CloudTrail</p> <ul> <li>logGroup \u2014 a CloudWatch Log Group supplied by AWS</li> </ul> </li> <li> <p>WAF</p> <ul> <li>webAclName \u2014 a Web ACL Name supplied by AWS</li> </ul> </li> <li> <p>EksFargate</p> <ul> <li> <p>kubernetesNamespaceName \u2014 a value of the kubernetes.namespace_name log record field</p> </li> <li> <p>kubernetesContainerName \u2014 a value of the kubernetes.container_name log record field</p> </li> </ul> </li> </ul> <p>Notes:</p> <ul> <li> <p>By default, your delivery stream name will be used as 'applicationName' and ARN as 'subsystemName'.</p> </li> <li> <p>To override the associated 'applicationName' or 'subsystemName', add a new parameter with the desired value.</p> <ul> <li> <p>Key: 'applicationName' , value - 'new-app-name'</p> </li> <li> <p>Key: 'subsystemName' , value - 'new-subsystem-name'</p> </li> </ul> </li> <li> <p>The source of the data in Firehose determines the \u2018integrationType\u2019 parameter value:</p> <ul> <li> <p>For CloudWatch logs, use \u2018CloudWatch_JSON\u2019.</p> </li> <li> <p>For CloudTrail logs in CloudWatch, use \u2018CloudWatch_CloudTrail\u2019.</p> </li> <li> <p>For logs coming from EKS Fargate using our guide, use \u2018EksFargate\u2019.</p> </li> <li> <p>For data sources matching the Coralogix log ingestion format, use \u2018Default\u2019.</p> </li> <li> <p>For other data sources, use \u2018RawText\u2019. This moves all the text to <code>text</code> field of log, adds severity of <code>Info</code>, and generates a current timestamp. All further parsing of these logs should be done using parsing rules.</p> </li> <li> <p>For logs coming from AWS WAF, use \u2018WAF\u2019. This requires configuration on WAF as follows.</p> </li> </ul> </li> </ul> <p></p> <p></p> <p></p> <ul> <li> <p>Without adding the 'integrationType' parameter, the 'Default' integration type is selected.</p> </li> <li> <p>For integration of type 'Default', the logs should be structured according to our REST API rules.</p> </li> </ul> <p>STEP 5. Set up a recovery bucket (recommended). Enabling source data backup ensures that the data can be recovered if record processing transformation does not produced the desired results.</p> <p></p> <p>STEP 6. Scroll down to 'Backup settings':</p> <ul> <li> <p>Source record backup in Amazon S3: We suggest selecting\u00a0Failed data only.</p> </li> <li> <p>S3 backup bucket: Choose an existing bucket or create a new one.</p> </li> <li> <p>Buffer hints, compression, encryption: Leave these fields as is.</p> </li> </ul> <p>STEP 7. Review your settings and select Create delivery stream.</p> <p>Logs subscribed to your delivery stream will be immediately sent and available for analysis within Coralogix.</p>"},{"location":"newoutput/aws-firehose/#dynamic-values-table","title":"Dynamic Values Table","text":"<p>For applicationName and subsystemName to be set dynamically, follow STEP 4 and set the \u2018dynamicMetadata' parameter to <code>true</code> along with the 'integrationType' parameter (e.g. CloudWatch_JSON, EksFargate). Use the following dynamic values.</p> TypeDynamic applicationNameDynamic subsystemNameNotesCloudWatch_JSONthe cloudwatch log groupnonesupplied by awsCloudWatch_CloudTrailthe cloudwatch log groupnonesupplied by awsDefault'applicationName' field'subsystemName' fieldneed to be supplied in the log to be usedEksFargate'kubernetes.namespace_name' field'kubernetes.container_name' fieldsupplied by the default configurationWAFThe web acl namenonesupplied by aws"},{"location":"newoutput/aws-firehose/#data-source-configuration","title":"Data Source Configuration","text":"<p>Below are a couple of ways to connect your data source to firehose.</p>"},{"location":"newoutput/aws-firehose/#cloudwatch-logs","title":"Cloudwatch logs","text":"<p>To start sending your logs to Coralogix you first need to create a subscription filter inside your Cloudwatch log group.</p> <p>First, create a new role in IAM for your Cloudwatch log group to allow sending data to firehose.</p> <p>Go to the IAM console and choose 'Roles' under 'Access management'.</p> <p>Click on 'Create role' on the right.</p> <p>Under 'Trusted entity type' choose 'Custom trust policy' and insert this policy.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"logs.&lt;region_code&gt;.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> <p>Note: change '' into your AWS region. e.g 'us-east-1' <p>Now on 'Add permissions' click on 'Create policy'.</p> <p>on the opened window click on the 'JSON' tab and insert this policy</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"firehose:PutRecord\",\n                \"firehose:PutRecordBatch\"\n            ],\n            \"Resource\": [\n                \"&lt;firehose_ARN&gt;\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Note: change '' to your firehose amazon resource name (arn). <p>After creating the policy go back to the role creation page and click on the refresh button</p> <p>Pick the newly created policy.</p> <p>Finally, give your role a name and create it.</p> <p>After creating the role, go to the Cloudwatch console and choose 'Logs groups' under the 'Logs' side menu.</p> <p>Create a new subscription filter for the relevant log group - '' -&gt; 'Subscription filters' -&gt; 'Create Kinesis Firehose subscription filter'. <p>Under 'Choose destination':</p> <ul> <li> <p>For 'Destination account' choose 'Current account'</p> </li> <li> <p>For 'Kinesis Firehose delivery stream' choose the created firehose delivery stream\u00a0</p> </li> </ul> <p>Scroll down to 'Grant permission':</p> <ul> <li>For 'Select an existing role' choose the role created above</li> </ul> <p>After that scroll down and click on 'Start streaming'.</p> <p>That's it logs coming to your cloudwatch log group will also be directed to firehose.</p> <p>Note: be sure to use the correct integration type inside your firehose configuration.</p>"},{"location":"newoutput/aws-firehose/#kinesis-data-stream","title":"Kinesis Data Stream","text":"<p>To start sending your Kinesis data stream logs to coralogix we need to connect the Data stream to Firehose.</p> <p>Go to the Kinesis Data Stream console and choose 'Create data stream'.</p> <p>Under 'Data stream configuration':</p> <ul> <li>Data stream name: Enter the name of the data stream</li> </ul> <p>Scroll down to 'Data stream capacity':</p> <ul> <li>Capacity mode: Choose 'On-demand'</li> </ul> <p>After that scroll down and click on 'Create data stream'.</p> <p>Note: to connect a kinesis data stream to a firehose delivery stream the delivery stream must use 'Amazon kinesis data streams' as its source instead of 'Direct PUT'.</p> <p>To get the most out of the platform, be sure to check out the documentation which will help you get started with everything from parsing and enrichment to alerting and data clustering.</p>"},{"location":"newoutput/aws-firehose/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-inspecto-integration/","title":"AWS Inspector","text":"<p>AWS Inspector is an automated vulnerability management service that continually scans AWS workloads for software vulnerabilities and unintended network exposure, improving the security and compliance of applications deployed on AWS. Using a hybrid approach of agent-based and agentless assessments, it provides comprehensive evaluations of network, host, and application security. The service is easy to set up and can be integrated with other AWS services.</p> <p>This tutorial demonstrates how to integrate AWS Inspector with Coralogix using Amazon EventBridge API destinations.</p>"},{"location":"newoutput/aws-inspecto-integration/#configuration","title":"Configuration","text":"<p>STEP 1. Create API destination and connection by following these EventBridge instructions.</p> <p>STEP 2. Create a new rule.</p> <ul> <li> <p>Select the appropriate Event bus.</p> </li> <li> <p>Select Rule Type with an event patten.</p> </li> <li> <p>Click Next.</p> </li> </ul> <p></p> <p>STEP 3. Define Event pattern.</p> <ul> <li> <p>Select Inspector2 as AWS service.</p> </li> <li> <p>Select Inspector Findings as Event type.</p> </li> </ul> <p></p> <p>STEP 4. Define the target.</p> <ul> <li> <p>Select EventBridge API Destination as Target Types.</p> </li> <li> <p>Select the destination you created in STEP 1 from the drop-down menu as API destination.</p> </li> <li> <p>Create a new Execution role. If you have an existing role in place, click Use existing role and select it from the list.</p> </li> <li> <p>Click Next.add tags if needed and Next &gt; Create rule</p> </li> </ul> <p>STEP 5. Add Tags if needed. Click Next.</p> <p>STEP 6. Click Create rule.</p>"},{"location":"newoutput/aws-inspecto-integration/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-kinesis-data-firehose-terraform-module/","title":"AWS Kinesis Data Firehose Terraform Module","text":"<p>Using Coralogix Terraform modules, you can easily install and manage Coralogix integrations with AWS services as modules in your infrastructure code. Our modules are open source and available on our\u00a0GitHub\u00a0and in the\u00a0Terraform Registry.</p> <p>This tutorial demonstrates how to install AWS Kinesis Data Firehose for logs and metrics.</p>"},{"location":"newoutput/aws-kinesis-data-firehose-terraform-module/#installation","title":"Installation","text":""},{"location":"newoutput/aws-kinesis-data-firehose-terraform-module/#logs","title":"Logs","text":"<p>For logs, install AWS Kinesis Data Firehose by adding this declaration to your Terraform project:</p> <pre><code>module \"cloudwatch_firehose_coralogix_logs\" {\n  source                         = \"coralogix/aws/coralogix//modules/firehose-logs\"\n  private_key                    = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXX\"\n  firehose_stream                = \"coralogix-firehose-logs\"\n  coralogix_region               = \"Europe\"\n  integration_type_logs          = \"Default\"\n  source_type_logs               = \"DirectPut\"\n}\n\n</code></pre>"},{"location":"newoutput/aws-kinesis-data-firehose-terraform-module/#metrics","title":"Metrics","text":"<p>For metrics, install AWS Kinesis Data Firehose by also adding this declaration to your Terraform project:</p> <pre><code>module \"cloudwatch_firehose_coralogix_metrics\" {\n  source           = \"coralogix/aws/coralogix//modules/firehose-metrics\"\n  private_key      = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXX\"\n  firehose_stream  = \"coralogix-firehose-metrics\"\n  coralogix_region = \"Europe\"\n}\n\n</code></pre>"},{"location":"newoutput/aws-kinesis-data-firehose-terraform-module/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Terraform ProviderExternal DocumentationGitHubTerraform Registry"},{"location":"newoutput/aws-kinesis-data-firehose-terraform-module/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-kinesis-with-lambda-function-terraform-module/","title":"AWS Kinesis with Lambda Function Terraform Module","text":"<p>Using Coralogix Terraform Modules, you can easily install and manage Coralogix integrations with AWS services as modules in your infrastructure code. This tutorial demonstrates how to install\u00a0AWS Kinesis with a Lambda Function.</p> <p>Our modules are open source and available on our\u00a0GitHub\u00a0and in the\u00a0Terraform Registry.</p>"},{"location":"newoutput/aws-kinesis-with-lambda-function-terraform-module/#installation","title":"Installation","text":"<p>Install our\u00a0AWS Kinesis with a Lambda Function\u00a0by adding this declaration to your Terraform project:</p> <pre><code>module \"kinesis\" {\n  source = \"coralogix/aws/coralogix//modules/kinesis\"\n\n  coralogix_region    = \"Europe\"\n  private_key         = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXX\"\n  ssm_enable          = \"false\"\n  layer_arn           = \"&lt;your layer arn&gt;\"\n  application_name    = \"kinesis\"\n  subsystem_name      = \"logs\"\n  kinesis_stream_name = \"&lt;your kinesis stream name&gt;\"\n}\n</code></pre>"},{"location":"newoutput/aws-kinesis-with-lambda-function-terraform-module/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Terraform ProviderExternal DocumentationGitHubTerraform Registry"},{"location":"newoutput/aws-kinesis-with-lambda-function-terraform-module/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-privatelink-lambda-configuration/","title":"AWS PrivateLink: Lambda Configuration","text":"<p>Once you have configured our AWS PrivateLink, align the VPC to your Lambda.</p>"},{"location":"newoutput/aws-privatelink-lambda-configuration/#lambda-configuration","title":"Lambda Configuration","text":""},{"location":"newoutput/aws-privatelink-lambda-configuration/#permissions","title":"Permissions","text":"<p>Grant the Lambda the permissions necessary to add virtual interfaces, as follows:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:DescribeNetworkInterfaces\",\n                \"ec2:CreateNetworkInterface\",\n                \"ec2:DeleteNetworkInterface\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n</code></pre> <p>Full instructions can be found here.</p>"},{"location":"newoutput/aws-privatelink-lambda-configuration/#align-the-vpc-to-the-lambda","title":"Align the VPC to the Lambda","text":"<p>STEP 1. Follow these instructions to align the VPC to the Lambda.</p> <p>STEP 2. Update the <code>CORALOGIX_URL</code> environment variable to match the FQDN endpoint for your Coralogix domain.</p>"},{"location":"newoutput/aws-privatelink-lambda-configuration/#aws-secrets-manager","title":"AWS Secrets Manager","text":"<p>If you intend to use AWS Secrets Manager with your Lambda, you must create another VPC endpoint for the\u00a0<code>com.amazonaws.&lt;AWS Region&gt;.secretsmanager</code>\u00a0service. Detailed instructions can be found here.</p>"},{"location":"newoutput/aws-privatelink-lambda-configuration/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-privatelink-vpc-peering-configuration/","title":"AWS PrivateLink: VPC Peering Configuration","text":"<p>To utilize our Coralogix AWS PrivateLink feature, it is essential to establish a VPC endpoint within the Coralogix AWS region corresponding to your Coralogix domain\u2014commonly referred to as a same-region VPC.</p> <p>In cases where your AWS resources for monitoring are located in a different region, you can achieve the required connectivity by employing VPC peering. This involves deploying your Lambda in a cross-region VPC, strategically positioned in proximity to the source.</p> <p>This tutorial provides step-by-step guidance on configuring your cross-region VPC setup. The configuration ensures that any traffic directed to the PrivateLink domain name follows a route through the VPC peering connection in the same region, ultimately reaching the PrivateLink endpoint.</p>"},{"location":"newoutput/aws-privatelink-vpc-peering-configuration/#prerequisites","title":"Prerequisites","text":"<p>When your Lambda is being deployed into a cross-region VPC, use VPC peering to allow the Lambda local VPC to communicate over the PrivateLink through the same-region VPC. To do this, configure the same-region VPC.</p>"},{"location":"newoutput/aws-privatelink-vpc-peering-configuration/#vpc-peering-configuration","title":"VPC Peering Configuration","text":"<p>STEP 1. Configure the DNS record to give it time to propagate.</p> <ul> <li> <p>Navigate to Route 53.</p> </li> <li> <p>Create a new private hosted zone for your Coralogix domain and align it with your Lambda local VPC.</p> </li> <li> <p>Click Created hosted zone.</p> </li> </ul> <p></p> <p>STEP 2. Configure an A record type pointing to the PrivateLink VPC endpoint.</p> <ul> <li> <p>In the private hosted zone, set the record name to <code>ingress.private</code>, with an alias to VPC endpoint - that is, an alias pointing to the VPC PrivateLink endpoint of your same-region VPC.</p> </li> <li> <p>Select the main regional endpoint that does not include availability zone references.</p> </li> <li> <p>Click Create records.</p> </li> </ul> <p></p> <p>Notes:</p> <ul> <li>The Route 53 rules may take some time to propagate.</li> </ul> <p>STEP 3. Set up the VPC peering connection between the two VPCs.</p> <ul> <li> <p>Navigate to the VPC console of the cross-region VPC.</p> </li> <li> <p>Select Peering Connections in the left-hand menu.</p> </li> <li> <p>Select Create Peering Connection.</p> </li> </ul> <p>STEP 4. Set the VPC ID (Requester) to the cross-region VPC that will host your Lambda.</p> <ul> <li> <p>In the local VPC to peer with section, select the region of your same-region VPC.</p> </li> <li> <p>Manually enter the VPC ID (Accepter) of the same-region VPC.</p> </li> <li> <p>Click Create peering connection.</p> </li> </ul> <p></p> <p>STEP 5. Find and accept the VPC peering request in the target region.</p> <ul> <li> <p>Switch regions.</p> </li> <li> <p>In the Peering Connections, find and accept the request.</p> </li> </ul> <p>STEP 6. Adjust your routing tables.</p> <ul> <li> <p>Adjust the routing tables of the cross-region VPC subnets.</p> </li> <li> <p>Validate that the routing table(s) in use by the subnets include routes to the same-region VPC CIDR range. The Target will be the peering connection you just created.</p> </li> <li> <p>Click Save changes.</p> </li> </ul> <p></p> <p>STEP 7. Once you\u2019ve added the route to all of your subnets, do the same for the same-region VPC. This ensures return traffic can be routed back to the cross-region VPC.</p> <p>Notes:</p> <ul> <li>You will likely want an Internet or NAT gateway configured for testing purposes.</li> </ul>"},{"location":"newoutput/aws-privatelink-vpc-peering-configuration/#next-steps","title":"Next Steps","text":"<p>Align the VPC to your Lambda. Instructions can be found here.</p>"},{"location":"newoutput/aws-privatelink-vpc-peering-configuration/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-resource-enrichment/","title":"AWS Resource Enrichment","text":"<p>Coralogix now offers AWS Resource Enrichment, allowing you to enrich your logs with tags from Amazon Web Services (AWS) EC2 instances. Use this feature to connect your business and operation metadata from AWS and gain greater insight into your data.</p>"},{"location":"newoutput/aws-resource-enrichment/#overview","title":"Overview","text":"<p>The feature enriches every log containing the following fields with metadata.</p> <code>ec2_instance_id</code>OpenTelemetry<code>instance_id</code>Fluentd<code>ec2_instance_id</code>Fluent Bit <p>Sample Otel Log Before Enrichment</p> <pre><code>{\n    resource:{\n        attributes:{\n            cloud.availability_zone:us-east-1b\n            cloud.platform:aws_ec2\n            cloud.provider:aws\n            cloud.region:us-east-1\n            ec2_instance_id:i-0c352ce4c433c58a5\n            host.image.id:ami-051f7e7f6c2f40dc1\n            host.name:ip-172-31-37-204.ec2.internal\n            host.type:t2.micro\n        }\n    }\n    scope:{\n        attributes:{}\n    }\n    resourceSchemaUrl:&lt;https://opentelemetry.io/schemas/1.6.1&gt;\n        logRecord:{\n            attributes:{\n                log.file.name:log2.log\n                log.file.path:log2.log\n            }\n        body:{\n            account_id:035955823196\n            application_name:my application test\n            ec2_instance_type:t2.micro\n            ec2_region:us-east-1\n            hosename:ip-172-31-37-204.ec2.internal\n            image_id:ami-051f7e7f6c2f40dc1\n            kubernetes:{\n                container_name:service-0\n                docker_id:12345186e060681175942786de75a102c679f13c3f0fb330231bdd94c5570e2\n                labels:{\n                    app.kubernetes.io\\\\/managed-by:Helm\n                    statefulset.kubernetes.io\\\\/pod-name:service-0\n                }\n                namespace_name:default\n                pod_id:123456-aefb-456e-b315-77b014de9d5b\n                pod_name:service-0\n                }\n            log:[INFO] 2022-11-15T12:55:36,768 kafka.streams.processor.internals.StateDirectory service-0 stream-thread [categorization-0-CleanupThread] Deleting obsolete state directory 0_2 for task 0_2 as 32827574178ms has elapsed (cleanup delay is 600000ms).\n            platform:aws_ec2\n            stream:stdout\n            time:2022-11-15T12:55:36.768697532Z\n        }\n    observedTimeUnixNano:1694069570457198600\n    }\n}\n\n</code></pre> <p>AWS Enrichment</p> <p></p> <p>Sample Otel Log After Enrichment</p> <pre><code>{\n    resource:{\n        attributes:{\n            ec2_instance_id_aws:{ ##enrichment part\n                companyId:32757\n                resourceId:i-0c352ce4c433c58a5\n                resourceType:aws:ec2:instance\n                tags:{\n                    tag_a:a\n                    tag_b:b\n                    }\n            }\n            cloud.availability_zone:us-east-1b\n            cloud.platform:aws_ec2\n            cloud.provider:aws\n            cloud.region:us-east-1\n            ec2_instance_id:i-01s32s14p423a98a5\n            host.image.id:ami-051f7e7f6c2f40dc1\n            host.name:ip-172-62-93-414.ec2.internal\n            host.type:t2.micro\n        }\n    }\n    scope:{\n        attributes:{}\n    }\n    resourceSchemaUrl:&lt;https://opentelemetry.io/schemas/1.6.1&gt;\n        logRecord:{\n            attributes:{\n                log.file.name:log.log\n                log.file.path:log.log\n            }\n        body:{\n            kubernetes:{\n                container_name:service-0\n                docker_id:12345186e060681175942786de75a102c679f13c3f0fb330231bdd94c5570e2\n                labels:{\n                    app.kubernetes.io\\\\/managed-by:Helm\n                    statefulset.kubernetes.io\\\\/pod-name:service-0\n                }\n                namespace_name:default\n                pod_id:123456-aefb-456e-b315-77b014de9d5b\n                pod_name:service-0\n                }\n            log:[INFO] 2022-11-15T12:55:36,768 kafka.streams.processor.internals.StateDirectory service-0 stream-thread [categorization-0-CleanupThread] Deleting obsolete state directory 0_2 for task 0_2 as 32827574178ms has elapsed (cleanup delay is 600000ms).\n            platform:aws_ec2\n            stream:stdout\n            time:2022-11-15T12:55:36.768697532Z\n        }\n    observedTimeUnixNano:1694069570457198600\n    }\n}\n\n</code></pre>"},{"location":"newoutput/aws-resource-enrichment/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Coralogix account set up in the domain corresponding to the region where your data is stored</p> </li> <li> <p>Coralogix Send-Your-Data API key</p> </li> </ul>"},{"location":"newoutput/aws-resource-enrichment/#setup","title":"Setup","text":""},{"location":"newoutput/aws-resource-enrichment/#installation-configuration","title":"Installation &amp; Configuration","text":"<p>We strongly recommend installing and configuring with OpenTelemetry. The default example below uses the configuration for OpenTelemetry.</p> <p>STEP 1. Ensure your logs contain a key with EC2 instance ID value:</p> <code>ec2_instance_id</code>OpenTelemetry<code>instance_id</code> (default key name) using Fluent-plugin-ec2-metadata pluginFluentD<code>ec2_instance_id</code>Fluent Bit <p>STEP 2. Add the resourcedetection and transform processors, enable the desired attributes, and add it to your config file. FluentD metadata labels and Fluent Bit metadata labels are configured differently.</p> <p>You must enable <code>host.id</code> for the enrichment to work.</p>"},{"location":"newoutput/aws-resource-enrichment/#opentelemetry-example","title":"OpenTelemetry Example","text":"<pre><code>processors:\n  resourcedetection:\n    detectors: [ec2]\n    override: true\n    ec2:\n      resource_attributes:\n        host.id:\n          enabled: true\n        host.name:\n          enabled: true\n        host.image.id:\n          enabled: true\n        host.type:\n          enabled: true\n        cloud.region:\n          enabled: true\n        cloud.account.id:\n          enabled: true\n        cloud.availability_zone:\n          enabled: true\n  transform:\n    error_mode: ignore\n    log_statements:\n      - context: log\n        statements:\n          - set(resource.attributes[\"ec2_instance_id\"],resource.attributes[\"host.id\"])\n          - delete_key(resource.attributes, \"host.id\")\n\n</code></pre>"},{"location":"newoutput/aws-resource-enrichment/#fluent-bit-example","title":"Fluent Bit Example","text":"<p>STEP 1. Add the AWS filter with the desired additional labels.</p> <pre><code>[FILTER]\n        Name aws\n        Match *\n        ec2_instance_type true\n        account_id true\n        hostname true\n\n</code></pre> <p>STEP 2. Specify the desired AWS metadata labels under the nested filter as part of the nest operation defining root fields.</p> <pre><code>[FILTER]\n        Name        nest\n        Match       kube.*\n        Operation   nest\n        Wildcard    kubernetes\n        Wildcard    account_id\n        Wildcard    hostname\n        Wildcard    az\n        Wildcard    ec2_instance_type\n        Wildcard    ec2_instance_id\n        Wildcard    log\n        Wildcard    log_obj\n        Wildcard    stream\n        Wildcard    time\n        Nest_under  json\n\n</code></pre>"},{"location":"newoutput/aws-resource-enrichment/#aws-lambda-deployment","title":"AWS Lambda Deployment","text":"<p>STEP 1. Deploy the AWS lambda function running in your AWS account. The platform collects logs from a chosen EC2 instance in your AWS account and sends them to Coralogix, enriching them with tags associated with your instance.</p> <p>STEP 2. Fill in the Application settings fields.</p> <ul> <li> <p>Input the Coralogix\u00a0domain\u00a0within which your account has been created in\u00a0CoralogixRegion.</p> </li> <li> <p>Input your Coralogix\u00a0Send-Your-Data API key\u00a0in\u00a0ApiKey.</p> </li> <li> <p>Do not change the default settings for the FunctionMemorySize and FunctionTimeout.</p> </li> </ul> <p></p> <p>STEP 3. Deploy your application to collect tags. It will collect tags every 10 minutes and send them to Coralogix.</p>"},{"location":"newoutput/aws-resource-enrichment/#aws-enrichment","title":"AWS Enrichment","text":"<p>STEP 1. From your Coralogix toolbar, navigate to Data Flow &gt; Data Enrichment. Scroll down the page to view the AWS Enrichment Option.</p> <p></p> <p>STEP 2. Select the chosen\u00a0AWS resource type.</p> <p>STEP 3. Select the key with your EC2 instance ID.</p> <code>resource.attributes.ec2_instance_id</code>OpenTelemetry<code>ec2_instance_id</code>Fluent Bit<code>instance_id</code>Fluentd <p>Note: If you have modified the default option, you may have a different key.</p> <p>STEP 4. Add the enrichment.</p> <p>STEP 5. Logs associated with the chosen AWS resource should now appear enriched with AWS tags in your dashboard. To view them, navigate to\u00a0Explore &gt; Logs or to LiveTail.</p>"},{"location":"newoutput/aws-resource-enrichment/#limitations","title":"Limitations","text":"<p>AWS Resource Enrichment feature grants the following:</p> <ul> <li> <p>Active resources (existing in the last 24h) / Coralogix team: 100,000</p> </li> <li> <p>50 tags/resources (equal to AWS limits)</p> </li> <li> <p>Tag key length &lt;= 127 (equal to AWS limits)</p> </li> <li> <p>Tag value length &lt;= 255 (equal to AWS limits)</p> </li> </ul>"},{"location":"newoutput/aws-resource-enrichment/#additional-resources","title":"Additional Resources","text":"GitHubLambda Documentation for Coralogix AWS EnrichmentAWS: Coralogix-Resource-Tabs Application"},{"location":"newoutput/aws-resource-enrichment/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-resource-metadata-collection/","title":"AWS Resource Metadata Collection","text":"<p>Deploy the AWS Resource Metadata Collection\u00a0AWS Lambda function in your AWS account. The function collects metadata of EC2 instances and AWS Lambda functions in the region of your AWS account and sends them to Coralogix.</p>"},{"location":"newoutput/aws-resource-metadata-collection/#overview","title":"Overview","text":"<p>AWS resources can be vast and interconnected. To better understand log data and troubleshoot issues, it's important to have context about which AWS resources are involved. The AWS Resource Metadata Collection integration collects information about AWS resources that are associated with log events. This contextual information can include details about the AWS service, resource tags, AWS region, timestamps for resource creation or modification, and any relevant custom tags specific to the organization's AWS environment.</p>"},{"location":"newoutput/aws-resource-metadata-collection/#benefits","title":"Benefits","text":"<p>The collection of EC2 instance and Lambda function metadata serves as a foundation for better AWS resource management, optimization, security, and efficient troubleshooting. It helps you make informed decisions and take actions based on a more comprehensive understanding of your AWS resources and their interactions.</p> <ul> <li> <p>Improve Troubleshooting. The metadata can provide valuable operational insights into your AWS environment. You can analyze resource-specific patterns, performance trends, and utilization to ensure your applications run smoothly. Having resource context helps in identifying the source of issues more quickly and accurately.</p> </li> <li> <p>Resource Optimization. By collecting metadata, you can understand the relationships between different AWS resources, such as how Lambda functions interact with specific EC2 instances or other services. This can be essential for monitoring and managing complex AWS architectures, and making informed decisions about resource usage and optimization.</p> </li> <li> <p>Security and Compliance. Resource metadata can assist in security monitoring, compliance reporting, and auditing.</p> </li> <li> <p>Cost Management. Understanding resource attributes can be valuable for cost management and allocation.</p> </li> </ul>"},{"location":"newoutput/aws-resource-metadata-collection/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>AWS account</p> </li> <li> <p>Permissions to create Lambda functions</p> </li> <li> <p>If you are using Secret Manager you should first deploy the\u00a0SM Lambda layer. Note that you should only deploy one layer per region.</p> </li> </ul>"},{"location":"newoutput/aws-resource-metadata-collection/#configuration","title":"Configuration","text":"<p>STEP 1. In your navigation pane, click Data Flow &gt; Integrations. View the list of available integrations.</p> <p>STEP 2. Select AWS Resource Metadata.</p> <p>STEP 3. Click + ADD NEW.</p> <p></p> <p>STEP 4. Input the Integration Details.</p> <p></p> <ul> <li> <p>Input a name for your integration.</p> </li> <li> <p>Select the authentication type, either APIKey or Existing Secret.</p> <ul> <li> <p>If using an API key, input an existing Coralogix Send-Your-Data API Key or click CREATE NEW KEY.</p> </li> <li> <p>If using an existing secret, enter the AWS Secret Name.</p> </li> </ul> </li> <li> <p>Mark the Collect Aliases checkbox if you want to collect the aliases of the resources.</p> </li> <li> <p>Select your AWS Region from the dropdown list.</p> </li> <li> <p>If you want to use AWS PrivateLink, click Advanced Settings and mark the Use AWS PrivateLink checkbox. AWS PrivateLink is a service that facilitates secure and private connections between VPCs and AWS services, bypassing the need for the public internet. It is worth noting that the integration might not succeed if AWS PrivateLink is not properly set up.</p> </li> </ul> <p>STEP 5. Click NEXT.</p> <p>STEP 6. View the instructions for your integration, then click CREATE CLOUDFORMATION.</p> <p></p> <p>STEP 7. You will be rerouted to the AWS website. Verify that all of the auto pre-populated values are correct, then click the acknowledgement checkboxes, and click Create Stack.</p> <p></p> <p>STEP 8. Go back to the Coralogix application and click COMPLETE to ensure your deployment is successful. This triggers a test to verify the deployment, the result of which can be seen on the next page as either Failed or Connected.</p> <p></p> <p>STEP 9. View your integration information.</p> <p></p> <p>STEP 10. Upon successful deployment, leverage the Coralogix APM Serverless Monitoring feature to access detailed insights into the Lambda functions operating within the deployed region.</p>"},{"location":"newoutput/aws-resource-metadata-collection/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Extension Packages"},{"location":"newoutput/aws-resource-metadata-collection/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-secrets-manager-lambda-layer/","title":"AWS Secrets Manager Lambda Layer","text":"<p>Deploy the AWS Secrets Manager Lambda layer to be used in any of our AWS integrations. Doing so ensures the security of your <code>ApiKey</code>, which is presented in the Lambda as a secret rather than an environment variable.</p>"},{"location":"newoutput/aws-secrets-manager-lambda-layer/#overview","title":"Overview","text":"<p>AWS Secrets Manager helps you to securely encrypt, store, and retrieve credentials for your databases and other services. Instead of hardcoding credentials in your apps, you can make calls to Secrets Manager to retrieve your credentials whenever needed. Secrets Manager helps you protect access to your IT resources and data by enabling you to rotate and manage access to your secrets.</p> <p>Use the AWS Lambda Secrets Manager layer in any of our AWS integrations. Deploy the layer with Terraform or as a serverless application.</p>"},{"location":"newoutput/aws-secrets-manager-lambda-layer/#serverless","title":"Serverless","text":"<p>Deploy the AWS Secrets Manager Lambda layer to be used in our serverless AWS integrations.</p> <p>STEP 1. Deploy the layer. Click Deploy here.</p> <p></p> <p>STEP 2. Scroll down. Click Deploy.</p> <p></p> <p>STEP 3. Once the layer is created, copy and paste the ARN into the <code>LayerARN</code> field of the relevant integration.</p> <p></p>"},{"location":"newoutput/aws-secrets-manager-lambda-layer/#terraform","title":"Terraform","text":"<p>Deploy the AWS Secrets Manager Lambda layer to be used with our Terraform AWS integrations.</p> <p>STEP 1. Run this code.</p> <pre><code>provider \"aws\" {\n}\n\nmodule \"lambda-secretLayer\" {\n  source = \"coralogix/aws/coralogix//modules/lambda-secretLayer\"\n\n}\n\noutput \"layer_arn\" {\n  value = module.lambda-secretLayer.lambda_layer_version_arn\n}\n\n</code></pre> <p>STEP 2. Once the layer is created, copy and paste the ARN as the\u00a0<code>layer_arn</code>\u00a0variable in the relevant integration, as in the example below.</p> <p>Once the layer is created, copy and paste the ARN as the\u00a0layer_arn\u00a0variable in the relevant integration, as in the example below. Set <code>secret_manager_enabled</code> to true.</p> <pre><code>provider \"aws\" {\n}\n\nmodule \"lambda-secretLayer\" {\n  source = \"coralogix/aws/coralogix//modules/lambda-secretLayer\"\n\n}\n\nmodule \"coralogix-shipper-s3\" {\n  source = \"coralogix/aws/coralogix//modules/s3\"\n  depends_on = [ module.lambda-secretLayer ]\n\n  coralogix_region   = \"Europe\"\n  private_key        = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXX\"\n  secret_manager_enabled    = true \n  layer_arn          = module.lambda-secretLayer.lambda_layer_version_arn\n  application_name   = \"s3\"\n  subsystem_name     = \"logs\"\n  s3_bucket_name     = \"test-bucket-name\"\n  integration_type   = \"s3\"\n}\n\n</code></pre> <p>In the event you want to use AWS Secrets Manager with a predefined secret that contains your Coralogix Send-Your-Data API key:</p> <ul> <li> <p>Set the variable <code>create_secret</code> to False</p> </li> <li> <p>Input the name of the secret that contains your Coralogix Send-Your-Data API key as <code>private_key</code></p> </li> </ul> <pre><code>provider \"aws\" {\n}\n\nmodule \"lambda-secretLayer\" {\n  source = \"coralogix/aws/coralogix//modules/lambda-secretLayer\"\n\n}\n\nmodule \"coralogix-shipper-s3\" {\n  source = \"coralogix/aws/coralogix//modules/s3\"\n  depends_on = [ module.lambda-secretLayer ]\n\n  coralogix_region   = \"Europe\"\n  private_key        = \"the name of the secret that contains the Coralogix send your data key\"\n  layer_arn          = module.lambda-secretLayer.lambda_layer_version_arn\n  application_name   = \"s3\"\n  subsystem_name     = \"logs\"\n  s3_bucket_name     = \"test-bucket-name\"\n  integration_type   = \"s3\"\n  create_secret      = \"False\"\n}\n\n</code></pre>"},{"location":"newoutput/aws-secrets-manager-lambda-layer/#additional-resources","title":"Additional Resources","text":"IntegrationsAmazon S3: Data Collection OptionsAWS CloudTrail: Data Collection OptionsAWS CloudWatch: Data Collection Options"},{"location":"newoutput/aws-secrets-manager-lambda-layer/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-sns-data-ingestion/","title":"AWS SNS Data Ingestion","text":"<p>Collect your AWS SNS messages in the Coralogix platform using our automatic Contextual Data Integration Package. The package automatically generates a URL to be used when creating an SNS subscription.</p>"},{"location":"newoutput/aws-sns-data-ingestion/#overview","title":"Overview","text":"<p>AWS SNS (Simple Notification Service) is a fully managed messaging service provided by Amazon Web Services (AWS). It enables you to send messages or notifications to distributed systems and application components. SNS supports multiple message formats, such as SMS, email, HTTP endpoints, mobile push notifications, and more, making it a versatile service for broadcasting messages to a large number of subscribers.</p> <p>Forwarding AWS SNS logs to Coralogix simplifies log aggregation, enriches monitoring capabilities, and streamlines issue diagnosis. By channeling AWS SNS logs into Coralogix, organizations gain a unified view of their notification events, enabling rapid detection of anomalies, proactive troubleshooting, and data-driven decision-making. This integration empowers teams to optimize message delivery, bolster system reliability, and ensure operational efficiency, utilizing Coralogix's analysis and visualization features to extract actionable insights from AWS SNS logs and maintain a resilient communication framework.</p>"},{"location":"newoutput/aws-sns-data-ingestion/#get-started","title":"Get Started","text":"<p>STEP 1.\u00a0In your navigation pane, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 2.\u00a0In the Contextual Data section, select AWS SNS and click\u00a0+ ADD.</p> <p></p> <p>STEP 4. Click ADD NEW.</p> <p>STEP 5.\u00a0Fill in the Integration Details:</p> <ul> <li> <p>Name.\u00a0Name your integration.</p> </li> <li> <p>Your API Key. Click\u00a0CREATE NEW KEY\u00a0to generate an API Key and name it.</p> </li> <li> <p>Application Name and Subsystem Name. Enter an\u00a0application and subsystem name.</p> </li> </ul> <p></p> <p>STEP 6.\u00a0Click\u00a0GENERATE URL. The URL for the integration will be automatically created. Use this when creating an SNS subscription.</p> <p></p>"},{"location":"newoutput/aws-sns-data-ingestion/#create-an-sns-subscription","title":"Create an SNS Subscription","text":"<p>STEP 1. Log in into your AWS account and create a subscription for the topic from which you would like to send messages.</p> <p></p> <p>STEP 2.\u00a0Fill in the Details:</p> <ul> <li> <p>Topic ARN.\u00a0This is the ARN of the topic you created.</p> </li> <li> <p>Protocol. Input HTTPS.</p> </li> <li> <p>Endpoint. Input the URL created by the Coralogix Integration Package.</p> </li> </ul> <p>STEP 3. Click Save.</p> <p>Notes:</p> <ul> <li>Read more about SNS subscriptions here.</li> </ul>"},{"location":"newoutput/aws-sns-data-ingestion/#example-log","title":"Example Log","text":"<pre><code>{\n  \"source_system\": \"awssns\",\n  \"awssns\": {\n    \"MessageId\": \"ab11a2b6-2b53-53b3-af61-fa1b95c7a52a\",\n    \"TopicArn\": \"arn:aws:sns:eu-central-1:1234567890:coralogix-integration\",\n    \"Subject\": \"Backend error\",\n    \"Message\": {\n      \"customeAlert\": \"Too many connections!\"\n    },\n    \"Timestamp\": \"2021-02-22T14:14:44.582Z\",\n    \"SignatureVersion\": \"1\",\n    \"Signature\": \"ioQos7NF5tDSLvUBKOBJjsi8VyMA4RWksfNV4K++mr5sOqv/4jUGSo8RRpCgE+du2NX07oq5j5tOHV/E02YDklOSZTzPHiR9fSNdV3wip4kZIEh+/CJfQIuHpLkJpFysw/Gkwxd4LDhDax+Fi1YmiFd2FaYKwgk9c2MJEarnUtAr5j7Nj/H32K4qU3F8Er+8efJ+nh+3EEAM9JnMPHCd7ryMtxoOQGv73pyROkMI+F28cx5v7lOxHQb0AqlO8uzCSksa2HByRgeTVS2akHO6tIBMJ5LoqaoBCwWmjhT8XWYODyMGGcdXiIFZdRA/mKrjZfcsCwqaVKjSnDpFZC1Rgw==\",\n    \"SigningCertURL\": \"https://sns.eu-central-1.amazonaws.com/SimpleNotificationService-010a507c1833636cd94bdb98bd911111.pem\",\n    \"UnsubscribeURL\": \"https://sns.eu-central-1.amazonaws.com/?Action=Unsubscribe&amp;amp;SubscriptionArn=arn:aws:sns:eu-central-1:1234567890:coralogix-integration:12345678990\"\n  }\n}\n</code></pre>"},{"location":"newoutput/aws-status-logs/","title":"AWS Status Logs","text":"<p>Collect your AWS Status log messages in the Coralogix platform using our automatic Contextual Data Integration Package. The package lets you enable AWS Status log data ingestion to allow you to see status updates on public AWS incidents.</p>"},{"location":"newoutput/aws-status-logs/#overview","title":"Overview","text":"<p>Amazon Web Services (AWS), offered by Amazon, is a comprehensive and widely used cloud computing platform that provides a vast array of on-demand cloud services, including computing power, storage, databases, machine learning, analytics, networking, and more. AWS allows organizations to rapidly scale resources and deploy applications across a global network of data centers, helping businesses innovate quickly, reduce costs, and achieve greater flexibility. With a rich ecosystem of services, tools, and solutions, AWS empowers businesses to build, test, and deploy applications with ease, accelerating their digital transformation and enabling them to drive innovation and growth in a highly scalable and reliable environment.</p> <p>Routing your AWS status logs to Coralogix streamlines log aggregation, augments monitoring efficiency, and expedites problem resolution. By funneling your AWS status logs into Coralogix's log management platform, you attain a consolidated view of your AWS infrastructure's condition, enabling rapid anomaly detection, proactive troubleshooting, and informed decision-making. This integration empowers teams to fine-tune resource allocation, fortify system reliability, and sustain operational effectiveness, leveraging Coralogix's analytics, alerts, and visualization tools to extract valuable insights from AWS status logs and ensure a robust and resilient cloud environment.</p>"},{"location":"newoutput/aws-status-logs/#get-started","title":"Get Started","text":"<p>STEP 1.\u00a0In your navigation pane, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 2.\u00a0In the Contextual Data section, select AWS and click\u00a0ADD.</p> <p></p> <p>STEP 3. Click Enable AWS Data Ingestion.</p>"},{"location":"newoutput/aws-status-logs/#example-log","title":"Example Log","text":"<pre><code>{\n    \"source_system\": \"aws\"\n        \"aws\":{\n            \"title\": \"Informational message: \u30a8\u30e9\u30fc\u7387\u304a\u3088\u3073\u30ec\u30a4\u30c6\u30f3\u30b7\u30fc\u306e\u4e0a\u6607 | Increased Error rates and Latencies\"\n            \"description\": \"We are deploying a second mitigation strategy to resolve elevated latency and the remaining level of errors for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region. We continue working towards recovery.\"\n    }\n}\n\n</code></pre>"},{"location":"newoutput/aws-status-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/aws-traffic-mirroring/","title":"AWS Traffic Mirroring Strategies","text":"<p>So, you\u2019ve installed Coralogix\u2019s STA and you would like to start analyzing your traffic and getting valuable insights but you\u2019re not sure that you\u2019re mirroring enough traffic or wondering if you might be mirroring too much data and could be getting more for less.</p> <p>In order to be able to detect everything, you have to capture everything and in order to be able to investigate security issues thoroughly, you need to capture every network packet.\u00a0</p> <p>More often than not, the data once labeled irrelevant and thrown away is found to be the missing piece in the puzzle when slicing and dicing the logs in an attempt to find a malicious attacker or the source of an information leak.</p> <p>However, as ideal as this might be, in reality, capturing every packet from every workstation and every server in every branch office is usually impractical and too expensive, especially for larger organizations. Just like in any other field of security, there is no real right or wrong here, it\u2019s more a matter of whether or not it is worth the trade-off in particular cases.</p> <p>There are several strategies that can be taken to minimize the overall cost of the AWS traffic monitoring solution and still get acceptable results. Here are some of the most commonly used strategies:</p>"},{"location":"newoutput/aws-traffic-mirroring/#1-mirror-by-resourceinformation-importance","title":"1. Mirror by Resource/Information Importance","text":"<ol> <li> <p>Guidelines: After mapping out the most critical assets for the organization from a business perspective, configure the mirroring to mirror only traffic to and from the most critical servers and services to be mirrored and analysed. For example, a bank will probably include all SWIFT related servers, for a software company it will probably include all traffic from and to their code repository, release location, etc.</p> </li> <li> <p>Rationale: The rationale behind this strategy is that mirroring the most critical infrastructures will still provide the ability to detect and investigate security issues that can harm the organization the most and will save money by not mirroring the entire infrastructure.</p> </li> <li> <p>Pros: By following this strategy, you will improve the visibility around the organization\u2019s critical assets and should be able to detect issues related to your organization\u2019s \u201ccrown jewels\u201d (if alerts are properly set) and to investigate such issues.</p> </li> <li> <p>Cons: Since this strategy won\u2019t mirror the traffic from non-crown jewels environments, you will probably fail to pinpoint the exact (or even approximate) path the attacker took in order to attack the organization\u2019s \u201ccrown jewels\u201d.</p> </li> <li> <p>Tips: If your organization uses a jump-box to connect to the crown jewels servers and environments, either configure the logs of that jump-box server to be as verbose as possible and store them on Coralogix with a long retention or mirror the traffic to the jumpbox server.</p> </li> </ol>"},{"location":"newoutput/aws-traffic-mirroring/#2-mirror-by-resourceinformation-risk","title":"2. Mirror by Resource/Information Risk","text":"<ol> <li> <p>Guidelines: After mapping out all the paths and services through which the most critical data of the organization is being transferred or manipulated, configure the mirroring to mirror only traffic to and from those services and routes. The main difference between this strategy and the one mentioned above is that it is focused on sensitive data rather than critical services as defined by the organization.</p> </li> <li> <p>Rationale: The rationale behind this strategy is that mirroring all the servers and services that may handle critical information will still provide the ability to detect and investigate security issues that can harm the organization the most and will save money by not mirroring the entire infrastructure.</p> </li> <li> <p>Pros: You will improve the visibility around the critical data across services and environments and you should be able to detect, by configuring the relevant alerts, attempts to modify or otherwise interfere with handling and transferring the organization\u2019s sensitive data</p> </li> <li> <p>Cons: Since this strategy won\u2019t mirror traffic from endpoints connecting to the services and paths used for transmission and manipulation of sensitive data, it might be difficult or even impossible to detect the identity of the attacker and the exact or even approximate path taken by the attacker.</p> </li> <li> <p>Tips: Collecting logs from firewalls and WAFs that control the connections from and to the Internet and sending the logs to Coralogix can help a great deal in creating valuable alerts and by correlating them with the logs from the STA can help identify the attacker (to some extent) and his/her chosen MO (Modus Operandi).</p> </li> </ol>"},{"location":"newoutput/aws-traffic-mirroring/#3-mirror-by-junction-points","title":"3. Mirror by Junction Points","text":"<ol> <li> <p>Guidelines: Mirror the data that passes through the critical \u201cjunction points\u201d such as WAFs, NLBs or services that most of the communication to the organization and its services goes through.</p> </li> <li> <p>Rationale: The idea behind this strategy is that in many organizations there are several \u201cjunction points\u201d such as WAFs, NLBs, or services that most of the communication to the organization and its services goes through. Mirroring this traffic can cover large areas of the organization\u2019s infrastructure by mirroring just a handful of ENIs.</p> </li> <li> <p>Pros: You will save money on mirroring sessions and avoid mirroring some of the data while still keeping a lot of the relevant information.</p> </li> <li> <p>Cons: Since some of the data (e.g. lateral connections between servers and services in the infrastructure) doesn\u2019t necessarily traverse the mirrored junction points, it won\u2019t be mirrored which will make it harder and sometimes even impossible to get enough information on the attack or even to be able to accurately detect it.</p> </li> <li> <p>Tips: Currently, AWS cannot mirror an NLB directly but it is possible and easy to mirror the server(s) that are configured as target(s) for that NLB. Also, you can increase the logs\u2019 verbosity on the non-monitored environments and services and forward them to Coralogix to compensate for the loss in traffic information.</p> </li> </ol>"},{"location":"newoutput/aws-traffic-mirroring/#4-mirror-by-most-common-access-paths","title":"4. Mirror by Most Common Access Paths","text":"<ol> <li> <p>Guidelines: Mirror traffic from every server is based on the expected and allowed set of network protocols that are most likely to be used to access it.</p> </li> <li> <p>Rationale: The idea behind this strategy is that servers that expose a certain service are more likely to be attacked via that same service. For example, an HTTP/S server is more likely to be attacked via HTTP/S than via other ports (at least at the beginning of the attack). Therefore, it makes some sense to mirror the traffic from each server based on the expected traffic to it.</p> </li> <li> <p>Pros: You will be able to save money by mirroring just part of the traffic that arrived or was sent from the organization\u2019s servers. You will be able to detect, by configuring the relevant alerts, some of the indications of an attack on your servers.</p> </li> <li> <p>Cons: Since you mirror only the expected traffic ports, you won\u2019t see unexpected traffic that is being sent or received to/from the server which can be of great value for a forensic investigation.</p> </li> <li> <p>Tips: Depending on your exact infrastructure and the systems and services in use, it might be possible to cover some of the missing information by increasing the services' log verbosity and forwarding them to Coralogix.</p> </li> </ol>"},{"location":"newoutput/aws-traffic-mirroring/#5-mirror-some-of-each","title":"5. Mirror Some of Each","text":"<ol> <li> <p>Guidelines: Randomly select a few instances of each role, region or subnet and mirror their traffic to the STA.</p> </li> <li> <p>Rationale: The idea behind this strategy is that it would be reasonable to assume that the attacker would not know which instances are mirrored and which are not, and also, many tools that are used by hackers are generic and will try to propagate through the network without checking if the instance is mirrored or not, therefore, if the attacker tries to move laterally in the network (manually or automatically), or to scan for vulnerable servers and services, it is very likely that the attacker will hit at least one of the mirrored instances (depending on the percentage of instances you have selected in each network region) and if alerts were properly configured, it will raise an alert.</p> </li> <li> <p>Pros: A high likelihood of detecting security issues throughout your infrastructure, especially the more generic types of malware and malicious activities.</p> </li> <li> <p>Cons: Since this strategy will only increase the chances of detecting an issue, it is still possible that you will \u201crun out of luck\u201d and the attacker will penetrate the machines that were not mirrored. Also, when it comes to investigations it might be very difficult or even impossible to create a complete \u201cstory\u201d based on the partial data that will be gathered.</p> </li> <li> <p>Tips: Since this strategy is based on a random selection of instances, increasing the operating system and auditing logs as well as other services logs and forwarding them to Coralogix for monitoring and analysis can sometimes help in completing the picture in such cases.</p> </li> </ol> <p>In addition to every strategy, you\u2019ll choose or develop, we would also recommend that you mirror the following. These will probably cost you near nothing but can be of great value when you\u2019ll need to investigate an issue or detect security issues (manually and automatically):</p> <ol> <li> <p>All DNS traffic - It is usually the tiniest traffic in terms of bytes/sec and packets/sec but can compensate for most black spots that will result in such trade-offs.</p> </li> <li> <p>Mirror traffic that should never happen - Suppose you have a publicly accessible HTTP server that is populated with new content only by scp from another server. In this case, mirroring should be done on the FTP access to the server, since that FTP is one of the most common methods to push new content to HTTP servers, mirroring FTP traffic to this server and defining an alert on such an issue will reveal attempts to replace the HTTP contents even before they have succeeded. This is just one example, there are many possible examples (ssh or nfs to Windows servers, RDP, SMB, NetBIOS and LDAP connections to Linux servers) you probably can come up with more based on your particular environment. The idea here is that since an attacker doesn\u2019t have any knowledge of the organization\u2019s infrastructure, the attacker will have to first scan hosts to see which operating systems are running and which services they are hosting, for example by trying to connect via SMB (a protocol mostly used by Windows computers) and if there is a response, the attacker would assume that it is Windows. Of course, the same applies to Linux.</p> </li> </ol>"},{"location":"newoutput/aws-traffic-mirroring/#cloud-access","title":"Cloud Access","text":"<p>In cloud infrastructures, instances and even the mirroring configuration are accessible via the Internet and therefore theoretically allows an attacker to find out whether an instance is mirrored and to act accordingly. Because of this, it is even more important to make sure that access to the cloud management console is properly secured and monitored.</p>"},{"location":"newoutput/aws-vpc-flow-logs-collection-options/","title":"AWS VPC Flow Logs: Data Collection Options","text":"<p>VPC Flow Logs is an AWS feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Send your VPC Flow Logs to Coralogix to enhance your data management, analysis, and monitoring capabilities.</p> <p>Coralogix provides multiple methods to collect your AWS VPC Flow Logs.</p>"},{"location":"newoutput/aws-vpc-flow-logs-collection-options/#custom-coralogix-experience","title":"Custom Coralogix Experience","text":"<p>Use any of our customized options to allow Coralogix to ingest the VPC Flow Logs stored in your S3 bucket and process them for further analysis and monitoring.</p> <ul> <li> <p>Automated Integration Packages (Recommended). Deploy this integration using our two-step, automated integration packages.</p> </li> <li> <p>Serverless Application Repository (SAR). Send your logs to Coralogix using our VPC Flow Logs integration via our AWS serverless application repository.</p> </li> <li> <p>Terraform. Install and manage this integration with AWS services as modules in your infrastructure code using our AWS VPC Flow Logs Terraform module.</p> </li> </ul>"},{"location":"newoutput/aws-vpc-flow-logs-collection-options/#open-source-tools","title":"Open Source Tools","text":"<p>To reduce costs, you have the option of sending us your VPC Flow Logs using our OpenTelemetry integration.</p>"},{"location":"newoutput/aws-vpc-flow-logs-collection-options/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/azure-activity-and-audit-logs-with-filebeat/","title":"Microsoft Azure Activity and Audit Logs with FileBeat","text":"<p>For us to be able to get audit logs from Azure, we are going to use the FileBeat Module.</p> <p>Azure audit events are sent into an EventHub, from which FileBeat pulls the logs and sends them to Coralogix.</p>"},{"location":"newoutput/azure-activity-and-audit-logs-with-filebeat/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>The logs have to be exported first to the event hubs https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create-kafka-enabled</p> </li> <li> <p>to export activity logs to event hubs users can follow the steps here https://docs.microsoft.com/en-us/azure/azure-monitor/platform/activity-log-export</p> </li> <li> <p>to export audit and sign-in logs to event hubs users can follow the steps here https://docs.microsoft.com/en-us/azure/active-directory/reports-monitoring/tutorial-azure-monitor-stream-logs-to-event-hub</p> </li> </ul>"},{"location":"newoutput/azure-activity-and-audit-logs-with-filebeat/#filebeat-configuration","title":"FileBeat Configuration","text":"<p>We need a Filebeat configured for using Coralogix as an output. Please follow this documentation if needed.</p> <p>We will enable the Azure plugin in FileBeat:</p> <pre><code>filebeat modules enable azure\u00a0\n</code></pre> <p>The module contains the following filesets:</p> <pre><code>activitylogs\n</code></pre> <p>Will retrieve Azure activity logs. Control-plane events on Azure Resource Manager resources. Activity logs provide insight into the operations that were performed on resources in your subscription.</p> <p><code>platformlogs</code></p> <p>Will retrieve Azure platform logs. Platform logs provide detailed diagnostic and auditing information for Azure resources and the Azure platform they depend on.</p> <p><code>signinlogs</code></p> <p>Will retrieve Azure Active Directory sign-in logs. The sign-ins report provides information about the usage of managed applications and user sign-in activities.</p> <p><code>auditlogs</code></p> <p>Will retrieve Azure Active Directory audit logs. The audit logs provide traceability through logs for all changes done by various features within Azure AD. Examples of audit logs include changes made to any resources within Azure AD like adding or removing users, apps, groups, roles, and policies.</p> <p>We will need to edit the configuration file, normally located in /etc/filebeat/modules.d/azure.yml</p> <pre><code>- module: azure\nactivitylogs:\nenabled: true\nvar:\n# eventhub name containing the activity logs, overwrite he default value if the logs are exported in a different eventhub\neventhub: \"insights-operational-logs\"\n# consumer group name that has access to the event hub, we advise creating a dedicated consumer group for the azure module\nconsumer_group: \"$Default\"\n# the connection string required to communicate with Event Hubs, steps to generate one here https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-get-connection-string\nconnection_string: \"\"\n# the name of the storage account the state/offsets will be stored and updated\nstorage_account: \"\"\n# the storage account key, this key will be used to authorize access to data in your storage account\nstorage_account_key: \"\"\nplatformlogs:\nenabled: false\n#\u00a0 var:\n#\u00a0 \u00a0 eventhub: \"\"\n#\u00a0 \u00a0 consumer_group: \"$Default\"\n#\u00a0 \u00a0 connection_string: \"\"\n#\u00a0 \u00a0 storage_account: \"\"\n#\u00a0 \u00a0 storage_account_key: \"\"\nauditlogs:\nenabled: false\n# \u00a0 var:\n# \u00a0 \u00a0 eventhub: \"insights-logs-auditlogs\"\n# \u00a0 \u00a0 consumer_group: \"$Default\"\n# \u00a0 \u00a0 connection_string: \"\"\n# \u00a0 \u00a0 storage_account: \"\"\n# \u00a0 \u00a0 storage_account_key: \"\"\nsigninlogs:\nenabled: false\n# \u00a0 var:\n# \u00a0 \u00a0 eventhub: \"insights-logs-signinlogs\"\n# \u00a0 \u00a0 consumer_group: \"$Default\"\n# \u00a0 \u00a0 connection_string: \"\"\n# \u00a0 \u00a0 storage_account: \"\"\n# \u00a0 \u00a0 storage_account_key: \"\"\n</code></pre> <p>We will need to create a storage account where Filebeat will save the position file. That Storage Account will be used in storage_account. Also, we need to retrieve the access key.</p> <p>Now we only need to restart the FileBeat service.</p>"},{"location":"newoutput/azure-activity-and-audit-logs-with-filebeat/#filebeat-azure-input-type","title":"Filebeat Azure input type","text":"<p>Please note- in case the module dosent work properly, please follow this guidance</p> <p>First of all, please disable the Azure module by using the command:</p> <pre><code>filebeat modules disable azure\u00a0\n</code></pre> <p>After disabling the module, it's mandatory to specify the input type in the Filebeat.yml. The input type should look:</p> <pre><code>filebeat.inputs:\n- type: azure-eventhub\n  eventhub: \"insights-operational-logs\"\n  consumer_group: \"$Default\"\n  connection_string: \"Endpoint=sb://.....\"\n  storage_account: \"...\"\n  storage_account_key: \".....\"\n</code></pre> <p>Fill in the values according to Azure, please note that after changing the config, its needed to restart the filebeat service.</p>"},{"location":"newoutput/azure-activity-logs/","title":"Azure Activity Logs","text":"<p>Collect Azure Activity logs and submit them to Coralogix for seamless integration.</p>"},{"location":"newoutput/azure-activity-logs/#overview","title":"Overview","text":"<p>Activity logs are a form of audit log that provide insight into the operations performed on each Azure resource in the subscription from the outside, known as the management plane. This can include the creation or deletion of resources, as well as updates to existing services.</p> <p>This tutorial explains how to collect Azure Activity logs and send them to Coralogix. To do so, you will need to configure an Activity log export and use our Event Hub integration to collect and submit the logs to the Coralogix platform.</p>"},{"location":"newoutput/azure-activity-logs/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Azure account with an active subscription</p> </li> <li> <p>EventHub Namespace [Note: If your EventHub has restricted public access you will need to enable VNet support using these optional configuration steps.]</p> </li> </ul>"},{"location":"newoutput/azure-activity-logs/#activity-log-export","title":"Activity Log Export","text":"<p>STEP 1. To configure Activity Log exports, navigate to your Subscription &gt; Activity log.</p> <p>STEP 2. Click Export Activity Logs.</p> <p></p> <p>STEP 3. In the next window, click + Add Diagnostic Setting.</p> <p>STEP 4. In the Diagnostic Setting, select your desired Categories and configure the Destination details to submit entries to your existing Event Hub.</p> <p></p>"},{"location":"newoutput/azure-activity-logs/#process-event-hub","title":"Process Event Hub","text":"<p>Now that we have your Activity Log entries being exported to your Event Hub, you\u2019ll need to deploy the Azure Event Hub integration to collect and submit the messages to the Coralogix platform.</p> <p>To do so, you can deploy via ARM template or Terraform:</p> <ul> <li> <p>ARM</p> <ul> <li> <p>Azure Event Hub ARM</p> </li> <li> <p>ARM Event Hub Integration Package</p> </li> </ul> </li> <li> <p>Terraform</p> <ul> <li>Azure Event Hub Terraform</li> </ul> </li> </ul>"},{"location":"newoutput/azure-activity-logs/#additional-resources","title":"Additional Resources","text":"DocumentationIntroduction to Microsoft Azure"},{"location":"newoutput/azure-activity-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/azure-devops-server-version-tags/","title":"Microsoft Azure DevOps Server Version Tags","text":""},{"location":"newoutput/azure-devops-server-version-tags/#integrate-coralogix-with-your-azure-devops-server-deployment-pipelines","title":"Integrate Coralogix with your Azure DevOps Server deployment pipelines","text":"<p>Coralogix supports integration with <code>Azure DevOps Server</code> webhooks, use webhooks to inform Coralogix when a new build is issued.</p> <ol> <li> <p>Sign in to Azure DevOps Server using your user credentials.</p> </li> <li> <p>Select your team project.</p> </li> <li> <p>Click settings.</p> </li> <li> <p>Go to Service hooks.</p> </li> <li> <p>Click add new service hooks.</p> </li> <li> <p>Select\u00a0\u201cWeb Hooks\"</p> </li> <li> <p>Select \u201cBuild Completed\u201d as the trigger.</p> </li> <li> <p>Select Build Status \u201cSucceeded\u201d \u00a0and click\u00a0Next.</p> <ol> <li>You may choose any filter as you desire.</li> </ol> </li> <li> <p>Add URL: https://webapi.coralogixstg.wpengine.com/api/v1/external/tfs?application=YOUR_APPLICATION_NAME&amp;subsystem=YOUR_SUBSYSTEM_NAME&amp;name=YOUR_TAG_NAME  </p> <p>If your account ends with .us you want to put .us in the URL. 1. Application - your desired application name - please use an identical name to correspond with your logs application name. -\u00a0must</p> <ol> <li>Subsystem - your desired system name - please use an identical name to correspond with your logs subsystem name. you can add more than 1 subsystem name separated by a comma.</li> </ol> </li> <li> <p>Headers - include headers with the following properties:</p> <ol> <li> <p>Authorization: Bearer  <p>The API key should be taken from\u00a0<code>Data\u00a0Flow\u00a0**&gt;**\u00a0API\u00a0Keys\u00a0**&gt;**\u00a0Alerts,\u00a0Rules\u00a0and\u00a0Tags\u00a0API\u00a0Key</code></p> <p></p> <p></p>"},{"location":"newoutput/azure-eventhub-trigger-function/","title":"Event Hub: Microsoft Azure Resource Manager (ARM)","text":"<p>Azure Event Hubs can ingest large volumes of data from various sources, such as applications, devices, and sensors. For monitoring purposes, you can configure your systems or applications to send relevant monitoring data as events to an Event Hub.</p> <p>Coralogix provides seamless integration with Azure cloud, allowing you to send your logs from anywhere and parse them according to your needs. Deploy the Event Hub integration to send Coralogix your JSON-formatted queue messages using the ARM template below.</p>"},{"location":"newoutput/azure-eventhub-trigger-function/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure account with an active subscription</li> </ul>"},{"location":"newoutput/azure-eventhub-trigger-function/#azure-resource-manager-template-deployment","title":"Azure Resource Manager Template Deployment","text":"<p>Sign into your Azure account and deploy the Event Hub integration by clicking here.</p> <p></p>"},{"location":"newoutput/azure-eventhub-trigger-function/#fields","title":"Fields","text":"FieldDescriptionSubscriptionAzure subscription within which you wish to deploy the integration.Must be the same as the monitored Event Hub namespaceResource GroupResource group in which you wish to deploy the integrationCoralogix RegionRegion associated with your Coralogix domainCustom URLCustom URL associated with your Coralogix account. Ignore if you do not have a custom URL.Coralogix Private KeyCoralogix\u00a0Send-Your-Data API keyCoralogix ApplicationMandatory metadata field sent with each log and helps to classify itCoralogix SubsystemMandatory metadata field sent with each log and helps to classify itEvent Hub Resource GroupName of the resource group that contains the Event HubEvent Hub NamespaceName of the Event Hub namespaceEvent Hub Instance NameName of the Event Hub instance to be monitoredEvent Hub Shared Access Policy NameName of the shared access policy of the Event Hub namespaceFunction App Service Plan TypeType of service plan to use for the integration.Consumption is cheapest with support for 'public' Event Hubs. Use Premium if you need to use VNet to configure access to restricted Event Hubs. <p>Notes:</p> <ul> <li> <p>The Event Hub integration allows parsing of queue messages in JSON format.</p> </li> <li> <p>Other format messages will not be processed and submitted to the Coralogix platform.</p> </li> </ul>"},{"location":"newoutput/azure-eventhub-trigger-function/#optional-configuration-options","title":"Optional Configuration Options","text":"<p>If your Event Hub has restricted access, review this\u00a0optional configuration\u00a0documentation\u00a0to learn about VNet support options.</p>"},{"location":"newoutput/azure-eventhub-trigger-function/#additional-resources","title":"Additional Resources","text":"GithubEvent Hub DocumentationTerraformTerraform Modules for Microsoft Azure Event HubMicrosoft Azure Functions Manual IntegrationsBlob StorageQueue StorageMicrosoft Azure Functions Serverless IntegrationCoralogix Azure serverless integration deployment container"},{"location":"newoutput/azure-eventhub-trigger-function/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/azure-metrics/","title":"Azure Metrics","text":"<p>The Coralogix Azure Metrics integration offers a simple and easy way to ingest Azure metrics into Coralogix.</p>"},{"location":"newoutput/azure-metrics/#overview","title":"Overview","text":"<p>Azure Monitor serves as the observability core within the Azure Cloud ecosystem. It plays a pivotal role by aggregating data from diverse origins, encompassing logs and metrics derived from Azure infrastructure and resources, custom applications, and agents running on virtual machines.</p> <p>Azure metrics are retrieved from Azure using the Azure Monitor REST API, which is a widely used API service in Azure that allows customers to gain insights into Azure resources.</p> <p>A valid Azure \"Application Registration\" featuring the \"Monitoring Reader\" role is required to collect metrics. This can be created before or during the integration process.</p>"},{"location":"newoutput/azure-metrics/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure Monitor must be enabled on your instance</li> </ul>"},{"location":"newoutput/azure-metrics/#azure-metrics-integration-deployment","title":"Azure Metrics Integration Deployment","text":"<p>STEP 1. From your Coralogix toolbar, go to Data Flow &gt; Integrations.</p> <p>STEP 2. From the Integrations section, select Azure Metrics.</p> <p></p> <p>STEP 3. Click ADD NEW.</p> <p></p> <p>STEP 4. If you don\u2019t have an App Registration set up in your Azure account, click GO TO AZURE PORTAL, then follow the steps below to apply the Monitoring Reader role to your\u00a0App Registration:</p> <p>STEP 4a. Log in to Azure Portal and select the subscription.</p> <p>STEP 4b. Select\u00a0Access Control (IAM)\u00a0from the left menu.</p> <p></p> <p>STEP 4c. Select Add role assignment from the top left + Add button from the Access Control panel.</p> <p>STEP 4d. In the Add role assignment panel, search for\u00a0Monitoring Reader\u00a0and select it.</p> <p></p> <p>STEP 4e. Under Members, select\u00a0User, group, or service principal\u00a0and then click the\u00a0+ Select members link.</p> <p>STEP 4f. Search for your App Registration in the right-hand panel and add this user to the list.</p> <p></p> <p>After review and assignment, you should see your\u00a0App\u00a0with\u00a0Monitoring Reader\u00a0permissions under the\u00a0Role Assignments\u00a0tab.</p> <p></p> <p>STEP 5. Go back to the Coralogix Azure Metrics integration tab and click NEXT.</p> <p></p> <p>STEP 6. Define your Settings.</p> <ul> <li> <p>Integration Name. A name for your integration (auto-filled with the default name).</p> </li> <li> <p>Application Name. [Optional] Application name, if present, will be the value of the \u201capplication name\u201d label of every collected metric (auto-filled with the default name).</p> </li> <li> <p>Application Label Selection. [Optional] select labels that will be used to create the application name. The first label value which matches a metric label will be used as the application name. For example, given three application labels, if the first does not match any metric labels, the value of the second label will be used as the application name.</p> </li> <li> <p>Subsystem Name. [Optional] The name of the subsystem for your application (auto-filled with the default name).</p> </li> <li> <p>Subsystem Label Selection. [Optional] select labels that will be used to create the subsystem name. The first label value which matches a metric label will be used as the subsystem name. For example, given three subsystem labels, if the first does not match any metric labels, the value of the second label will be used as the subsystem name.</p> </li> <li> <p>Tenant ID. Copy and paste from the Azure Application Registration.</p> </li> <li> <p>Subscription ID. Copy and paste from the Azure Application Registration.</p> </li> <li> <p>Client ID. Copy and paste from the Azure Application Registration.</p> </li> <li> <p>Client Secret. Copy and paste from the Certificates &amp; Secrets section of the Azure Application Registration.</p> </li> </ul> <p>STEP 7. Click NEXT.</p> <p>STEP 8. Define Filtering Options.</p> <p></p> <ul> <li> <p>Resource Types Filter / Select Metric Namespace. Select resource types from the dropdown list to filter the list of metric namespaces to show in the next step. If none are selected, all metric namespaces will be available.</p> </li> <li> <p>Resource Name Filter. Specify a Regular Expression (RegEx) to serve as a filter based on the name of the Azure resource being monitored. Doing so sets a rule that specifies the pattern the resource names should follow. This enables you to selectively include or exclude resources based on the defined pattern, helping you focus on the specific subset of resources that meet your monitoring criteria.</p> </li> </ul> <p>STEP 9. Select from the dropdown list which Metric Namespaces to bring into Coralogix. If none are selected, all metric namespaces will be brought in.</p> <p>STEP 10. Click NEXT.</p> <p></p>"},{"location":"newoutput/azure-metrics/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/azure-platform-monitoring/","title":"Azure Platform Monitoring","text":"<p>Microsoft Azure platform monitoring focuses on capturing platform logs - Microsoft Entra ID, Activity, and Resource logs - from various components within your environment.</p>"},{"location":"newoutput/azure-platform-monitoring/#overview","title":"Overview","text":"<p>Azure platform logs, automatically generated, provide detailed diagnostic and auditing information for Azure resources and the Azure platform they depend on. They are divided as follows:</p> <ul> <li> <p>Microsoft Entra ID logs, which capture activity at the authentication layer</p> </li> <li> <p>Azure Activity logs, which capture activity at the subscription layer</p> </li> <li> <p>Resource logs, which capture activity at the resource layer</p> </li> </ul>"},{"location":"newoutput/azure-platform-monitoring/#microsoft-entra-id-logs","title":"Microsoft Entra ID Logs","text":"<p>Microsoft Entra ID logs (previously Azure Active Directory logs) capture all the actions performed against your Active Directory domain. This includes the history of sign-in activity and an audit trail of changes made in Azure AD for a particular tenant. This type of log is focused on the Authentication layer of Azure. Find out more here.</p>"},{"location":"newoutput/azure-platform-monitoring/#activity-logs","title":"Activity Logs","text":"<p>Activity logs provide insight into the operations performed\u00a0on\u00a0each Azure resource in the subscription from the outside, known as the\u00a0management plane. This can include the creation or deletion of resources or updates to existing services. There's a single activity log for each Azure subscription. Find out more here.</p>"},{"location":"newoutput/azure-platform-monitoring/#resource-logs","title":"Resource Logs","text":"<p>Resource logs [previously referred to as diagnostic logs] capture resource-specific audit information, providing insight into operations performed within an Azure resource. This is known as the data plane. Examples include a connection made to a PostgreSQL server or when a Blob is created, read, or deleted from a storage account. The contents of resource logs vary according to the Azure service and resource type. Find out more here.</p>"},{"location":"newoutput/azure-platform-monitoring/#additional-resources","title":"Additional Resources","text":"DocumentationIntroduction to Microsoft Azure"},{"location":"newoutput/azure-platform-monitoring/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up. Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/azure-resource-logs/","title":"Azure Resource Logs","text":"<p>Collect Azure Resource logs and send them to Coralogix for seamless integration.</p>"},{"location":"newoutput/azure-resource-logs/#overview","title":"Overview","text":"<p>Resource logs [previously referred to as diagnostic logs] capture resource-specific audit information, providing insight into operations performed within an Azure resource. This is known as the data plane. Examples include a connection made to a PostgreSQL server or when a Blob is created, read, or deleted from a storage account. The contents of resource logs vary according to the Azure service and resource type.</p> <p>This tutorial demonstrates how to collect Azure resource audit logs and submit them to Coralogix. To do so, you will need to configure your resource\u2019s Diagnostic Settings and leverage our Event Hub integration for the collection and submission of those logs to the Coralogix platform.</p>"},{"location":"newoutput/azure-resource-logs/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Azure account with an active subscription</p> </li> <li> <p>EventHub Namespace [Note: If your EventHub has restricted public access you will need to enable VNet support using these optional configuration steps.]</p> </li> </ul>"},{"location":"newoutput/azure-resource-logs/#resource-audit-log-export","title":"Resource Audit Log Export","text":"<p>STEP 1. To configure resource audit logs, navigate to your desired resource.</p> <p>STEP 2. Under Monitoring, click Diagnostic Settings.</p> <p>STEP 3. Click + Add diagnostic setting.</p> <p>STEP 4. In the Diagnostic Setting window, select the Audit category. Configure the Destination Details to submit entries to your existing Event Hub.</p> <p></p> <p>Notes:</p> <ul> <li> <p>Not all resource types have audit logs.</p> </li> <li> <p>In this example, we\u2019re looking at a PostgreSQL database.</p> </li> </ul>"},{"location":"newoutput/azure-resource-logs/#process-event-hub","title":"Process Event Hub","text":"<p>Now that your resource audit log entries are being exported to your Event Hub, you\u2019ll need to deploy the Azure Event Hub integration to collect and submit the messages to the Coralogix platform.</p> <p>To do so, you can deploy via ARM template or Terraform:</p> <ul> <li> <p>ARM</p> <ul> <li> <p>Azure Event Hub ARM</p> </li> <li> <p>ARM Event Hub Integration Package</p> </li> </ul> </li> <li> <p>Terraform</p> <ul> <li>Azure Event Hub Terraform</li> </ul> </li> </ul>"},{"location":"newoutput/azure-resource-logs/#additional-resources","title":"Additional Resources","text":"DocumentationIntroduction to Microsoft Azure"},{"location":"newoutput/azure-resource-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/azure-resource-manager-arm-integration-packages/","title":"Azure Resource Manager (ARM) Integration Packages","text":"<p>Access our Azure Resource Manager (ARM) Integration Packages to automatically deploy our various Microsoft Azure integrations. Extend your platform capabilities with packages and sources, without expending unnecessary time and resources. Gain insights into role, user, group and directory management, successful and failed sign-in events, and application management data that helps you understand your users\u2019 experience and immediately troubleshoot any errors.</p>"},{"location":"newoutput/azure-resource-manager-arm-integration-packages/#overview","title":"Overview","text":"<p>Coralogix offers Azure Resource Manager (ARM) custom template deployments to build your Azure log and metric pipelines using our Azure Resources Integration Packages. Each template creates a unique dedicated resource group, to which Azure Monitor streams logs or metrics. Additionally, an Azure function is created for sending monitoring data to Coralogix, and storage accounts are established to which the function writes its own log messages about successful and failed transmissions.</p> <p>Choose from any of our ARM custom template deployments:</p> <ul> <li> <p>Event Hub</p> </li> <li> <p>Blob Storage</p> </li> <li> <p>Queue Storage</p> </li> <li> <p>Diagnostic Data</p> </li> </ul>"},{"location":"newoutput/azure-resource-manager-arm-integration-packages/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure account with an active subscription</li> </ul>"},{"location":"newoutput/azure-resource-manager-arm-integration-packages/#deploy-the-arm-integration-package","title":"Deploy the ARM Integration Package","text":"<p>STEP 1. In your Coralogix menu bar, navigate to Data Flow &gt; Integrations. View the list of available integrations in Coralogix Apps.</p> <p></p> <p>STEP 2. Click CONNECT in the ARM deployment of choice.</p> <p>STEP 3. View application details in the App Overview. For more information, click on the links under Integration Details. You will also see a list of extension packages available for the integration.</p> <p></p> <p>STEP 4. Click + ADD NEW.</p> <p>STEP 5. Define your Settings.</p> <p></p> Common Field Description API Key Coralogix\u00a0Send-Your-Data API key Application Name Mandatory\u00a0metadata field sent with each log and helps to classify it Subsystem Name Mandatory\u00a0metadata field\u00a0sent with each log and helps to classify it <p>STEP 6. Click NEXT.</p>"},{"location":"newoutput/azure-resource-manager-arm-integration-packages/#configure-the-arm-template","title":"Configure the ARM Template","text":"<p>STEP 1. Click OPEN ARM.</p> <p>STEP 2. You will rerouted to your Microsoft Azure account in a new tab. Configure the custom ARM template by filling in the relevant fields.</p> <p></p>"},{"location":"newoutput/azure-resource-manager-arm-integration-packages/#common-fields","title":"Common Fields","text":"Field Description Subscription Azure subscription within which you wish to deploy the integration. Must be the same as the monitored Event Hub namespace Coralogix Region Region associated with your Coralogix\u00a0domain Custom URL Custom URL associated with your Coralogix account. Ignore if you do not have a custom URL."},{"location":"newoutput/azure-resource-manager-arm-integration-packages/#event-hub-diagnostic-data","title":"Event Hub / Diagnostic Data","text":"Field Description Event Hub Resource Group Name of the resource group that contains the Event Hub Event Hub Namespace Name of the Event Hub namespace Event Hub Instance Name Name of the Event Hub instance to be monitored Event Hub Shared Access Policy Name Name of the shared access policy of the Event Hub namespace Function App Service Plan Type Type of service plan to use for the integration. Consumption is cheapest with support for \u2018public\u2019 Event Hubs. Use Premium if you need to use VNet to configure access to restricted Event Hubs."},{"location":"newoutput/azure-resource-manager-arm-integration-packages/#blob-storage","title":"Blob Storage","text":"Field Description Storage Account Name Name of the storage account containing the Blob container. Must be Storagev2 (general purpose v2) type. Storage Account Resource Group Resource Group name of the storage account containing the Blob container to be monitored Blob Container Name Name of the Blob container to be monitored Event Grid System Topic Name Name of a pre-existing Event Grid system topic for the storage account containing the Blob container. Leave as \u2018New\u2019 to create one. Newline Pattern Newline pattern expected within the Blob storage documents Prefix Filter Prefix filter to apply to the Blob container. Use \u2018NoFilter\u2019 to refrain from filtering by prefix. Wildcards are not allowed. Use the following format:\u00a0/subfolder1/subfolder2/. Suffix Filter Suffix filter to apply to the Blob container. Use \u2018NoFilter\u2019 to refrain from filtering by suffix. Wildcards are not allowed. Use the following format:\u00a0.log. Function App Service Plan Type Type of service plan for the function app. Choose \u2018Premium\u2019 if you need vNet support."},{"location":"newoutput/azure-resource-manager-arm-integration-packages/#queue-storage","title":"Queue Storage","text":"Field Description Storage Account Name Name of the storage account containing the storage queue. Must be of StorageV2 (general purpose V2) type. Storage Account Resource Group Name of the storage account resource group containing the storage queue to be monitored Storage Queue Name Name of the Storage Queue to be monitored Function App Service Plan Type Type of the function app service plan. Choose \u2018Premium\u2019 if you need VNet support. <p>STEP 3. Click Next: Review + create &gt;.</p> <p>STEP 4. Review your configuration settings. If correct, click Create.</p>"},{"location":"newoutput/azure-resource-manager-arm-integration-packages/#validation","title":"Validation","text":"<p>STEP 1. Revert back to Coralogix Apps in the previous tab and click COMPLETE to ensure your deployment is successful. This triggers a test to verify the deployment, the result of which is displayed as either Active or Failed.</p> <p></p> <p>STEP 2. Deploy the extension package of your choice to complement your integration needs.</p> <p>STEP 3. Once the verification process is complete and you have deployed your extension package, view your logs in your Coralogix dashboard.</p>"},{"location":"newoutput/azure-resource-manager-arm-integration-packages/#remove-modify-the-arm-integration-package","title":"Remove / Modify the ARM Integration Package","text":"<p>To remove / modify this integration, you will need to delete the dedicated resource group created during deployment.</p> <p>STEP 1. In your Coralogix menu bar, navigate to Data Flow &gt; Extensions. Access the ARM integration package.</p> <p>STEP 2. In the opening screen, click on the More Options icon [\u2026] under Actions. Select DELETE.</p> <p></p> <p>STEP 3. Click DELETE INTEGRATION.</p> <p></p> <p>STEP 4. You will rerouted to your Microsoft Azure account. Select the relevant resource group. Click Delete resource group.</p> <p></p> <p>STEP 5. Click COMPLETE in your Coralogix UI to revert back to the App Overview.</p> <p></p>"},{"location":"newoutput/azure-resource-manager-arm-integration-packages/#additional-resources","title":"Additional Resources","text":"DocumentationIntroduction to Microsoft AzureExtension Packages"},{"location":"newoutput/azure-resource-manager-arm-integration-packages/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/azure-status-logs/","title":"Microsoft Azure Status Logs","text":"<p>Collect your Microsoft Azure Status log messages in the Coralogix platform using our automatic Contextual Data Integration Package. The package lets you enable Azure Status log data ingestion to allow you to see status updates on public Azure incidents.</p>"},{"location":"newoutput/azure-status-logs/#overview","title":"Overview","text":"<p>Microsoft Azure, commonly referred to as Azure, is a comprehensive cloud computing platform and infrastructure offered by Microsoft. It provides a vast array of cloud services that encompass computing, storage, databases, networking, analytics, artificial intelligence, Internet of Things (IoT), and more. Azure allows organizations to build, deploy, and manage applications and services across a global network of data centers, offering scalability, flexibility, and reliability. With a wide range of tools, frameworks, and languages, Azure enables businesses to accelerate digital transformation, enhance productivity, and innovate by harnessing the power of cloud technology and integrating seamlessly with existing IT environments.</p> <p>Forwarding your Azure status logs to Coralogix simplifies log consolidation, enhances monitoring capabilities, and streamlines issue resolution. By directing Azure status logs to Coralogix, you gain a unified perspective on your Azure infrastructure's status, enabling swift identification of irregularities, proactive problem-solving, and informed decision-making. This integration empowers teams to optimize resource allocation, bolster system dependability, and uphold operational efficiency, utilizing Coralogix's analytical, alerts, and visualization features to extract actionable insights from Azure status logs and ensure a resilient cloud environment.</p>"},{"location":"newoutput/azure-status-logs/#get-started","title":"Get Started","text":"<p>STEP 1.\u00a0In your navigation pane, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 2.\u00a0In the Contextual Data section, select Azure and click\u00a0ADD.</p> <p></p> <p>STEP 3. Click Enable Azure Data Ingestion.</p>"},{"location":"newoutput/azure-status-logs/#example-log","title":"Example Log","text":"<pre><code>{\n    \"source_system\": \"azure\",\n    \"azure\": {\n        \"uri\": \"azure-front-door-connectivity-issues-and-increased-latency-applying-mitigation\",\n        \"title\": \"Azure Front Door - Connectivity issues and increased latency - Applying Mitigation\",\n        \"date\": \"2021-07-12T16:00:05Z\",\n        \"description\": \"&amp;lt;p&amp;gt;Starting approximately 16:00 UTC, on 12 Jul 2021, a subset of customers primarily in US, Canada region may experience connection timeouts and increased latency to Azure front door and Azure CDN services. Retries may be successful on the resources\\/environments experiencing issues.&amp;lt;\\/p&amp;gt;&amp;lt;p&amp;gt;&amp;lt;br&amp;gt;&amp;lt;\\/p&amp;gt;&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Current Status&amp;lt;\\/strong&amp;gt;: Initial investigation indicates an increase in traffic in US region, which exceeded the available capacity in 3 edge locations in the US. We have completed&amp;nbsp;rebalancing traffic to other available edge locations nearby. Customers should see signs of successful connections. We are monitoring the service health to ensure full recovery. The next update will be in 60 minutes, or as events warrant.&amp;lt;\\/p&amp;gt;\",\n        \"categories\": [\"Azure Front Door\", \"Content Delivery Network\", \"Public Non-Regional\"],\n        \"tags\": []\n    }\n}\n\n</code></pre>"},{"location":"newoutput/azure-status-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/bitbucket-data-ingestion/","title":"Bitbucket Data Ingestion","text":"<p>Collect your Bitbucket messages in the Coralogix platform using our automatic Contextual Data Integration Package. The package automatically generates a URL to be used when creating a Bitbucket webhook.</p>"},{"location":"newoutput/bitbucket-data-ingestion/#overview","title":"Overview","text":"<p>Bitbucket is a web-based platform that facilitates version control and collaboration for software development projects, owned by Atlassian. Bitbucket offers Git and Mercurial repositories, allowing teams to efficiently manage and track changes to their codebase. It provides features like pull requests, code reviews, issue tracking, and continuous integration, enabling developers to collaborate seamlessly, maintain code quality, and streamline the development workflow. Bitbucket supports both public and private repositories, making it a versatile choice for individual developers and teams of all sizes.</p> <p>Send your Bitbucket logs to Coralogix to enhance log consolidation, strengthen monitoring capabilities, and streamline issue resolution. By directing Bitbucket logs into Coralogix, you can achieve a comprehensive view of your code repository activities, enabling swift anomaly detection, proactive debugging, and informed decision-making. This integration empowers teams to optimize development workflows, fortify system reliability, and maintain operational effectiveness, leveraging Coralogix's analytics, alerts, and visualization tools to extract valuable insights from Bitbucket logs and ensure a productive and resilient software development environment.</p>"},{"location":"newoutput/bitbucket-data-ingestion/#get-started","title":"Get Started","text":"<p>STEP 1.\u00a0In your navigation pane, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 2.\u00a0In the Contextual Data section, select Bitbucket and click\u00a0+ ADD.</p> <p></p> <p>STEP 3. Click ADD NEW.</p> <p>STEP 4.\u00a0Fill in the Integration Details:</p> <ul> <li> <p>Name.\u00a0Name your integration.</p> </li> <li> <p>Your API Key. Click\u00a0CREATE NEW KEY\u00a0to generate an API Key and name it.</p> </li> <li> <p>Application Name and Subsystem Name. Enter an\u00a0application and subsystem name.</p> </li> </ul> <p></p> <p>STEP 5.\u00a0Click\u00a0GENERATE URL. The URL for the integration will be automatically created. Use this when creating a Bitbucket webhook.</p> <p></p>"},{"location":"newoutput/bitbucket-data-ingestion/#create-a-bitbucket-webhook","title":"Create a Bitbucket Webhook","text":"<p>Create a Bitbucket webhook using your URL.</p> <p>STEP 1.\u00a0Log in to your Bitbucket account.</p> <p>STEP 2. If you have not yet done so, create a project.</p> <p>STEP 3. On the left top corner click on your repository. Select the repository from which you would like to collect events.</p> <p></p> <p>STEP 4.\u00a0Click Repository Settings in the new screen.</p> <p></p> <p>STEP 5. Click the Webhooks tab in the left hand menu.</p> <p>STEP 6. Complete the Add new webhook form using the URL generated by Coralogix and select the event for which you want to trigger the collection.</p> <p></p> <p>STEP 7. Click Save.</p>"},{"location":"newoutput/bitbucket-data-ingestion/#example-log","title":"Example Log","text":"<pre><code>{\n    \"source_system\": \"bitbucket\",\n    \"bitbucket\": {\n        \"changes\": {\n            \"description\": {\n                \"new\": \"My test repo\",\n                \"old\": \"My first repo\"\n            }\n        },\n        \"repository\": {\n            \"scm\": \"git\",\n            \"website\": null,\n            \"uuid\": \"{0a519024-7bc7-49f8-add8-2fbe1d2cf66c}\",\n            \"links\": {\n                \"self\": {\n                    \"href\": \"https://api.bitbucket.org/2.0/repositories/test_user/testrepo\"\n                },\n                \"html\": {\n                    \"href\": \"https://bitbucket.org/test_user/testrepo\"\n                },\n                \"avatar\": {\n                    \"href\": \"https://bytebucket.org/ravatar/%7B0a519024-7bc7-49f8-add8-2fbe1d2cf66c%7D?ts=default\"\n                }\n            },\n            \"project\": {\n                \"links\": {\n                    \"self\": {\n                        \"href\": \"https://api.bitbucket.org/2.0/workspaces/test_user/projects/AD\"\n                    },\n                    \"html\": {\n                        \"href\": \"https://bitbucket.org/test_user/workspace/projects/AD\"\n                    },\n                    \"avatar\": {\n                        \"href\": \"https://bitbucket.org/account/user/test_user/projects/AD/avatar/32?ts=1623065966\"\n                    }\n                },\n                \"type\": \"project\",\n                \"name\": \"TestProject\",\n                \"key\": \"AD\",\n                \"uuid\": \"{12345678-b705-40c8-af4c-7905c047d73b}\"\n            },\n            \"full_name\": \"test_user/testrepo\",\n            \"owner\": {\n                \"display_name\": \"John Smith\",\n                \"uuid\": \"{1234abcd-0000-aaaa-1111-0123456789ab}\",\n                \"links\": {\n                    \"self\": {\n                        \"href\": \"https://api.bitbucket.org/2.0/users/%1234abcd-0000-aaaa-1111-0123456789ab\"\n                    },\n                    \"html\": {\n                        \"href\": \"https://bitbucket.org/%1234abcd-0000-aaaa-1111-0123456789ab/\"\n                    },\n                    \"avatar\": {\n                        \"href\": \"https://secure.gravatar.com/avatar/d8dee7d2f2ac223263989224f7f99fe3?d=https%3A%2F%2Favatar-management--avatars.us-west-2.prod.public.atl-paas.net%2Finitials%2FAP-2.png\"\n                    }\n                },\n                \"type\": \"user\",\n                \"nickname\": \"John Smith\",\n                \"account_id\": \"01234567892cc1006957e666\"\n            },\n            \"workspace\": {\n                \"slug\": \"test_user\",\n                \"type\": \"workspace\",\n                \"name\": \"John Smith\",\n                \"links\": {\n                    \"self\": {\n                        \"href\": \"https://api.bitbucket.org/2.0/workspaces/test_user\"\n                    },\n                    \"html\": {\n                        \"href\": \"https://bitbucket.org/test_user/\"\n                    },\n                    \"avatar\": {\n                        \"href\": \"https://bitbucket.org/workspaces/test_user/avatar/?ts=1623065881\"\n                    }\n                },\n                \"uuid\": \"{12345678-1111-2222-aaaa-74e612ecd2ae}\"\n            },\n            \"type\": \"repository\",\n            \"is_private\": true,\n            \"name\": \"TestRepo\"\n        },\n        \"actor\": {\n            \"display_name\": \"John Smith\",\n            \"uuid\": \"{12345678-1111-2222-bbbb-74e612ecd2ae}\",\n            \"links\": {\n                \"self\": {\n                    \"href\": \"https://api.bitbucket.org/2.0/users/%1234abcd-0000-aaaa-1111-0123456789ab\"\n                },\n                \"html\": {\n                    \"href\": \"https://bitbucket.org/%1234abcd-0000-aaaa-1111-0123456789ab/\"\n                },\n                \"avatar\": {\n                    \"href\": \"https://secure.gravatar.com/avatar/d8dee7d2f2ac223263989224f7f99fe3?d=https%3A%2F%2Favatar-management--avatars.us-west-2.prod.public.atl-paas.net%2Finitials%2FAP-2.png\"\n                }\n            },\n            \"type\": \"user\",\n            \"nickname\": \"John Smith\",\n            \"account_id\": \"01234567892cc1006957e666\"\n        }\n    }\n}\n\n</code></pre>"},{"location":"newoutput/bitbucket-version-tags/","title":"Bitbucket Version Tags","text":"<p>Coralogix supports integration with <code>Bitbucket</code> webhooks, which can be used to inform Coralogix when a new build has been issued.</p> <p>Note: This document includes cluster dependent URL\u2019s. Please refer to the following table to select the correct Coralogix Bitbucket Version Tags API endpoint for your Coralogix Portal domain\u2019s extension (.com/.us/.in):</p> <p>[table id=91 /]</p>"},{"location":"newoutput/bitbucket-version-tags/#configuration","title":"Configuration","text":"<ol> <li> <p>Log into Bitbucket using your user credentials.</p> </li> <li> <p>Click your desired Repository to select it.</p> </li> <li> <p>Click \"Repository settings\" on the left.</p> </li> <li> <p>Under General \\ Repository details, click \"Webhooks\".</p> </li> <li> <p>Click \"Add webhook\".</p> </li> <li> <p>Add a Title (for example: \u201cCoralogix builds\u201d).</p> </li> <li> <p>In the URL box, you will enter the link corresponding to the Coralogix Bitbucket Version Tags endpoint for your cluster (as described in the note above).</p> </li> </ol> <p>URL Parameters</p> <ul> <li> <p>API Endpoint.</p> </li> <li> <p>Alerts, Rules, and Tags API Key.</p> <ul> <li> <p>You can get this key from: \"<code>https://&lt;YourCoralogixPortalHere&gt;/#/integration/apikey</code>\"</p> </li> <li> <p>or by navigating to: <code>Data Flow --&gt; API Keys --&gt; Alerts, Rules and Tags API Key</code>)</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Application Name:</p> <ul> <li>Enter your desired Application name. Please use a name corresponding to your logs \"Application\" name.</li> </ul> </li> <li> <p>Subsystem Name:</p> <ul> <li> <p>Enter your desired Subsystem name. Please use a name corresponding to your logs \"Subsystem\" name.</p> </li> <li> <p>You can specify more than 1 Subsystem name, separated by a commas.</p> </li> </ul> </li> <li> <p>Tag Name</p> </li> </ul> <p>URL Example</p> <p>https://webapi.coralogixstg.wpengine.com/api/v1/external/bitbucket/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx?application=bitbucket&amp;subsystem=build-tag-test&amp;name=bitbucket-tag-test</p> <p>(Please note the question mark\u00a0\u2018?\u2019\u00a0immediately after the API key. You could copy and paste this example, and replace the API Endpoint, Alerts and Tags API Key, and additional parameters with your own values).</p> <ol> <li>Under \"Triggers\", please select Build status updated\".</li> </ol> <p></p> <ol> <li>On the right bottom, please click \"Save\".</li> </ol>"},{"location":"newoutput/blobstorage-microsoft-azure-functions/","title":"Blob Storage via Event Grid: Microsoft Azure Resource Manager (ARM)","text":"<p>Coralogix provides a seamless integration with Azure cloud, allowing you to send your logs from anywhere and parse them according to your needs.</p> <p>The Azure Blob Storage via Event Grid integration allows parsing of Azure Blobs, triggered by an EventGrid subscription notification.</p> <p>Notes:</p> <ul> <li> <p>This integration only supports whole log processing. It does not support tail functionality.</p> </li> <li> <p>It does not monitor for changes to log files.</p> </li> </ul>"},{"location":"newoutput/blobstorage-microsoft-azure-functions/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Azure account with an active subscription</p> </li> <li> <p>[Optional] Pre-existing Event Grid system topic aligned with Storage v2 /General Purpose v2 storage topic type</p> </li> </ul>"},{"location":"newoutput/blobstorage-microsoft-azure-functions/#azure-resource-manager-template-deployment","title":"Azure Resource Manager Template Deployment","text":"<p>Sign into your Azure account and deploy the Blob Storage via Event Grid trigger integration by clicking here.</p> <p></p>"},{"location":"newoutput/blobstorage-microsoft-azure-functions/#fields","title":"Fields","text":"FieldDescriptionSubscriptionAzure subscription for which you wish to deploy the integration. Must be the same as the monitored storage account.Resource GroupResource group into which you wish to deploy the integrationCoralogix RegionRegion associated with your Coralogix domainCustom URLCustom URL associated with your Coralogix account. Ignore if you do not have a custom URL.Coralogix Private KeyCoralogix Send-Your-Data API keyCoralogix ApplicationMandatory metadata field sent with each log and helps to classify itCoralogix SubsystemMandatory metadata field sent with each log and helps to classify itStorage Account NameName of the storage account containing the Blob container. Must be Storagev2 (general purpose v2) type.Storage Account Resource GroupResource Group name of the storage account containing the Blob container to be monitoredBlob Container NameName of the Blob container to be monitoredEvent Grid System Topic NameName of a pre-existing Event Grid system topic for the storage account containing the Blob container. Leave as 'New' to create one.Newline PatternNewline pattern expected within the Blob storage documentsPrefix FilterPrefix filter to apply to the Blob container. Use 'NoFilter' to refrain from filtering by prefix. Wildcards are not allowed. Use the following format: <code>/subfolder1/subfolder2/</code>.Suffix FilterSuffix filter to apply to the Blob container. Use 'NoFilter' to refrain from filtering by suffix. Wildcards are not allowed. Use the following format: <code>.log</code>.Function App Service Plan TypeType of service plan for the function app. Choose 'Premium' if you need vNet support."},{"location":"newoutput/blobstorage-microsoft-azure-functions/#optional-configuration-options","title":"Optional Configuration Options","text":"<p>If your Blob container belongs to a restricted storage account, review this optional configuration documentation to learn about VNet support options.</p>"},{"location":"newoutput/blobstorage-microsoft-azure-functions/#additional-resources","title":"Additional Resources","text":"GithubBlob Storage DocumentationMicrosoft Azure Functions Manual IntegrationsEvent HubQueue Storage"},{"location":"newoutput/blobstorage-microsoft-azure-functions/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/browser-sdk-installation-guide/","title":"NPM Browser SDK Installation Guide","text":"<p>As part of our Real User Monitoring (RUM) toolkit, Coralogix offers multi-faceted Error Tracking, enabled by our NPM RUM Browser SDK.</p> <p>Integrated into the front end of web applications, this lightweight code tool detects and captures errors that arise within users' browsers, including JavaScript runtime errors, unhandled exceptions, network errors, and application (custom logic) errors. The SDK collects essential error information and additional contextual data, such as browser details and URLs, and securely sends it to our platform through logs for further analysis.</p> <p>For information on our CDN Browser SDK, view this page.</p>"},{"location":"newoutput/browser-sdk-installation-guide/#prerequisites","title":"Prerequisites","text":"<p>Deploy our\u00a0RUM Integration Package. This includes creating your RUM API key, which is required for the Browser SDK setup.</p>"},{"location":"newoutput/browser-sdk-installation-guide/#setup","title":"Setup","text":""},{"location":"newoutput/browser-sdk-installation-guide/#configuration","title":"Configuration","text":"<p>STEP 1. Install the Coralogix NPM Browser SDK package.</p> <pre><code>npm install @coralogix/browser\n\n</code></pre> <p>STEP 2. Initialize the SDK.</p> <pre><code>    CoralogixRum.init({\n      public_key: 'abc-123-456',\n      application: 'natan-test',\n      version: '1.0',\n      coralogixdDomain: 'EU2',\n      user_context: {\n       user_email: 'test@test.com',\n       user_name: 'aa',\n       user_id: '123'\n      },\n      labels: {\n        payment: 'visa'\n      }\n    });\n\n</code></pre> <p>Input the following basic parameters.</p> Basic Parameters Description public_key The SDK uses\u00a0your Coralogix RUM API Key\u00a0to authenticate you. application Details for the specific user, including user_email, user_name, and user_id. This can be sent later to Coralogix if you do not have these details during installation. version Input your application version. This allows Coralogix to match your code to your data. coralogixDomain Input your Coralogix domain. user_context Details for the specific user, including user_email, user_name, and user_id. This can be sent later to Coralogix if you do not have these details during the installation process. labels Input labels of your choosing. <p>You also have the options to add other instrumentation options with other advanced parameters.</p> <pre><code>export interface CoralogixOtelWebConfig {\n  /** Publicly-visible `public_key` value */\n  public_key: string;\n\n  /** Sets a value for the `application` attribute */\n  application: string;\n\n  /** Coralogix account domain */\n  coralogixdDomain: CoralogixDomain;\n\n  /** Configuration for user context. */\n  user_context?: UserContextConfig;\n\n  /** Turns on/off internal debug logging */\n  debug?: boolean;\n\n  /** Sets a value for the 'app.version' attribute */\n  version?: string;\n\n  /** Sets labels added to every Span. */\n  labels?: CoralogixRumLabels;\n\n  /**\n   * Applies for XHR and Fetch URLs. URLs that partially match any regex in ignoreUrls will not be traced.\n   * In addition, URLs that are _exact matches_ of strings in ignoreUrls will also not be traced.\n   * */\n  ignoreUrls?: Array&lt;string | RegExp&gt;;\n\n  /**\n   A pattern for error messages which should not be sent to Coralogix. By default, all errors will be sent.\n   * */\n  ignoreErrors?: Array&lt;string | RegExp&gt;;\n\n  /** Configuration for instrumentation modules. */\n  instrumentations?: CoralogixOtelWebOptionsInstrumentations;\n\n  /** Add Traceparent to headers in order to start a trace */\n  traceParentInHeader?: TraceHeaderConfiguration;\n}\n\n</code></pre> <p>Note:</p> <ul> <li><code>traceParentInHeader</code>: If you are using OpenTelemetry instrumentation in your backend services, use this flag to ensure your frontend creates spans.</li> </ul>"},{"location":"newoutput/browser-sdk-installation-guide/#ignore-errors","title":"Ignore Errors","text":"<p>Coralogix provides a feature that allows you to selectively ignore errors. This functionality lets you focus on the critical issues impacting user experience and streamline your error-tracking process.</p> <p>Using Regex, you can set up the SDK to reject all requests with a full or partial URL, as in the following example.</p> <pre><code>ignoreUrls: [\n    '&lt;https://my-server.com/user/123&gt;', // will ignore all requests match this url\n    /.*\\\\.svg/, // will ignore all requests match this regex - all svg files\n    /.*material-override.*/, // will ignore all requests match this regex - all urls with the string material-override\n  ],\n\n</code></pre> <p>You also have the option setup the SDK to reject all errors with a particular message or string, as in the following example.</p> <pre><code>ignoreErrors: [\n    'This is a custom error', // will ignore all errors with this message\n    /.*my-error.*/, // will ignore all errors match this regex - all errors with the string my-error\n  ],\n\n</code></pre>"},{"location":"newoutput/browser-sdk-installation-guide/#upload-your-source-maps","title":"Upload Your Source Maps","text":"<p>Install the RUM CLI tool and upload your source maps. Find out more here.</p>"},{"location":"newoutput/browser-sdk-installation-guide/#next-steps","title":"Next Steps","text":"<p>Get started with Error Tracking. Use our dedicated User Manual for support.</p>"},{"location":"newoutput/browser-sdk-installation-guide/#additional-resources","title":"Additional Resources","text":"DocumentationReal User MonitoringRUM Integration PackageError TrackingError Tracking: User Manual"},{"location":"newoutput/browser-sdk-installation-guide/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/","title":"Python OpenTelemetry Instrumentation","text":"<p>This tutorial shows simple examples of instrumenting Python applications for\u00a0metrics\u00a0and\u00a0traces\u00a0using OpenTelemetry and sending them to Coralogix. While the examples utilize a Flask web framework, it is also possible to instrument using other frameworks such as\u00a0Tornado, Django, and\u00a0more.</p>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#overview","title":"Overview","text":"<p>OpenTelemetry supports\u00a0auto, programmatic,\u00a0and manual\u00a0instrumentation for Python applications. While auto instrumentation is the easiest way to instrument an application for traces, it significantly reduces your level of control. For instance, it denies you the ability to explicitly define relationships between functions or effectively propagate context to other applications.</p> <p>This guide provides examples of auto, programmatic, and manual instrumentation.</p>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Your Coralogix Send-Your-Data API Key and domain</p> </li> <li> <p>Python 3.9+</p> </li> </ul>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#initial-setup-and-dependencies-installation-for-flask-application","title":"Initial Setup and Dependencies Installation for Flask Application","text":"<p>Prerequisite steps for this integration include setting up a Python virtual environment, creating a Python file, and installing the required packages for a Flask application.</p> <p>STEP 1. Set up a Python virtual environment.</p> <pre><code># create python virtual environment\npython -m venv env\n\n# activate python environment\nsource env/bin/active\n\n# create app.py\ntouch app.py\n\n</code></pre> <p>STEP 2. Add a\u00a0requirements.txt\u00a0file and install dependencies.</p> <pre><code>flask==2.3.8\nopentelemetry-distro\nopentelemetry-exporter-otlp\nopentelemetry-instrumentation\nopentelemetry-instrumentation-flask\ncoralogix-opentelemetry==0.1.3\nWerkzeug==2.3.8\n\n</code></pre> <pre><code>pip install -r requirements.txt\n\n</code></pre> <p>Notes:</p> <ul> <li>Werkzeug is pinned to version 2.3.8, given\u00a0this issue.</li> </ul>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#auto-instrumentation","title":"Auto Instrumentation","text":"<p>Auto instrumentation works by running your application within another wrapper application provided by OpenTelemetry. This wrapper handles collecting traces from all\u00a0compatible\u00a0parts of your application.</p>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#traces","title":"Traces","text":"<p>Example with a simple Flask app \u2018app.py\u2019.</p> <pre><code>from random import randint\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/roll\")\ndef roll_dice():\n    return str(do_roll())\n\ndef do_roll():\n    res = randint(1, 6)\n    return f\"you've rolled {res}\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)\n\n</code></pre> <p>STEP 1.\u00a0You can now run the application using the\u00a0<code>opentelemetry-instrument</code>\u00a0wrapper. Before proceeding, configure the auto instrumentation tool. The wrapper is configured using either CLI arguments or environment variables.</p> <p>The following example uses environment variables.</p> <pre><code># set exporter headers. These are the headers used when calling the Coralogix Endpoint\nexport OTEL_EXPORTER_OTLP_HEADERS='Authorization=Bearer%20[your-private-key], CX-Application-Name=dev-traces, CX-Subsystem-Name=dev-traces'\n\n# set the coralogix endpoint for traces\nexport OTEL_EXPORTER_OTLP_ENDPOINT='ingress.&lt;cx_domain&gt;:443'\n\n# set the exporter protocol, in this case we're using grpc\nexport OTEL_TRACES_EXPORTER=otlp_proto_grpc\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>The values of the environment variable\u00a0OTEL_EXPORTER_OTLP_HEADERS\u00a0must be URL encoded. The example uses the value %20 to represent a space between the bearer and the token.</p> </li> <li> <p>Select the Coralogix\u00a0OpenTelemetry endpoint\u00a0associated with your Coralogix\u00a0domain.</p> </li> <li> <p>View\u00a0this page\u00a0for a list of all the environment variable options for configuring the\u00a0<code>opentelemetry-instrument</code>\u00a0tool.</p> </li> </ul> <p>STEP 2. Once you\u2019ve set your environment variables, run the application.</p> <pre><code>opentelemetry-instrument \\\\\n    --traces_exporter console,otlp \\\\ # export traces to console and otel protocol\n    --metrics_exporter none \\\\ # disable metrics\n    --service_name py-auto-instr-test \\\\ # service name is required\n    python app.py\n\n</code></pre> <p>The application will be served on port\u00a0<code>5001</code>. A trace can be triggered by calling the endpoint\u00a0<code>localhost:5001/roll</code>.</p>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#metrics","title":"Metrics","text":"<p>To add metrics, add\u00a0<code>otlp</code>\u00a0as an argument to the\u00a0<code>--metrics_exporter</code>\u00a0flag.</p> <pre><code>opentelemetry-instrument \\\\\n    --traces_exporter console,otlp \\\\\n    --metrics_exporter console,otlp \\\\ # send metrics to console and otel exporters\n    --service_name py-auto-instr-test \\\\\n    python app.py\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>The metrics collected will depend on supporting the underlying framework being instrumented. View\u00a0this page\u00a0for a list of all supported frameworks.</p> </li> <li> <p>Forwarding metrics to Coralogix is required for leveraging Coralogix platform functionalities that depend on service applicative metrics from OpenTelemetry (OTEL). However, it is not required for utilizing Coralogix APM features.</p> </li> </ul>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#programatic-instrumentation","title":"Programatic Instrumentation","text":""},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#traces_1","title":"Traces","text":"<p>STEP 1. \u00a0Create a file\u00a0<code>app.py</code>\u00a0with the following content:</p> <pre><code>from os import environ\nimport random\n\nfrom flask import Flask\n\n# opentelemetry imports\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom opentelemetry.sdk.resources import Resource, SERVICE_NAME\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom opentelemetry.sdk.trace.sampling import StaticSampler, Decision\nfrom opentelemetry import trace\n\n# coralogix sampler\nfrom coralogix_opentelemetry.trace.samplers import CoralogixTransactionSampler\n\n</code></pre> <p>STEP 2.\u00a0Define the headers necessary for calling the Coralogix endpoint and use them to define an exporter and tracer. An exporter is the mechanism responsible for exporting/sending traces during runtime.</p> <p>Notes:</p> <ul> <li> <p>CX_ENDPOINT: Select the Coralogix\u00a0OpenTelemetry endpoint\u00a0associated with your Coralogix\u00a0domain.</p> </li> <li> <p>The\u00a0<code>headers</code>\u00a0variable is a\u00a0<code>string</code>\u00a0and not a\u00a0<code>dictionary</code>.</p> </li> </ul> <pre><code>headers = ', '.join([\n    f'Authorization=Bearer%20{environ.get(\"CX_TOKEN\")}',\n    'CX-Application-Name=my-instrumented-app',\n    'CX-Subsystem-Name=my-instrumented-app'\n])\n\ntracer_provider = TracerProvider(\n    resource=Resource.create({\n        SERVICE_NAME: 'my-instrumented-app'\n    }),\n    sampler=CoralogixTransactionSampler(StaticSampler(Decision.RECORD_AND_SAMPLE))\n)\n\n# set up an OTLP exporter to send spans to coralogix directly.\nexporter = OTLPSpanExporter(\n    endpoint=environ.get('CX_ENDPOINT'),\n    headers=headers\n)\n\n# set up a span processor to send spans to the exporter\nspan_processor = SimpleSpanProcessor(exporter)\n# span_processor = ConsoleSpanExporter(exporter)\n\n# add the span processor to the tracer provider\ntracer_provider.add_span_processor(span_processor)\ntrace.set_tracer_provider(tracer_provider)\n\ninstrumentor = FlaskInstrumentor()\ntracer = trace.get_tracer_provider().get_tracer(__name__)\n\n</code></pre> <p>STEP 3.\u00a0Define the Flask application and instrument the functions.</p> <pre><code># initialise flask app\napp = Flask(__name__)\ninstrumentor.instrument_app(app) # Instrument the flask app\n\n@app.route(\"/roll\")\ndef roll():\n    res = dice()\n    span = trace.get_current_span()\n    span.set_attribute(\"roll.value\", res) # Set custom tags\n    return f\"you rolled {res}\\\\n\"\n\n# example of adding a span to a function using a decorator with the tracer we defined in step 2\n@tracer.start_as_current_span(\"diceFunc\")\ndef dice():\n    return random.randint(1, 6)\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)\n\n</code></pre> <p>The code above has 2 functions:\u00a0<code>roll</code>\u00a0and\u00a0<code>dice</code>. Select one depending on the specifics of your use case and codebase.</p> Function Description Instrumentation roll The HTTP handle for the\u00a0/roll\u00a0path Instrumented by defining a trace within the function body. dice A simple function that returns a number between 1 and 6 Instrumented using a decorator <p>STEP 4.\u00a0To run the app, set the required environment variables:</p> <ul> <li> <p>CX_TOKEN</p> </li> <li> <p>CX_ENDPOINT</p> </li> </ul> <p>Notes:</p> <ul> <li>When using a Python virtual environment, these environment variables must be set inside that environment.</li> </ul> <p>Run the dev server:</p> <pre><code>python app.py\n\n</code></pre> <p>To produce a trace, go to: http://localhost:5001/roll.</p> <p>STEP 5. Confirm that your traces are being sent to Coralogix. From your Coralogix toolbar, navigate to\u00a0Explore\u00a0&gt;\u00a0Tracing.\u00a0You should now see the traces generated by your application.</p>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#metrics_1","title":"Metrics","text":"<p>Forwarding metrics to Coralogix is required for leveraging Coralogix platform functionalities that depend on service applicative metrics from OpenTelemetry (OTEL). However, it is not required for utilizing Coralogix APM features.</p> <p>STEP 1.\u00a0Define the metrics exporter and metrics provider.</p> <pre><code>from opentelemetry import metrics\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import (\n    PeriodicExportingMetricReader,\n)\n\nfrom opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter\nfrom os import environ\n\n# define headers. Single string with comma separated key-value pairs\n# Authorization, CX-Application-Name and CX-Subsystem-Name are mandatory\n# when calling the coralogix endpoint directly.\nheaders = ', '.join([\n    f'Authorization=Bearer%20{environ.get(\"CX_TOKEN\")}',\n    \"CX-Application-Name=manual-instro-traces\",\n    \"CX-Subsystem-Name=manual-instro-traces\",\n])\n\n# set up an OTLP exporter to send spans to coralogix directly.\nexporter = OTLPMetricExporter(\n    endpoint=environ.get('CX_ENDPOINT'),\n    headers=headers,\n)\n\nmetric_reader = PeriodicExportingMetricReader(exporter)\nprovider = MeterProvider(metric_readers=[metric_reader])\n\n# Sets the global default meter provider\nmetrics.set_meter_provider(provider)\n\n# Creates a meter from the global meter provider\nmeter = metrics.get_meter(\"my.meter.name\")\n\n</code></pre> <p>Above, the\u00a0<code>meter</code>\u00a0created a counter metric called\u00a0<code>http.request.counter</code>. The counter counts the number of requests made to the\u00a0<code>/roll</code>\u00a0endpoint.</p> <p>View\u00a0this page\u00a0for more information on the\u00a0<code>meter</code>\u00a0and a list of other metric types that can be defined.</p> <p>STEP 3.\u00a0Add the application logic and instrument the\u00a0<code>/roll</code>\u00a0endpoint.</p> <pre><code>from random import randint\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/roll\")\ndef roll_dice():\n    # increment the counter\n    http_req_counter.add(1, {\"request.path\": '/roll'})\n    return str(do_roll())\n\ndef do_roll():\n    res = randint(1, 6)\n    return f\"you've rolled {res}\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)\n\n</code></pre> <p>Note that counter is incremented each time the\u00a0<code>/roll</code>\u00a0endpoint is called.</p> <p>STEP 4.\u00a0Execute.</p> <p>To run the app as before, set the required environment variables:</p> <ul> <li> <p>CX_TOKEN</p> </li> <li> <p>CX_ENDPOINT</p> </li> </ul> <pre><code>python app.py\n</code></pre> <p>STEP 5. Confirm that your metrics are being sent to Coralogix. From your Coralogix toolbar, navigate to\u00a0Grafana\u00a0&gt;\u00a0Explore\u00a0&gt;\u00a0Metric Browser. Search for the name of your metric.</p>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#manual-instrumentation","title":"Manual Instrumentation","text":"<p>Implement the same example above using manual instrumentation.</p>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#traces_2","title":"Traces","text":"<p>STEP 1.\u00a0Create a file\u00a0<code>app.py</code>\u00a0and import the required packages.</p> <pre><code>from os import environ\nfrom random import randint\n\nfrom flask import Flask\n\n# opentelemetry imports\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource, SERVICE_NAME\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom opentelemetry.sdk.trace.sampling import StaticSampler, Decision\n\n# coralogix sampler \nfrom coralogix_opentelemetry.trace.samplers import CoralogixTransactionSampler\n\n</code></pre> <p>STEP 2.\u00a0Define the headers necessary for calling the Coralogix endpoint and use them to define an exporter and tracer. An exporter is the mechanism responsible for exporting / sending traces during runtime.</p> <p>Notes:</p> <ul> <li> <p>CX_ENDPOINT: Select the Coralogix\u00a0OpenTelemetry endpoint\u00a0associated with your Coralogix\u00a0domain.</p> </li> <li> <p>The\u00a0<code>headers</code>\u00a0variable is a\u00a0<code>string</code>\u00a0and not a\u00a0<code>dictionary</code>.</p> </li> </ul> <pre><code># define headers. Single string with comma separated key-value pairs\n# Authorization, CX-Application-Name and CX-Subsystem-Name are mandatory when calling the coralogix endpoint directly.\nheaders = ', '.join([\n    f'Authorization=Bearer%20{environ.get(\"CX_TOKEN\")}',\n    \"CX-Application-Name=&lt;APPLICATION NAME&gt;\",\n    \"CX-Subsystem-Name=&lt;SUBSYSTEM NAME&gt;\",\n])\n\n\n# create a tracer provider\ntracer_provider = TracerProvider(\n    resource=Resource.create({\n        SERVICE_NAME: '&lt;SERVICE NAME&gt;'\n    }),\n    sampler=CoralogixTransactionSampler(StaticSampler(Decision.RECORD_AND_SAMPLE)))\n\n# set up an OTLP exporter to send spans to coralogix directly.\nexporter = OTLPSpanExporter(\n    endpoint=environ.get('CX_ENDPOINT'),\n    headers=headers,\n)\n\n# set up a span processor to send spans to the exporter\nspan_processor = SimpleSpanProcessor(exporter)\n# span_processor = ConsoleSpanExporter(exporter)\n\n# add the span processor to the tracer provider\ntracer_provider.add_span_processor(span_processor)\n\n# set trace provider\ntrace.set_tracer_provider(tracer_provider)\n\n# create a tracer\ntracer = trace.get_tracer_provider().get_tracer(__name__)\n</code></pre> <pre><code># initialise flask app\napp = Flask(__name__)\n\n# example of adding a span to a function using a context manager\n# this is useful when you want to dynamic attribuf\"http://{os.getenv('OTEL_COLLECTOR_SERVICE_HOST')}:4318\"tes to the span\n@app.route(\"/roll\")\ndef roll():\n    # create a span\n    with tracer.start_as_current_span(\"roll_v2\") as span:\n        span.set_attribute(\"http.method\", \"GET\")\n        span.set_attribute(\"http.host\", \"localhost\")\n        span.set_attribute(\"http.url\", \"/v2/rolldice\")\n        res = dice()\n        span.set_attribute(\"roll.value\", res)\n        return f\"you rolled {res}\\\\n\"\n\n# example of adding a span to a function using a decorator\n@tracer.start_as_current_span(\"diceFunc\")\ndef dice():\n    return randint(1, 6)\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)\n\n</code></pre> <p>The code above has 2 functions:\u00a0<code>roll</code>\u00a0and\u00a0<code>dice</code>. Select one depending on the specifics of your use-case and codebase.</p> Function Description Instrumentation roll The HTTP handle for the\u00a0/roll\u00a0path Instrumented by defining a trace within the function body. dice A simple function that returns a number between 1 and 6 Instrumented using a decorator <p>STEP 4.\u00a0To run the app, set the required environment variables:</p> <ul> <li> <p>CX_TOKEN</p> </li> <li> <p>CX_ENDPOINT</p> </li> </ul> <p>Notes:</p> <ul> <li>When using a Python virtual environment, these environment variables must be set inside that environment.</li> </ul> <pre><code>python app.py\n</code></pre> <p>STEP 5. Confirm that your traces are being sent to Coralogix. From your Coralogix toolbar, navigate to\u00a0Explore\u00a0&gt;\u00a0Tracing.\u00a0You should now see the traces generated by your application.</p>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#metrics_2","title":"Metrics","text":"<p>Like traces, metrics can also be instrumented manually. Choose this option if there are specific metrics that you wish to extract from your application runtime context.</p> <p>Notes:</p> <ul> <li> <p>The dependencies are the same as those utilized in the\u00a0<code>requirements.txt</code>\u00a0above for traces.</p> </li> <li> <p>Forwarding metrics to Coralogix is required for leveraging Coralogix platform functionalities that depend on service applicative metrics from OpenTelemetry (OTEL). However, it is not required for utilizing Coralogix APM features.</p> </li> </ul> <p>STEP 1.\u00a0Define the metrics exporter and metrics provider.</p> <pre><code>from opentelemetry import metrics\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import (\n    PeriodicExportingMetricReader,\n)\n\nfrom opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter\nfrom os import environ\n\n# define headers. Single string with comma separated key-value pairs\n# Authorization, CX-Application-Name and CX-Subsystem-Name are mandatory\n# when calling the coralogix endpoint directly.\nheaders = ', '.join([\n    f'Authorization=Bearer%20{environ.get(\"CX_TOKEN\")}',\n    \"CX-Application-Name=manual-instro-traces\",\n    \"CX-Subsystem-Name=manual-instro-traces\",\n])\n\n# set up an OTLP exporter to send spans to coralogix directly.\nexporter = OTLPMetricExporter(\n    endpoint=environ.get('CX_ENDPOINT'),\n    headers=headers,\n)\n\nmetric_reader = PeriodicExportingMetricReader(exporter)\nprovider = MeterProvider(metric_readers=[metric_reader])\n\n# Sets the global default meter provider\nmetrics.set_meter_provider(provider)\n\n# Creates a meter from the global meter provider\nmeter = metrics.get_meter(\"my.meter.name\")\n\n</code></pre> <p>Above, the\u00a0<code>meter</code>\u00a0created a counter metric called\u00a0<code>http.request.counter</code>. The counter counts the number of requests made to the\u00a0<code>/roll</code>\u00a0endpoint.</p> <p>View\u00a0this page\u00a0for more information on the\u00a0<code>meter</code>\u00a0and a list of other metric types that can be defined.</p> <p>STEP 3.\u00a0Add the application logic and instrument the\u00a0<code>/roll</code>\u00a0endpoint.</p> <pre><code>from random import randint\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/roll\")\ndef roll_dice():\n    # increment the counter\n    http_req_counter.add(1, {\"request.path\": '/roll'})\n    return str(do_roll())\n\ndef do_roll():\n    res = randint(1, 6)\n    return f\"you've rolled {res}\"\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001)\n\n</code></pre> <p>Note that counter is incremented each time the\u00a0<code>/roll</code>\u00a0endpoint is called.</p> <p>STEP 4.\u00a0Execute.</p> <p>To run the app as before, set the required environment variables:</p> <ul> <li> <p>CX_TOKEN</p> </li> <li> <p>CX_ENDPOINT</p> </li> </ul> <pre><code>python app.py\n</code></pre> <p>STEP 5. Confirm that your metrics are being sent to Coralogix. From your Coralogix toolbar, navigate to\u00a0Grafana\u00a0&gt;\u00a0Explore\u00a0&gt;\u00a0Metric Browser. Search for the name of your metric.</p>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#service-flows","title":"Service Flows","text":"<p>For customers with functional Python OpenTelemetry instrumentation that was configured manually, this section guides reconfiguring the existing setup to define, report, and monitor Coralogix Service Flows.</p> <p>For new customers or those who haven't configured the Python OpenTelemetry instrumentation manually, you are required to follow the Manual Instrumentation instructions above. The steps in this section are included in those instructions.</p> <p>Add packages to the requirements.txt file:</p> <pre><code>coralogix-opentelemetry==0.1.3\n\n</code></pre> <p>And then run:</p> <pre><code>pip install -r requirements.txt\n\n</code></pre> <p>Add to the code:</p> <pre><code>from coralogix_opentelemetry.trace.samplers import CoralogixTransactionSampler\n# create a tracer provider\ntracer_provider = TracerProvider(sampler=CoralogixTransactionSampler(StaticSampler(Decision.RECORD_AND_SAMPLE)))\n\n</code></pre>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#additional-resources","title":"Additional Resources","text":"<p>For more information on Python OpenTelemetry instrumentation, view the official OpenTelemetry documentation.</p>"},{"location":"newoutput/capture-opentelemetry-traces-from-your-python-applications/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/cdn-browser-sdk-installation-guide/","title":"CDN Browser SDK Installation Guide","text":"<p>As part of our\u00a0Real User Monitoring\u00a0(RUM) toolkit, Coralogix offers multi-faceted\u00a0Error Tracking,\u00a0enabled by our\u00a0CDN RUM Browser SDK.</p>"},{"location":"newoutput/cdn-browser-sdk-installation-guide/#overview","title":"Overview","text":"<p>Perfect for developers seeking client-side SDKs in web development and faster load times, our CDN Browser SDK detects and captures errors that arise within users\u2019 browsers, unhandled exceptions, network errors, and application (custom logic) errors. The SDK collects essential error information and additional contextual data, such as browser details and URLs, and securely sends it to our platform through logs for further analysis.</p> <p>For information on our NPM Browser SDK, view this page.</p>"},{"location":"newoutput/cdn-browser-sdk-installation-guide/#configuration","title":"Configuration","text":"<p>Add the CDN script to your application.</p> <ul> <li> <p>[Recommended] The latest browser version: https://cdn.rum-ingress-coralogixstg.wpengine.com/coralogix/browser/latest/coralogix-browser-sdk.js</p> </li> <li> <p>A specific browser version: https://cdn.rum-ingress-coralogixstg.wpengine.com/coralogix/browser/[version]/coralogix-browser-sdk.js. Replace [version] with a version from our\u00a0Releases page.</p> </li> </ul> <pre><code>&lt;head&gt;\n  ...\n&lt;script src=\"&lt;https://cdn.rum-ingress-coralogixstg.wpengine.com/coralogix/browser/latest/coralogix-browser-sdk.js&gt;\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n\n</code></pre>"},{"location":"newoutput/cdn-browser-sdk-installation-guide/#initialization","title":"Initialization","text":"<p>Initialize the SDK using a JS or TS file.</p>"},{"location":"newoutput/cdn-browser-sdk-installation-guide/#js-file","title":"JS File","text":"<pre><code>window.CoralogixRum.init({\n  application: 'app-name',\n  environment: 'production',\n  public_key: 'abc-123-456',\n  coralogixDomain: 'EU2',\n  version: 'v1.0.3',\n  labels: {\n    payment: 'visa',\n  },\n  ignoreErrors: ['some error message to ignore'],\n});\n\n</code></pre>"},{"location":"newoutput/cdn-browser-sdk-installation-guide/#ts-file","title":"TS File","text":"<pre><code>window.CoralogixRum.init({\n  application: 'app-name',\n  environment: 'production',\n  public_key: 'abc-123-456',\n  coralogixDomain: 'EU2',\n  version: 'v1.0.3',\n  labels: {\n    payment: 'visa',\n  },\n  ignoreErrors: ['some error message to ignore'],\n});\n\n// In case of warning from TSC\ndeclare global {\n  interface Window {\n    CoralogixRum:any;\n  }\n}\n\n</code></pre>"},{"location":"newoutput/cdn-browser-sdk-installation-guide/#next-steps","title":"Next Steps","text":"<p>Get started\u00a0with\u00a0Error Tracking. Use our dedicated\u00a0user manual\u00a0for support.</p>"},{"location":"newoutput/cdn-browser-sdk-installation-guide/#additional-resources","title":"Additional Resources","text":"DocumentationReal User MonitoringRUM Integration PackageError TrackingError Tracking: User Manual"},{"location":"newoutput/cdn-browser-sdk-installation-guide/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/cloud-security-posture-cspm/","title":"Cloud Security Posture Management (CSPM) - AWS","text":"<p>Cloud Security Posture Management (CSPM) helps to mitigate and minimize cloud data security breaches and to assess the overall posture of the entire cloud environment against best practices and compliance standards to help remediate issues.</p> <p>CSPM tools verify that cloud configurations follow security best practices and compliance standards such as CIS, Azure, and GCP benchmarks as well as PCI and HIPAA frameworks. As companies are increasingly moving to the cloud, CSPM is becoming a necessary aspect of security insights.</p> <p>The CSPM can be installed using the following methods:</p> <ol> <li> <p>Docker container</p> </li> <li> <p>Kubernetes CronJob</p> </li> </ol> <p>For each installation method, we need to pass the following environment variables:</p> API_KEYUnder \"Send your data\" on your Coralogix accountAPPLICATION_NAMESet the application nameSUBSYSTEM_NAMESet the subsystem nameCOMPANY_IDCompany ID from the settings screen in your Coralogix accountCORALOGIX_ENDPOINT_HOSTCoralogix GRPC endpoint associated with your Coralogix domainCLOUD_PROVIDERThe Cloud Provider that CSPM will be deployed into in lowercase (e.g. aws, gcp, etc)TESTER_LISTIf specified, will run the tests on the specified service, otherwise will run tests on all the AWS services. leave empty to run all testers, otherwise, comma separated per tester name without spacesREGION_LISTIf specified, will check only the specified regions (For global services like AWS S3, IAM and Route53, make sure you add region \"global\"). Otherwise, the tests will be conducted in all regions. leave empty to run on all regions, otherwise, comma separated per region name without spacesAWS_DEFAULT_REGIONAWS default region for authenticationROLE_ARN_LISTAn additional role(s) that can be assumed from other AWS accounts to scan. leave empty to run on the current account, for additional accounts, add in a comma-separated manner per role ARN without spaces.Note: please follow the instructions below for multi-account configurationCORALOGIX_ALERT_API_KEY(Optional parameter) Under \"Alerts, Rules and Tags API Key\" on your Coralogix account.When providing this variable, a custom enrichment for failed resources will be created in Coralogix's account at the end of each run if specified"},{"location":"newoutput/cloud-security-posture-cspm/#installing-as-a-docker-container","title":"Installing as a Docker container","text":""},{"location":"newoutput/cloud-security-posture-cspm/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>EC2 Instance</p> <ul> <li> <p>The minimum Instance Type should be \"t3a.medium\"</p> </li> <li> <p>Internet connectivity (thorough Internet/NAT Gateway)</p> </li> </ul> </li> <li> <p>IAM Role with the following policy attached to the instance</p> </li> <li> <p>Docker installed on the EC2 instance - refer to Docker documentation</p> </li> </ol> <p>Note that the instance type will affect the run time, so it's up to a personal preference and is affected by the environment size.</p>"},{"location":"newoutput/cloud-security-posture-cspm/#deploying","title":"Deploying","text":"<p>After prerequisites are met, download the docker image using the following command (if the following command hasn't run, the image will still be downloaded automatically in the next step):</p> <pre><code>docker pull coralogixrepo/snowbit-cspm\n</code></pre> <p>In order to automate the process, use Crontab in the following manner:</p> <p>Create the crontab using your favorite editor</p> <pre><code>sudo crontab -e\n</code></pre> <p>Inside the document, on the bottom, paste the following one-liner (note that the API_KEY and the CORALOGIX_ENDPOINT_HOST fields are mandatory)</p> <pre><code>*/10 * * * * docker rm snowbit-cspm ; docker rmi coralogixrepo/snowbit-cspm ; docker run --name snowbit-cspm -d -e PYTHONUNBUFFERED=1 -e CLOUD_PROVIDER=\"choose_the_cloud_provider - aws / gcp\" -e AWS_DEFAULT_REGION=\"eu-west-1\" -e CORALOGIX_ENDPOINT_HOST=\"coralogix_grpc_endpoint\" -e APPLICATION_NAME=\"application_name\" -e COMPANY_ID=&lt;coralogix_company_ID&gt; -e SUBSYSTEM_NAME=\"subsystem_name\" -e TESTER_LIST=\"\" -e API_KEY=\"send_your_data_api_key\" -e REGION_LIST=\"\" -e ROLE_ARN_LIST=\"\" -e CORALOGIX_ALERT_API_KEY=\"\" --network host -v ~/.aws:/root/.aws coralogixrepo/snowbit-cspm\n</code></pre> <p>The above command will try to run every 10* minutes, and consists of two commands:</p> <ul> <li> <p>docker rm snowbit-cspm - removes the last docker container if exists</p> </li> <li> <p>docker rmi coralogixrepo/snowbit-cspm - will remove and redownload the docker images to ensure it have the latest images locally available.</p> </li> <li> <p>docker run --name snowbit-cspm [Options...] - runs a new container sequence</p> </li> </ul> <p>** The actual scheduling is set in the Coralogix security tab - scroll to the \"Configuring the scan settings\" section for further details</p>"},{"location":"newoutput/cloud-security-posture-cspm/#installing-as-a-kubernetes-cronjob-in-eks-via-kubectl-and-eksctl","title":"Installing as a Kubernetes CronJob in EKS \u2013 via kubectl and eksctl","text":"<p>In order to deploy the image in a pod that follows the principle of least privileged, we should use a service account that assumes the needed role at the start of each job and doesn't inherit its permission from his host node.</p>"},{"location":"newoutput/cloud-security-posture-cspm/#prerequisites_1","title":"Prerequisites","text":"<ol> <li> <p>An existing Amazon EKS cluster</p> </li> <li> <p>Version 2.7.21 or later or 1.25.46 or later of the AWS CLI installed and configured on your device or AWS CloudShell</p> </li> <li> <p>The kubectl command line tool is installed on your device or AWS CloudShell</p> </li> <li> <p>The eksctl command line tool is installed on your device or AWS CloudShell</p> </li> <li> <p>An existing kubectl config file that contains your cluster configuration</p> </li> <li> <p>IAM policy with the following permissions</p> </li> </ol>"},{"location":"newoutput/cloud-security-posture-cspm/#before-deploying","title":"Before Deploying","text":"<p>Determine whether you have an existing IAM OIDC provider for your cluster:</p> <ul> <li>Retrieve your cluster's OIDC provider ID and store it in a variable:</li> </ul> <pre><code>oidc_id=$(aws eks describe-cluster --name my-cluster --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5)\n</code></pre> <ul> <li>Determine whether an IAM OIDC provider with your cluster's ID is already in your account</li> </ul> <pre><code>aws iam list-open-id-connect-providers | grep $oidc_id\n</code></pre> <p>Note: If the output is returned from the previous command, then you already have a provider for your cluster and you can skip the next step. If no output is returned, then you must create an IAM OIDC provider for your cluster</p>"},{"location":"newoutput/cloud-security-posture-cspm/#deploying_1","title":"Deploying","text":"<p>If the output from the above test is blank, create an IAM OIDC identity provider for your cluster with the following command. Replace \"my-cluster\" with your own value</p> <pre><code>eksctl utils associate-iam-oidc-provider --cluster my-cluster --approve\n</code></pre> <p>Configuring a Kubernetes service account to assume an IAM role (the policy ARN is the policy created in section 6 of the above prerequisites)</p> <pre><code>eksctl create iamserviceaccount --name my-service-account --namespace default --cluster my-cluster --role-name \"my-role\" --attach-policy-arn arn:aws:iam::111111111111:policy/my-policy --approve\n</code></pre> <p>Use the following CronJob configurations:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: snowbit-cspm-cronjob\n  namespace: &lt;optional - when using the default, remove entire row&gt;\nspec:\n  schedule: \"*/10 * * * *\"\n  successfulJobsHistoryLimit: 1\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          serviceAccountName: &lt;my-service-account&gt;\n          hostNetwork: true\n          containers:\n          - image: coralogixrepo/snowbit-cspm\n            name: snowbit-cspm-cronjob\n            command: [\"python3\"]\n            args: [\"lambda_function.py\"]\n            env:\n              - name: API_KEY\n                value: \"send_your_data_api_key\" \n              - name: CORALOGIX_ENDPOINT_HOST\n                value: \"coralogix_endpoint\"\n              - name: CLOUD_PROVIDER\n                value: \"choose_the_cloud_provider - aws / gcp\"\n              - name: APPLICATION_NAME\n                value: \"application_name\"\n              - name: SUBSYSTEM_NAME\n                value: \"subsystem_name\"\n              - name: TESTER_LIST\n                value: \"\"\n              - name: REGION_LIST\n                value: \"\"\n              - name: ROLE_ARN_LIST\n                value: \"\"\n              - name: COMPANY_ID\n                value: \"&lt;coralogix_company_ID&gt;\"\n              - name: PYTHONUNBUFFERED\n                value: \"1\"\n              - name: AWS_DEFAULT_REGION\n                value: \"eu-west-1\"\n              - name: CORALOGIX_ALERT_API_KEY\n                value: \"\"\n          restartPolicy: OnFailure\n</code></pre> <p>Save the above content into a .yaml file and execute the following command:</p> <pre><code>kubectl create -f Cronjob.yaml\n</code></pre>"},{"location":"newoutput/cloud-security-posture-cspm/#configuring-the-scan-settings","title":"Configuring the scan settings","text":"<p>Inside the security tab in your Coralogix account, you will find the SCAN SETTINGS button:</p> <ol> <li> <p>Scan Now will start the scan of the selected environment(s) in up to 10 minutes from pressing (according to the configured cronjob)</p> </li> <li> <p>Scan Schedule allows choosing the frequency of the scans, and the start time of each scan. the default is every 24 hours</p> </li> </ol> <p></p>"},{"location":"newoutput/cloud-security-posture-cspm/#multi-account-configuration","title":"Multi-Account Configuration","text":"<p>Definitions</p> <ul> <li> <p>Primary Account - the account that the CSPM is initially was deployed at</p> </li> <li> <p>Additional Account - an account that you wish to be scanned on top of the Primary Account</p> </li> </ul>"},{"location":"newoutput/cloud-security-posture-cspm/#for-each-additional-account","title":"For each Additional Account","text":"<ol> <li> <p>go to IAM &gt; Roles and click \"Create role\"</p> </li> <li> <p>choose \"AWS account\"</p> </li> <li> <p>Below, choose \"Another AWS account\" and paste the account ID of the primary account</p> </li> <li> <p>for permissions, create and add the following policy</p> </li> <li> <p>Give the Role a name and Click \"Create role\"</p> </li> <li> <p>Go to the role and copy his ARN</p> </li> </ol>"},{"location":"newoutput/cloud-security-posture-cspm/#for-primary-account","title":"For Primary Account","text":"<ol> <li> <p>Add the following policy to the existing CSPM Role (done in the initial deployment)</p> </li> <li> <p>Fill the resource key according to the amount of roles to assume</p> </li> </ol> <p>Example for one additional account:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"CSPMMultiAccountAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": \"arn:aws:iam::123456789012:role/cspm_additional_account_to_scan\"\n        }\n    ]\n}\n</code></pre>"},{"location":"newoutput/cloud-security-posture-cspm/#add-the-environmental-variable-to-the-dockerkubernetes-command","title":"Add the Environmental Variable to the Docker\\Kubernetes command","text":"<ol> <li> <p>use the ROLE_ARN_LIST environmental variable as listed above to point each run to use the role ARN in all the additional accounts.</p> </li> <li> <p>when using more than one account, use comma-separated strings without spaces.</p> </li> </ol>"},{"location":"newoutput/cloud-security-posture-cspm-gcp/","title":"GCP Security Posture Management (CSPM)","text":"<p>Security Posture Management (CSPM) helps to mitigate and minimize cloud data security breaches and to assess the overall posture of the entire cloud environment against best practices and compliance standards to help remediate issues.</p> <p>CSPM tools verify that cloud configurations follow security best practices and compliance standards such as CIS, Azure, and GCP benchmarks as well as PCI and HIPAA frameworks. As companies are increasingly moving to the cloud, CSPM is becoming a necessary aspect of security insights.</p> <p>The CSPM can be installed using a Docker container</p> <p>Coralogix GRPC endpoints</p> Irelandng-api-grpc.coralogixstg.wpengine.comStockholmng-api-grpc.eu2.coralogixstg.wpengine.comSingaporeng-api-grpc.coralogixsg.comMumbaing-api-grpc.app.coralogix.inUnited Statesng-api-grpc.coralogix.us <p>For each installation method, we need to pass the following environment variables:</p> API_KEYUnder \"Send your data\" on your Coralogix accountAPPLICATION_NAMESet the application nameSUBSYSTEM_NAMESet the subsystem nameCOMPANY_IDThe company ID from the settings screen in your Coralogix accountCORALOGIX_ENDPOINT_HOSTCoralogix GRPC endpointCLOUD_PROVIDERThe Cloud Provider that CSPM will be deployed into in lowercase (e.g. aws, gcp, etc)TESTER_LISTIf specified, will run the tests on the specified service, otherwise will run tests on all the GCP services. leave empty to run all testers, otherwise, comma separated per tester name without spacesREGION_LISTIf specified, will check only the specified regions (For global services like IAM and DNS, make sure you add region \"global\"). Otherwise, the tests will be conducted in all regions. leave empty to run on all regions, otherwise, comma separated per region name without spacesGOOGLE_APPLICATION_CREDENTIALSPath to credentials file inside the containerCORALOGIX_ALERT_API_KEY(Optional parameter) Under \"Alerts, Rules and Tags API Key\" on your Coralogix account.When providing this variable, a custom enrichment for failed resources will be created in Coralogix's account at the end of each run if specified"},{"location":"newoutput/cloud-security-posture-cspm-gcp/#installing-as-a-docker-container","title":"Installing as a Docker container","text":""},{"location":"newoutput/cloud-security-posture-cspm-gcp/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Verify the following APIs are enabled on the project before running</p> <ul> <li> <p>Cloud Resource Manager API</p> </li> <li> <p>API Keys API</p> </li> <li> <p>Cloud SQL Admin API</p> </li> <li> <p>Compute Engine API</p> </li> <li> <p>Cloud Key Management Service (KMS) API</p> </li> <li> <p>BigQuery API</p> </li> <li> <p>Kubernetes Engine API</p> </li> </ul> </li> <li> <p>Create a Role with the following permissions (IAM &amp; Admin &gt; Roles)</p> <ul> <li> <p>If you have access to use gcloud command and wish to automate the role creation process, download the following file</p> </li> <li> <p>Now run the command gcloud iam roles create test_coralogix_role1 --project --file=/path/to/yaml.yml</p> </li> </ul> </li> <li> <p>Create a service account and attach the previously created role (IAM &amp; Admin &gt; Service Accounts)</p> </li> <li> <p>Create VM Instance</p> <ul> <li> <p>Attach the service account (3) to the instance</p> </li> <li> <p>E2-small is sufficient</p> </li> <li> <p>Internet connectivity</p> </li> </ul> </li> <li> <p>Docker installed on the VM instance - refer to Docker documentation</p> </li> </ol> <p>Note that the instance type will affect the run time, so it's up to a personal preference and is affected by the environment size.</p>"},{"location":"newoutput/cloud-security-posture-cspm-gcp/#deploying","title":"Deploying","text":"<p>After prerequisites are met, download the docker image using the following command (if the following command hasn't run, the image will still be downloaded automatically in the next step):</p> <pre><code>docker pull coralogixrepo/snowbit-cspm\n</code></pre> <p>In order to automate the process, use Crontab in the following manner:</p> <p>Create the crontab using your favorite editor</p> <pre><code>sudo crontab -e\n</code></pre> <p>Inside the document, on the bottom, paste the following one-liner (note that the API_KEY and the CORALOGIX_ENDPOINT_HOST fields are mandatory)</p> <pre><code>*/10 * * * *  docker rm snowbit-cspm ; docker rmi coralogixrepo/snowbit-cspm:latest ; docker run --name snowbit-cspm -d -e PYTHONUNBUFFERED=1 -e CLOUD_PROVIDER=\"gcp\" -e COMPANY_ID=&lt;coralogix_company_ID&gt; -e CORALOGIX_ENDPOINT_HOST=\"coralogix_grpc_endpoint\" -e APPLICATION_NAME=\"application_name\" -e SUBSYSTEM_NAME=\"subsystem_name\" -e TESTER_LIST=\"\" -e API_KEY=\"send_your_data_api_key\" -e REGION_LIST=\"\" -e GOOGLE_APPLICATION_CREDENTIALS=\"path_to_credentials_file_in_the_container\" -v &lt;local_folder_that_contains_gcp_credentials&gt;:&lt;location_to_map_the_credentials_inside_the_container&gt; coralogixrepo/snowbit-cspm\n</code></pre>"},{"location":"newoutput/cloud-security-posture-cspm-gcp/#configuring-the-scan-settings","title":"Configuring the scan settings","text":"<p>Inside the security tab in your Coralogix account, you will find the SCAN SETTINGS button:</p> <ol> <li> <p>Scan Now will start the scan of the selected environment(s) in up to 10 minutes from pressing (according to the configured cronjob)</p> </li> <li> <p>Scan Schedule allows choosing the frequency of the scans, and the start time of each scan. the default is every 24 hours</p> </li> </ol> <p></p>"},{"location":"newoutput/cloud-security-posture-management-cspm/","title":"Cloud Security Posture Management (CSPM)","text":"<p>Reduce your attack surface and prevent risk of cloud data security breaches by assessing the overall security posture of your entire cloud environment against best practices and compliance standards using our Cloud Security Posture Management (CSMP).</p>"},{"location":"newoutput/cloud-security-posture-management-cspm/#what-is-cspm","title":"What is CSPM?","text":"<p>Cloud Security Posture Management (CSPM) is a cybersecurity approach that focuses on continuously monitoring and improving the security of cloud resources and configurations. CSPM tools automate the process of assessing cloud environments, verifying that cloud configurations follow security best practices and compliance standards such as ISO, CIS AWS, SOC 2, PCI, and HIPAA frameworks. This provides organizations with real-time visibility, proactive risk mitigation, and the ability to maintain a strong security posture across their cloud deployments. As companies increasingly move to the cloud, CSPM has become a necessary aspect of security insights.</p>"},{"location":"newoutput/cloud-security-posture-management-cspm/#how-it-works","title":"How It Works","text":"<p>Our CSPM solution is based on an agent that runs in the customer\u2019s cloud environment. The CSPM agent performs tests to assess the security posture - checking if the current cloud setting is aligned with the recommended best practice or not. The test results are then sent to Coralogix to be displayed in the Coralogix Security UI.</p> <p>In order to set up CSPM, you will be required to take the following steps:</p> <ol> <li> <p>Create roles necessary for resource scanning;</p> </li> <li> <p>Install the CSPM agent using a virtual machine or orchestration tool such as Kubernetes;</p> </li> <li> <p>Configure settings, including scanning and environment settings, as well as those designed to send scanning results to Coralogix for visualization in your dashboard.</p> </li> </ol>"},{"location":"newoutput/cloud-security-posture-management-cspm/#supported-services","title":"Supported Services","text":"<p>Below you can find a list of the services that we currently support.</p>"},{"location":"newoutput/cloud-security-posture-management-cspm/#cspm-testing-availability","title":"CSPM Testing Availability","text":"<ul> <li> <p>AWS (named: \"aws\"): WAF, KMS, Lambda, S3, RDS, EC2, IAM, CloudTrail, CloudFront, VPC, DocumentDB, FSx, DynamoDB/Dax, SQS, Api Gateway, Security Hub, ECR, ELB (v1/v2), EBS, Backup, EMR, AutoScaling, EKS, CloudWatch, Redshift, OpenSearch, NetworkFirewall, Elasticache</p> </li> <li> <p>GCP (named: \"gcp\"): CloudDNS, CloudSQL, API Keys, GCE, GKE, VPC, Resource manager, Cloud Storage, Cloud Logging, Cloud Load Balancer, PubSub, Dataproc, KMS, IAM</p> </li> </ul>"},{"location":"newoutput/cloud-security-posture-management-cspm/#sspm-testing-availability","title":"SSPM Testing Availability","text":"<ul> <li>GitHub (named: \"github\")</li> </ul>"},{"location":"newoutput/cloud-security-posture-management-cspm/#create-roles","title":"Create Roles","text":"<p>Create roles to grant relevant permissions to scan resources necessary for the CSPM agent.</p>"},{"location":"newoutput/cloud-security-posture-management-cspm/#aws","title":"AWS","text":"<p>STEP 1. In each AWS account containing resources to scan, go to AWS IAM and create a role with these permissions:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"CSPM\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"access-analyzer:Get*\",\n                \"access-analyzer:List*\",\n                \"acm:Describe*\",\n                \"apigateway:Get*\",\n                \"application-autoscaling:Describe*\",\n                \"autoscaling-plans:Describe*\",\n                \"autoscaling-plans:GetScalingPlanResourceForecastData\",\n                \"autoscaling:Describe*\",\n                \"autoscaling:GetPredictiveScalingForecast\",\n                \"cloudformation:BatchDescribeTypeConfigurations\",\n                \"cloudformation:Describe*\",\n                \"cloudformation:DetectStack*\",\n                \"cloudformation:EstimateTemplateCost\",\n                \"cloudformation:Get*\",\n                \"cloudformation:List*\",\n                \"cloudformation:ValidateTemplate\",\n                \"cloudfront:DescribeFunction\",\n                \"cloudfront:Get*\",\n                \"cloudfront:List*\",\n                \"cloudtrail:Describe*\",\n                \"cloudtrail:Get*\",\n                \"cloudtrail:List*\",\n                \"cloudtrail:LookupEvents\",\n                \"cloudwatch:Describe*\",\n                \"cloudwatch:Get*\",\n                \"cloudwatch:List*\",\n                \"dms:Describe*\",\n                \"ec2:Describe*\",\n                \"ec2:ExportClientVpn*\",\n                \"ec2:Get*\",\n                \"ec2:List*\",\n                \"ec2:Search*\",\n                \"ec2messages:Get*\",\n                \"eks:Describe*\",\n                \"eks:List*\",\n                \"elasticache:Describe*\",\n                \"elasticache:List*\",\n                \"elasticloadbalancing:Describe*\",\n                \"elasticmapreduce:Describe*\",\n                \"elasticmapreduce:Get*\",\n                \"elasticmapreduce:List*\",\n                \"elasticmapreduce:ViewEventsFromAllClustersInConsole\",\n                \"emr-containers:Describe*\",\n                \"emr-containers:List*\",\n                \"emr-serverless:Get*\",\n                \"emr-serverless:List*\",\n                \"es:Describe*\",\n                \"es:Get*\",\n                \"es:List*\",\n                \"iam:Generate*\",\n                \"iam:Get*\",\n                \"iam:List*\",\n                \"iam:Simulate*\",\n                \"imagebuilder:Get*\",\n                \"imagebuilder:List*\",\n                \"kms:Describe*\",\n                \"kms:Get*\",\n                \"kms:List*\",\n                \"lambda:Get*\",\n                \"lambda:List*\",\n                \"network-firewall:Describe*\",\n                \"network-firewall:List*\",\n                \"organizations:Describe*\",\n                \"organizations:List*\",\n                \"rds:Describe*\",\n                \"redshift:Describe*\",\n                \"redshift:List*\",\n                \"redshift:ViewQueries*\",\n                \"rolesanywhere:Get*\",\n                \"rolesanywhere:list*\",\n                \"route53:Get*\",\n                \"route53:List*\",\n                \"route53:TestDNSAnswer\",\n                \"route53domains:CheckDomain*\",\n                \"route53domains:Get*\",\n                \"route53domains:List*\",\n                \"route53domains:ViewBilling\",\n                \"s3:Describe*\",\n                \"s3:List*\",\n                \"s3:GetBucketPublicAccessBlock\",\n                \"s3:GetBucketPolicyStatus\",\n                \"s3:GetEncryptionConfiguration\",\n                \"s3:GetAccountPublicAccessBlock\",\n                \"s3:GetBucketLogging\",\n                \"s3:GetBucketVersioning\",\n                \"s3:GetBucketAcl\",\n                \"s3:GetBucketLocation\",\n                \"s3:GetBucketPolicy\",\n                \"servicequotas:Get*\",\n                \"servicequotas:List*\",\n                \"ses:Describe*\",\n                \"ses:Get*\",\n                \"ses:List*\",\n                \"sqs:Get*\",\n                \"sqs:List*\",\n                \"ssm:Describe*\",\n                \"ssm:Get*\",\n                \"ssm:List*\",\n                \"sts:Get*\",\n                \"tag:Get*\",\n                \"waf-regional:Get*\",\n                \"waf-regional:List*\",\n                \"waf:Get*\",\n                \"waf:List*\",\n                \"wafv2:Describe*\",\n                \"wafv2:Get*\",\n                \"wafv2:List*\",\n                \"elasticfilesystem:List*\",\n                \"elasticfilesystem:Get*\",\n                \"backup:List*\",\n                \"backup:Get*\",\n                \"redshift:Describe*\",\n                \"redshift:ViewQueriesInConsole\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n</code></pre> <p>STEP 2. If the CSPM agent will be deployed in a different AWS account than your resources, follow this step.</p> <p>1. In the AWS account where the CSPM agent is deployed:</p> <p>a. Go to AWS IAM and create a role with this permission. In the \u201cResource\u201d field, specify the Role ARN of all roles created in STEP 1 above.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"CSPM\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n                        \"Resource\": [\n                            \"arn:aws:iam::111222333444:role/MyRole1\",\n                            \"arn:aws:iam::111222333455:role/MyRole2\"\n                        ]\n        }\n    ]\n}\n\n</code></pre> <p>b. Copy the Role ARN to be used below.</p> <p>2. In each of the AWS accounts where resources to scan exist (including the account where the CSPM agent will be deployed, if it contains resources to be scanned):</p> <p>a. Go to AWS IAM and select the role created in STEP 1 above.</p> <p>b. In the Trust Relationships tab, add this section while pasting the Role ARN copied from STEP 2.1.b above into the \u201cAWS\u201d field:</p> <pre><code>{   \n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::111222333444:role/MyRole\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre> <p>c. Copy the Role ARN of this role to be used in the CSPM agent configuration file below.</p>"},{"location":"newoutput/cloud-security-posture-management-cspm/#gcp","title":"GCP","text":"<p>STEP 1. Create a service account with the following permissions in the Google organization, folder, or project in which you have resources to scan.</p> <pre><code>apikeys.keys.list\nbigquery.datasets.get\ncloudkms.cryptoKeys.getIamPolicy\ncloudkms.cryptoKeys.list\ncloudkms.keyRings.list\ncloudsql.instances.get\ncloudsql.instances.list\ncompute.firewalls.list\ncompute.instances.list\ncompute.networks.list\ncompute.projects.get\ncompute.regions.get\ncompute.regions.list\ncompute.sslPolicies.get\ncompute.subnetworks.list\ncompute.targetHttpsProxies.list\ncontainer.clusters.list\ndns.managedZones.get\ndns.managedZones.list\niam.serviceAccountKeys.list\niam.serviceAccounts.list\nlogging.logMetrics.list\nmonitoring.alertPolicies.list\nmonitoring.notificationChannels.get\nmonitoring.notificationChannels.list\nresourcemanager.projects.get\nresourcemanager.projects.getIamPolicy\nstorage.buckets.get\nstorage.buckets.getIamPolicy\nstorage.buckets.list\ncompute.zones.list\n\n</code></pre> <p>STEP 2. Copy the email addresses of the service accounts you created, to be used in the CSPM agent configuration file.</p>"},{"location":"newoutput/cloud-security-posture-management-cspm/#github","title":"GitHub","text":"<p>STEP 1. Define a user as the owner for all of the organizations you wish to scan, while following this procedure.</p> <p>STEP 2. Create a personal access token (classic) for this user. Grant the the following scopes to this new token:</p> <pre><code>`read:audit_log`\n`read:enterprise`\n`read:gpg_key`\n`read:org`\n`read:project`\n`read:public_key`\n`read:repo_hook`\n`read:ssh_signing_key`\n`read:user`\n`repo`\n`user:email`\n\n</code></pre>"},{"location":"newoutput/cloud-security-posture-management-cspm/#installation-using-a-virtual-machine","title":"Installation using a Virtual Machine","text":""},{"location":"newoutput/cloud-security-posture-management-cspm/#prerequisites","title":"Prerequisites","text":"<p>The following are minimum hardware requirements:</p> <ul> <li> <p>4 GiB RAM (To scan large cloud environments with 1000+ resources, use 8 GiB RAM or more.)</p> </li> <li> <p>2+ CPU cores</p> </li> </ul>"},{"location":"newoutput/cloud-security-posture-management-cspm/#installation","title":"Installation","text":"<p>STEP 1. Access a virtual machine (such as an AWS EC2 instance) sized according to the guidelines in the prerequisites. Obtain terminal access (for example via SSH).</p> <p>STEP 2. Install\u00a0Docker.</p> <p>STEP 3. This step will vary depending on whether you are using AWS or GCP.</p> <p>For AWS:</p> <ul> <li> <p>Select the EC2 instance. Click Actions &gt; Security &gt; Modify IAM role.</p> <ul> <li> <p>To scan resources in the same account as the EC2 instance, select the role created in STEP 1 in Create Roles &gt; AWS section above.</p> </li> <li> <p>To only scan resources in an account other than where the EC2 instance is deployed, select the role created in STEP 2.2 in the Create Roles &gt; AWS section above.</p> </li> </ul> </li> <li> <p>After the role selection, click Update IAM role.</p> </li> </ul> <p>For GCP:</p> <ul> <li> <p>Open the\u00a0Google Cloud Console.</p> </li> <li> <p>Navigate to the Compute Engine section.</p> </li> <li> <p>Select the desired GCE instance.</p> </li> <li> <p>Click Edit to modify the instance.</p> </li> <li> <p>In the Service account section, click Add item.</p> </li> <li> <p>Select the service account created in the Create Roles &gt; GCP section above.</p> </li> <li> <p>Click Save.</p> </li> </ul> <p>STEP 4. Save the default configuration file as <code>local.yaml</code>\u00a0in a local directory in the EC2.</p> <ul> <li> <p>Configure the relevant sections for the resources you wish to scan:</p> <ul> <li> <p>Configuration File &gt; AWS configuration file</p> </li> <li> <p>Configuration File &gt; GCP configuration file</p> </li> <li> <p>Configuration File&gt; GitHub configuration file</p> </li> </ul> </li> <li> <p>Configure the Coralogix section following in the steps in Configuration File &gt; Coralogix Configuration.</p> </li> </ul> <p>STEP 5. Run the following command\u00a0in a terminal from the folder within which the <code>local.yaml</code> file is located:</p> <pre><code>docker run -d -v ./local.yaml:/cxa/config/local.yaml -e RUST_LOG=info coralogixrepo/cspm-agent:latest\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>For AWS, if the same CSPM agent is used for scanning resources in multiple AWS accounts, repeat STEP 4 and STEP 5 for each AWS account you wish to scan.</p> </li> <li> <p>Use a different name for the configuration file for each AWS account (e.g. <code>local-myaccount.yaml</code>, <code>local-myaccount1.yaml</code>, <code>local-myaccount2.yaml</code>, etc.)</p> </li> </ul>"},{"location":"newoutput/cloud-security-posture-management-cspm/#installation-using-an-orchestration-tool","title":"Installation using an Orchestration Tool","text":"<p>STEP 1. Install kubectl.</p> <p>STEP 2. Save this file as <code>cspm-agent.yaml</code>\u00a0in the current directory:</p> <ul> <li> <p>File for AWS</p> </li> <li> <p>File for GCP</p> </li> </ul> <p>STEP 3. This step will vary depending on whether you are using AWS or GCP.</p> <p>For AWS:</p> <p>Attach the role to the cluster by following these steps.</p> <ul> <li> <p>Open the Amazon EKS console and select your cluster.</p> </li> <li> <p>Select Configuration in the left-hand navigation pane.</p> </li> <li> <p>Select Security in the cluster details page.</p> </li> <li> <p>Under Service account settings, select Add or remove IAM roles from service accounts.</p> </li> <li> <p>Select the relevant namespace and service account.</p> </li> <li> <p>Choose Add IAM role and select this role:</p> <ul> <li> <p>To scan resources in the same account as the cluster, select the role created in STEP 1 in the Create Roles &gt; AWS section above.</p> </li> <li> <p>To only scan resources in an account other than where the cluster is deployed, select the role created in STEP 2.2 in the Create Roles &gt; AWS section above.</p> </li> </ul> </li> <li> <p>Save the changes.</p> </li> </ul> <p>For GCP:</p> <p>Create a GKE node pool and attach the service account by following these steps.</p> <ul> <li> <p>Open the\u00a0Google Cloud Console.</p> </li> <li> <p>Navigate to the GKE section.</p> </li> <li> <p>Select the desired GKE cluster.</p> </li> <li> <p>Click on the Node Pools tab.</p> </li> <li> <p>Click on the Add Node Pool button.</p> </li> <li> <p>In the Node pool details section, provide the name CSPM-POOL for the node pool.</p> </li> <li> <p>In the Service account section, click Add item.</p> </li> <li> <p>Select the service account created in Create Roles &gt; GCP section above.</p> </li> <li> <p>Configure other settings for the node pool as per your requirements.</p> </li> <li> <p>Click Create to create the node pool.</p> </li> </ul> <p>STEP 4. Save the default configuration file as <code>local.yaml</code>\u00a0in the same directory in which the <code>cspm-agent.yaml</code> file is saved. Configure the <code>local.yaml</code>\u00a0file as follows:</p> <ul> <li> <p>Configure the relevant sections for the resources you wish to scan:</p> <ul> <li> <p>Configuration File &gt; AWS Configuration</p> </li> <li> <p>Configuration File &gt; GCP Configuration</p> </li> <li> <p>Configuration File&gt; GitHub Configuration</p> </li> </ul> </li> <li> <p>Configure the Coralogix section following in the steps in Configuration File &gt; Coralogix Configuration.</p> </li> </ul> <p>STEP 5. Run the following commands from the directory where the <code>cspm-agent.yaml</code> and <code>local.yaml</code> files are saved:</p> <pre><code>kubectl create configmap cspm-config --from-file=./local.yaml\nkubectl apply -f cspm-agent.yaml\n\n</code></pre> <p>STEP 6. To stay up to date with the latest agent version, use a script to automatically rerun these commands periodically. It will automatically update the running agent when a newer version is released.</p> <ul> <li> <p>We recommend verifying each new version in a staging environment before updating it in production.</p> </li> <li> <p>Run the above script daily as\u00a0a cron job\u00a0or use your favorite CI/CD platform to only update minor versions.</p> </li> <li> <p>Here are some example links:</p> <ul> <li> <p>https://argocd-image-updater.readthedocs.io/en/stable/</p> </li> <li> <p>https://fluxcd.io/flux/guides/image-update/</p> </li> </ul> </li> </ul> <p>Notes:</p> <ul> <li> <p>For AWS, if the same CSPM agent is used for scanning resources in multiple AWS accounts, repeat STEP 4 and STEP 5 for each AWS account you wish to scan.</p> </li> <li> <p>Use a different name for the configuration file for each AWS account (e.g. <code>local-myaccount.yaml</code>).</p> </li> </ul>"},{"location":"newoutput/cloud-security-posture-management-cspm/#configuration","title":"Configuration","text":"<p>A configuration file is used to define scanning and environment settings essential for the CSPM agent. Download the default configuration file and save it locally as instructed above. The configuration will vary depending on whether AWS, GCP, or GitHub is used. Once complete, configure the Coralogix section to send the scanning results to be displayed in your Coralogix dashboard.</p>"},{"location":"newoutput/cloud-security-posture-management-cspm/#aws-configuration","title":"AWS Configuration","text":"<p>Unmark the AWS section and configure the relevant keys:</p> <pre><code># AWS\naws:\n# Default region\nregion: \"eu-west-1\"\n# Retries in case the AWS API fails\nretries: 100\n# Optional role to use for querying the data\niam_role: \"my-role\"\n# Regions to include for testers\ninclude_regions: [ \"sa-east-1\", \"us-east-2\", \"us-west-1\", \"us-west-2\" ]\n# In cross account access settings, test results from this master account are going to be skipped\nmaster_account_id: 123456789012\n\n</code></pre> Key Description region Fill in the AWS region where the CSPM agent is deployed (even if the agent is not expected to scan the AWS account where it is deployed) retries Select the number of scan retries in case of failure. iam_role Optional: for scanning resources in a different AWS account, fill in the Role ARN copied in [STEP 2.2.c in the Create Roles &gt; AWS section above]. include_regions [Optional] List all AWS regions of resources to scan, in addition to the \u201cregion\u201d specified above, if any. master_account_id [Optional] For scanning resources in a different AWS account, fill in the AWS account ID where the CSPM agent is deployed."},{"location":"newoutput/cloud-security-posture-management-cspm/#gcp-configuration","title":"GCP Configuration","text":"<p>Unmark the GCP section and configure the relevant keys:</p> <pre><code># GCP\ngcp:\n  organization_configs:\n  - organization_id: my-org-id\n    organization_number: 123456789123\n    client_email: \"***\"\n  folder_configs: \n  - folder_id: my-folder-id\n    folder_number: 123456789123\n    client_email: \"***\"\n    project_configs:\n  - project_id: my-project-id\n        project_number: 123456789123\n    client_email: \"***\"\n  postgres_max_connections: \"100\"\n  sql_server_user_connections: \"100\"\n# Your organization's email domain\norganization_email_domain: \"coralogixstg.wpengine.com\"\n# Duration in seconds \nstorage_bucket_retention_period: 86400\n\n</code></pre> <ul> <li> <p>For each service account created in Create Roles &gt; GCP section above, fill in the relevant section according to the level where the service account was created (Google organization, folder or project):</p> <ul> <li> <p><code>organization_id</code> / <code>folder_id</code> / <code>project_id</code></p> </li> <li> <p><code>organization_number</code> / <code>folder_number</code> / <code>project_number</code></p> </li> <li> <p><code>client_email</code> : the email identifier of the service account</p> </li> </ul> </li> <li> <p>For multiple service accounts in the same level (Google organization, folder or project), duplicate the section starting with \u201c-\u201d. Make sure to keep the same structure and spaces.</p> </li> <li> <p>Remove the redundant sections with no service accounts in that level.</p> </li> <li> <p><code>postgres_max_connections</code> : For scanning Postgres DB resource, set the maximum number of connections that the CSPM agent would initiate to a Postgres DB in parallel.</p> </li> <li> <p><code>sql_server_user_connections</code> : For scanning SQL Server resource, set the maximum number of connections that the CSPM agent would initiate to an SQL Server in parallel.</p> </li> <li> <p><code>organization_email_domain</code> : Fill in the organization\u2019s email domain. The CSPM tests will use this to identify external users.</p> </li> <li> <p><code>storage_bucket_retention_period</code> : Fill in the expected retention period in seconds for storage buckets. The CSPM tests will verify that this retention period is enforced.</p> </li> </ul>"},{"location":"newoutput/cloud-security-posture-management-cspm/#github-configuration","title":"GitHub Configuration","text":"<p>Unmark the GitHub section and configure the relevant keys:</p> <pre><code># GitHub\ngithub:\n# GitHub (classic) private access token. The required permissions are: \n# read:audit_log, read:enterprise, read:gpg_key, read:org, read:project, read:public_key, read:repo_hook, read:ssh_signing_key, read:user, repo, user:email\ntoken: \"ghp_99eiK0RNNRgYwLK8UT34PZybr1nDZoR\"\n# Which (GitHub) organizations to include in the testing. Note that the account has to have \"owner\" permissions for some testers. \norgs: [\"my-org\"]\n\n</code></pre> Key Description token Fill in the token created in STEP 2 in the Create Roles &gt; GitHub section above. orgs Fill in all the organization names you wish to scan."},{"location":"newoutput/cloud-security-posture-management-cspm/#coralogix-configuration","title":"Coralogix Configuration","text":"<p>Configure the Coralogix section to send the scanning results to be displayed in Coralogix:</p> <pre><code>### CORALOGIX \n# Coralogix connection configuration\n###\n\ncoralogix:\n  # gRPC endpoint of the Coralogix cluster. Check the docs &lt;https://coralogixstg.wpengine.com/docs/cloud-security-posture-cspm/&gt;\n  grpc_endpoint: https://ng-api-grpc.&lt;coralogix-domain&gt;\n  # logs endpoint of the Coralogix cluster. Check the docs &lt;https://coralogixstg.wpengine.com/docs/coralogix-domain/&gt;\n  logs_endpoint: https://ingress.&lt;coralogix-domain&gt;\n  # Your Coralogix private key\n  api_key: 877E1EB0-EBE2-4EEF-9170-9B418F98F654\n  # Meta data application name for Coralogix\n  application_name: MyApplication\n  # Meta data subsystem name for Coralogix\n  subsystem_name: MySubsystem\n  # If true doesn't actually send the data\n  dry_run: true\n\n</code></pre> Key Description <code>grpc_endpoint</code> Update the  in the URL using your Coralogix domain. <code>logs_endpoint</code> Update the  in the URL using your Coralogix domain. The CSPM agent sends the scanning results to this URL. <code>api_key</code> Your Coralogix Send-Your-Data API key <code>application_name</code> / <code>subsystem_name</code> Metadata fields that will be added to the results displayed in your Coralogix dashboard and will allow you to filter. <code>dry_run</code> false: Results sent to Coralogix true: For debugging mode, scan the resources without sending the logs to Coralogix."},{"location":"newoutput/cloud-security-posture-management-cspm/#scanning","title":"Scanning","text":"<p>Coralogix triggers a scan, referred to as a run, by default every 24 hours. You can change this setting by navigating to Security in your Coralogix toobar, after logging into your Coralogix instance. Click on SCAN SETTINGS in the upper right-hand corner.</p> <p></p>"},{"location":"newoutput/cloud-security-posture-management-cspm/#additional-resources","title":"Additional Resources","text":"DocumentationDownloading Your Security Report"},{"location":"newoutput/cloud-security-posture-management-cspm/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/cloud-security-quick-start/","title":"Coralogix Cloud Security Quick Start","text":"<p>Coralogix Cloud Security helps you detect security threats across all of your network traffic with rapid setup and without the need for additional tooling. Once running, Cloud Security easily integrates all your security logs for a multidimensional view of your security posture and gives you the ability to perform deep and wide forensic investigations.</p> <p>Cloud Security runs on your AWS account to provide real-time monitoring and analysis of your infrastructure.</p>"},{"location":"newoutput/cloud-security-quick-start/#with-coralogix-sta-you-can-do-the-following","title":"With Coralogix STA you can do the following","text":"<ul> <li> <p>Detect system intrusions</p> </li> <li> <p>Monitor your entire enterprise for unauthorized changes</p> </li> <li> <p>Centrally manage and analyze all security-related logs</p> </li> </ul>"},{"location":"newoutput/cloud-security-quick-start/#known-issues-limitations","title":"Known Issues / Limitations","text":"<ol> <li> <p>The Cloud Security instance MUST be installed in a VPC that has access to the Internet in order to send the logs to Coralogix</p> </li> <li> <p>The Cloud Security instance hasn\u2019t been tested in private VPCs</p> </li> <li> <p>The Cloud Security can be installed only in the following regions:</p> <ul> <li> <p>eu-west-1 (Ireland)</p> </li> <li> <p>ap-south-1 (Mumbai)</p> </li> <li> <p>us-east-1 (N. Virginia)</p> </li> <li> <p>us-east-2 (Ohio)</p> </li> <li> <p>us-west-1 (N. California)</p> </li> <li> <p>us-west-2 (Oregon)</p> </li> </ul> </li> <li> <p>Currently, the Cloud Security solution doesn\u2019t offer access to the actual packets that were captured</p> </li> <li> <p>No alerts are created in Coralogix during the installation. Security alerts must be created manually</p> </li> <li> <p>Mirroring or autoscaling instances are not supported</p> </li> <li> <p>Mirroring of non-Nitro-based instances are not supported</p> </li> <li> <p>Currently, you need to create individual Mirror Sessions for each Network Interface that you want to mirror</p> </li> </ol>"},{"location":"newoutput/cloud-security-quick-start/#next-steps","title":"Next Steps","text":"<p>The STA has several key concepts that you should understand to make the most of it.</p> <p>Of-course you can jump straight to the Installation part, use the default configurations and later on do later on the desired drill down.</p> <p>See the following links to navigate through the STA\u2019s documentation.</p> <p>How to install Coralogix STA</p>"},{"location":"newoutput/cloudflare-audit-logs/","title":"Cloudflare Data Ingestion","text":"<p>Collect your Cloudflare events in the Coralogix platform using our automatic Contextual Data Integration Package.</p>"},{"location":"newoutput/cloudflare-audit-logs/#overview","title":"Overview","text":"<p>Cloudflare is a prominent technology company that offers a range of internet services aimed at enhancing security, performance, and reliability for websites and online applications. By operating as a content delivery network (CDN), Cloudflare optimizes the delivery of web content and mitigates cyber threats like DDoS attacks by distributing traffic across its global network of servers. Additionally, Cloudflare provides services such as DNS management, load balancing, and firewall protection, enabling businesses to improve their online presence and user experience while safeguarding against various online threats.</p> <p>Sending your Cloudflare events to Coralogix enables centralized log management, proactive monitoring, security analysis, and performance optimization. This integration empowers you to consolidate event data, set up custom alerts, detect security threats, identify performance bottlenecks, and maintain compliance, ultimately enhancing application security, reliability, and user experience through data-driven insights and actionable intelligence.</p>"},{"location":"newoutput/cloudflare-audit-logs/#get-started","title":"Get Started","text":"<p>STEP 1.\u00a0In your navigation pane, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 2.\u00a0In the Contextual Data section, select Cloudflare and click\u00a0+ ADD.</p> <p></p> <p>STEP 4. Click ADD NEW.</p> <p>STEP 5.\u00a0Fill in the Integration Details:</p> <p></p> <ul> <li> <p>Integration Name.\u00a0Name your integration.</p> </li> <li> <p>Authentication Email. The e-mail address you use to login to Cloudflare.</p> </li> <li> <p>Cloudflare API Key. Go to your Cloudflare account, click on My Profile -&gt; API Tokens -&gt; Global API Key -&gt; View and copy the Global API key.</p> </li> </ul> <p></p> <ul> <li> <p>AccountName Subdomain. Either your Cloudflare Account Name or you can provide another name which will be your subsystem name in Coralogix.</p> </li> <li> <p>Account ID. In your Cloudflare account home page, go to the URL and copy the Account ID from the URL.</p> </li> </ul> <p></p> <p>STEP 6.\u00a0In the Coralogix integration page, click\u00a0SAVE.</p> <p></p>"},{"location":"newoutput/cloudflare-audit-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/cloudflare-coralogix/","title":"Cloudflare","text":"<p>Cloudflare Enterprise customers have access to Logpush service, which allows you to forward logs to cloud service providers like AWS. This tutorial demonstrates how to send your logs to Coralogix.</p>"},{"location":"newoutput/cloudflare-coralogix/#send-logs-directly","title":"Send Logs Directly","text":""},{"location":"newoutput/cloudflare-coralogix/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Coralogix Send-Your-Data API key</p> </li> <li> <p>Cloudflare ZoneID / AccountID - Find the relevant id in the cloudflare dashboard under \u2018Websites\u2019 -&gt; \u2018\u2019 -&gt; \u2018API (scroll down)\u2019."},{"location":"newoutput/cloudflare-coralogix/#configuration","title":"Configuration","text":"<p>To start sending data directly to Coralogix, select the type of logs (data-sets) and fields that will be sent here.</p> <p>To create the logpush job, call the API using the terminal. Input your Coralogix\u00a0domain\u00a0to create your endpoint URL:\u00a0<code>https://ingress.**&lt;cx_domain&gt;**/cloudflare/v1/logs</code>.  </p> <pre><code>curl -s https://api.cloudflare.com/client/v4/zones/&lt;zone_id&gt;/logpush/jobs -X POST \\\n-H \"Content-Type:application/json\" \\\n-d '{\n  \"name\": \"logpush-to-coralogix\",\n  \"logpull_options\": \"fields=BotScoreCloudflare,BotScoreSrc,BotTags,CacheCacheStatus,CacheResponseBytes,CacheResponseStatus,CacheTieredFill,ClientASN,ClientCountry,ClientDeviceType,ClientIP,ClientIPClass,ClientMTLSAuthCertFingerprint,ClientMTLSAuthStatus,ClientRequestBytes,ClientRequestHost,ClientRequestMethod,ClientRequestPath,ClientRequestProtocol,ClientRequestReferer,ClientRequestScheme,ClientRequestSource,ClientRequestURI,ClientRequestUserAgent,ClientSSLCipher,ClientSSLProtocol,ClientSrcPort,ClientTCPRTTMs,ClientXRequestedWith,EdgeCFConnectingO2O,EdgeColoCode,EdgeColoID,EdgeEndTimestamp,EdgePathingOp,EdgePathingSrc,EdgePathingStatus,EdgeRateLimitAction,EdgeRateLimitID,EdgeRequestHost,EdgeResponseBodyBytes,EdgeResponseBytes,EdgeResponseCompressionRatio,EdgeResponseContentType,EdgeResponseStatus,EdgeServerIP,EdgeTimeToFirstByteMs,FirewallMatchesActions,FirewallMatchesRuleIDs,FirewallMatchesSources,JA3Hash,OriginDNSResponseTimeMs,OriginIP,OriginRequestHeaderSendDurationMs,OriginResponseBytes,OriginResponseDurationMs,OriginResponseHTTPExpires,OriginResponseHTTPLastModified,OriginResponseHeaderReceiveDurationMs,OriginResponseStatus,OriginResponseTime,OriginSSLProtocol,OriginTCPHandshakeDurationMs,OriginTLSHandshakeDurationMs,ParentRayID,RayID,RequestHeaders,ResponseHeaders,SecurityLevel,SmartRouteColoID,UpperTierColoID,WAFAction,WAFFlags,WAFMatchedVar,WAFProfile,WAFRuleID,WAFRuleMessage,WorkerCPUTime,WorkerStatus,WorkerSubrequest,WorkerSubrequestCount,ZoneID,ZoneName&amp;timestamps=unixnano\",\n  \"destination_conf\": \"&lt;https://ingress.&lt;coralogix_domain&gt;/cloudflare/v1/logs&gt;?header_Authorization=Bearer%20&lt;Send_your_data_key&gt;&amp;header_timestamp-format=UnixNano&amp;header_dataset=HTTPRequests\",\n  \"max_upload_bytes\": 5000000,\n  \"max_upload_records\": 1000,\n  \"dataset\": \"http_requests\",\n  \"enabled\": true,\n  \"frequency\": \"low\"\n}' \\\n-H \"X-Auth-Email: &lt;Your_Auth_Email&gt;\" \\\n-H \"X-Auth-Key: &lt;Your_API_Key&gt;\"\n</code></pre> <p>Notes:</p> <ul> <li> <p>Replace  with your site zone id,  with your coralogix Send-Your-Data API key and 'X-Auth-Email' 'X-Auth-Key' with your cloudflare credentials. <li> <p>To change the dataset sent change the 'dataset' field and `header_dataset` inside the 'destination_conf' field using the table below.</p> </li> <li> <p>To change the fields sent change the 'logpull_options' field, each dataset has different fields.</p> </li> <li> <p>Each dataset has a different 'timestamp' key, providing it in the 'logpull_options' field is required, also using the unixnano format for the timestamp is also required. Find out more about log fields here.</p> </li> <li> <p>To configure Account-scoped datasets use 'https://api.cloudflare.com/client/v4/accounts//logpush/jobs'. <li> <p>By default, the integration will set <code>application_name</code> as <code>Cloudflare</code>, and <code>subsystem_name</code> as the data set name. To overwrite these parameters, add the following:</p> <ul> <li> <p><code>header_CX-Application-Name</code> - application name override</p> </li> <li> <p><code>header_CX-Subsystem-Name</code> - subsystem name override</p> </li> </ul> </li> Dataset nameHeader name'Timestamp' keyScopedns_logsDNSLogsTimestampZonefirewall_eventsFirewallEventsDatetimeZonehttp_requestsHTTPRequestsEdgeStartTimestampZonenel_reportsNELReportsTimestampZonespectrum_eventsSpectrumEventsTimestampZoneaudit_logsAuditLogsWhenAccountgateway_dnsGatewayDNSDatetimeAccountgateway_httpGatewayHTTPDatetimeAccountgateway_networkGatewayNetworkDatetimeAccountnetwork_analytics_logsNetworkAnalyticsLogsDatetimeAccountaccess_requestsAccessRequestsCreatedAtAccountcasb_findingsCASBFindingsDetectedTimestampAccountdns_firewall_logsDNSFirewallLogsTimestampAccountmagic_ids_detectionsMagicIDSDetectionsTimestampAccountworkers_trace_eventsWorkersTraceEventsEventTimestampMsAccount <p>After creating the logpush, view it in the terminal.</p> <pre><code>curl -s https://api.cloudflare.com/client/v4/zones/&lt;Zone_ID&gt;/logpush/jobs -X GET \\\n-H \"X-Auth-Email: &lt;Your_Auth_Email&gt;\" \\\n-H \"X-Auth-Key: &lt;Your_Auth-Key&gt;\"\n</code></pre> <p>Or in the dashboard itself under \u2018Websites\u2019 -&gt; \u2018\u2019 -&gt; \u2018Analytics\u2019 -&gt; \u2018Logs\u2019."},{"location":"newoutput/cloudflare-coralogix/#send-logs-via-s3-bucket","title":"Send Logs via S3 Bucket","text":""},{"location":"newoutput/cloudflare-coralogix/#prerequisites_1","title":"Prerequisites","text":"<p>AWS S3 bucket -\u00a0Follow the tutorial to send logs from the S3 bucket to Coralogix: https://coralogixstg.wpengine.com/integrations/data-collection-s3/</p>"},{"location":"newoutput/cloudflare-coralogix/#configuration_1","title":"Configuration","text":"<p>To enable the Clouflare Logpush service:</p> <ol> <li> <p>Log in to the Cloudflare dashboard</p> </li> <li> <p>Ensure the Enterprise domain you want to use with Logpush is selected</p> </li> <li> <p>Select the Analytics app in the top menu</p> </li> <li> <p>Select the Logs section in the secondary menu </p> </li> <li> <p>Select Connect a service. A modal window opens where you will need to complete several steps. </p> </li> <li> <p>Under Select service, pick your Amazon S3 and click Next. </p> </li> <li> <p>Enter or select the following:</p> <ul> <li> <p>Bucket path</p> </li> <li> <p>Daily subfolders</p> </li> <li> <p>Bucket region </p> </li> </ul> </li> <li> <p>Copy the policy from the field below \"Grant Cloudflare access to upload files to your bucket\"</p> </li> <li> <p>In new window go to S3 &gt; your bucket name &gt; Permissions &gt; Bucket Policy</p> </li> <li> <p>Click on the Edit button at the Bucket Policy.</p> </li> <li> <p>Paste the copied policy and save changes.</p> </li> <li> <p>In the Cloudflare dashboard, click Validate access.</p> </li> <li> <p>Follow the on-screen instructions to enter the Ownership token (included in a file Cloudflare sends to your Amazon S3 bucket).      Open a file with token on your Amazon S3 bucket and paste the token into Ownership token field: </p> </li> <li> <p>Click the \"Prove ownership\" button.</p> </li> <li> <p>Next in the Customize log screen, select the data set in which you're interested (currently HTTP requests or Spectrum events).      You can keep the default fields to include in your log or make changes. You can add or remove fields at a later time by modifying your settings in Logs &gt; Logpush (select the wrench icon). </p> </li> <li> <p>To finish enabling Logpush, click Save and Start Pushing</p> </li> </ol> <p>Once connected, Cloudflare lists the provider you just configured under Logs &gt; Logpush. This is where you can make changes or remove the provider. </p> <p>If all steps were executed properly, you should see files in your S3 bucket and also in Coralogix.</p>"},{"location":"newoutput/cloudflare-coralogix/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/cloudtrail-s3-terraform/","title":"AWS CloudTrail via S3 with Terraform","text":"<p>Streamline the process of ingesting and analyzing logs from your AWS resources using our automated AWS CloudTrail via S3 with Terraform integration package.</p>"},{"location":"newoutput/cloudtrail-s3-terraform/#overview","title":"Overview","text":"<p>AWS CloudTrail\u00a0is an Amazon Web Services (AWS) service that helps you enable operational and risk auditing, governance, and compliance of your AWS account. Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail.</p> <p>Coralogix ingests the logs stored in your Amazon S3 bucket and process them for further analysis and monitoring.</p> <p>Sending CloudTrail logs to Coralogix via Terraform provides a streamlined and automated approach to configure and manage the necessary AWS resources for CloudTrail log delivery.</p>"},{"location":"newoutput/cloudtrail-s3-terraform/#benefits","title":"Benefits","text":"<p>Use the AWS CloudTrail via S3 with Terraform integration package to enjoy:</p> <ul> <li> <p>Robust Security Monitoring. Terraform and Coralogix together offer a unified solution to send AWS CloudTrail logs, ensuring a centralized and robust security audit trail. Rapidly detect and respond to potential security threats or unauthorized access.</p> </li> <li> <p>Swift Incident Response. Coralogix's real-time visibility, driven by Terraform, accelerates incident response by quickly identifying and analyzing unusual patterns or suspicious activities in AWS CloudTrail logs.</p> </li> <li> <p>Efficient Compliance. Terraform automation guarantees consistent delivery of CloudTrail logs to Coralogix, streamlining compliance management and providing a centralized solution for auditing and reporting.</p> </li> <li> <p>Comprehensive Infrastructure Monitoring. The integration of CloudTrail logs with Coralogix via Terraform allows for a holistic view of your AWS infrastructure, enabling correlation with other logs and metrics for comprehensive monitoring and troubleshooting.</p> </li> <li> <p>Scalable Configuration. Terraform's infrastructure as code ensures scalable and repeatable deployment of CloudTrail log forwarding, adapting seamlessly to changes in your AWS environment and supporting both small and large-scale infrastructures.</p> </li> </ul>"},{"location":"newoutput/cloudtrail-s3-terraform/#configuration","title":"Configuration","text":"<p>STEP 1. From your Coralogix toolbar, navigate to Data Flow &gt; Extensions.</p> <p>STEP 2. In the Integrations section, select AWS CloudTrail via S3 with Terraform.</p> <p></p> <p>STEP 3. Click + SETUP INTEGRATION.</p> <p>STEP 4. Input your integration details.</p> <p></p> <ul> <li> <p>Integration Name. Enter a name for your integration.</p> </li> <li> <p>Authentication Type. Select the authentication type: API Key or Existing Secret.</p> <ul> <li> <p>If using an API key, enter your Send-Your-Data API key or click CREATE A NEW KEY to create a new API key for the integration.</p> </li> <li> <p>If using an existing secret, enter the secret name.</p> </li> </ul> </li> <li> <p>Application Name. Enter an application name. The default name is AWS.</p> </li> <li> <p>Subsystem Name. Enter a subsystem name. The default name is CloudTrail.</p> </li> <li> <p>S3 Bucket Name. Enter the name of your S3 bucket.</p> </li> <li> <p>Enable AWS Secrets Manager (Advanced Settings). Enabling AWS Secrets Manager is necessary if you want to keep your Send-Your-Data API key as a secret in AWS Secrets Manager.</p> </li> <li> <p>Lambda Layer ARN (Advanced Settings). If you are using Secret Manager, this is the ARN of the Coralogix Security lambda layer.</p> </li> <li> <p>Newline Pattern (Advanced Settings). The pattern for where to split new lines.</p> </li> <li> <p>Blocking Pattern (Advanced Settings). [Optional] The pattern for line blocking.</p> </li> <li> <p>Buffer Size (Advanced Settings). The Coralogix logger buffer size. Default is <code>134217728</code>.</p> </li> <li> <p>Sampling Rate (Advanced Settings). Allows you to send messages with a specific sampling rate. Default is <code>1</code>.</p> </li> <li> <p>Debug (Advanced Settings). Enables Coralogix Logger Debug mode.</p> </li> <li> <p>Lambda Function Memory Size (Advanced Settings). The memory limit of the lambda function. Default is <code>1024</code>MB.</p> </li> <li> <p>Lambda Function Timeout (Advanced Settings). The timeout limit of the lambda function. Default is <code>300</code> seconds.</p> </li> <li> <p>Lambda Function Architecture (Advanced Settings). The architecture of the lambda function. Default is <code>x86_64</code>.</p> </li> <li> <p>S3 Path Prefix (Advanced Settings). [Optional] The S3 path prefix to watch.</p> </li> <li> <p>S3 Path Suffix (Advanced Settings). [Optional] The S3 path suffix to watch.</p> </li> <li> <p>Notification Email (Advanced Settings). [Optional] The email address for failure notifications.</p> </li> <li> <p>Custom S3 Bucket (Advanced Settings). [Optional] The name of the S3 bucket in which the lambda zip code is saved.</p> </li> </ul> <p>STEP 5. Click NEXT.</p> <p>STEP 6. Copy the declaration from the integration screen and add it to your Terraform project.</p> <p></p> <p>STEP 7. Check the box next to \u201cI\u2019ve added the configuration blocks and completed the Terraform deployment as instructed.\u201d Click COMPLETE.</p> <p>STEP 8. [Optional] Deploy the AWS CloudTrail extension package\u00a0to complement your integration needs.</p> <p>STEP 9. View your logs by navigating to Explore &gt; Logs in your Coralogix toolbar. Find out more here.</p>"},{"location":"newoutput/cloudtrail-s3-terraform/#additional-resources","title":"Additional Resources","text":"DocumentationAWS CloudTrail: Data Collection OptionsCoralogix Terraform Registry"},{"location":"newoutput/cloudtrail-s3-terraform/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/cloudwatch-metric-firehose-delivery-stream/","title":"AWS CloudWatch Metrics Processing","text":"<p>Amazon CloudWatch monitors your Amazon Web Services (AWS) resources and applications you run on AWS in real time. Use the Coralogix destination to easily forward metrics for your AWS resources to Coralogix using CloudWatch Metric Stream and Firehose Delivery Stream.</p> <p>This guide demonstrates how to:</p> <ul> <li> <p>Use CloudWatch to collect and track metrics and send them to Coralogix</p> </li> <li> <p>Visualize metrics in your Coralogix dashboard</p> </li> <li> <p>Import pre-built dashboards and set up alerts in your Coralogix account</p> </li> </ul>"},{"location":"newoutput/cloudwatch-metric-firehose-delivery-stream/#metrics-collection","title":"Metrics Collection","text":"<p>There are two ways of sending AWS service metrics to Coralogix:</p> <ul> <li> <p>Real-time CloudWatch metrics processing using CloudWatch Metric Stream and Firehose Delivery Stream. Opt for this if you\u2019d like to use a native AWS solution without an agent. Use the Amazon CloudWatch pricing calculator to estimate the costs of this option. The instructions below guide you through this option.</p> </li> <li> <p>Poll CloudWatch metrics using Telegraf. Opt for this if you are already using this shipper or seeking to reduce costs.</p> </li> </ul>"},{"location":"newoutput/cloudwatch-metric-firehose-delivery-stream/#configuration","title":"Configuration","text":"<p>1. Set up CloudWatch Metric Stream and Firehose Delivery Stream.</p> <p>2. Identify the AWS services that publish metrics to CloudWatch and their namespaces. Use them in your configuration to collect the metrics.</p> <p>Note: Namespaces are case-sensitive.</p>"},{"location":"newoutput/cloudwatch-metric-firehose-delivery-stream/#dashboard","title":"Dashboard","text":"<p>1. Download and import the metric stream version of your RDS dashboard.</p> <p>Download JSON</p> <p>2. Download and import the metric stream version of ElastiCache dashboard.</p> <p>Download JSON</p>"},{"location":"newoutput/cloudwatch-metric-firehose-delivery-stream/#alerts","title":"Alerts","text":"<p>Use PromQL to create metrics alerts:</p> <ul> <li>RDS CPU utilization above 80%</li> </ul> <pre><code>1avg(amazonaws_com_AWS_RDS_CPUUtilization{DBInstanceIdentifier!=\"\"}) by (DBInstanceIdentifier)\n</code></pre> <ul> <li>RDS Writer CPU utilization above 90%</li> </ul> <pre><code>1avg(amazonaws_com_AWS_RDS_CPUUtilization{DBClusterIdentifier!=\"\",Role=\"WRITER\"}) by (Role,DBClusterIdentifier)\n</code></pre> <ul> <li>ElastiCache miss above 50%</li> </ul> <pre><code>1(sum(amazonaws_com_AWS_ElastiCache_CacheMisses_sum{})/(sum(amazonaws_com_AWS_ElastiCache_CacheHits_sum{})+sum(amazonaws_com_AWS_ElastiCache_CacheMisses_sum{})))*100\n</code></pre>"},{"location":"newoutput/cloudwatch-metric-firehose-delivery-stream/#validation","title":"Validation","text":"<p>To validate your configuration, access your Coralogix - Grafana dashboard.</p> <p>1. On the right-hand corner of your dashboard, click on the Grafana drop-down tab.</p> <p></p> <p>2. Once you\u2019ve accessed your Coralogix-Grafana dashboard, click on Explore tab in the left-hand browser.</p> <p></p> <p>3. Click on the drop-down arrow of the Metrics browser and input <code>`RDS`</code> (RDS metrics) and <code>`ElastiCache`</code> (ElastiCache metrics) in the \u201cSelect a Metric\u201d column.</p>"},{"location":"newoutput/cloudwatch-metric-firehose-delivery-stream/#limitations","title":"Limitations","text":"<p>CloudWatch Metric Streams does not send metrics that are older than 2 hours. This means that some CloudWatch metrics are calculated at the end of a day and reported with the beginning timestamp of the same day. This includes S3 daily storage metrics and some billing metrics.</p> <p>Should you need these metrics, we recommend using Cloudwatch Exporter using Prometheus alongside our new CloudWatch integration designed to retrieve those metrics. For updated information, contact Coralogix Support.</p>"},{"location":"newoutput/cloudwatch-metric-firehose-delivery-stream/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/","title":"AWS CloudWatch Metric Streams with Kinesis Data Firehose","text":"<p>Amazon Kinesis Data Firehose\u00a0delivers real-time streaming data to destinations like Amazon Simple Storage Service (Amazon S3), Amazon Redshift, or Amazon OpenSearch Service (successor to Amazon Elasticsearch Service), and now supports delivering streaming data to Coralogix, an AWS Partner Network (APN) Advanced Technology Partner.</p> <p>Use this seamless integration for easy monitoring, flexibility with minimum maintenance, and maximum scalability. There is no limit on the number of delivery streams, so it can be used for getting data from multiple AWS services.</p>"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#setup-options","title":"Setup Options","text":"<ul> <li> <p>Automated Integration Package (Recommended). Streamline the ingesting and analyzing metrics from your AWS resources using our automated integration package setup below.</p> </li> <li> <p>Manual Integration.\u00a0Alternatively, use our\u00a0manual integration below.</p> </li> <li> <p>Terraform. Install and manage the Firehose Metrics integration with AWS services as modules in your infrastructure code.</p> </li> <li> <p>CloudFormation. Install our\u00a0AWS Kinesis Firehose Metrics with CloudFormation Template\u00a0and incorporate the necessary configurations and settings via CloudFormation to automate your Firehose metrics collection setup and management.</p> </li> </ul>"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#automated-integration-package","title":"Automated\u00a0Integration Package","text":"<p>Streamline your setup process using our automated integration package CloudWatch Metrics via Firehose. This package lets you preconfigure and deploy a template, replicating the manual setup.</p> <p>Once completed, select from various out-of-the-box\u00a0extension packages: AWS EBS, AWS EC2, AWS ElastiCache, or AWS RDS.</p> <p>Each tailored extension unlocks a set of predefined items \u2013 such as alerts, parsing rules, dashboards, saved views, and actions \u2013 allowing you to jumpstart Coralogix monitoring of your external-facing resources.</p>"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#configuration","title":"Configuration","text":"<p>STEP 1. From your Coralogix toolbar, navigate to\u00a0Data Flow\u00a0&gt;\u00a0Integrations.</p> <p>STEP 2.\u00a0In the Integrations section, select\u00a0CloudWatch Metrics via Firehose.</p> <p></p> <p>STEP 3.\u00a0Click\u00a0ADD NEW.</p> <p></p> <p>STEP 4.\u00a0Input your integration details.</p> <ul> <li> <p>Integration Name.\u00a0Enter a name for your integration and either enter your\u00a0Send-Your-Data API key\u00a0or click\u00a0CREATE A NEW KEY\u00a0to create a new API key for the integration.</p> </li> <li> <p>Application Name.\u00a0Enter an\u00a0application name. The default name is AWS.</p> </li> <li> <p>Subsystem Name.\u00a0Enter a\u00a0subsystem name. The default name is Firehose.</p> </li> <li> <p>CloudWatch Metric Namespaces.\u00a0Select the namespaces you want to bring from the dropdown menu into the Coralogix platform.</p> </li> <li> <p>Enable Additional Statistics.\u00a0Enabling\u00a0additional statistics\u00a0is\u00a0recommended\u00a0to receive detailed percentile data on key AWS metrics. However, enabling it may incur additional costs from AWS. The configuration used for the additional statistics is as follows:</p> </li> </ul> <pre><code>[ { \"AdditionalStatistics\": [\"p50\", \"p75\", \"p95\", \"p99\"], \"IncludeMetrics\": [ {\"Namespace\": \"AWS/EBS\", \"MetricName\": \"VolumeTotalReadTime\"}, {\"Namespace\": \"AWS/EBS\", \"MetricName\": \"VolumeTotalWriteTime\"}, {\"Namespace\": \"AWS/ELB\", \"MetricName\": \"Latency\"}, {\"Namespace\": \"AWS/ELB\", \"MetricName\": \"Duration\"}, {\"Namespace\": \"AWS/Lambda\", \"MetricName\": \"PostRuntimeExtensionsDuration\"}, {\"Namespace\": \"AWS/S3\", \"MetricName\": \"FirstByteLatency\"}, {\"Namespace\": \"AWS/S3\", \"MetricName\": \"TotalRequestLatency\"} ] } ]\n</code></pre> <ul> <li>AWS Region.\u00a0Select your AWS region from the dropdown menu.</li> </ul> <p>Optional [Recommended]:</p> <ul> <li> <p>Enable Metrics Tags Processors. Enabling the Metrics Tags Processors involves using the mentioned Transformation Lambda to enhance metrics by adding AWS resource tags using the abovementioned settings.</p> </li> <li> <p>AWS PrivateLink (Advanced Settings). Enabling the use of AWS PrivateLink is\u00a0recommended\u00a0to ensure a secure and private connection between your VPCs and AWS services. Find out more\u00a0here.</p> </li> </ul> <p>STEP 6.\u00a0Click\u00a0NEXT.</p> <p>STEP 7.\u00a0Review the instructions for your integration and click\u00a0CREATE CLOUDFORMATION.</p> <p>STEP 8.\u00a0You will be rerouted to the AWS website. Verify that all of the auto pre-populated values are correct. Click\u00a0Create Stack.</p> <p>STEP 9.\u00a0Go back to the Coralogix application and click\u00a0COMPLETE\u00a0to close the module. Revert to the integration page.</p> <p>STEP 10.\u00a0[Optional] Deploy the\u00a0extension package\u00a0of your choice to complement your integration needs.</p> <ul> <li> <p>AWS EBS</p> </li> <li> <p>AWS EC2</p> </li> <li> <p>AWS ElastiCache</p> </li> <li> <p>AWS RDS</p> </li> </ul> <p>STEP 11.\u00a0View the metrics in your Coralogix dashboard using\u00a0Custom Dashboards [recommended] or hosted\u00a0Grafana View.</p>"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#manual-setup","title":"Manual Setup","text":""},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#prerequisites","title":"Prerequisites","text":"<ul> <li>Metrics bucket configured in your Coralogix dashboard. Data Flow &gt; Setup Archive</li> </ul>"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#configuration_1","title":"Configuration","text":"<p>STEP 1. Go to the Kinesis Data Firehose console and choose \u2018Create delivery stream\u2019.</p> <p>STEP 2. Under \u2018Choose source and destination\u2019.</p> <ul> <li> <p>Source: Direct PUT</p> </li> <li> <p>Destination: Coralogix</p> </li> <li> <p>Delivery stream name: Fill in the desired stream name.</p> </li> </ul> <p>STEP 3. Scroll down to \u2018Destination settings\u2019.</p> <ul> <li>HTTP endpoint URL: Choose a Firehose HTTP endpoint URL based on your Coralogix domain and region.</li> </ul> <p>STEP 4. Scroll down to \u2018Parameters\u2019:</p> <ul> <li> <p>Private key: Enter your Coralogix\u00a0Send Your Data \u2013 API Key.</p> </li> <li> <p>Content encoding: Select GZIP.</p> </li> <li> <p>Retry duration: Choose 300 seconds.</p> </li> </ul> <p>By default, your delivery stream arn and name will be used as \u2018applicationName\u2019 and \u2018subsystemName\u2019.</p> <p>Add a new parameter with the desired value to override the associated 'applicationName' or 'subsystemName'.</p> <ul> <li> <p>Key: 'applicationName' , value - 'new-app-name'</p> </li> <li> <p>Key: 'subsystemName' , value - 'new-subsystem-name'</p> </li> </ul> <p>The source of the data in Firehose determines the \u2018integrationType\u2019 parameter value:</p> <ul> <li>Key: \u2018integrationType\u2019 , value: \u2018CloudWatch_Metrics_OpenTelemetry070_WithAggregations\u2032</li> </ul> <p>STEP 5. Scroll down to \u2018Backup settings\u2019:</p> <ul> <li> <p>Source record backup in Amazon S3: We suggest selecting\u00a0\u2018Failed data only\u2019.</p> </li> <li> <p>S3 backup bucket: Choose an existing bucket or create a new one.</p> </li> <li> <p>Buffer hints, compression, encryption: Leave these fields as is.</p> </li> </ul> <p>STEP 6. Review your settings and choose \u2018Create delivery stream\u2019.</p> <p>Metrics subscribed to your delivery stream will be immediately sent and available for analysis within Coralogix.</p>"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#data-source-configuration","title":"Data Source Configuration","text":""},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#cloudwatch-metrics","title":"Cloudwatch Metrics","text":"<p>To start sending your metrics to Coralogix, you first need to create a metric stream.</p> <p>STEP 1. Go to the Cloudwatch console and choose \u2018Streams\u2019 under the \u2018Metrics\u2019 side menu.</p> <p>STEP 2. Click on \u2018Create metric stream\u2019.</p> <p>STEP 3. Under \u2018Metrics to be streamed\u2019:</p> <ul> <li>Choose what metrics to send</li> </ul> <p>STEP 4. Scroll down to \u2018Configuration\u2019:</p> <ul> <li> <p>For \u2018Select configuration option\u2019 choose \u2018Select an existing Firehose owned by your account\u2019</p> </li> <li> <p>For \u2018Select your Kinesis Data Firehose stream\u2019 choose the delivery stream created above</p> </li> </ul> <p>STEP 5. Scroll down to \u2018Change output format\u2019</p> <ul> <li>Make sure that \u2018OpenTelemetry 0.7\u2019 is selected</li> </ul> <p>STEP 6. Scroll down to \u2018Custom metric stream name\u2019 and pick a name for the metrics stream.</p> <p>STEP 7. Scroll down and click on \u2018Create metric stream\u2019.</p> <p>After a few minutes, the metrics will start streaming to Coralogix, and you will see them on the Grafana dashboard.</p>"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#transformation-lambda","title":"Transformation Lambda","text":"<p>[Optional] CloudWatch Metric Streams Lambda transformation function can be used as a Kinesis Firehose transformation function, to enrich the metrics from CloudWatch Metric Streams with AWS resource tags.</p> <p>This installation is\u00a0optional,\u00a0and you can install the transformation Lambda if you\u2019d like to take advantage of having AWS resource tags as labels in your metrics data. Find out more\u00a0here.</p> <p>Take the following steps to install the transformation Lambda.</p> <p>STEP 1. Create a new AWS Lambda function in your designated region with the following parameters:</p> <ul> <li> <p>Runtime:\u00a0<code>Custom runtime on Amazon Linux 2</code></p> </li> <li> <p>Handler:\u00a0<code>bootstrap</code></p> </li> <li> <p>Architecture:\u00a0<code>arm64</code></p> </li> </ul> <p>STEP 2. Download the bootstrap ZIP\u00a0file from the releases in the repository. Unless instructed otherwise, use the latest release. Upload the <code>bootstrap.zip</code> as the code source for Lambda.</p> <p>STEP 3. Make sure to set the memory. We recommend starting with\u00a0<code>512 MB</code>\u00a0and, depending on the number of metrics you export and the speed of Lambda processing, see if you need to increase it.</p> <p>STEP 4. Adjust the role of the Lambda function as described in\u00a0the necessary permissions.</p> <p>STEP 5. Optionally, add environment variables to configure the Lambda, as described in the configuration.</p> <p>STEP 6.\u00a0The Lambda function is ready for use in\u00a0Kinesis Data Firehose Data Transformation. Please note the ARN and provide it in the relevant section of the Kinesis Data Firehose configuration.</p> <p>To prevent delay in the delivery of your data and depending on the size of your setup, we recommend adjusting your Lambda buffer hint and Kinesis Data Firehose buffer size configuration accordingly. For the most optimal experience, we recommend setting the Lambda buffer hint \u00a00.2 MB\u00a0and the Kinesis Data Firehose buffer size to\u00a01 MB.\u00a0Note that this may cause more frequent Lambda runs, resulting in higher costs.</p> <p>You can check the staleness of your data in your Kinesis Data Firehose delivery stream in the Monitoring tab by looking at the \u2018Delivery to HTTP endpoint data freshness\u2019. Seeing the staleness value grow may indicate that the Lambda function runs are too slow. In such a case, you may increase the Lambda\u2019s memory and set the buffering configuration as described above.</p>"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#terraform-module-setup","title":"Terraform - Module Setup","text":"<p>Using\u00a0Coralogix Terraform modules, you can easily install and manage Coralogix integrations with AWS services as modules in your infrastructure code. Our open-source modules are available on our\u00a0GitHub\u00a0and in the\u00a0Terraform Registry. Visit our full AWS Kinesis Firehose Terraform Module documentation for more information.</p>"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#metrics","title":"Metrics","text":"<p>For metrics, install\u00a0AWS Kinesis Data Firehose\u00a0by also adding this declaration to your Terraform project:</p> <pre><code>module \"cloudwatch_firehose_coralogix_metrics\" {\n  source           = \"coralogix/aws/coralogix//modules/firehose-metrics\"\n  private_key      = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXX\"\n  firehose_stream  = \"coralogix-firehose-metrics\"\n  coralogix_region = \"Europe\"\n}\n\n</code></pre>"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#amazon-cloudwatch-extensions","title":"Amazon CloudWatch Extensions","text":"<p>Coralogix offers a variety of out-of-the-box data\u00a0extension packages: AWS EBS, AWS EC2, AWS ElastiCache, or AWS RDS.</p> <p>Each tailored extension unlocks a set of predefined items \u2013 such as alerts, parsing rules, dashboards, saved views, and actions \u2013 allowing you to jumpstart Coralogix monitoring of your external-facing resources.</p>"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#destination-errors","title":"Destination Errors","text":"<p>Review these common destination errors presented by Firehose and their possible solutions</p> Message Solution The delivery timed out before a response was received and will be retried. If this error persists, contact the AWS Firehose service team. None needed \u2013 no data loss Delivery to the endpoint was unsuccessful. See Troubleshooting HTTP Endpoints in the Firehose documentation for more information. Response received with status code. 502\u2026 Coralogix returned an HTTP 502 error code; Firehose will resend the data. None needed \u2013 no data loss"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#limitations","title":"Limitations","text":"<p>CloudWatch Metric Streams does not send metrics that have a timestamp older than 2 hours. This means that some CloudWatch metrics that are calculated at the end of a day and reported with the beginning timestamp of the same day would not be streamed. This includes S3 daily storage metrics and some billing metrics.</p> <p>Should you need these metrics, we\u00a0recommend\u00a0using\u00a0Cloudwatch Exporter using Prometheus\u00a0alongside our new CloudWatch integration designed to retrieve those metrics. For updated information, contact\u00a0Coralogix Support.</p>"},{"location":"newoutput/cloudwatch-metricstreams-kinesis-data-firehose/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/cockroachdb/","title":"CockroachDB","text":""},{"location":"newoutput/cockroachdb/#overview","title":"Overview","text":"<p>This guide demonstrates the process of integrating Coralogix with a self-managed CockroachDB instance from Cockroach Labs. Initially, we will set up a CockroachDB instance on an EC2 instance, following the outlined steps below.</p>"},{"location":"newoutput/cockroachdb/#cockroachdb-instance-setup","title":"CockroachDB Instance Setup","text":"<p>STEP 1. Bring up an Ubuntu EC2 instance. This example uses a t2.xlarge instance. Run the DB instance on your EC2 instance by following steps these steps.</p> <p>STEP 2. Start the CockroachDB instance as a single node cluster using the command listed here.</p> <pre><code>cockroach start-single-node --insecure\n</code></pre> <p>STEP 3. Start a Prometheus server on that same machine following these instructions.</p> <p>STEP 4. Create a configuration file.</p> <p>The example config file below includes a scrape_config section to scrape the data from the CockroachDB and a remote_write section that sends the data to Coralogix.</p> <pre><code># my global config\nglobal:\n  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\n  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\n  # scrape_timeout is set to the global default (10s).\n\n# Alertmanager configuration\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          # - alertmanager:9093\n\n# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\nrule_files:\n  # - \"first_rules.yml\"\n  # - \"second_rules.yml\"\n\n# A scrape configuration containing exactly one endpoint to scrape:\n# Here it's Prometheus itself.\nscrape_configs:\n  - job_name: 'cockroachdb'\n    metrics_path: '/_status/vars'\n    # Insecure mode:\n    scheme: 'http'\n    # Secure mode:\n    # scheme: 'https'\n    tls_config:\n      insecure_skip_verify: true\n\n    static_configs:\n    - targets: ['localhost:8080']\n      labels:\n        cluster: 'my-cockroachdb-cluster'\n\nremote_write:\n- url: https://ingress.coralogix.us/prometheus/v1\n  name: 'cockroachlabs'\n  remote_timeout: 120s\n  bearer_token: 'MY_CX_API_KEY'\n</code></pre>"},{"location":"newoutput/cockroachdb/#monitoring-cockroachdb-performance-metrics","title":"Monitoring CockroachDB Performance Metrics","text":"<p>Once you start the Prometheus server, it will push the CockroachDB performance metrics into Coralogix, which you can view in Custom Dashboards or your Grafana-hosted dashboard.</p>"},{"location":"newoutput/cockroachdb/#custom-dashboards","title":"Custom Dashboards","text":"<p>Here is a screenshot of CockreachDB performance metrics in a Coralogix custom dashboard.</p> <p></p> <p>The following are the dashboard artifacts:</p> <pre><code>{\n  \"id\": \"0a1Tqgj3vl8LsGBzDwmn0\",\n  \"name\": \"CockroachDB Performance\",\n  \"description\": \"Displays the performance of cockoach DB\",\n  \"layout\": {\n    \"sections\": [\n      {\n        \"id\": {\n          \"value\": \"9c1aa22d-3c7b-eb45-c636-864e09f5fe1a\"\n        },\n        \"rows\": [\n          {\n            \"id\": {\n              \"value\": \"8e02b618-fe69-90ab-45b6-11c1f8a20576\"\n            },\n            \"appearance\": {\n              \"height\": 24\n            },\n            \"widgets\": [\n              {\n                \"id\": {\n                  \"value\": \"d856f898-bda6-57bb-d9a3-2e9f683c8787\"\n                },\n                \"title\": \"New Markdown\",\n                \"definition\": {\n                  \"markdown\": {\n                    \"markdownText\": \"# ![](https://2023.allthingsopen.org/wp-content/uploads/2023/05/Silver_CockroachLabs.jpg)\"\n                  }\n                },\n                \"appearance\": {\n                  \"width\": 0\n                }\n              },\n              {\n                \"id\": {\n                  \"value\": \"f9060fb8-bf1f-09a0-05b3-211cf73869ee\"\n                },\n                \"title\": \"Live Node Count\",\n                \"definition\": {\n                  \"gauge\": {\n                    \"query\": {\n                      \"metrics\": {\n                        \"promqlQuery\": {\n                          \"value\": \"sum(liveness_livenodes{cluster=~\\\"{{cluster}}\\\"})\"\n                        },\n                        \"aggregation\": \"AGGREGATION_UNSPECIFIED\",\n                        \"filters\": []\n                      }\n                    },\n                    \"min\": 0,\n                    \"max\": 100,\n                    \"showInnerArc\": true,\n                    \"showOuterArc\": true,\n                    \"unit\": \"UNIT_NUMBER\",\n                    \"thresholds\": [\n                      {\n                        \"from\": 0,\n                        \"color\": \"var(--c-severity-log-verbose)\"\n                      },\n                      {\n                        \"from\": 33,\n                        \"color\": \"var(--c-severity-log-warning)\"\n                      },\n                      {\n                        \"from\": 66,\n                        \"color\": \"var(--c-severity-log-error)\"\n                      }\n                    ],\n                    \"dataModeType\": \"DATA_MODE_TYPE_HIGH_UNSPECIFIED\",\n                    \"thresholdBy\": \"THRESHOLD_BY_UNSPECIFIED\"\n                  }\n                },\n                \"appearance\": {\n                  \"width\": 0\n                }\n              },\n              {\n                \"id\": {\n                  \"value\": \"c3d96ad2-eb5c-be56-af9b-36460c5a8fbb\"\n                },\n                \"title\": \"Go Routine Count\",\n                \"definition\": {\n                  \"gauge\": {\n                    \"query\": {\n                      \"metrics\": {\n                        \"promqlQuery\": {\n                          \"value\": \"sum(sys_goroutines{cluster=~\\\"{{cluster}}\\\"})\"\n                        },\n                        \"aggregation\": \"AGGREGATION_UNSPECIFIED\",\n                        \"filters\": []\n                      }\n                    },\n                    \"min\": 0,\n                    \"max\": 1000,\n                    \"showInnerArc\": true,\n                    \"showOuterArc\": true,\n                    \"unit\": \"UNIT_NUMBER\",\n                    \"thresholds\": [\n                      {\n                        \"from\": 0,\n                        \"color\": \"var(--c-severity-log-verbose)\"\n                      },\n                      {\n                        \"from\": 33,\n                        \"color\": \"var(--c-severity-log-warning)\"\n                      },\n                      {\n                        \"from\": 66,\n                        \"color\": \"var(--c-severity-log-error)\"\n                      }\n                    ],\n                    \"dataModeType\": \"DATA_MODE_TYPE_HIGH_UNSPECIFIED\",\n                    \"thresholdBy\": \"THRESHOLD_BY_UNSPECIFIED\"\n                  }\n                },\n                \"appearance\": {\n                  \"width\": 0\n                }\n              },\n              {\n                \"id\": {\n                  \"value\": \"e45cec9e-c0a9-6a32-8cb7-1429791b9980\"\n                },\n                \"title\": \"SQL Memory (MB)\",\n                \"definition\": {\n                  \"gauge\": {\n                    \"query\": {\n                      \"metrics\": {\n                        \"promqlQuery\": {\n                          \"value\": \"sum(sql_mem_root_current{cluster=~\\\"{{cluster}}\\\"})\"\n                        },\n                        \"aggregation\": \"AGGREGATION_UNSPECIFIED\",\n                        \"filters\": []\n                      }\n                    },\n                    \"min\": 0,\n                    \"max\": 10000000,\n                    \"showInnerArc\": true,\n                    \"showOuterArc\": true,\n                    \"unit\": \"UNIT_NUMBER\",\n                    \"thresholds\": [\n                      {\n                        \"from\": 0,\n                        \"color\": \"var(--c-severity-log-verbose)\"\n                      },\n                      {\n                        \"from\": 33,\n                        \"color\": \"var(--c-severity-log-warning)\"\n                      },\n                      {\n                        \"from\": 66,\n                        \"color\": \"var(--c-severity-log-error)\"\n                      }\n                    ],\n                    \"dataModeType\": \"DATA_MODE_TYPE_HIGH_UNSPECIFIED\",\n                    \"thresholdBy\": \"THRESHOLD_BY_UNSPECIFIED\"\n                  }\n                },\n                \"appearance\": {\n                  \"width\": 0\n                }\n              },\n              {\n                \"id\": {\n                  \"value\": \"b1dfd3a8-ee84-a28a-5707-3226c87c61e2\"\n                },\n                \"title\": \"Replicas per store\",\n                \"definition\": {\n                  \"gauge\": {\n                    \"query\": {\n                      \"metrics\": {\n                        \"promqlQuery\": {\n                          \"value\": \"sum(replicas{cluster=~\\\"{{cluster}}\\\"})\"\n                        },\n                        \"aggregation\": \"AGGREGATION_UNSPECIFIED\",\n                        \"filters\": []\n                      }\n                    },\n                    \"min\": 0,\n                    \"max\": 1000,\n                    \"showInnerArc\": true,\n                    \"showOuterArc\": true,\n                    \"unit\": \"UNIT_NUMBER\",\n                    \"thresholds\": [\n                      {\n                        \"from\": 0,\n                        \"color\": \"var(--c-severity-log-verbose)\"\n                      },\n                      {\n                        \"from\": 33,\n                        \"color\": \"var(--c-severity-log-warning)\"\n                      },\n                      {\n                        \"from\": 66,\n                        \"color\": \"var(--c-severity-log-error)\"\n                      }\n                    ],\n                    \"dataModeType\": \"DATA_MODE_TYPE_HIGH_UNSPECIFIED\",\n                    \"thresholdBy\": \"THRESHOLD_BY_UNSPECIFIED\"\n                  }\n                },\n                \"appearance\": {\n                  \"width\": 0\n                }\n              }\n            ]\n          },\n          {\n            \"id\": {\n              \"value\": \"d1e9dd53-1589-0c95-2db8-68a8c9e440ab\"\n            },\n            \"appearance\": {\n              \"height\": 19\n            },\n            \"widgets\": [\n              {\n                \"id\": {\n                  \"value\": \"0f3cc7cd-cb6b-d306-557d-c19798aa61ec\"\n                },\n                \"title\": \"Memory Usage\",\n                \"definition\": {\n                  \"lineChart\": {\n                    \"legend\": {\n                      \"isVisible\": true,\n                      \"columns\": [],\n                      \"groupByQuery\": false\n                    },\n                    \"tooltip\": {\n                      \"showLabels\": false,\n                      \"type\": \"TOOLTIP_TYPE_ALL\"\n                    },\n                    \"queryDefinitions\": [\n                      {\n                        \"id\": \"f1413d5b-0e93-0a13-607a-54ca9d33864d\",\n                        \"query\": {\n                          \"metrics\": {\n                            \"promqlQuery\": {\n                              \"value\": \"sum(sys_rss{cluster=~\\\"{{cluster}}\\\"})\"\n                            },\n                            \"filters\": []\n                          }\n                        },\n                        \"seriesNameTemplate\": \"sys_rss\",\n                        \"seriesCountLimit\": \"20\",\n                        \"unit\": \"UNIT_UNSPECIFIED\",\n                        \"scaleType\": \"SCALE_TYPE_LINEAR\",\n                        \"name\": \"Query 1\",\n                        \"isVisible\": true,\n                        \"colorScheme\": \"cold\",\n                        \"resolution\": {\n                          \"bucketsPresented\": 96\n                        },\n                        \"dataModeType\": \"DATA_MODE_TYPE_HIGH_UNSPECIFIED\"\n                      },\n                      {\n                        \"id\": \"51323169-44a2-810d-9d12-4df2a4d22135\",\n                        \"query\": {\n                          \"metrics\": {\n                            \"promqlQuery\": {\n                              \"value\": \"sum(sys_cgo_allocbytes{cluster=~\\\"{{cluster}}\\\"})\"\n                            },\n                            \"filters\": []\n                          }\n                        },\n                        \"seriesNameTemplate\": \"sys_cgo_allocbytes\",\n                        \"seriesCountLimit\": \"20\",\n                        \"unit\": \"UNIT_UNSPECIFIED\",\n                        \"scaleType\": \"SCALE_TYPE_LINEAR\",\n                        \"isVisible\": true,\n                        \"colorScheme\": \"cold\",\n                        \"resolution\": {\n                          \"bucketsPresented\": 96\n                        },\n                        \"dataModeType\": \"DATA_MODE_TYPE_HIGH_UNSPECIFIED\"\n                      },\n                      {\n                        \"id\": \"d3fafaac-a0e8-9a1d-76a8-d3578ba1434c\",\n                        \"query\": {\n                          \"metrics\": {\n                            \"promqlQuery\": {\n                              \"value\": \"sum(sys_go_allocbytes{cluster=~\\\"{{cluster}}\\\"})\"\n                            },\n                            \"filters\": []\n                          }\n                        },\n                        \"seriesNameTemplate\": \"sys_go_allocbytes\",\n                        \"seriesCountLimit\": \"20\",\n                        \"unit\": \"UNIT_UNSPECIFIED\",\n                        \"scaleType\": \"SCALE_TYPE_LINEAR\",\n                        \"isVisible\": true,\n                        \"colorScheme\": \"cold\",\n                        \"resolution\": {\n                          \"bucketsPresented\": 96\n                        },\n                        \"dataModeType\": \"DATA_MODE_TYPE_HIGH_UNSPECIFIED\"\n                      },\n                      {\n                        \"id\": \"be8a5da2-5aba-1321-ff02-5bb814f3dc71\",\n                        \"query\": {\n                          \"metrics\": {\n                            \"promqlQuery\": {\n                              \"value\": \"sum(sys_cgo_totalbytes{cluster=~\\\"{{cluster}}\\\"})\"\n                            },\n                            \"filters\": []\n                          }\n                        },\n                        \"seriesNameTemplate\": \"sys_cgo_totalbytes\",\n                        \"seriesCountLimit\": \"20\",\n                        \"unit\": \"UNIT_UNSPECIFIED\",\n                        \"scaleType\": \"SCALE_TYPE_LINEAR\",\n                        \"isVisible\": true,\n                        \"colorScheme\": \"cold\",\n                        \"resolution\": {\n                          \"bucketsPresented\": 96\n                        },\n                        \"dataModeType\": \"DATA_MODE_TYPE_HIGH_UNSPECIFIED\"\n                      }\n                    ]\n                  }\n                },\n                \"appearance\": {\n                  \"width\": 0\n                }\n              },\n              {\n                \"id\": {\n                  \"value\": \"0a7c51b9-b4cb-7987-256a-b55f799151a2\"\n                },\n                \"title\": \"Runnable Goroutines per CPU\",\n                \"definition\": {\n                  \"lineChart\": {\n                    \"legend\": {\n                      \"isVisible\": true,\n                      \"columns\": [],\n                      \"groupByQuery\": true\n                    },\n                    \"tooltip\": {\n                      \"showLabels\": false,\n                      \"type\": \"TOOLTIP_TYPE_ALL\"\n                    },\n                    \"queryDefinitions\": [\n                      {\n                        \"id\": \"1108b91b-e9c8-839f-20e4-6a92a34b7011\",\n                        \"query\": {\n                          \"metrics\": {\n                            \"promqlQuery\": {\n                              \"value\": \"sum(sys_runnable_goroutines_per_cpu{cluster=~\\\"{{cluster}}\\\"})\"\n                            },\n                            \"filters\": []\n                          }\n                        },\n                        \"seriesNameTemplate\": \"goroutines\",\n                        \"seriesCountLimit\": \"20\",\n                        \"unit\": \"UNIT_UNSPECIFIED\",\n                        \"scaleType\": \"SCALE_TYPE_LINEAR\",\n                        \"name\": \"Query 1\",\n                        \"isVisible\": true,\n                        \"colorScheme\": \"cold\",\n                        \"resolution\": {\n                          \"bucketsPresented\": 96\n                        },\n                        \"dataModeType\": \"DATA_MODE_TYPE_HIGH_UNSPECIFIED\"\n                      }\n                    ]\n                  }\n                },\n                \"appearance\": {\n                  \"width\": 0\n                }\n              }\n            ]\n          },\n          {\n            \"id\": {\n              \"value\": \"d81dbd99-ecd5-7059-690b-5795b4fc7021\"\n            },\n            \"appearance\": {\n              \"height\": 19\n            },\n            \"widgets\": [\n              {\n                \"id\": {\n                  \"value\": \"3771ada3-7b0b-8d6d-d900-d9a8d75dca47\"\n                },\n                \"title\": \"CPU Time\",\n                \"definition\": {\n                  \"lineChart\": {\n                    \"legend\": {\n                      \"isVisible\": true,\n                      \"columns\": [],\n                      \"groupByQuery\": false\n                    },\n                    \"tooltip\": {\n                      \"showLabels\": false,\n                      \"type\": \"TOOLTIP_TYPE_ALL\"\n                    },\n                    \"queryDefinitions\": [\n                      {\n                        \"id\": \"108f4c9f-3267-a71c-a114-05f4a9860673\",\n                        \"query\": {\n                          \"metrics\": {\n                            \"promqlQuery\": {\n                              \"value\": \"sum(rate(sys_cpu_user_ns{cluster=~\\\"{{cluster}}\\\"}))\"\n                            },\n                            \"filters\": []\n                          }\n                        },\n                        \"seriesNameTemplate\": \"sys_cpu_user_ns\",\n                        \"seriesCountLimit\": \"20\",\n                        \"unit\": \"UNIT_UNSPECIFIED\",\n                        \"scaleType\": \"SCALE_TYPE_LINEAR\",\n                        \"name\": \"Query 1\",\n                        \"isVisible\": true,\n                        \"colorScheme\": \"cold\",\n                        \"resolution\": {\n                          \"bucketsPresented\": 96\n                        },\n                        \"dataModeType\": \"DATA_MODE_TYPE_HIGH_UNSPECIFIED\"\n                      },\n                      {\n                        \"id\": \"477a447e-5e72-3303-0d34-e206bb2cee56\",\n                        \"query\": {\n                          \"metrics\": {\n                            \"promqlQuery\": {\n                              \"value\": \"sum(rate(sys_cpu_sys_ns{cluster=~\\\"{{cluster}}\\\"}))\"\n                            },\n                            \"filters\": []\n                          }\n                        },\n                        \"seriesNameTemplate\": \"sys_cpu_sys_ns\",\n                        \"seriesCountLimit\": \"20\",\n                        \"unit\": \"UNIT_UNSPECIFIED\",\n                        \"scaleType\": \"SCALE_TYPE_LINEAR\",\n                        \"isVisible\": true,\n                        \"colorScheme\": \"cold\",\n                        \"resolution\": {\n                          \"bucketsPresented\": 96\n                        },\n                        \"dataModeType\": \"DATA_MODE_TYPE_HIGH_UNSPECIFIED\"\n                      }\n                    ]\n                  }\n                },\n                \"appearance\": {\n                  \"width\": 0\n                }\n              },\n              {\n                \"id\": {\n                  \"value\": \"ffcb3fe1-1c34-5561-273e-8ff73c41db87\"\n                },\n                \"title\": \"GC Pause Time\",\n                \"definition\": {\n                  \"lineChart\": {\n                    \"legend\": {\n                      \"isVisible\": true,\n                      \"columns\": [],\n                      \"groupByQuery\": true\n                    },\n                    \"tooltip\": {\n                      \"showLabels\": false,\n                      \"type\": \"TOOLTIP_TYPE_ALL\"\n                    },\n                    \"queryDefinitions\": [\n                      {\n                        \"id\": \"737e908f-6eb4-1de3-5d10-1d831feaf777\",\n                        \"query\": {\n                          \"metrics\": {\n                            \"promqlQuery\": {\n                              \"value\": \"sum(rate(sys_gc_pause_ns{cluster=~\\\"{{cluster}}\\\"}))\"\n                            },\n                            \"filters\": []\n                          }\n                        },\n                        \"seriesNameTemplate\": \"sys_gc_pause_ns\",\n                        \"seriesCountLimit\": \"20\",\n                        \"unit\": \"UNIT_UNSPECIFIED\",\n                        \"scaleType\": \"SCALE_TYPE_LINEAR\",\n                        \"name\": \"Query 1\",\n                        \"isVisible\": true,\n                        \"colorScheme\": \"cold\",\n                        \"resolution\": {\n                          \"bucketsPresented\": 96\n                        },\n                        \"dataModeType\": \"DATA_MODE_TYPE_HIGH_UNSPECIFIED\"\n                      }\n                    ]\n                  }\n                },\n                \"appearance\": {\n                  \"width\": 0\n                }\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  },\n  \"variables\": [\n    {\n      \"name\": \"cluster\",\n      \"definition\": {\n        \"multiSelect\": {\n          \"selected\": [],\n          \"source\": {\n            \"metricLabel\": {\n              \"metricName\": \"sys_uptime\",\n              \"label\": \"cluster\"\n            }\n          },\n          \"selection\": {\n            \"all\": {}\n          },\n          \"valuesOrderDirection\": \"ORDER_DIRECTION_ASC\"\n        }\n      },\n      \"displayName\": \"Cluster\"\n    }\n  ],\n  \"filters\": [\n    {\n      \"source\": {\n        \"logs\": {\n          \"operator\": {\n            \"equals\": {\n              \"selection\": {\n                \"list\": {\n                  \"values\": []\n                }\n              }\n            }\n          },\n          \"observationField\": {\n            \"keypath\": [\n              \"applicationname\"\n            ],\n            \"scope\": \"DATASET_SCOPE_LABEL\"\n          }\n        }\n      },\n      \"enabled\": true,\n      \"collapsed\": false\n    },\n    {\n      \"source\": {\n        \"logs\": {\n          \"operator\": {\n            \"equals\": {\n              \"selection\": {\n                \"list\": {\n                  \"values\": []\n                }\n              }\n            }\n          },\n          \"observationField\": {\n            \"keypath\": [\n              \"subsystemname\"\n            ],\n            \"scope\": \"DATASET_SCOPE_LABEL\"\n          }\n        }\n      },\n      \"enabled\": true,\n      \"collapsed\": false\n    }\n  ],\n  \"relativeTimeFrame\": \"86400s\",\n  \"annotations\": []\n}\n</code></pre>"},{"location":"newoutput/cockroachdb/#grafana-dashboard","title":"Grafana Dashboard","text":"<p>Here are screenshots of CockroachDB performance metrics in your Grafana-hosted dashboard. Others can be found here.</p> <p></p> <p></p> <p>The following are the dashboard artifacts:</p> <p>wget https://raw.githubusercontent.com/cockroachdb/cockroach/master/monitoring/grafana-dashboards/by-cluster/runtime.json</p> <p>wget https://raw.githubusercontent.com/cockroachdb/cockroach/master/monitoring/grafana-dashboards/by-cluster/storage.json</p> <p>wget https://raw.githubusercontent.com/cockroachdb/cockroach/master/monitoring/grafana-dashboards/by-cluster/sql.json</p> <p>wget https://raw.githubusercontent.com/cockroachdb/cockroach/master/monitoring/grafana-dashboards/by-cluster/replication.json</p>"},{"location":"newoutput/cockroachdb/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/collect-cloudwatch-metrics-telegraf/","title":"Collect CloudWatch Metrics with Telegraf","text":"<p>Amazon CloudWatch monitors your Amazon Web Services (AWS) resources and applications you run on AWS in real time. Use the Coralogix destination to easily forward metrics for your AWS resources to Coralogix using Telegraf.</p> <p>This guide demonstrates how to:</p> <ul> <li> <p>Use CloudWatch to collect and track metrics and send them to Coralogix using Telegraf</p> </li> <li> <p>Visualize metrics in your Coralogix dashboard</p> </li> <li> <p>Import pre-built dashboards and set up alerts in your Coralogix account</p> </li> </ul>"},{"location":"newoutput/collect-cloudwatch-metrics-telegraf/#metrics-collection","title":"Metrics Collection","text":"<p>There are two ways of sending AWS service metrics to Coralogix:</p> <ul> <li> <p>Real-time CloudWatch metrics processing using CloudWatch Metric Stream and Firehose Delivery Stream. Opt for this if you\u2019d like to use a native AWS solution without an agent by setting up a CloudWatch metric stream and Firehose delivery steam. Use the Amazon CloudWatch pricing calculator to estimate the costs of this option.</p> </li> <li> <p>Poll CloudWatch metrics using Telegraf. Opt for this if you are already using this shipper or seeking to reduce costs. The instructions below guide you through this option.</p> </li> </ul>"},{"location":"newoutput/collect-cloudwatch-metrics-telegraf/#prerequisites","title":"Prerequisites","text":"<p>1. Install Telegraf.</p> <p>2. Identify the AWS services that publish metrics to CloudWatch and their namespaces. Use them in your configuration to collect the metrics.</p> <p>Note: Namespaces are case-sensitive.</p>"},{"location":"newoutput/collect-cloudwatch-metrics-telegraf/#configuration","title":"Configuration","text":"<p>1. Create a configuration file. The example below reads and sends RDS and ElastiCache metrics to Coralogix in 5-minute intervals.</p> <pre><code># Global tags can be specified here in key=\"value\" format.\n[global_tags]\n  dc = \"us-east-2\" \n  user = \"$USER\"\n  env = \"CoE\"\n\n# Configuration for telegraf agent\n[agent]\n  interval = \"10s\"\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  collection_jitter = \"0s\"\n  flush_interval = \"10s\"\n  flush_jitter = \"0s\"\n  precision = \"0s\"\n\n  hostname = \"\"\n  omit_hostname = false\n\n###############################################################################\n#                            OUTPUT PLUGINS                                   #\n###############################################################################\n\n# Configuration for sending metrics to Coralogix\n[[outputs.opentelemetry]]\n   service_address = \"coralogix-metrics.endpoint\"\n   insecure_skip_verify = true\n   compression = \"gzip\"\n   [outputs.opentelemetry.coralogix]\n     private_key = \"your-coralogix-api-key\"\n     application = \"prod\"\n     subsystem = \"cloudwatch\"\n\n###############################################################################\n#                             INPUT PLUGINS                                   #\n###############################################################################\n\n# # Pull Metric Statistics from Amazon CloudWatch\n[[inputs.cloudwatch]]\n  region = \"us-east-2\"\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and\n  ##    web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  access_key = \"your-aws-account-access-key\"\n  secret_key = \"your-aws-account-secret-key\"\n#   # token = \"\"\n#   # role_arn = \"\"\n#   # web_identity_token_file = \"\"\n#   # role_session_name = \"\"\n#   # profile = \"\"\n#   # shared_credential_file = \"\"\n#\n#   ## The minimum period for Cloudwatch metrics is 1 minute (60s). However not\n#   ## all metrics are made available to the 1 minute period. Some are collected\n#   ## at 3 minute, 5 minute, or larger intervals.\n#   ## See https://aws.amazon.com/cloudwatch/faqs/#monitoring.\n#   ## Note that if a period is configured that is smaller than the minimum for a\n#   ## particular metric, that metric will not be returned by the Cloudwatch API\n#   ## and will not be collected by Telegraf.\n#   #\n#   ## Requested CloudWatch aggregation Period (required)\n#   ## Must be a multiple of 60s.\n  period = \"5m\"\n#\n#   ## Collection Delay (required)\n#   ## Must account for metrics availability via CloudWatch API\n  delay = \"5m\"\n#\n#   ## Recommended: use metric 'interval' that is a multiple of 'period' to avoid\n#   ## gaps or overlap in pulled data\n  interval = \"5m\"\n# Case sensitive check https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/aws-services-cloudwatch-metrics.html\n  namespaces = [\"AWS/ElastiCache\",\"AWS/RDS\"]\n</code></pre> <p>In order to send your data to Coralogix, you are\u00a0required\u00a0to declare the following variables in your configuration:</p> <ul> <li> <p><code>service_address</code>: In order to send metrics to Coralogix, you will need to include your account\u2019s specific\u00a0domain\u00a0in the Coralogix endpoint:\u00a0<code>otel-metrics.&lt;domain&gt;:443</code>.</p> <ul> <li>For example, if you are located in Region US1, your <code>service_address</code> should appear as: <code>otel-metrics.coralogix.us:443</code>.</li> </ul> </li> <li> <p><code>private_key</code>: Access your Coralogix\u00a0Send-Your-Data API key. Your key is recorded in the override file as a secret in order to ensure that this sensitive information remains protected and unexposed.</p> </li> <li> <p><code>application</code>\u00a0&amp;\u00a0<code>subsystem</code>: Customize and organize your data in your Coralogix dashboard using\u00a0application\u00a0names.</p> </li> </ul> <p>Note: Find out more about different attributes available for the CloudWatch statistics input plugin here.</p> <p>2. If you would like to add additional dimensions to your CloudWatch metrics, add them in the <code>global_tags</code> section of your configuration, as in the example below.</p> <pre><code>[global_tags]\n  dc = \"us-east-2\"\n  user = \"$USER\"\n  env = \"CoE\"\n</code></pre> <p>3. Save the configuration file and restart Telegraf. You should now see your metrics on your Coralogix dashboard.</p> <p></p>"},{"location":"newoutput/collect-cloudwatch-metrics-telegraf/#dashboard","title":"Dashboard","text":"<p>1. Download and import the Telegraf version of your RDS dashboard.</p> <p>Download JSON</p> <p></p> <p>2. Download and import the Telegraf version of your ElastiCache dashboard.</p> <p>Download JSON</p>"},{"location":"newoutput/collect-cloudwatch-metrics-telegraf/#alerts","title":"Alerts","text":"<p>Use PromQL to create metrics alerts:</p> <ul> <li>RDS CPU utilization above 80%</li> </ul> <pre><code>1avg(cloudwatch_aws_rds_cpu_utilization_average{db_instance_identifier!=\"\"}) by (db_instance_identifier)\n\n</code></pre> <ul> <li>RDS Writer CPU utilization above 90%</li> </ul> <pre><code>1avg(cloudwatch_aws_rds_cpu_utilization_average{db_cluster_identifier!=\"\",role=\"WRITER\"}) by (db_cluster_identifier)\n\n</code></pre> <ul> <li>ElastiCache miss above 50%</li> </ul> <pre><code>1(sum(cloudwatch_aws_elasti_cache_cache_misses_sum{})/(sum(cloudwatch_aws_elasti_cache_cache_hits_sum{})+sum(cloudwatch_aws_elasti_cache_cache_misses_sum{})))*100\n\n</code></pre>"},{"location":"newoutput/collect-cloudwatch-metrics-telegraf/#validation","title":"Validation","text":"<p>To validate your configuration, access your Coralogix - Grafana dashboard.</p> <ol> <li>On the right-hand corner of your dashboard, click on the Grafana drop-down tab.</li> </ol> <p></p> <p>2. Once you\u2019ve accessed your Coralogix-Grafana dashboard, click on Explore tab in the left-hand browser.</p> <p></p> <p>3. Click on the drop-down arrow of the Metrics browser and input <code>rds</code> (RDS metrics) and <code>elasti_cache</code> (ElastiCache metrics) in the \u201cSelect a Metric\u201d column.</p>"},{"location":"newoutput/collect-cloudwatch-metrics-telegraf/#additional-resources","title":"Additional Resources","text":"<p>Getting Started with Telegraf</p> <p>Telegraf Repository</p> <p>Enrich metrics with EC2 tags</p>"},{"location":"newoutput/collect-cloudwatch-metrics-telegraf/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/","title":"Collect Kubernetes Events using OpenTelemetry","text":"<p>The following tutorial demonstrates how to collect Kubernetes Events using OpenTelemetry.</p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#overview","title":"Overview","text":"<p>The Kubernetes event is valuable telemetry data to help understand what is happening for a given resource inside the cluster, providing the cluster administrator and engineer more visibility using Kubernetes as their hosting platform.</p> <p>OpenTelemetry has established a new receiver called k8sobjects, used to collect and export required data to any destination, including Coralogix.</p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#set-up-kubernetes-events","title":"Set Up Kubernetes Events","text":"<p>To set up Kubernetes events, use the following configuration.</p> <pre><code>receivers:\n  k8sobjects:\n    objects:\n      - name: events\n        mode: watch\n                group: events.k8s.io\n\nexporters:\n    coralogix:\n    application_name: '${APP_NAME}'\n    application_name_attributes:\n        - '${APP_NAME}'\n    logs:\n      endpoint: '&lt;endpoint&gt;'\n    metrics:\n      endpoint: '&lt;endpoint&gt;'\n    private_key: ${CORALOGIX_PRIVATE_KEY}\n    subsystem_name: 'nodes'\n    subsystem_name_attributes:\n        - service.name\n    timeout: 1m\n    traces:\n      endpoint: '&lt;endpoint&gt;'\n\nservice:\n    pipelines:\n      logs:\n        receivers: [k8sobjects]\n        exporters: [coralogix]\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>k8sobjects serves as an OpenTelemetry receiver collecting logs, making the task of configuration simple.</p> </li> <li> <p>The configuration watches all of the events through all of the namespaces and sends them to Coralogix under the configured <code>**application_name**</code> and <code>subystem_name</code>. Learn more about Coralogix application and subsystem names here.</p> </li> <li> <p>Input the following variables:</p> <ul> <li> <p><code>**private_key**</code>: Coralogix Send-Your-Data API key</p> </li> <li> <p><code>**endpoint**</code>: OpenTelemetry endpoint associated with your Coralogix domain</p> </li> </ul> </li> </ul>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#send-events-to-a-specific-subsystem-in-coralogix","title":"Send Events to a Specific Subsystem in Coralogix","text":"<p>The configuration above will send the Kubernetes events data to the default <code>node</code> sub_system in Coralogix. This occurs because the k8sobject receiver collects data from events produced by another system and does not have information regarding the app that produced the event.</p> <p>To work around this default behavior, leverage the Resource OpenTelemetry processor, which grants the possibility of setting attributes on the collected data before exporting it.</p> <pre><code>receivers:\n  k8sobjects:\n    objects:\n      - name: events\n        mode: watch\n                group: events.k8s.io\n\nprocessors:\n  resource:\n    attributes:\n    - key: service.name\n      value: \"kube-events\"\n      action: upsert\n\nexporters:\n    coralogix:\n    application_name: '${APP_NAME}'\n    application_name_attributes:\n        - '${APP_NAME}'\n    logs:\n      endpoint: '&lt;endpoint&gt;'\n    metrics:\n      endpoint: '&lt;endpoint&gt;'\n    private_key: ${CORALOGIX_PRIVATE_KEY}\n    subsystem_name: 'nodes'\n    subsystem_name_attributes:\n        - service.name\n    timeout: 1m\n    traces:\n      endpoint: '&lt;endpoint&gt;'\n\nservice:\n    pipelines:\n      logs:\n        receivers: [k8sobjects]\n                processors: [resource]\n        exporters: [coralogix]\n\n</code></pre> <p>This configuration will ship all of the events collected to the sub_system <code>kube-events</code>, making it easier to identify and contextualize events.</p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#keep-log-contents-clean","title":"Keep Log Contents Clean","text":"<p>OpenTelemetry ships all the OpenTelemetry log objects to Coralogix and because of this, resulting in many unnecessary properties.</p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#opentelemetry","title":"OpenTelemetry","text":"<p>Transform your data using the Transform OpenTelemetry processor to reduce costs on bandwidth and unnecessary storage and parsing rules in your Coralogix account.</p> <pre><code>receivers:\n  k8sobjects:\n    objects:\n      - name: events\n        mode: watch\n                group: events.k8s.io\n\nprocessors:\n  resource:\n    attributes:\n    - key: service.name\n      value: \"kube-events\"\n      action: upsert\n    transform:\n        log_statements:\n            - context: log  \n        statements: \n          - keep_keys(body, [\"type\", \"action\", \"eventTime\", \"reason\", \"regarding\", \"reportingController\", \"note\", \"series\", \"metadata\", \"deprecatedFirstTimestamp\", \"deprecatedLastTimestamp\"])\n\nexporters:\n    coralogix:\n    application_name: '${APP_NAME}'\n    application_name_attributes:\n        - '${APP_NAME}'\n    logs:\n      endpoint: '&lt;endpoint&gt;'\n    metrics:\n      endpoint: '&lt;endpoint&gt;'\n    private_key: ${CORALOGIX_PRIVATE_KEY}\n    subsystem_name: 'nodes'\n    subsystem_name_attributes:\n        - service.name\n    timeout: 1m\n    traces:\n      endpoint: '&lt;endpoint&gt;'\n\nservice:\n    pipelines:\n      logs:\n        receivers: [k8sobjects]\n                processors: [resource, transform]\n        exporters: [coralogix]\n\n</code></pre> <p>This configuration enables storing only important event data and drops all other related properties.</p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#coralogix-dashboard","title":"Coralogix Dashboard","text":"<p>Leverage Coralogix parsing rules to further clean up your data set, using the following parsing rule configuration.</p> <p></p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#reduce-the-amount-of-collected-data","title":"Reduce the Amount of Collected Data","text":"<p>The number of events produced by Kubernetes may easily reach the millions, especially if one has big clusters with many nodes and applications scaling up and down all the time. Collect only that data that really matters to you using our suggestions below.</p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#collect-only-warning-events","title":"Collect Only Warning Events","text":"<p>Currently, Kubernetes has two different types of events: <code>Normal</code> and <code>Warning</code>. As we have the ability to filter events according to their type, you may choose to collect only <code>Warning</code> events, as these events are key to troubleshooting.</p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#filter-the-event-reason","title":"Filter the Event Reason","text":"<p>The reason field describes, in a readable manner, why an action was taken. You may apply some filters to avoid the reasons you want to drop.</p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#coralogix-block-feature","title":"Coralogix Block Feature","text":"<p>OpenTelemetry provides a filter processor which allows one to filter out telemetry data, enabling a reduction in data sent to Coralogix.</p> <p>At present, the filter processor provides only simple filters for log metadata and prevents filters (such as <code>reason: BackoffLimitExceeded and kind: Job</code>) from being combined. This may lead to important data being deleted.</p> <p>If the same reasons are produced by different controllers, leverage the Coralogix Dynamic Blocking Feature under the parsing rules and utilize the powerful regex system available.</p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#alerting-for-events","title":"Alerting for Events","text":"<p>Once your data appears in your Coralogix dashboard, create alerts to give you visibility about important events inside the cluster that may lead to outages or issues.</p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#failedcreatepodsandbox","title":"FailedCreatePodSandBox","text":"<p>This error usually occurs due to issues with networking, although it can also be caused by suboptimal system\u00a0resource limit configuration, such as the number of available IPs, your Subnet being exhausted, or the instances using all the network interfaces available on the node.</p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#evicted","title":"Evicted","text":"<p>A pod eviction is a characteristic function of Kubernetes used in certain scenarios,\u00a0such as node NotReady, insufficient node resources, and expelling pods to other nodes.</p>"},{"location":"newoutput/collect-kubernetes-events-using-opentelemetry/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/coralogix-action-extension/","title":"Coralogix Actions","text":"<p>Coralogix simplifies the triggering of third-party services based on specific search results and values under designated keys.</p>"},{"location":"newoutput/coralogix-action-extension/#creating-an-action","title":"Creating an Action","text":"<p>STEP 1. Navigate to the Coralogix Explore &gt; Logs screen.</p> <p>STEP 2. Click on Settings in the upper right-hand corner. Select MANAGE ACTIONS from the drop-down menu.</p> <p></p> <p>STEP 3. A pop-up window displaying a list of defined Actions will appear. If the list is empty, a new Untitled Action will be opened.</p> <p></p> <p>STEP 4. If this isn't your first Action, click + ADD NEW ACTION to create a new one.</p> <p>STEP 5. Name the new Action in the Untitled Action pop-up.</p> <p>STEP 6. In the URL field, add the external tool's URL. For example: <code>https://test-my-url.com?type={{ $m.subsystemName === \"frontend\" ? \"Front\" : \"Back\" }}</code>.</p> <p></p> <p>When using curly brackets {{}}, a dropdown menu with three variable types will appear: $p, $m, $d.</p> VariableDescription$pVariables which can be used with any log (START_DATE, END_DATE, SELECTED_VALUE)$mCoralogix metadata fields (applicationName, subsystemName, severity, etc.)$dSelected log <p>Notes:</p> <ul> <li> <p>URLs must begin with 'http://' or 'https://'. Other URL structures trigger an instructional message.</p> </li> <li> <p>JavaScript syntax is supported for \"Actions,\" allowing for conditions like <code>{{ condition ? \"true value\" : \"false value\" }}</code>.</p> </li> <li> <p>Nearly all JavaScript functions are supported.</p> </li> </ul> <p>STEP 7.\u00a0[Optional] Advanced: Restrict the Action to specific applications and/or subsystems. Share it with teammates by activating the Make public when saved toggle.</p> <p></p> <p>STEP 8. Click RESET to clear the entered data. To save, click SAVE ACTION.</p>"},{"location":"newoutput/coralogix-action-extension/#manage-custom-actions","title":"Manage Custom Actions","text":"<p>STEP 1.\u00a0In your Coralogix navigation pane, click on\u00a0Explore\u00a0&gt; Logs screen page.</p> <p>STEP 2. Click on\u00a0Settings\u00a0in upper right-hand corner. In the drop-down menu, select MANAGE ACTIONS.</p> <p>STEP 3. A pop-up window, containing a list of defined\u00a0Actions, will be displayed.</p> <p>STEP 4. Take various actions:</p> <ul> <li> <p>Click on the rectangle icon to clone an Action for easy editing and saving under a new name.</p> </li> <li> <p>Use the eye icon to disable/enable an Action.</p> </li> <li> <p>Click the arrow button to edit or delete an Action.</p> </li> <li> <p>Click + ADD NEW ACTION to define a new Action.</p> </li> <li> <p>Use the group of small gray rectangles on the left side of an Action name to move and set the order within Private/Shared sections.</p> </li> </ul>"},{"location":"newoutput/coralogix-action-extension/#custom-action-usage","title":"Custom Action Usage","text":"<p>STEP 1. Access Coralogix Explore &gt; Logs screen.</p> <p>STEP 2. Define a query.</p> <p>STEP 3. Trigger the action by:</p> <ul> <li> <p>Clicking on (\u2026) next to a specific log or JSON key. Only Actions without \"$p.SELECTED_VALUE\" in the URL are visible.</p> </li> <li> <p>Clicking on a value lists all defined Actions, including those with \"$p.SELECTED_VALUE\" in the URL. If defined with \"$p.SELECTED_VALUE,\" the value from that JSON key is passed.</p> </li> </ul> <p></p> <p></p> <p>Once an Action is chosen, the defined URL opens in a new tab.</p>"},{"location":"newoutput/coralogix-action-extension/#usage-examples","title":"Usage Examples","text":""},{"location":"newoutput/coralogix-action-extension/#common-usage","title":"Common Usage","text":"<p>Jira bug action</p> <pre><code>http://&lt;company_name&gt;.atlassian.net//secure/CreateIssueDetails!init.jspa?pid=&lt;Jira_PID&gt;&amp;issuetype=&lt;Jira_issueType&gt;&amp;description={{$d.logRecord.body}}&amp;summary={{$d.msg}}\n</code></pre> <p>Query logs based on a key in the logs</p> <pre><code>https://&lt;team_name&gt;.app.coralogix.us/#/query-new/logs?query=&lt;key&gt;%3A%22{{$d.&lt;key&gt;}}%22&amp;time=from:{{$p.start_date}},to:{{$p.end_date}}\n</code></pre> <p>Query spans based on a key in the logs</p> <pre><code>https://&lt;team_name&gt;.app.coralogix.us/#/query-new/tracing?query=&lt;spans_key&gt;:%22{{$d.&lt;logs_key&gt;}}%22&amp;time=from:{{$p.start_date}},to:{{$p.end_date}}&amp;page=0&amp;onlyRootSpans=false&amp;tracingTableView=spans\n</code></pre> <p>Jump to Grafana dashboard with filter from a key in the log</p> <pre><code>https://&lt;team_name&gt;.app.coralogix.us/#/grafana/d/WhANBmU4k/&lt;grafana_dashboard_name&gt;?orgId=342&amp;var-&lt;grafana_filter_name&gt;={{$d.&lt;log_key&gt;}}\n</code></pre>"},{"location":"newoutput/coralogix-action-extension/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/coralogix-amazon-web-services-aws-privatelink-endpoints/","title":"AWS PrivateLink","text":"<p>AWS PrivateLink provides private connectivity between virtual private clouds (VPCs), supported AWS services, and your on-premises networks without exposing your traffic to the public internet. Interface VPC endpoints, powered by PrivateLink, connect you to services hosted by Coralogix. While Coralogix monitoring traffic is always secure, PrivateLink provides stable connectivity, a reduction in traffic costs, and even greater security by maintaining data on the AWS network.</p> <p>This tutorial provides AWS Coralogix PrivateLink endpoints and instructions for standard configuration.</p>"},{"location":"newoutput/coralogix-amazon-web-services-aws-privatelink-endpoints/#use-cases","title":"Use Cases","text":"<p>The primary use case for PrivateLink with Coralogix is connectivity for monitored applications running in AWS VPCs. To use Coralogix PrivateLink, you must create a VPC endpoint in the Coralogix AWS region matching your Coralogix domain. This is referred to as same-region VPC.</p> <p></p> <p>If your AWS resources to be monitored are in a different region, you can leverage VPC peering to meet the requirements by launching your Lambda in a cross-region VPC, local to the source.</p> <p></p>"},{"location":"newoutput/coralogix-amazon-web-services-aws-privatelink-endpoints/#privatelink-endpoints","title":"PrivateLink Endpoints","text":"<p>Coralogix exposes the AWS PrivateLink endpoint in all Coralogix AWS regions.</p> Coralogix DomainCoralogix AWS RegionService NameOpenTelemetry -Otel-TracesOtel-MetricsOtel-LogsCoralogixLambdaTelemetryCoralogixLogsPrometheusRemoteWritecoralogixstg.wpengine.comeu-west-1 (Ireland)com.amazonaws.vpce.eu-west-1.vpce-svc-01f6152d495e211f0ingress.private.coralogixstg.wpengine.com:443ingress.private.coralogixstg.wpengine.com:443https://ingress.private.coralogixstg.wpengine.com/logs/v1/singleshttps://ingress.private.coralogixstg.wpengine.com/prometheus/v1coralogix.inap-south1 (India)com.amazonaws.vpce.ap-south-1.vpce-svc-0eb807f14d645a973ingress.private.coralogix.in:443ingress.private.coralogix.in:443https://ingress.private.coralogix.in/logs/v1/singleshttps://ingress.private.coralogix.in/prometheus/v1coralogix.usus-east2(US)com.amazonaws.vpce.us-east-2.vpce-svc-067fdf46ffae1ed0eingress.private.coralogix.us:443ingress.private.coralogix.us:443https://ingress.private.coralogix.us/logs/v1/singleshttps://ingress.private.coralogix.us/prometheus/v1eu2.coralogixstg.wpengine.comeu-north-1 (Stockholm)com.amazonaws.vpce.eu-north-1.vpce-svc-041b21c87be842c08ingress.private.eu2.coralogixstg.wpengine.com:443ingress.private.coralogixsg.com:443https://ingress.private.eu2.coralogixstg.wpengine.com/logs/v1/singleshttps://ingress.private.eu2.coralogixstg.wpengine.com/prometheus/v1coralogixsg.comap-southeast-1(Singapore)com.amazonaws.vpce.ap-southeast-1.vpce-svc-0e4cd83852ff2869bingress.private.coralogixsg.com:443ingress.private.coralogixsg.com:443https://ingress.private.coralogixsg.com/logs/v1/singleshttps://ingress.private.coralogixsg.com/prometheus/v1cx498.coralogixstg.wpengine.comus-west-2(Oregon)com.amazonaws.vpce.us-west-2.vpce-svc-0f6436ddb210e5dbbingress.private.cx498-aws-us-west-2.coralogixstg.wpengine.com:443ingress.private.cx498-aws-us-west-2.coralogixstg.wpengine.com:443https://ingress.private.cx498-aws-us-west-2.coralogixstg.wpengine.com:443/logs/v1/singleshttps://ingress.private.cx498-aws-us-west-2.coralogixstg.wpengine.com:443/prometheus/v1"},{"location":"newoutput/coralogix-amazon-web-services-aws-privatelink-endpoints/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>If you use an integration involving Amazon S3, you must ensure that the VPC in which your Lambda is deployed has an S3 Service Gateway configured.</p> </li> <li> <p>If you intend to use AWS Secrets Manager with your Lambda, you must create another VPC endpoint for the\u00a0<code>com.amazonaws.&lt;AWS Region&gt;.secretsmanager</code>\u00a0service. Detailed instructions can be found here.</p> </li> </ul>"},{"location":"newoutput/coralogix-amazon-web-services-aws-privatelink-endpoints/#vpc-configuration","title":"VPC Configuration","text":"<p>To use Coralogix PrivateLink, you must create a VPC endpoint in the Coralogix AWS region matching your Coralogix domain. This is referred to as same-region VPC. For example, the <code>coralogixstg.wpengine.com</code> domain is hosted in <code>eu-west-1</code>. A same-region VPC must be deployed in <code>eu-west-1</code>.</p> <p>If your AWS resources to be monitored are in a different region, you can leverage VPC peering to meet the requirements by launching your Lambda in a cross-region VPC, local to the source. Cross-region VPC configuration instructions can be found here.</p> <p>STEP 1. Create a VPC endpoint.</p> <ul> <li> <p>Connect to the AWS console in your Coralogix AWS region.</p> </li> <li> <p>Navigate to the Endpoints section.</p> </li> <li> <p>Click Create endpoint.</p> </li> </ul> <p></p> <p>STEP 2. Name the VPC endpoint and select the service category: PrivateLink Ready partner services.</p> <p></p> <p>STEP 3. Input the Service name associated with your Coralogix AWS region, as per the above table.</p> <p></p> <p>STEP 4. Click\u00a0Verify service.</p> <ul> <li> <p>You should receive the following message: Service name verified.</p> </li> <li> <p>If you do not receive this message, contact us via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p> </li> </ul> <p></p> <p>STEP 5. Select a VPC in which to create the endpoint.</p> <p></p> <p>STEP 6. Expand the Additional settings section and Enable DNS name.</p> <p></p> <p>STEP 7. Select a security group to enable traffic to this VPC endpoint.</p> <ul> <li>The security group must accept inbound traffic in port 443 (TCP).</li> </ul> <p></p> <p>STEP 8. Click Create endpoint.</p> <p>STEP 9. Verify your configuration.</p> <ul> <li>Ensure the VPC endpoint status appears as Available.</li> </ul> <p></p> <p>STEP 10. Enter the connected VPC and type the following command, adjusted per region:</p> <pre><code>#example US region\n#telnet ingress.private.coralogix.us\ntelnet &lt;ingress.private.&lt;region&gt; 443\n</code></pre>"},{"location":"newoutput/coralogix-amazon-web-services-aws-privatelink-endpoints/#next-steps","title":"Next Steps","text":"<ul> <li> <p>If your AWS resources to be monitored are in a different region than your Coralogix domain, you have the option of leveraging VPC peering to meet the requirements by launching your Lambda in a cross-region VPC, local to the source. Cross-region VPC configuration instructions can be found here.</p> </li> <li> <p>Align the VPC to your Lambda. Instructions can be found here.</p> </li> </ul>"},{"location":"newoutput/coralogix-amazon-web-services-aws-privatelink-endpoints/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/coralogix-apis/","title":"Getting Started With Coralogix APIs","text":"<p>Optimize Coralogix's observability monitoring and unlock its most powerful features by using our wide range of APIs. Use them to send data to Coralogix, build visualizations, manage your data, and query it. Read on to learn about our Data Ingestion, Data Management, and Data Query APIs and how to use them.</p>"},{"location":"newoutput/coralogix-apis/#send-your-data-to-coralogix","title":"Send Your Data to Coralogix","text":""},{"location":"newoutput/coralogix-apis/#data-ingestion-apis","title":"Data Ingestion APIs","text":"<p>Data is ingested seamlessly and reliably into the Coralogix platform using our Data Ingestion APIs.</p> <ul> <li> <p>HTTP</p> <ul> <li>REST API. Send us your logs using either our <code>/logs</code> or <code>/singles</code> endpoint.</li> </ul> </li> <li> <p>gRPC</p> <ul> <li> <p>Custom logs. Send your custom logs to Coralogix using our OpenTelemetry-compatible endpoint.</p> </li> <li> <p>Custom metrics. Employ our OpenTelemetry-compatible custom metric endpoint, including serverless computing and quick cURL-like calls, to send counters, gauges, and histograms to Coralogix.</p> </li> <li> <p>Custom traces. Send your custom traces to Coralogix using our OpenTelemetry-compatible endpoint.</p> </li> </ul> </li> </ul>"},{"location":"newoutput/coralogix-apis/#manage-your-data","title":"Manage Your Data","text":"<p>The Data Management APIs enable you to configure the Coralogix platform, customize your user interface, and optimize it for your observability requirements.</p> <ul> <li> <p>Alerts API. Coralogix gives you the ability to create monitors that actively check system performance and notify you when there are changes to your data. Our Alerts API allows you to define, query, and manage your alerts.</p> </li> <li> <p>Parsing Rules API. Use our log parsing rules to process, parse, and restructure log data for monitoring and analysis in the Coralogix platform. Create, read, update, or delete these rules and rule groups for your data.</p> </li> <li> <p>Enrichment API. Easily enrich your log data with business, operations, or security information using our Enrichment API. Automatically add fields to your JSON logs based on specific matches in your log data, using a predefined custom data source of your choice.</p> </li> <li> <p>Hosted Grafana API. Visualize your logs, metrics, and traces using our Grafana-hosted view without the need for any plugins. We provide a secure Grafana API to manage your Grafana-hosted dashboard, allowing you to create, edit, export, import, and query your data in that platform.</p> </li> <li> <p>Data Usage Service API. Coralogix provides a Data Usage Service API in support of our\u00a0Detailed Data Usage Report, which presents you with the data you\u2019ve sent to Coralogix, per policy, for either the current month or retroactively 30 or 90 days. The API allows you to query your data consumption in a given time period.</p> </li> <li> <p>TCO Optimizer HTTP API. Define, query, and manage your TCO log policy overrides.</p> </li> <li> <p>TCO Tracing Policy gRPC API. Define, query, and manage your\u00a0TCO tracing policy criteria.</p> </li> <li> <p>Insights API. Manage our Insights Detection service, automatically detecting possible threats and security related anomalies in your traffic.</p> </li> <li> <p>Webhooks API. Define, query, and manage your webhooks.</p> </li> <li> <p>S3 Archive Setup API. Many of our customers send us their telemetry data via their Amazon S3 bucket. Our S3 Archive Setup gRPC API allows you to view bucket definitions, set a target bucket and to define archive retentions.</p> </li> <li> <p>Send-Your-Data Management API. Coralogix offers its customers the option of creating multiple Send-Your-Data API keys with advanced security settings, allowing you to minimize security vulnerabilities and utilize different keys across different systems, deployment methods, and teams. Our\u00a0recommended best practice\u00a0when sending us data is to create multiple keys for your organization with all keys enjoying our advanced security settings. This feature is supported by our Send-Your-Data Management API.</p> </li> <li> <p>Recording Rules API. Coralogix recording rules allow you to pre-process and derive new time series from existing ones. The Recording Rules API allows you to manage these rules, which are executed in the background at regular intervals.</p> </li> <li> <p>SLO API. Efficiently manage your Service Level Objectives (SLOs) programmatically. This API will let you retrieve, create, update and delete your SLOs.</p> </li> <li> <p>Service Removal API. Even if services no longer exist on your side, the catalog in Coralogix lists all previously imported services indefinitely. With the Service Removal API, you can manually remove one or more unused services from your Coralogix subscription.</p> </li> <li> <p>Incidents Management API. Manage your reported incidents by listing individual or batch metadata, aggregating incidents, as well as assigning/unassigning incidents or resolving chosen events.\u00a0</p> </li> </ul>"},{"location":"newoutput/coralogix-apis/#query-your-data","title":"Query Your Data","text":"<p>Access and query your data using our Data Query APIs.</p> <ul> <li>Direct Query API. Run\u00a0DataPrime\u00a0or\u00a0Lucene\u00a0queries of your indexed and\u00a0archived\u00a0logs without the need to access your Coralogix UI.</li> </ul>"},{"location":"newoutput/coralogix-apis/#platform-as-a-service-paas","title":"Platform as a Service (PaaS)","text":"<p>Avoid implementing our APIs on your own by taking advantage of our own automated options, without having to write your own code:</p> <ul> <li> <p>Coralogix Terraform Provider</p> </li> <li> <p>Coralogix Kubernetes Operator</p> </li> </ul>"},{"location":"newoutput/coralogix-apis/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/","title":"AWS Lambda Telemetry Exporter","text":"<p>This tutorial demonstrates how to set up and install the Coralogix AWS Lambda Telemetry Exporter.</p> <p>This integration is one of two options - complete and basic - for monitoring Lambda, a requirement of our cutting-edge Serverless Monitoring feature.</p> <ul> <li> <p>This tutorial demonstrates how to set up Lambda monitoring to get basic telemetry, including logs, which requires setting up the Coralogix AWS Lambda Telemetry Exporter.</p> </li> <li> <p>To set up Lambda monitoring to get complete telemetry, including traces, view the relevant documentation\u00a0here.</p> </li> </ul>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#overview","title":"Overview","text":"<p>The Coralogix AWS Lambda Telemetry Exporter is an AWS Lambda extension that uses AWS Lambda Telemetry API and seamlessly collects Lambda function logs, as well as Lambda platform logs, metrics, and traces. It is currently available as an open beta.</p> <p>The Coralogix AWS Lambda Telemetry Exporter supersedes the previously offered Coralogix Extension for AWS Lambda (version 1.x.y) by extending its capabilities beyond logs. If you\u2019re already using the Coralogix Extension for AWS Lambda, you can use it for now or migrate to the Telemetry Exporter to enjoy the new goodies early. We recommend using the Telemetry Exporter in new deployments.</p> <p>This document assumes version 0.6.2 (layer version 25) of the Coralogix AWS Lambda Telemetry Exporter. If you\u2019re using an older version, we highly recommend updating.</p>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#setting-up-lambda-monitoring","title":"Setting up Lambda monitoring","text":"<p>To monitor AWS Lambda functions using Coralogix, follow these steps.</p> <p>STEP 1. Set up the Coralogix AWS Resource Metadata Collection.</p> <p>STEP 2a. To get full telemetry, including tracing, set up the AWS Lambda Auto-Instrumentation. This is available for Node.js and Python (3.8 or newer). For other runtimes choose Step 2b.</p> <p>STEP 2b. Alternatively, to get logs and basic telemetry, set up just the Coralogix AWS Lambda Telemetry Exporter</p> <p>STEP 3. Learn what the Serverless Monitoring feature has to offer.</p> <p>The document you\u2019re viewing now is for Step 2b. You may want to proceed with Step 2a instead.</p>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>An active AWS account with permissions to manage Lambda functions.</p> </li> <li> <p>At least one AWS Lambda function selected for monitoring</p> </li> </ul>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#setup","title":"Setup","text":"<p>The Coralogix AWS Lambda Telemetry Exporter is available with any of the following:</p> <ul> <li> <p>Using ARN of a lambda layer published by Coralogix (recommended)</p> </li> <li> <p>Copying the telemetry exporter into a container image (recommended for Docker)</p> </li> <li> <p>Deploying your copy of the lambda layer to your AWS account from AWS Serverless Repository</p> </li> </ul>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#installation-published-layer-arn","title":"Installation: Published Layer ARN","text":"<p>To deploy or update the Coralogix AWS Lambda Telemetry Exporter, select the ARN corresponding to your AWS region and CPU architecture from the following list:</p> <pre><code>arn:aws:lambda:ap-south-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:ap-south-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:eu-north-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:eu-north-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:eu-west-3:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:eu-west-3:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:eu-west-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:eu-west-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:eu-west-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:eu-west-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:ap-northeast-3:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:ap-northeast-3:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:ap-northeast-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:ap-northeast-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:ap-northeast-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:ap-northeast-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:ca-central-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:ca-central-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:sa-east-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:sa-east-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:ap-southeast-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:ap-southeast-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:ap-southeast-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:ap-southeast-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:eu-central-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:eu-central-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:us-east-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:us-east-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:us-east-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:us-east-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:us-west-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:us-west-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:us-west-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:us-west-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:af-south-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:af-south-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:ap-east-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:ap-east-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:ap-southeast-3:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:ap-southeast-3:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:eu-south-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:eu-south-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:me-south-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:me-south-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:ap-south-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:ap-south-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:ap-southeast-4:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:ap-southeast-4:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:eu-central-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:eu-central-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:eu-south-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:eu-south-2:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:me-central-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:me-central-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:il-central-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\narn:aws:lambda:il-central-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-arm64:25\narn:aws:lambda:ca-west-1:625240141681:layer:coralogix-aws-lambda-telemetry-exporter-x86_64:25\n</code></pre> <p>STEP 1. Configure the <code>coralogix-aws-lambda-telemetry-exporter</code>.</p> <ul> <li> <p>Access the Lambda function that you would like to monitor and configure the environment variables: Configuration &gt; Environment variables</p> </li> <li> <p>View the list of configuration variables in the Configuration section below.</p> </li> </ul> <p>STEP 2. Add the Telemetry Exporter layer to the chosen function.</p> <ul> <li>Access the Lambda function from which you want to send telemetry data to Coralogix: Layers &gt; Add Layer&gt; Specify an ARN &gt; paste ARN from the list provided above &gt; Add</li> </ul> <p>Notes:</p> <ul> <li>If you are unsure about the architecture, it is displayed under Function runtime settings.</li> </ul> <p></p> <ul> <li>If that function was already using <code>coralogix-lambda-extension</code>, we recommend removing it: Layers &gt; Edit &gt; Select the Lambda layer &gt; Remove</li> </ul>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#installation-container-image-lambda","title":"Installation: Container Image Lambda","text":"<p>To deploy your lambda as a container image and inject an extension as part of your function, build from the exporter image found here.</p> <pre><code>FROM coralogixrepo/coralogix-aws-lambda-telemetry-exporter:0.6.2 AS coralogix-extension\nFROM public.ecr.aws/lambda/python:3.8\n# Layer code\nWORKDIR /opt\nCOPY --from=coralogix-extension /opt/ .\n# Function code\nWORKDIR /var/task\nCOPY app.py .\nCMD [\"app.lambda_handler\"]\n</code></pre> <p>In this example, python3.8 is being used as the runtime, but all runtimes are supported with our lambda extension.</p> <p>Notes:</p> <ul> <li> <p>There is no latest tag for the <code>coralogix-aws-lambda-telemetry-exporter</code> image; you must specify the version explicitly.</p> </li> <li> <p>Find information on available versions/tags here.</p> </li> </ul>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#configuration","title":"Configuration","text":"<ul> <li> <p>CX_DOMAIN: Coralogix domain within which you\u2019ve set up your account.</p> </li> <li> <p>CX_API_KEY: Coralogix Send Your Data - API Key.</p> </li> <li> <p>CX_SECRET: As an alternative to providing the Send Your Data \u2013 API Key directly in CX_API_KEY you can use AWS Secrets Manager to store the key, and provide the name of the secret in CX_SECRET. See the Secrets Manager section below for more details.</p> </li> <li> <p>CX_REPORTING_STRATEGY: Acceptable values include <code>LOW_OVERHEAD</code>, <code>REPORT_AFTER_INVOCATION</code> and <code>REPORT_DURING_AND_AFTER_INVOCATION</code>. For info on how to choose the optimal strategy for your Lambda function, see the Reporting Strategies section below.</p> </li> <li> <p>CX_APPLICATION &amp; CX_SUBSYSTEM (optional): Defaults to AWS account number and Lambda function, respectively. Learn more about application and subsystem names on your Coralogix dashboard here.</p> </li> <li> <p>CX_LOG_METADATA_ENABLED (optional): Defaults to <code>true</code>. Set to false to exclude the <code>cx_metadata</code> from logs in order to reduce their size.</p> </li> <li> <p>CX_OTLP_SERVER_ENABLED (optional): Defaults to <code>true</code>. Set to false to disable the OTLP server and free up the <code>4317</code> and <code>4318</code> ports.</p> </li> <li> <p>CX_LOG_ONLY (optional): When set to <code>true</code> disables metrics and tracing.</p> </li> <li> <p>CX_LOG_MODE (optional): Defaults to <code>structured</code>. Acceptable values are:</p> <ul> <li> <p><code>disabled</code> - no logs will be delivered to Coralogix</p> </li> <li> <p><code>structured</code> - logs will be wrapped in a JSON and delivered to Coralogix</p> </li> </ul> </li> <li> <p>CX_METRICS_MODE (optional): Defaults to <code>platform_report</code>. Acceptable values are:</p> <ul> <li> <p><code>disabled</code> - no metrics will be delivered to Coralogix</p> </li> <li> <p><code>platform_report</code> - metrics based on AWS Telemetry API Platform Report event will be delivered to Coralogix</p> </li> </ul> </li> <li> <p>CX_TRACING_MODE (optional): Defaults to otel when OTEL instrumentation is detected. Otherwise defaults to telemetry_api. Acceptable values are:</p> <ul> <li> <p><code>disabled</code> - no traces will be delivered to Coralogix</p> </li> <li> <p><code>telemetry_api</code> - will generate spans based on data provided by the AWS Lambda Telemetry API.</p> </li> <li> <p><code>otel</code> - will expect to receive spans from the function via OTLP. This mode is meant to be used in conjunction with OTEL instrumentation of the function and the CX_OTLP_SERVER_ENABLED enabled. Warning: <code>otel</code> tracing mode delays emitting logs until the end of the invocation even when the reporting strategy is set to <code>REPORT_DURING_AND_AFTER_INVOCATION</code> .</p> </li> </ul> </li> <li> <p>CX_TRACE_SAMPLING_MODE (optional): Defaults to <code>all</code>. Acceptable values are:</p> <ul> <li> <p><code>all</code> - all traces are sent to Coralogix</p> </li> <li> <p><code>follow_xray</code> - the telemetry exporter will respect AWS XRay\u2019s decision which traces should be sampled</p> </li> </ul> </li> <li> <p>CX_TAGS_ENABLED (optional): Defaults to <code>false</code>. Setting it to <code>true</code> will enable enrichment of the telemetry with the tags of the Lambda function. See the Enrichment with tags section to learn how to correctly set this up.</p> </li> <li> <p>CX_TAG_CACHE_VALIDITY_MS (optional): Defaults to 300000 (5 minutes). Controls how often the tags will be refreshed.</p> </li> <li> <p>CX_MESSAGE_SIZE_LIMIT (optional): Defaults to 30000. Log messages longer than CX_MESSAGE_SIZE_LIMIT bytes will be truncated.</p> </li> <li> <p>CX_EXCLUDED_SPAN_ATTRIBUTES (optional): Defaults to <code>disabled</code>. Acceptable values are:</p> <ul> <li> <p><code>disabled</code></p> </li> <li> <p><code>predefined</code> - will remove known vulnerable attributes. Currently <code>http.request.header.x-api-key</code> and <code>http.request.header.authorization</code>. More may be added in the future.</p> </li> <li> <p>Any regex that will match attributes should be removed. For example: <code>http\\\\.request\\\\.header\\\\.x-api-key|http\\\\.request\\\\.header\\\\.authorization</code></p> </li> </ul> </li> <li> <p>CX_REPORTING_DELAY_MS (optional): Defaults to 15000. Controls how often telemetry is sent in <code>LOW_OVERHEAD</code> and <code>REPORT_DURING_AND_AFTER_INVOCATION</code> modes</p> </li> <li> <p>CX_MAX_SHUTDOWN_FLUSH_DELAY_MS (optional): Defaults to 600. Controls how long telemetry exporter will wait for the remaining telemetry to arrive during shutdown.</p> </li> <li> <p>CX_SPAN_SENDING_THRESHOLD (optional): Defaults to 2048. Telemetry exporter will send function spans early if the number of spans exceeds this threshold.</p> </li> </ul>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#reporting-strategies","title":"Reporting Strategies","text":"<p>Due to the serverless nature of AWS Lambda and Lambda extensions, the <code>telemetry-exporter</code> cannot freely do its job after and between invocations of the monitored Lambda function. A tradeoff has to be made between timely delivery of the telemetry and keeping overhead costs low.</p> <p>The optimal strategy choice depends on how often the Lambda function is invoked and how long it runs. The <code>coralogix-aws-lambda-telemetry-exporter</code> offers three reporting strategies that enable you to adjust reporting to the characteristics of your function.</p> <ul> <li> <p><code>LOW_OVERHEAD</code> : Recommended for frequently called functions. Telemetry is batched across many invocations of the function, and the <code>telemetry-exporter</code> avoids keeping the Lambda execution environment active after an invocation is complete (and in turn avoids additional billable time to the invocations). This may result in long delays (in the order of minutes) in the delivery of telemetry.</p> </li> <li> <p><code>REPORT_AFTER_INVOCATION</code> : Recommended for rarely called functions with short to moderate execution times. Telemetry is reported at the end of each invocation, limiting the amount of billable time for each invocation.</p> </li> <li> <p><code>REPORT_DURING_AND_AFTER_INVOCATION</code> : Recommended for rarely called functions with long execution times (15s or more). Telemetry is reported in regular intervals during the execution of the function and then after the execution completes. This strategy will add 2-3s of billable time to each invocation.</p> </li> </ul> <p>Notes:</p> <ul> <li> <p>Regardless of the chosen reporting strategy, technical limitations may place constraints on delivery times for some telemetry data.</p> </li> <li> <p>Rather than being delivered directly after the invocation, some data (such as the total billable time of an invocation) is delivered during the next invocation of the Lambda function instance.</p> </li> </ul>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#using-metrics","title":"Using Metrics","text":"<p>Check out our tutorial here.</p>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#enrichment-with-tags","title":"Enrichment with Tags","text":"<p>Coralogix AWS Lambda Telemetry Exporter can read the tags of the AWS Lambda function and add them to all reported telemetry (logs/metrics/traces). This feature is disabled by default and can be enabled with <code>CX_TAGS_ENABLED</code>, but it requires prior configuration of permissions for the Lambda function. The configuration can be done in the following steps:</p> <p>STEP 1. In AWS Console access the Lambda function that you would like to configure and navigate to: Configuration &gt; Permissions</p> <p>STEP 2. Click on the name of the execution role. This will bring to the IAM configuration of that role.</p> <p>STEP 3. Click Add Permissions &gt; Create Inline Policy</p> <p>STEP 4. Switch to <code>JSON</code> view and paste the following policy JSON:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"lambda:GetFunction\",\n            \"Resource\": \"&lt;paste lambda ARN here&gt;\"\n        }\n    ]\n}\n</code></pre> <p>STEP 5. Replace the <code>&lt;paste lambda ARN here&gt;</code> with the ARN of the Lambda function. (you can find it on the main page of the function\u2019s configuration)</p> <p>STEP 6. Click Review Policy</p> <p>STEP 7. Name the policy, for example <code>ListLambdaTags</code>.</p> <p>STEP 8. Click Create Policy.</p> <p>STEP 9. Set the <code>CX_TAGS_ENABLED</code> environment variable to <code>true</code>.</p>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#privatelink","title":"PrivateLink","text":"<p>The Coralogix AWS Lambda Telemetry Exporter can send telemetry to Coralogix over AWS Private Link by implementing the following steps:</p> <p>STEP 1. Set up PrivateLink for your VPC.</p> <p>STEP 2. Configure the lambda function to run in that VPC.</p> <p>STEP 3. Change the <code>CX_DOMAIN</code> environment variable, by adding <code>private.</code> at the beginning. For example, if your <code>CX_DOMAIN</code> used to be <code>coralogixstg.wpengine.com</code> , now it should appear as <code>private.coralogixstg.wpengine.com</code>.</p>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#code-signing","title":"Code Signing","text":"<p>The Coralogix AWS Lambda Telemetry Exporter Lambda layers are signed with <code>arn:aws:signer:eu-west-1:625240141681:/signing-profiles/coralogix_lambda</code> AWS Signer profile. You may need to add this profile to your code signing configuration if you have configured Lambda to require signed code.</p>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#secrets-manager","title":"Secrets Manager","text":"<p>Coralogix AWS Lambda Telemetry Exporter can retrieve the Send Your Data \u2013 API Key from AWS Secrets Manager. It requires prior configuration of permissions for the Lambda function. The configuration can be done in the following steps:</p> <ul> <li> <p>Create a secret with the Send-Your-Data API key in AWS Secrets Manager. You can lear how to create Secrets Manager secrets from this instruction https://docs.aws.amazon.com/secretsmanager/latest/userguide/hardcoded.html#hardcoded_step-1</p> </li> <li> <p>In AWS Console access the Lambda function that you would like to configure and go to: Configuration &gt; Permissions</p> </li> <li> <p>Click on the name of the execution role. This will bring to the IAM configuration of that role.</p> </li> <li> <p>Click Add Permissions &gt; Create inline policy</p> </li> <li> <p>Switch to <code>JSON</code> view and paste the following policy JSON:</p> </li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"secretsmanager:GetSecretValue\",\n            \"Resource\": \"&lt;paste ARN of the secret&gt;\"\n        }\n    ]\n}\n</code></pre> <ul> <li> <p>Replace the <code>&lt;paste ARN of the secret&gt;</code> with the ARN of the Secret.</p> </li> <li> <p>Click Review Policy</p> </li> <li> <p>Give it a name. For example <code>GetSecretValue</code></p> </li> <li> <p>Click Create Policy</p> </li> <li> <p>Set the <code>CX_SECRET</code> environment variable to the name of the secret</p> </li> <li> <p>Remove the <code>CX_PRIVATE_KEY</code> environment variable if you added it before</p> </li> </ul>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#update-coralogix-aws-lambda-telemetry-exporter","title":"Update Coralogix AWS Lambda Telemetry Exporter","text":""},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#update-the-procedure-for-arn-based-installations","title":"Update the Procedure for ARN-Based Installations","text":"<p>STEP 1. Look up the latest layer version by reviewing the list of ARNs presented earlier. The number at the end of the ARN is the layer version.</p> <p>STEP 2. Change the Telemetry Exporter layer version used by the function: Layers &gt; Edit &gt; Change to the updated layer version &gt; Save</p>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#update-the-procedure-for-aws-serverless-repository-based-installations","title":"Update the Procedure for AWS Serverless Repository-Based Installations","text":"<p>STEP 1. Deploy the Coralogix-AWS-Lambda-Telemetry-Exporter in your AWS Serverless Application Repository. You are required to deploy it in the same account and region as your Lambda function.</p> <p>STEP 2. Verify the deployment by clicking on \u2018Layers\u2019 in the left-hand menu. You should see that the two previously deployed Lambda layers have new versions. Keep in mind that the version number of the Lambda layer is simply incremented with each deployment and does not correspond to the semantic version shown in the AWS Serverless repository.</p> <p>STEP 3. Change the Telemetry Exporter layer version used by the function: Layers &gt; Edit &gt; Change to latest available layer version &gt; Save</p>"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#additional-resources","title":"Additional Resources","text":"DocumentationAWS Lambda Auto InstrumentationServerless Monitoring"},{"location":"newoutput/coralogix-aws-lambda-telemetry-exporter/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/coralogix-aws-shipper/","title":"Forward AWS Logs via Lambda Shipper","text":"<p>Our newest AWS integration offers the most seamless way to link up with Coralogix. Using a predefined Lambda function, you can send your AWS logs and events to your Coralogix subscription for in-depth analysis, monitoring, and troubleshooting.</p>"},{"location":"newoutput/coralogix-aws-shipper/#overview","title":"Overview","text":"<p>This integration guide focuses on connecting your AWS environment and Coralogix using AWS Lambda functions. To complete this integration, you may either use the Coralogix platform UI, CloudFormation templates from AWS, AWS SAM applications, or a dedicated Terraform module from our GitHub repository.</p> <p>Our latest integration is based on the <code>[coralogix-aws-shipper](https://github.com/coralogix/coralogix-aws-shipper)</code>, which is written in Rust. It is designed with a focus on optimizing memory safety and runtime performance. Rust improves control over system resources without sacrificing safety. As a result, your integration is more resource-efficient.</p> <p>We will show you how to complete our predefined Lambda function template to simplify the integration. Your task will be to provide specific configuration parameters, based on the service that you wish to connect. The reference list for these parameters is provided below.</p> <p>Note: As we improve <code>[coralogix-aws-shipper](https://github.com/coralogix/coralogix-aws-shipper)</code>, we invite you to contribute, ask questions, and report issues in the repository.</p>"},{"location":"newoutput/coralogix-aws-shipper/#supported-services","title":"Supported Services","text":"<p>Amazon S3, CloudTrail, VPC Flow Logs and more</p> <p>This integration is based on S3. Your Amazon S3 bucket can receive log files from all kinds of services, such as CloudTrail, VPC Flow Logs, Redshift, Network Firewall, or different types of load balancers (ALB/NLB/ELB). This data is then sent to Coralogix for analysis.</p> <p>You may also include SNS/SQS in the pipeline so that the integration triggers upon notification.</p> <p>Amazon CloudWatch</p> <p>Coralogix can be configured to directly receive data directly from your CloudWatch log group. In this case, the S3 is not used as an intermediary.</p> <p>Amazon Kinesis</p> <p>Coralogix can receive stream data from your AWS account. This option does not use S3 and lets you connect to services directly.</p>"},{"location":"newoutput/coralogix-aws-shipper/#amazon-msk-kafka","title":"Amazon MSK &amp; Kafka","text":"<p>Coralogix can be configured to receive data directly from your\u00a0MSK\u00a0or\u00a0Kafka\u00a0cluster.</p>"},{"location":"newoutput/coralogix-aws-shipper/#amazon-ecr-image-security-scan","title":"Amazon ECR Image Security Scan","text":"<p>Coralogix can be configured to receive ECR\u00a0Image Scanning.</p> <p>Note: Although <code>coralogix-aws-shipper</code> handles all of the AWS product integrations, some of the parameters are product-specific. Consult the Configuration Parameters for product-specific requirements.</p>"},{"location":"newoutput/coralogix-aws-shipper/#deployment-options","title":"Deployment Options","text":"<p>\u26a0\ufe0f Important: Before you get started, ensure that your AWS user has the permissions to create Lambda functions and IAM roles.</p>"},{"location":"newoutput/coralogix-aws-shipper/#integrate-using-the-coralogix-platform-recommended","title":"Integrate using the Coralogix Platform (Recommended)","text":"<p>The fastest way to deploy your predefined Lambda function is from within the Coralogix platform. Fill out an integration form and confirm the integration from your AWS account. Product integrations can be by navigating to Data Flow &gt; Integrations in your Coralogix toolbar. For detailed UI instructions, please read the Integration Packages tutorial.</p>"},{"location":"newoutput/coralogix-aws-shipper/#quick-create-a-cloudformation-stack","title":"Quick Create a CloudFormation Stack","text":"<p>You can always launch the CloudFormation stack by filling out a Quick Create template. This is done from within your AWS Management Console. Log into your AWS account and click the button below to deploy the CloudFormation stack.</p> <p></p> <p>If you use AWS CLI, you can use a CloudFormation template from our repository.</p>"},{"location":"newoutput/coralogix-aws-shipper/#deploy-the-aws-serverless-application","title":"Deploy the AWS Serverless Application","text":"<p>Alternatively, you may use the SAM deployment link. The procedure is very similar to filling out the above Quick Create template.</p>"},{"location":"newoutput/coralogix-aws-shipper/#terraform-module","title":"Terraform Module","text":"<p>If you are using Terraform to launch your infrastructure, you can access <code>[coralogix-aws-shipper](https://github.com/coralogix/coralogix-aws-shipper)</code> it via our Terraform Module. Use the parameters defined in the repository README, as they better reflect the configuration process.</p>"},{"location":"newoutput/coralogix-aws-shipper/#configuration-parameters","title":"Configuration Parameters","text":"<p>This document explains the basic config options for your template. You will need these values to launch your integration. For additional optional parameters, view our Advanced Configuration options.</p> <p>Use the tables below as a guide to properly configure your deployment. The provided configuration variables are for the Serverless or CloudFormation deployment options. The variable requirements are slightly different if you wish to deploy with Terraform. Please refer to the Terraform Module for further details.</p>"},{"location":"newoutput/coralogix-aws-shipper/#universal-configuration","title":"Universal Configuration","text":"<p>Use an existing Coralogix Send-Your-Data API key to make the connection or create one as you fill our pre-made template. Additionally, make sure your integration is region-specific.</p> <p>Note: You should always deploy the AWS Lambda function in the same AWS Region as your resource (e.g. the S3 bucket).</p> Parameter Description Default Value Required Application name This will also be the name of the CloudFormation stack that creates your integration. It can include letters (A\u2013Z and a\u2013z), numbers (0\u20139) and dashes (-). \u2714\ufe0f IntegrationType Choose the AWS service that you wish to integrate with Coralogix. Can be one of: S3, CloudTrail, VpcFlow, CloudWatch, S3Csv, SNS, SQS, Kinesis, CloudFront\u2019 S3 \u2714\ufe0f CoralogixRegion Your data source should be in the same region as the integration stack. You may choose from one of the default Coralogix regions: [Custom, EU1, EU2, AP1, AP2, US1, US2]. If this value is set to Custom you must specify the Custom Domain to use via the CustomDomain parameter. Custom \u2714\ufe0f CustomDomain If you choose a custom domain name for your private cluster, Coralogix will send telemetry from the specified address (e.g. custom.coralogixstg.wpengine.com). ApplicationName The name of the application for which the integration is configured. Advanced Configuration specifies dynamic value retrieval options. \u2714\ufe0f SubsystemName Specify the name of your subsystem. For a dynamic value, refer to the Advanced Configuration section. For CloudWatch, leave this field empty to use the log group name. ApiKey The Send-Your-Data API Key validates your authenticity. This value can be a direct Coralogix API Key or an AWS Secret Manager ARN containing the API Key. \u2714\ufe0f StoreAPIKeyInSecretsManager Enable this to store your API Key securely. Otherwise, it will remain exposed in plain text as an environment variable in the Lambda function console. True \u2714\ufe0f"},{"location":"newoutput/coralogix-aws-shipper/#s3cloudtrailvpcflows3csv-configuration","title":"S3/CloudTrail/VpcFlow/S3Csv Configuration","text":"<p>This is the most flexible type of integration, as it is based on receiving log files to Amazon S3. First, your bucket can receive log files from all kinds of other services, such as CloudTrail, VPC Flow Logs, Redshift, Network Firewall or different types of load balancers (ALB/NLB/ELB). Once the data is in the bucket, a pre-made Lambda function will then transmit it to your Coralogix account.</p> <p>Tip: The S3 integration supports generic data. You can ingest any generic text, JSON, and CSV data stored in your S3 bucket.</p> <p>Figure 1: Sending data directly from an S3 bucket. Your applications will deposit their logs and events in a specified S3 Bucket. Each S3 Bucket event will trigger a Lambda invocation, thus sending the data to your Coralogix Account.</p> <p></p>"},{"location":"newoutput/coralogix-aws-shipper/#adding-snssqs","title":"Adding SNS/SQS","text":"<p>If you don\u2019t want to send data directly as it enters S3, you can also use SNS/SQS to maintain notifications before any data is sent from your bucket to Coralogix. For this, you need to set the <code>SNSTopicArn</code> or <code>SQSTopicArn</code> parameters.</p> <p>Figure 2: First trigger via SNS, then send to an S3 bucket. Alternatively, you can configure a notification to invoke the Lambda. The SNS/SQS is triggered as your S3 Bucket event occurs.</p> <p></p> <p>Note: All resources, such as S3 or SNS/SQS, should be provisioned already. If you are using an S3 bucket as a resource, please make sure it is clear of any Lambda triggers located in the same AWS region as your new function.</p> Parameter Description Default Value Required S3BucketName Specify the name of the AWS S3 bucket that you want to monitor. \u2714\ufe0f S3KeyPrefix Specify the prefix of the log path within your S3 bucket. This value is ignored if you use the SNSTopicArn/SQSTopicArn parameter. CloudTrail/VpcFlow 'AWSLogs/' S3KeySuffix Filter for the suffix of the file path in your S3 bucket. This value is ignored if you use the SNSTopicArn/SQSTopicArn parameter. CloudTrail '.json.gz', VpcFlow '.log.gz' NewlinePattern Enter a regular expression to detect a new log line for multiline logs, e.g., \\n(?=\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2}.\\d{3}). SNSTopicArn The ARN for the SNS topic that contains the SNS subscription responsible for retrieving logs from Amazon S3. SQSTopicArn The ARN for the SQS queue that contains the SQS subscription responsible for retrieving logs from Amazon S3 CSVDelimiter Specify a single character to be used as a delimiter when ingesting a CSV file with a header line. This value is applicable when the S3Csv integration type is selected, for example, \",\" or \" \". ,"},{"location":"newoutput/coralogix-aws-shipper/#cloudwatch-configuration","title":"CloudWatch Configuration","text":"<p>Coralogix can be configured to receive data directly from your CloudWatch log group. This option does not use S3. You must provide the log group name as a parameter during setup.</p> <p>Figure 3: CloudWatch logs are streamed directly to Coralogix via Lambda.</p> <p></p> Parameter Description Default Value Required CloudWatchLogGroupName Provide a comma-separated list of CloudWatch log group names to monitor, for example, (log-group1, log-group2, log-group3). \u2714\ufe0f"},{"location":"newoutput/coralogix-aws-shipper/#sns-configuration","title":"SNS Configuration","text":"<p>To receive SNS messages directly to Coralogix, use the SNSIntegrationTopicARN parameter. This differs from the above use of <code>SNSTopicArn</code>, which notifies based on S3 events.</p> Parameter Description Default Value Required SNSIntegrationTopicARN Provide the ARN of the SNS topic to which you want to subscribe for retrieving messages. \u2714\ufe0f"},{"location":"newoutput/coralogix-aws-shipper/#sqs-configuration","title":"SQS Configuration","text":"<p>To receive SQS messages directly to Coralogix, use the <code>SQSIntegrationTopicARN</code> parameter. This differs from the above use of <code>SQSTopicArn</code>, which notifies based on S3 events.</p> Parameter Description Default Value Required SQSIntegrationTopicARN Provide the ARN of the SQS queue to which you want to subscribe for retrieving messages. \u2714\ufe0f"},{"location":"newoutput/coralogix-aws-shipper/#kinesis-configuration","title":"Kinesis Configuration","text":"<p>We can receive direct stream data from your AWS account. This option does not use S3. Your Kinesis stream ARN is a required parameter in this case.</p> <p>Figure 4: Streaming Kinesis data from AWS Services directly to Coralogix</p> <p></p> Parameter Description Default Value Required KinesisStreamARN Provide the ARN of the Kinesis Stream to which you want to subscribe for retrieving messages. \u2714\ufe0f"},{"location":"newoutput/coralogix-aws-shipper/#kafka-configuration","title":"Kafka Configuration","text":"Parameter Description Default Value Required KafkaBrokers Comma-delimited list of Kafka brokers to establish a connection with. \u2714\ufe0f KafkaTopic Subscribe to this Kafka topic for data consumption. \u2714\ufe0f KafkaBatchSize Specify the size of data batches to be read from Kafka during each retrieval. 100 KafkaSecurityGroups Comma-delimited list of Kafka security groups for secure connection setup. \u2714\ufe0f KafkaSubnets Comma-delimited list of Kafka subnets to use when connecting to Kafka. \u2714\ufe0f"},{"location":"newoutput/coralogix-aws-shipper/#msk-configuration","title":"MSK Configuration","text":"<p>Your Lambda function must be in a VPC that has access to the MSK cluster. You can configure your VPC via the provided\u00a0VPC configuration parameters.</p> Parameter Description Default Value Required MSKBrokers Comma-delimited list of MSK brokers to connect to. \u2714\ufe0f KafkaTopic Subscribe to this Kafka topic for data consumption. \u2714\ufe0f"},{"location":"newoutput/coralogix-aws-shipper/#generic-configuration-optional","title":"Generic Configuration (Optional)","text":"<p>These are optional parameters if you wish to 1) receive notification emails, 2) exclude certain logs or 3) send messages to Coralogix at a particular rate.</p> Parameter Description Default Value Required NotificationEmail A failure notification will be sent to this email address. BlockingPattern Enter a regular expression to identify lines excluded from being sent to Coralogix. For example, use MainActivity.java:\\d{3} to match log lines with MainActivity followed by exactly three digits. SamplingRate Send messages at a specific rate, such as 1 out of every N logs. For example, if your value is 10, a message will be sent for every 10th log. 1 \u2714\ufe0f"},{"location":"newoutput/coralogix-aws-shipper/#lambda-configuration-optional","title":"Lambda Configuration (Optional)","text":"<p>These are the default presets for Lambda. Read Troubleshooting for more information on changing these defaults.</p> Parameter Description Default Value Required FunctionMemorySize Specify the memory size for the Lambda function in megabytes. 1024 \u2714\ufe0f FunctionTimeout Set a timeout for the Lambda function in seconds. 300 \u2714\ufe0f LogLevel Specify the log level for the Lambda function, choosing from the following options: INFO, WARN, ERROR, DEBUG. WARN \u2714\ufe0f LambdaLogRetention Set the CloudWatch log retention period (in days) for logs generated by the Lambda function. 5 \u2714\ufe0f"},{"location":"newoutput/coralogix-aws-shipper/#vpc-configuration-optional","title":"VPC Configuration (Optional)","text":"<p>Use the following options if you need to configure a private link with Coralogix.</p> Parameter Description Default Value Required LambdaSubnetID Specify the ID of the subnet where the integration should be deployed. \u2714\ufe0f LambdaSecurityGroupID Specify the ID of the Security Group where the integration should be deployed. \u2714\ufe0f UsePrivateLink Set this to true if you will be using AWS PrivateLink. false \u2714\ufe0f"},{"location":"newoutput/coralogix-aws-shipper/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"newoutput/coralogix-aws-shipper/#aws-privatelink","title":"AWS PrivateLink","text":"<p>If you want to bypass using the public internet, you can use AWS PrivateLink to facilitate secure connections between your VPCs and AWS Services. This option is available under the VPC Configuration tab. To turn it on, either check off the Use Private Link box in the Coralogix UI or set the parameter to <code>true</code>. For additional instructions on AWS PrivateLink, please follow our dedicated tutorial.</p>"},{"location":"newoutput/coralogix-aws-shipper/#dynamic-values","title":"Dynamic Values","text":"<p>If you wish to use dynamic values for the Application and Subsystem Name parameters, consider the following:</p> <ul> <li> <p>JSON support: To reference dynamic values from the log, use\u00a0$.my_log.field. For the CloudTrail source, use\u00a0$.eventSource.</p> </li> <li> <p>S3 folder: Use the following tag: {{s3_key.value}} where the value is the folder level. For example, if the file path that triggers the event is AWSLogs/112322232/ELB1/elb.log or AWSLogs/112322232/ELB2/elb.log and you want ELB1 and ELB2 to be the subsystem, your subsystemName should be {{s3_key.3}}</p> </li> </ul>"},{"location":"newoutput/coralogix-aws-shipper/#troubleshooting","title":"Troubleshooting","text":"<p>Timeout errors</p> <p>If you see \"Task timed out after\", you need to increase the Lambda Timeout value. You can do this from the AWS Lambda function settings under Configuration &gt; General Configuration.</p> <p>Not enough memory</p> <p>If you see \"Task out of memory\", you should increase the Lambda maximum Memory value. In the AWS Lambda function settings, go to Configuration &gt; General Configuration.</p> <p>Verbose logs</p> <p>To add more verbosity to your function logs, set RUST_LOG to DEBUG.</p> <p>\u26a0\ufe0f Warning: Remember to change it back to INFO after troubleshooting.</p> <p>Changing defaults</p> <ul> <li> <p>Set the MAX_ELAPSED_TIME variable for default change (default = 250).</p> </li> <li> <p>Set BATCHES_MAX_SIZE (in MB) sets batch max size before sending to Coralogix. This value is limited by the max payload accepted by the Coralogix endpoint (default = 4).</p> </li> <li> <p>Set BATCHES_MAX_CONCURRENCY sets the maximum amount of concurrent batches that can be sent.</p> </li> </ul>"},{"location":"newoutput/coralogix-aws-shipper/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/coralogix-cli/","title":"Coralogix CLI","text":"<p>Coralogix is on a mission to provide a CLI that will enable you to do all your Coralogix operations without the web interface involved. Currently, we support:</p> <ol> <li> <p>Quota management to help you manage quota across your teams.</p> </li> <li> <p>SAML management allows the management of SAML SSO\u00a0configuration by admin users.</p> </li> <li> <p>Team management allows admins to create teams and invite users.</p> </li> <li> <p>LiveTail (a live log stream from all your services). You can filter your logs by Coralogix metadata fields such as application name, subsystem name, and severity and also by querying the data itself. Live tail is much like how the stern command in K8s streams logs in real-time.\u00a0</p> </li> </ol>"},{"location":"newoutput/coralogix-cli/#install-the-coralogix-cli-for-macoslinux","title":"Install the Coralogix CLI for MacOS/Linux","text":""},{"location":"newoutput/coralogix-cli/#download","title":"Download:","text":"<pre><code>curl -O https://coralogix-public.s3-eu-west-1.amazonaws.com/cxctl/latest/cxctl-macOS.gz\n</code></pre> <p>Or</p> <pre><code>curl -O https://coralogix-public.s3-eu-west-1.amazonaws.com/cxctl/latest/cxctl-Linux.gz\n</code></pre>"},{"location":"newoutput/coralogix-cli/#unzip","title":"Unzip","text":"<pre><code>gunzip -N cxctl-*.gz\n</code></pre>"},{"location":"newoutput/coralogix-cli/#make-executable","title":"Make Executable:","text":"<pre><code>chmod +x cxctl\n</code></pre>"},{"location":"newoutput/coralogix-cli/#set-environment-variables","title":"Set Environment Variables:","text":"<p>Required for Live Tail:</p> <pre><code>export CORALOGIX_API_KEY=YOUR_LOGS_API_KEY\n</code></pre> <p>The API key to use can be found under Data Flow / API Keys / Logs Query Key:</p> <p></p> <p></p>"},{"location":"newoutput/coralogix-cli/#scan-option","title":"Scan Option:","text":"<p>[table id=36 /]</p>"},{"location":"newoutput/coralogix-cli/#scan-examples","title":"Scan Examples","text":"<p>[table id=37 /]</p>"},{"location":"newoutput/coralogix-cli/#histogram","title":"Histogram:","text":"<p>Prints a time-based log count histogram.</p> <p>[table id=36 /]</p>"},{"location":"newoutput/coralogix-cli/#log-streaming","title":"Log Streaming","text":"<p>In order to stream the logs in real-time (as they are being archived) use the following command:</p> <pre><code>./cxctl livetail --region \"region-name\" --api-key &lt;api-key&gt; --format pretty --application \"app-name\" --subsystem \"subsystem-name\"\n</code></pre> <p>A Coralogix API key can be passed to the scanner through the `--api-key` argument or as an environment variable</p> <p>Streaming supports the following regions:</p> RegionUS1EU1EU1AP1AP2<code>region-name</code><code>us</code><code>eu</code><code>se</code><code>in</code><code>sg</code> <p>By default, the logs use the `pretty` format. A `raw` format is also available which prints out the logs as JSON objects</p> <p>The scanner will tail the logs infinitely by default but can stop after a specified number of log lines using --count</p>"},{"location":"newoutput/coralogix-cli/#livetail-option","title":"LiveTail Option:","text":"<p>[table id=38 /]</p>"},{"location":"newoutput/coralogix-domain/","title":"Coralogix Domain","text":"<p>Coralogix offers independent domains around the globe allowing you to maintain compliance with local data storage requirements. The domain where your account is hosted will determine the endpoint for your integrations, API configurations, and more.</p>"},{"location":"newoutput/coralogix-domain/#domains","title":"Domains","text":"<p>The following section will provide you with the region and domain for your Coralogix account determined by your Coralogix Account URL.</p> Coralogix Domain Coralogix Region AWS Region Team Hostname coralogix.us US1 us-east-2 (Ohio) .app.coralogix.us cx498.coralogixstg.wpengine.com US2 us-west-2 (Oregon) .app.cx498.coralogixstg.wpengine.com coralogixstg.wpengine.com EU1 eu-west-1 (Ireland) .coralogixstg.wpengine.com eu2.coralogixstg.wpengine.com EU2 eu-north-1 (Stockholm) .app.eu2.coralogixstg.wpengine.com coralogix.in AP1 (IN) ap-south-1 (Mumbai) .app.coralogix.in coralogixsg.com AP2 (SG) ap-southeast-1 (Singapore) .app.coralogixsg.com"},{"location":"newoutput/coralogix-domain/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email to\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/coralogix-endpoints/","title":"Coralogix Endpoints","text":"<p>Coralogix offers a regional endpoint for sending data from all observability sources into the Coralogix platform.</p> <p>This document provides generally available endpoints. AWS PrivateLink endpoints can be found here.</p> <p>Notes:</p> <ul> <li> <p>The tables below provide the most up-to-date endpoints provided by Coralogix. Other endpoints utilized in Coralogix integrations remain available.</p> </li> <li> <p>Find out more about your Coralogix domain here.</p> </li> </ul>"},{"location":"newoutput/coralogix-endpoints/#endpoints","title":"Endpoints","text":""},{"location":"newoutput/coralogix-endpoints/#opentelemetry","title":"OpenTelemetry","text":"Coralogix DomainCoralogix AWS RegionEndpointcoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]ingress.coralogixstg.wpengine.com:443coralogix.inap-south1[AP1 - India]ingress.coralogix.in:443coralogix.usus-east2[US1 - Ohio]ingress.coralogix.us:443eu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]ingress.eu2.coralogixstg.wpengine.com:443coralogixsg.comap-southeast-1[AP2 - Singapore]ingress.coralogixsg.com:443cx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]ingress.cx498-aws-us-west-2.coralogixstg.wpengine.com:443"},{"location":"newoutput/coralogix-endpoints/#coralogix-logs","title":"Coralogix Logs","text":"Coralogix DomainCoralogix AWS RegionEndpointcoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]https://ingress.coralogixstg.wpengine.com/logs/v1/singlescoralogix.inap-south1[AP1 - India]https://ingress.coralogix.in/logs/v1/singlescoralogix.usus-east2[US1 - Ohio]https://ingress.coralogix.us/logs/v1/singleseu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]https://ingress.eu2.coralogixstg.wpengine.com/logs/v1/singlescoralogixsg.comap-southeast-1[AP2 - Singapore]https://ingress.coralogixsg.com/logs/v1/singlescx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]https://ingress.cx498-aws-us-west-2.coralogixstg.wpengine.com/logs/v1/singles"},{"location":"newoutput/coralogix-endpoints/#coralogix-rest-api-bulk","title":"Coralogix REST API Bulk","text":"Coralogix DomainCoralogix AWS RegionEndpointcoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]https://ingress.coralogixstg.wpengine.com/logs/v1/bulkcoralogix.inap-south1[AP1 - India]https://ingress.coralogix.in/logs/v1/bulkcoralogix.usus-east2[US1 - Ohio]https://ingress.coralogix.us/logs/v1/bulkeu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]https://ingress.eu2.coralogixstg.wpengine.com/logs/v1/bulkcoralogixsg.comap-southeast-1[AP2 - Singapore]https://ingress.coralogixsg.com/logs/v1/bulkcx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]https://ingress.cx498.coralogixstg.wpengine.com/logs/v1/bulk"},{"location":"newoutput/coralogix-endpoints/#coralogix-rest-api-singles","title":"Coralogix REST API Singles","text":"Coralogix DomainCoralogix AWS RegionEndpointcoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]https://ingress.coralogixstg.wpengine.com/logs/v1/singlescoralogix.inap-south1[AP1 - India]https://ingress.coralogix.in/logs/v1/singlescoralogix.usus-east2[US1 - Ohio]https://ingress.coralogix.us/logs/v1/singleseu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]https://ingress.eu2.coralogixstg.wpengine.com/logs/v1/singlescoralogixsg.comap-southeast-1[AP2 - Singapore]https://ingress.coralogixsg.com/logs/v1/singlescx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]https://ingress.cx498.coralogixstg.wpengine.com/logs/v1/singles"},{"location":"newoutput/coralogix-endpoints/#prometheus-remotewrite","title":"Prometheus RemoteWrite","text":"Coralogix DomainCoralogix AWS RegionEndpointcoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]https://ingress.coralogixstg.wpengine.com/prometheus/v1coralogix.inap-south1[AP1 - India]https://ingress.coralogix.in/prometheus/v1coralogix.usus-east2[US1 - Ohio]https://ingress.coralogix.us/prometheus/v1eu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]https://ingress.eu2.coralogixstg.wpengine.com/prometheus/v1coralogixsg.comap-southeast-1[AP2 - Singapore]https://ingress.coralogixsg.com/prometheus/v1cx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]https://ingress.cx498-aws-us-west-2.coralogixstg.wpengine.com/prometheus/v1"},{"location":"newoutput/coralogix-endpoints/#tls-tcp-syslog","title":"TLS / TCP Syslog","text":"Coralogix DomainCoralogix AWS RegionEndpointcoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]syslog.coralogixstg.wpengine.com:6514coralogix.inap-south1[AP1 - India]syslog.coralogix.in:6514coralogix.usus-east2[US1 - Ohio]syslog.coralogix.us:6514eu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]syslog.eu2.coralogixstg.wpengine.com:6514coralogixsg.comap-southeast-1[AP2 - Singapore]syslog.coralogixsg.com:6514cx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]syslog.cx498.coralogix.com:6514"},{"location":"newoutput/coralogix-endpoints/#management","title":"Management","text":"Coralogix DomainCoralogix AWS RegionEndpointcoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]https://ng-api-http.coralogixstg.wpengine.com/api/v1/dataprime/querycoralogix.inap-south1[AP1 - India]https://ng-api-http.app.coralogix.in/api/v1/dataprime/querycoralogix.usus-east2[US1 - Ohio]https://ng-api-http.coralogix.us/api/v1/dataprime/queryeu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]https://ng-api-http.eu2.coralogixstg.wpengine.com/api/v1/dataprime/querycoralogixsg.comap-southeast-1[AP2 - Singapore]https://ng-api-http.coralogixsg.com/api/v1/dataprime/querycx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]https://ng-api-http.cx498.coralogixstg.wpengine.com/api/v1/dataprime/query"},{"location":"newoutput/coralogix-endpoints/#external-alerts","title":"External Alerts","text":"Coralogix DomainCoralogix AWS RegionEndpointcoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]https://api.coralogixstg.wpengine.com/api/v1/external/alertscoralogix.inap-south1[AP1 - India]https://api.coralogix.in/api/v1/external/alertscoralogix.usus-east2[US1 - Ohio]https://api.coralogix.us/api/v1/external/alertseu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]https://api.eu2.coralogixstg.wpengine.com/api/v1/external/alertscoralogixsg.comap-southeast-1[AP2 - Singapore]https://api.coralogixsg.com/api/v1/external/alertscx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]https://api.cx498.coralogixstg.wpengine.com/api/v1/external/alerts"},{"location":"newoutput/coralogix-endpoints/#firehose","title":"Firehose","text":"Coralogix DomainCoralogix AWS RegionEndpointcoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]https://firehose-ingress.coralogixstg.wpengine.com/firehosecoralogix.inap-south1[AP1 - India]https://firehose-ingress.coralogix.in/firehosecoralogix.usus-east2[US1 - Ohio]https://firehose-ingress.coralogix.us/firehoseeu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]https://firehose-ingress.eu2.coralogixstg.wpengine.com/firehosecoralogixsg.comap-southeast-1[AP2 - Singapore]https://firehose-ingress.coralogixsg.com/firehosecx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]https://firehose-ingress.cx498.coralogixstg.wpengine.com/firehose"},{"location":"newoutput/coralogix-endpoints/#grpc","title":"gRPC","text":"Coralogix DomainCoralogix AWS RegionEndpointcoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]https://ng-api-grpc.coralogixstg.wpengine.comcoralogix.inap-south1[AP1 - India]https://ng-api-grpc.app.coralogix.incoralogix.usus-east2[US1 - Ohio]https://ng-api-grpc.coralogix.useu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]https://ng-api-grpc.eu2.coralogixstg.wpengine.comcoralogixsg.comap-southeast-1[AP2 - Singapore]https://ng-api-grpc.coralogixsg.comcx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]https://ng-api-grpc.cx498.coralogixstg.wpengine.com"},{"location":"newoutput/coralogix-endpoints/#scim","title":"SCIM","text":"Coralogix DomainCoralogix AWS RegionEndpointcoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]https://ng-api-http.coralogixstg.wpengine.com/scimcoralogix.inap-south1[AP1 - India]https://ng-api-http.app.coralogix.in/scimcoralogix.usus-east2[US1 - Ohio]https://ng-api-http.coralogix.us/scimeu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]https://ng-api-http.eu2.coralogixstg.wpengine.com/scimcoralogixsg.comap-southeast-1[AP2 - Singapore]https://ng-api-http.coralogixsg.com/scimcx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]https://ng-api-http.cx498.coralogixstg.wpengine.com/scim"},{"location":"newoutput/coralogix-endpoints/#opensearch","title":"OpenSearch","text":"Coralogix DomainCoralogix AWS RegionEndpointcoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]https://coralogix-esapi.coralogixstg.wpengine.com:9443/*coralogix.inap-south1[AP1 - India]https://es-api.app.coralogix.in:9443/*coralogix.usus-east2[US1 - Ohio]https://esapi.coralogix.us:9443/*eu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]https://es-api.eu2.coralogixstg.wpengine.com:9443/*coralogixsg.comap-southeast-1[AP2 - Singapore]https://es-api.coralogixsg.com:9443/*cx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]https://es-api.cx498.coralogixstg.wpengine.com:9443/*"},{"location":"newoutput/coralogix-endpoints/#metrics-cardinality","title":"Metrics Cardinality","text":"Coralogix DomainCoralogix AWS RegionEndpointcoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]https://ng-api-http.coralogixstg.wpengine.com/api/metrics/cardinalitycoralogix.inap-south1[AP1 - India]https://ng-api-http.coralogix.in/api/metrics/cardinalitycoralogix.usus-east2[US1 - Ohio]https://ng-api-http.coralogix.us/api/metrics/cardinalityeu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]https://ng-api-http.eu2.coralogixstg.wpengine.com/api/metrics/cardinalitycoralogixsg.comap-southeast-1[AP2 - Singapore]https://ng-api-http.coralogixsg.com/api/metrics/cardinalitycx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]https://ng-api-http.cx498.coralogixstg.wpengine.com/api/metrics/cardinality"},{"location":"newoutput/coralogix-endpoints/#loadbalancer-stats-ips","title":"LoadBalancer Stats IPs","text":"Coralogix DomainCoralogix AWS RegionLoadBalancerStats IPscoralogixstg.wpengine.comeu-west-1[EU1 - Ireland]34.253.20.018.203.53.23152.212.0.182coralogix.inap-south1[AP1 - India]3.109.88.813.127.95.18443.204.151.45coralogix.usus-east2[US1 - Ohio]18.119.84.1983.22.174.33.139.112.94eu2.coralogixstg.wpengine.comeu-north-1[EU2 - Stockholm]13.53.114.4116.170.111.13113.50.102.213coralogixsg.comap-southeast-1[AP2 - Singapore]52.77.57.11552.221.106.20718.142.194.181cx498.coralogixstg.wpengine.comus-west-2[US2 - Oregon]44.228.187.149"},{"location":"newoutput/coralogix-endpoints/#limits-quotas","title":"Limits &amp; Quotas","text":"<p>Coralogix places the following limits on endpoints:</p> <ul> <li> <p>A hard limit of 10MB of data to our OpenTelemetry endpoint, with a recommendation of 2MB</p> </li> <li> <p>A hard limit of 2411724 bytes of data to our Prometheus RemoteWrite endpoint, with a recommendation for any amount less than this limit</p> </li> </ul> <p>Limits apply to single requests, regardless of timespan.</p>"},{"location":"newoutput/coralogix-endpoints/#additional-resources","title":"Additional Resources","text":"GitHubCoralogix Telemetry ShippersCoralogix OpenTelemetry ExporterDocsCoralogix DomainCoralogix Lambda Telemetry"},{"location":"newoutput/coralogix-endpoints/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/coralogix-entities/","title":"Coralogix Entities","text":"<p>Coralogix entities are divided into Organizations, Teams, &amp; Groups.</p>"},{"location":"newoutput/coralogix-entities/#overview-concepts","title":"Overview &amp; Concepts","text":""},{"location":"newoutput/coralogix-entities/#organization","title":"Organization","text":"<p>An\u00a0organization\u00a0is a unit associated with a single Coralogix customer managed by an Organization Administrator. Multiple domains may be treated as part of one organization. Organizations are divided into working teams, which group users based on their project, department, or other relevant criteria.</p> <ul> <li> <p>Create an Organization</p> </li> <li> <p>Organization Domains</p> </li> <li> <p>Organization Quotas</p> </li> <li> <p>Organization Admins</p> </li> </ul>"},{"location":"newoutput/coralogix-entities/#teams","title":"Teams","text":"<p>Teams are used to group users from one organization together based on their project, department, or any other relevant criteria. An organization may have multiple teams, each consisting of multiple users. Each team is a platform environment that has a unique URL and settings. All account management is executed on the team level.</p> <p>Team members may belong to one or more groups through which role and data scope-based access is assigned. For example, you can have a Development Team with users from the Engineering, Operations, and Support groups.</p> <p>When an organization is set up, the Org Admin is prompted to create a team to engage with the Coralogix platform. Subsequent users join existing teams or create new ones.</p> <ul> <li> <p>Create a Team</p> </li> <li> <p>Manage Existing Teams</p> </li> <li> <p>SSO Login</p> </li> <li> <p>Session Length</p> </li> <li> <p>Quota Manager</p> </li> <li> <p>Archive Retention Policy</p> </li> </ul>"},{"location":"newoutput/coralogix-entities/#groups","title":"Groups","text":"<p>Coralogix users enjoy role and data scope-based access based on their group membership, each assigned one or more\u00a0roles. This association allows you to\u00a0grant permissions and manage access\u00a0granularly, aligning with your organization\u2019s structure and requirements.</p> <p>Only Organization and Platform administrators have the predefined permissions to manage Groups.</p> <ul> <li>Assign User Roles via Groups</li> </ul>"},{"location":"newoutput/coralogix-extensions-for-aws-lambda/","title":"Coralogix Extensions for Amazon Web Services (AWS) Lambda","text":"<p>Coralogix provides an easy way to collect your Cloud watch metrics. The preferred and easiest integration method will be to use our AWS\u00a0Serverless Application Repository. Our code is open source and you can see it on\u00a0Github</p>"},{"location":"newoutput/coralogix-extensions-for-aws-lambda/#requirements","title":"Requirements","text":"<ul> <li>Your AWS user should have permissions to create lambdas and IAM roles.</li> </ul>"},{"location":"newoutput/coralogix-extensions-for-aws-lambda/#installation-lambda-layer","title":"Installation - Lambda layer","text":"<ul> <li> <p>Navigate to\u00a0the Application Page.</p> </li> <li> <p>Based on your lambda write the runtime in 'CompatibleRuntimes'</p> </li> <li> <p>Based on your lambda architecture put 'true' in either 'AMD64SupportEnabled' or 'ARM64SupportEnabled' and 'false' in the other</p> </li> <li> <p>Click Deploy.</p> </li> </ul>"},{"location":"newoutput/coralogix-extensions-for-aws-lambda/#installation-container-image-lambda","title":"Installation - Container image lambda","text":"<p>In case you deploy your lambda as container image, to inject extension as part of your function just copy it to your image:</p> <pre><code>FROM coralogixrepo/coralogix-lambda-extension:latest AS coralogix-extension\nFROM public.ecr.aws/lambda/python:3.8\n# Layer code\nWORKDIR /opt\nCOPY --from=coralogix-extension /opt/ .\n# Function code\nWORKDIR /var/task\nCOPY app.py .\nCMD [\"app.lambda_handler\"]\n</code></pre> <p>Note: in this example, python3.8 is being used as the runtime but all runtimes are supported with our lambda extension.</p>"},{"location":"newoutput/coralogix-extensions-for-aws-lambda/#usage","title":"Usage","text":"<p>Once the deployment is done the Layer is ready and you can add it to any Lambda you wish to collect logs from. This is how:</p> <ol> <li> <p>Go to the Lambda functions from which you want to send logs to Coralogix, choose the 'Layers' component, and click 'Add Layer'</p> </li> <li> <p>Select Custom Layers, choose the coralogix-extension layer that matches your lambda architecture, select the latest version and click 'Add'</p> </li> <li> <p>Go to the environment variables section in your Lambda function and add the following ENV variables:</p> <ol> <li> <p>CORALOGIX_PRIVATE_KEY - The Send-Your-Data API key, a unique ID that represents your company.</p> </li> <li> <p>CORALOGIX_APP_NAME - A mandatory metadata field that is sent with the logs. Helps identify the logs sent by this Lambda function.</p> </li> <li> <p>CORALOGIX_SUB_SYSTEM - A mandatory metadata field sent with the logs. Helps identify the logs sent by this Lambda function.</p> </li> <li> <p>CORALOGIX_LOG_URL (Optional) - use https:///api/v1/logs     Using the table below find the appropriate 'Cluster URL' ."},{"location":"newoutput/coralogix-extensions-for-aws-lambda/#coralogix-endpoints","title":"Coralogix Endpoints     Region Logs Endpoint     EU <code>api.coralogixstg.wpengine.com</code>   EU2 <code>api.eu2.coralogixstg.wpengine.com</code>   US <code>api.coralogix.us</code>   SG <code>api.coralogixsg.com</code>   IN <code>api.app.coralogix.in</code>    <p>You're all set! Data will begin streaming to Coralogix.</p> <p>Need any help with your integration? ping us on our in-app chat or at support@coralogixstg.wpengine.com.</p>","text":""},{"location":"newoutput/coralogix-features-tour/","title":"Coralogix Features Tour","text":"<p>The Coralogix monitoring platform ingests data from any digital source and transforms it using our core features, unleashed with our out-of-the-box extension packages. Take advantage of these evolving features to fully understand your system, analyze changes in its behavior, and respond to incidents before they become problems.</p> <p></p>"},{"location":"newoutput/coralogix-features-tour/#core-features","title":"Core Features","text":"<p>Here is a list of our core features.</p> <ul> <li> <p>Data Collection and Centralized Storage. Coralogix provides integrations with popular logging frameworks and libraries, enabling you to easily transfer your data to Coralogix for centralized storage and analysis. With our unique Streama\u00a9 technology, you can analyze your data without needing to index it, thereby avoiding costly storage expenses.</p> </li> <li> <p>Data transformation and parsing. Coralogix automatically parses and structures your telemetry data.</p> <ul> <li> <p>Parse your data to make it easier to search, filter, and analyze. Get started with our Parsing Rules Cheat Sheet.</p> </li> <li> <p>Enrich your data with business, operations, or security information, including IP-based geographical information, that may not be available at runtime. AWS customers can enrich their logs with tags from their AWS EC2 instances to connect their business and operation metadata, and gain greater insight into their data.</p> </li> <li> <p>Record new metric time series to produce leaner and more quickly queried metrics.</p> </li> </ul> </li> <li> <p>Data query. Extract specific information or insights from your collected logs, metrics, and traces using Lucene, SQL, or our advanced\u00a0DataPrime\u00a0query language. We make it possible for you to directly query your archived data at a speed 5x faster than Athena.</p> </li> <li> <p>Alerting. By querying logs and metrics, you can set up alerts and notifications based on specific conditions or thresholds. These alerts can notify you when predefined events occur or when metrics deviate from normal behavior.</p> </li> <li> <p>Monitoring. Monitor your data using our advanced Explore screen, where you can drill down into your logs and traces. Engage with our Events2Metrics\u00a0functionality to generate metrics from your\u00a0spans\u00a0and\u00a0logs\u00a0to optimize storage without sacrificing important data. What\u2019s more, we offer application performance monitoring (APM)\u00a0which decorates our standard observability options with additional information.</p> </li> </ul> <p></p> <ul> <li>Visualizations. Create unlimited, personalized\u00a0custom dashboards catered to your specific observability needs, or take advantage of our pre-built dashboards to help you analyze and visualize log data. We also offer Grafana and Kubernetes dashboards.</li> </ul> <p></p> <ul> <li>Compliance and security. Coralogix prioritizes data security and compliance. It offers encryption at rest and in transit, access controls, and adheres to industry-standard security practices. The platform helps businesses meet compliance requirements, such as GDPR and HIPAA, by providing features like data anonymization and audit logs.</li> </ul> <p>Our features, along with unparalleled customer support and excellent cost optimization, make us one of the few observability providers that can help you grow, optimize, and save money at the same time. To get started, click here.</p>"},{"location":"newoutput/coralogix-logstash-integration/","title":"Logstash","text":"<p>Coralogix provides seamless integration with Logstash, so you can send your logs from anywhere and parse them according to your needs.</p>"},{"location":"newoutput/coralogix-logstash-integration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Logstash installed</li> </ul>"},{"location":"newoutput/coralogix-logstash-integration/#best-practices","title":"Best Practices","text":"<p>We recommend using the generic http output plugin with this integration, given its high level of configurability and metric support for monitoring output.</p>"},{"location":"newoutput/coralogix-logstash-integration/#installation","title":"Installation","text":"<p>STEP 1. Share the Ruby code snippet depicting the event structure as it flows through Logstash.</p> <ul> <li> <p>[Optional] Use this opportunity to set dynamic application and subsystem fields.</p> </li> <li> <p>The example below adopts a JSON structure and has these fields: application, subsystem and host.</p> </li> </ul> <pre><code>filter {\n  ruby {code =&gt; \"\n                event.set('[@metadata][application]', event.get('application'))\n                event.set('[@metadata][subsystem]', event.get('subsystem'))\n                event.set('[@metadata][event]', event.to_json)\n                event.set('[@metadata][host]', event.get('host'))\n                \"}\n}\n</code></pre> <ul> <li>If you prefer that the fields application, subsystem and host remain static, replace the <code>event.get</code> with a plain string, as in the example below.</li> </ul> <pre><code>filter {\n  ruby {code =&gt; \"\n                event.set('[@metadata][application]', MyApplicationName)\n                event.set('[@metadata][subsystem]', MySubsystemName)\n                event.set('[@metadata][event]', event.to_json)\n                event.set('[@metadata][host]', event.get('host'))\n                \"}\n}\n</code></pre> <p>STEP 2. Once the Event is ready, configure the output itself to send the logs.</p> <ul> <li> <p>Input your Send-Your-Data API key.</p> </li> <li> <p>Select the Coralogix REST API Singles endpoint associated with your Coralogix domain.</p> </li> </ul> <pre><code>output {\n    http {\n        url =&gt; \"&lt;Coralogix REST API singles endpoint&gt;\"\n        http_method =&gt; \"post\"\n        headers =&gt; [\"authorization\", \"Bearer &lt;Coralogix Send-Your-Data API key&gt;\"]\n        format =&gt; \"json_batch\"\n        codec =&gt; \"json\"\n        mapping =&gt; {\n            \"applicationName\" =&gt; \"%{[@metadata][application]}\"\n            \"subsystemName\" =&gt; \"%{[@metadata][subsystem]}\"\n            \"computerName\" =&gt; \"%{[@metadata][host]}\"\n            \"text\" =&gt; \"%{[@metadata][event]}\"\n        }\n        http_compression =&gt; true\n        automatic_retries =&gt; 5\n        retry_non_idempotent =&gt; true\n        connect_timeout =&gt; 30\n        keepalive =&gt; false\n        }\n}\n</code></pre>"},{"location":"newoutput/coralogix-logstash-integration/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/coralogix-operator/","title":"Coralogix Operator","text":"<p>The Coralogix Operator is an open-source Kubernetes Operator that automates configuration and management of Coralogix APIs in a Kubernetes environment.</p>"},{"location":"newoutput/coralogix-operator/#overview","title":"Overview","text":"<p>The Coralogix Operator is built upon a Kubernetes Operator pattern, enabling you to define the desired state of your Coralogix account using declarative configuration files named custom resource definitions (CRDs). It leverages the Kubernetes reconcile loop to ensure this desired state.</p> <p>Note: The Coralogix Operator is in alpha, with the current version being unstable.</p>"},{"location":"newoutput/coralogix-operator/#management-of-functionalities","title":"Management of Functionalities","text":"<p>The Operator manages the following Coralogix functionalities:</p> <ul> <li> <p>alerting</p> </li> <li> <p>parsing rules</p> </li> <li> <p>recording rules</p> </li> </ul> <p>A full list of rule configuration samples can be found\u00a0here.</p>"},{"location":"newoutput/coralogix-operator/#leveraging-prometheusrules","title":"Leveraging PrometheusRules","text":"<p>For those customers using Prometheus Operator, the Operator can also be used to manage the CRD PrometheusRules. The Operator leverages existing PrometheusRules to manage Coralogix\u00a0recording and\u00a0alerting rules, ensuring the smoothest possible integration.</p>"},{"location":"newoutput/coralogix-operator/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster</li> </ul>"},{"location":"newoutput/coralogix-operator/#installation","title":"Installation","text":"<p>The following steps demonstrate how to get started. Full instructions can be found here.</p> <p>STEP 1. Add our Helm charts repository to the local repos list. This command will create a repository named\u00a0<code>coralogix</code>.</p> <pre><code>helm repo add coralogix https://cgx.jfrog.io/artifactory/coralogix-charts-virtual\nhelm repo update\n\n</code></pre> <p>STEP 2. Update the Helm values with your Alerts, Rules and Tags API Key and the region associated with your Coralogix domain, as in the example configuration below:</p> <pre><code>coralogixOperator:\n  region: \"EUROPE1\"\n  prometheusRules:\n    enabled: false\n\nsecret:\n  data:\n    apiKey: &lt;YOUR API KEY HERE&gt;\n</code></pre> <p>Notes:</p> <ul> <li> <p><code>apiKey</code>: Access your Alerts, Rules and Tags API Key by navigating to Data Flow &gt; API Keys in your Coralogix toolbar.</p> </li> <li> <p><code>region</code>: Should be formatted as follows - APAC1, APAC2, EUROPE1, EUROPE2, USA1, USA2, STG.</p> </li> <li> <p><code>prometheusRules</code>: Must be set to false.</p> </li> </ul> <p>STEP 3. Run the following command to install the helm chart on your cluster:</p> <pre><code>helm install &lt;my-release&gt; coralogix/coralogix-operator\n\n</code></pre> <p>STEP 3. To uninstall the helm chart, run the following:</p> <pre><code>helm delete &lt;my-release&gt;\n\n</code></pre>"},{"location":"newoutput/coralogix-operator/#manage-your-coralogix-account-using-crds","title":"Manage Your Coralogix Account Using CRDs","text":"<p>Once the Coralogix Operator is installed and running on your cluster, manage your Coralogix account using CRDs. The following section provides samples of available features.</p>"},{"location":"newoutput/coralogix-operator/#alerting-management","title":"Alerting Management","text":"<p>The Coralogix Operator provides a feature to manage alerts using the Alert CRD. Here is a sample CRD definition using PromQL.</p> <pre><code>apiVersion: coralogixstg.wpengine.com/v1alpha1\nkind: Alert\nmetadata:\n  name: promql-alert-example\nspec:\n  name: promql alert example\n  description: alert from k8s operator\n  severity: Critical\n  notificationGroups:\n    - notifications:\n        - notifyOn: TriggeredOnly\n          integrationName: WebhookAlerts\n          retriggeringPeriodMinutes: 1\n        - notifyOn: TriggeredAndResolved\n          emailRecipients: [ \"example@coralogixstg.wpengine.com\" ]\n          retriggeringPeriodMinutes: 1440\n    - groupByFields: [ \"coralogix.metadata.sdkId\" ]\n      notifications:\n        - notifyOn: TriggeredOnly\n          integrationID: 2235\n          retriggeringPeriodMinutes: 1\n        - notifyOn: TriggeredAndResolved\n          emailRecipients: [ \"example@coralogixstg.wpengine.com\" ]\n          retriggeringPeriodMinutes: 1440\n  scheduling:\n    daysEnabled: [\"Wednesday\", \"Thursday\"]\n    timeZone: UTC+02\n    startTime: 08:30\n    endTime: 20:30\n  alertType:\n    metric:\n      promql:\n        searchQuery: http_requests_total{status!~\\\\\"4..\\\\\"}\n        conditions:\n          alertWhen: More\n          threshold: 3\n          sampleThresholdPercentage: 50\n          timeWindow: TwelveHours\n          minNonNullValuesPercentage: 10\n\n</code></pre> <p>Save this file as <code>prometheusalert.yaml</code> and then apply the following alert:</p> <pre><code> kubectl apply -f prometheusalert.yaml\n\n</code></pre> <p>Once Coralogix Operator syncs the alert, you will be able to view it by navigating to Alerts in your Coralogix UI.</p> <p></p> <p></p> <p>Find more information about the Alert CRD API definition in our Coralogix Operator API documentation.</p>"},{"location":"newoutput/coralogix-operator/#recording-rules-management","title":"Recording Rules Management","text":"<p>The Coralogix Operator provides a feature to manage recording rules using the RecordingRuleGroupSet CRD. Here is a sample CRD definition using PromQL.</p> <pre><code>apiVersion: coralogixstg.wpengine.com/v1alpha1\nkind: RecordingRuleGroupSet\nmetadata:\n  name: kube-state-metrics\nspec:\n  groups:\n    - name: kube-state-metrics\n      intervalSeconds: 60\n      rules:\n        - expr: kube_pod_info * on(pod, node) group_left(resource) sum by (pod, node, resource)(kube_pod_container_resource_requests{resource=\"memory\"})\n          record: workload_pod_resources_memory:kube_pod_container_resource_requests:join\n\n</code></pre> <p>Find more information about the Alert CRD API definition in our Coralogix Operator API documentation.</p>"},{"location":"newoutput/coralogix-operator/#parsing-rules-management","title":"Parsing Rules Management","text":"<p>The Coralogix Operator provides a feature to manage parsing rules using the RuleGroup CRD. Here is a sample CRD definition using parsing log fields.</p> <pre><code>apiVersion: coralogixstg.wpengine.com/v1alpha1\nkind: RuleGroup\nmetadata:\n  name: parsing-rule\nspec:\n  name: parsing-rule\n  description: rule-group from k8s operator\n  applications: [\"application-name\"]\n  subsystems: [\"subsystems-name\"]\n  severities: [\"Warning\", \"Info\"]\n  subgroups:\n    - rules:\n        - name: HttpRequestParser2\n          description: Parse the fields of the HTTP request - will be applied after HttpRequestParser1\n          parse:\n            sourceField: text\n            destinationField: text\n            regex: (?P&lt;remote_addr&gt;\\\\\\\\d{1,3}.\\\\\\\\d{1,3}.\\\\\\\\d{1,3}.\\\\\\\\d{1,3})\\\\\\\\s*-\\\\\\\\s*(?P&lt;user&gt;[^ ]+)\\\\\\\\s*\\\\\\\\[(?P&lt;timestemp&gt;\\\\\\\\d{4}-\\\\\\\\d{2}\\\\\\\\-\\\\\\\\d{2}T\\\\\\\\d{2}\\\\\\\\:\\\\\\\\d{2}\\\\\\\\:\\\\\\\\d{2}\\\\\\\\.\\\\\\\\d{1,6}Z)\\\\\\\\]\\\\\\\\s*\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"(?P&lt;method&gt;[A-z]+)\\\\\\\\s[\\\\\\\\/\\\\\\\\\\\\\\\\]+(?P&lt;request&gt;[^\\\\\\\\s]+)\\\\\\\\s*(?P&lt;protocol&gt;[A-z0-9\\\\\\\\/\\\\\\\\.]+)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"\\\\\\\\s*(?P&lt;status&gt;\\\\\\\\d+)\\\\\\\\s*(?P&lt;body_bytes_sent&gt;\\\\\\\\d+)?\\\\\\\\s*?\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"(?P&lt;http_referer&gt;[^\\\\\"]+)\\\\\\\\\\\\\"\\\\\\\\s*\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"(?P&lt;http_user_agent&gt;[^\\\\\"]+)\\\\\\\\\\\\\"\\\\\\\\s(?P&lt;request_time&gt;\\\\\\\\d{1,6})\\\\\\\\s*(?P&lt;response_time&gt;\\\\\\\\d{1,6})\n\n</code></pre>"},{"location":"newoutput/coralogix-operator/#leveraging-prometheusrules_1","title":"Leveraging PrometheusRules","text":"<p>For those customers using Prometheus Operator, the Operator can also be used to manage the CRD PrometheusRules. Samples of available features can be found below.</p>"},{"location":"newoutput/coralogix-operator/#recording-rules-management-with-prometheusrules","title":"Recording Rules Management with PrometheusRules","text":"<p>Leverage existing PrometheusRules to manage Coralogix recording rules with the addition of the label <code>app.coralogixstg.wpengine.com/track-recording-rules: \"true\"</code> to PrometheusRules, as in the example below.</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  labels:\n    ## Coralogix label to indicate to the Coralogix Operator\n    ## Create Recording Rules on Coralogix using this PrometheusRule\n    app.coralogixstg.wpengine.com/track-recording-rules: \"true\"\n  name: kube-state-metrics\n  namespace: observability\nspec:\n  groups:\n    - name: kube-state-metrics\n      rules:\n        - expr: kube_pod_info * on(pod, node) group_left(resource) sum by (pod, node, resource)(kube_pod_container_resource_requests{resource=\"memory\"})\n          labels:\n            namespace: observability\n          record: workload_pod_resources_memory:kube_pod_container_resource_requests:join\n\n</code></pre>"},{"location":"newoutput/coralogix-operator/#alerting-management-with-prometheusrules","title":"Alerting Management with PrometheusRules","text":"<p>Leverage existing PrometheusRules to manage Coralogix alerts with the addition of the label <code>app.coralogixstg.wpengine.com/track-alerting-rules: \"true\"</code> to PrometheusRules, as in the example below.</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  labels:\n    ## Coralogix label to indicate to the Coralogix Operator\n    ## Create Recording Rules on Coralogix using this PrometheusRule\n    app.coralogixstg.wpengine.com/track-alerting-rules: \"true\"\n  name: kube-state-metrics\n  namespace: observability\nspec:\n  groups:\n    - name: kube-state-metrics\n      rules:\n        - alert: PodFrequentlyRestarting\n          annotations:\n            description: &gt;-\n              Pod {{$labels.pod}} in namespace: observability was restarted more\n              than 3 times\n            summary: Pod is restarting frequently\n          expr: &gt;-\n            sum(increase(kube_pod_container_status_restarts_total{container=~\"kube-state-metrics\",\n            job=\"kube-state-metrics\", pod=~\"kube-state-metrics-.*\"}[5m])) by\n            (pod, namespace) &gt; 3\n          labels:\n            namespace: observability\n            severity: critical\n\n</code></pre>"},{"location":"newoutput/coralogix-operator/#additional-resources","title":"Additional Resources","text":"GitHubCoralogix Operator GitHub RepositoryCoralogix Operator APIExternalKubebuilderBlobIntroduction to Kubernetes Observability"},{"location":"newoutput/coralogix-operator/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/coralogix-rabbitmq-agent/","title":"RabbitMQ Metrics","text":"<p>With tens of thousands of users, RabbitMQ is one of the most popular open source message brokers. RabbitMQ is used worldwide at small startups and large enterprises.</p> <p>Getting Metrics from RabbitMQ Management API allow you to monitor and track your brokers performance.</p> <p>To Achieve this we will use Telegraf to fetch, and ship RabbitMQ Metrics into Coralogix.</p>"},{"location":"newoutput/coralogix-rabbitmq-agent/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Telegraf installed</p> </li> <li> <p>Coralogix account with metric bucket</p> </li> </ul>"},{"location":"newoutput/coralogix-rabbitmq-agent/#configuration","title":"Configuration","text":"<p>To verify you have a working Telegraf install please run:</p> <pre><code># telegraf --version\nTelegraf 1.23.4 (git: HEAD 5b48f5da)\n</code></pre> <p>Now let's modify the configuration to pickup RabbitMQ Metrics and send them to Coralogix.</p> <p>Edit telegraf configuration file, normally located in <code>/etc/telegraf/telegraf.conf</code></p> <p>Here is a sample config:</p> <pre><code>[[inputs.rabbitmq]]\n  ## Management Plugin url. (default: http://localhost:15672)\n   url = \"\"\n  ## Tag added to rabbitmq_overview series; deprecated: use tags\n   name = \"\"\n  ## Credentials\n   username = \"\"\n   password = \"\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain &amp; host verification\n  # insecure_skip_verify = false\n\n  ## Optional request timeouts\n  ##\n  ## ResponseHeaderTimeout, if non-zero, specifies the amount of time to wait\n  ## for a server's response headers after fully writing the request.\n  # header_timeout = \"3s\"\n  ##\n  ## client_timeout specifies a time limit for requests made by this client.\n  ## Includes connection time, any redirects, and reading the response body.\n  # client_timeout = \"4s\"\n\n  ## A list of nodes to gather as the rabbitmq_node measurement. If not\n  ## specified, metrics for all nodes are gathered.\n  # nodes = [\"rabbit@node1\", \"rabbit@node2\"]\n\n  ## A list of queues to gather as the rabbitmq_queue measurement. If not\n  ## specified, metrics for all queues are gathered.\n  ## Deprecated in 1.6: Use queue_name_include instead.\n  # queues = [\"telegraf\"]\n\n  ## A list of exchanges to gather as the rabbitmq_exchange measurement. If not\n  ## specified, metrics for all exchanges are gathered.\n  # exchanges = [\"telegraf\"]\n\n  ## Metrics to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all metrics\n  ## Currently the following metrics are supported: \"exchange\", \"federation\", \"node\", \"overview\", \"queue\"\n  # metric_include = []\n  # metric_exclude = []\n\n  ## Queues to include and exclude. Globs accepted.\n  ## Note that an empty array for both will include all queues\n  # queue_name_include = []\n  # queue_name_exclude = []\n\n  ## Federation upstreams to include and exclude specified as an array of glob\n  ## pattern strings.  Federation links can also be limited by the queue and\n  ## exchange filters.\n  # federation_upstream_include = []\n  # federation_upstream_exclude = []\n\n[[outputs.opentelemetry]]\n   service_address = \"&lt;yourclusterURL&gt;:443\"\n   insecure_skip_verify = true\n   compression = \"gzip\"\n   [outputs.opentelemetry.coralogix]\n     private_key = \"&lt;private_key&gt;\"\n     application = \"&lt;application&gt;\"\n     subsystem = \"&lt;subsystem&gt;\"\n\n\n</code></pre> <p>The fields needed to be filled are:</p> <ul> <li> <p>url - Coralogix OpenTelemetry endpoint</p> </li> <li> <p>name - Tag added to rabbitmq_overview series; deprecated: use tags</p> </li> <li> <p>username - RabbitMQ AdminUI username</p> </li> <li> <p>password - RabbitMQ AdminUI password</p> </li> <li> <p>private_key- Coralogix Send-Your-Data API key</p> </li> <li> <p>application- Application name, which will be added to your metric attributes</p> </li> <li> <p>subsystem- Subsystem name, which will be added to your metric attributes</p> </li> <li> <p>service_address- Cluster endpoint location</p> </li> </ul>"},{"location":"newoutput/coralogix-rabbitmq-agent/#additional-resources","title":"Additional Resources","text":"DocumentationRabbitMQ Plugin"},{"location":"newoutput/coralogix-rabbitmq-agent/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/coralogix-rest-api/","title":"Coralogix REST API","text":"<p>Send us your logs using our Rest API <code>/singles</code> or <code>/bulk</code> endpoint.</p>"},{"location":"newoutput/coralogix-rest-api/#documentation","title":"Documentation","text":"<ul> <li> <p>Documentation for Rest API <code>/singles</code> endpoint</p> </li> <li> <p>Documentation for Rest API <code>/bulk</code> endpoint</p> </li> </ul>"},{"location":"newoutput/coralogix-rest-api/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/coralogix-sta-kubernetes-context-enrichment/","title":"Kubernetes Context Enrichment w/Coralogix STA","text":"<p>If there are Kubernetes nodes among the instances that are mirrored to the STA, the STA can automatically enrich source and destination IPs in its events with information about the pod or node that generated this traffic.</p> <p>To get this to work, follow these simple steps:</p>"},{"location":"newoutput/coralogix-sta-kubernetes-context-enrichment/#self-hosted-kubernetes-non-eks","title":"Self-hosted Kubernetes (Non - EKS)","text":"<ol> <li> <p>During the STA's installation set an S3 bucket to store the configuration (Using the <code>ConfigS3Bucket</code> CloudFormation/Terraform parameter)</p> </li> <li> <p>Create a kube config file (that allows at least to run <code>kubectl get nodes</code> and <code>kubectl get pods</code>) and upload it to that S3 bucket at the root of the bucket under the name <code>kube.config</code></p> </li> </ol>"},{"location":"newoutput/coralogix-sta-kubernetes-context-enrichment/#aws-eks","title":"AWS EKS","text":"<ul> <li> <p>During the STA's installation set an empty S3 bucket to store the configuration (Using the <code>ConfigS3Bucket</code> CloudFormation/Terraform parameter)</p> </li> <li> <p>Open AWS EC2 console and click the STA instance, navigate to the Security tab and click the STA's IAM role</p> </li> <li> <p>Copy the role's ARN to clipboard or to a text file</p> </li> <li> <p>Click the policy named \"cgx_sta\", click on \"Add additional permissions\", click on \"Choose a service\" and select \"STS\"</p> </li> <li> <p>In the \"Actions\" section, click the tiny triangle next to \"Write\" and select \"AssumeRole\"</p> </li> <li> <p>In the \"Resources\" section, click the tiny triangle next to \"Resources\" and click \"Add ARN\" and at the top text box under \"Specify ARN for role\" replace all the text with the ARN you copied at step 3 and click \"Add\", click \"Review policy\" and then on \"Save changes\"</p> </li> <li> <p>Connect to your AWS environment via CLI and run the following command:</p> </li> <li> <p>Use this command on your EKS management instance to generate the kube config file:</p> </li> <li> <p><code>aws --profile &lt;aws_profile&gt; eks update-kubeconfig --name &lt;cluster_name&gt; --region &lt;aws_region&gt;</code></p> </li> <li> <p><code>kubectl edit configmap aws-auth -n kube-system</code></p> </li> <li> <p>Under <code>mapRoles:</code> add the following content (for rolearn paste the ARN you copied at step 3 and for username copy and paste the last part after the slash):</p> </li> </ul> <pre><code>    - rolearn: arn:aws:iam::123456789012:role/test-sta-spot-k8s-eks-CoralogixSTASpotsManagerRole-4ECRD39DWTKT\n      username: test-sta-spot-k8s-eks-CoralogixSTASpotsManagerRole-4ECRD39DWTKT\n      groups:\n      - system:masters\n</code></pre> <ul> <li> <p>Save and quit the editor</p> </li> <li> <p>Upload the <code>~/.kube/config</code> file to the S3 bucket chosen to hold the configuration under the name <code>kube.config</code></p> </li> </ul>"},{"location":"newoutput/coralogix-sta-kubernetes-context-enrichment/#azure-aks","title":"AZURE AKS","text":"<p>during the STA's installation, you provided several arguments representing Azure's storage variables.</p> <p>in case AKS and STA are using same virtual network and subnet:</p> <ul> <li>Upload the <code>~/.kube/config</code> file to the Storage container chosen to hold the configuration under the name <code>kube.config</code> - thats it!</li> </ul>"},{"location":"newoutput/coralogix-sta-kubernetes-context-enrichment/#final-steps","title":"FINAL STEPS","text":"<ul> <li> <p>The following steps are essentially optional and will be automatically carried out by the system but carrying them out now will force applying the new settings immediately:</p> <ul> <li> <p>Log in to the STA via SSH and run the following commands:</p> <ul> <li> <p><code>sta-force-sync-configs</code> (if it returns with a message saying it has collided with one of the STA's core services, wait for three minutes and try again)</p> </li> <li> <p><code>sta-restart-enrichment-k8s-context</code></p> </li> </ul> </li> </ul> </li> <li> <p>You should now see that events from the STA are now enriched with on of the fields <code>source_ip_k8sinfo</code> or <code>destination_ip_k8sinfo</code> or both:</p> </li> </ul> <p></p>"},{"location":"newoutput/coralogix-sta-virtual-tap/","title":"Coralogix STA Virtual Tap","text":"<p>The Coralogix STA is capable of detecting security-related issues in network traffic and host activities (with Wazuh integration). Until now it could receive the mirrored traffic in only two ways: AWS VPC Traffic Mirroring and physical network taps or SPAN ports.</p> <p>We are excited to inform you that we have launched our own virtual tap which can be used to mirror traffic from instances which are not supported for AWS VPC Traffic Mirroring, from containerized environments or from environments in which it won't make economic sense to use AWS VPC Traffic Mirroring.</p> <p>The Coralogix STA Virtual Tap can run as a container (either as a Docker container or as a Kubernetes pod) and can both mirror the instance's traffic by using eBPF as well as function as a Wazuh agent and collect host based information and handle tasks defined by the Wazuh manager's configuration.</p>"},{"location":"newoutput/coralogix-sta-virtual-tap/#kubernetes-installation","title":"Kubernetes Installation","text":"<p>To install the Coralogix STA Virtual Tap copy the following file to a file:</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: coralogix-vtap-wazuh\n  namespace: default\n  labels:\n    k8s-app: coralogix-sta-ng\nspec:\n  selector:\n      matchLabels:\n        kubernetes.io/os: linux\n  template:\n    metadata:\n      labels:\n        kubernetes.io/os: linux\n    spec:\n      hostNetwork: true\n      containers:\n      - name: coralogix-vtap-wazuh\n        image: coralogixrepo/sta-virtual-tap-docker\n        securityContext:\n          privileged: true\n        env:\n        - name: STA_SNIFFING_FILTER\n          value: \"not dst port 4789\"\n        - name: STA_SNIFFING_DEST\n          value: \"&lt;REMOTE_STORAGE_CONFIGURATION|RELEVANT_INTERNAL_IP&gt;\"\n        - name: STA_DISABLE_WAZUH\n          value: \"&lt;TRUE|FALSE&gt;\"\n        - name: WAZUH_MANAGER\n          value: \"&lt;WAZUH_MANAGER_NLB_DNS_NAME&gt;\"\n        - name: TAP_INTERFACE\n          value: \"&lt;eth0(commonly_used_CHANGE_IF_DIFFERENT)&gt;\"\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n### Uncomment when using Wazuh\n#      volumeMounts:\n#        - name: rootfs\n#          mountPath: /rootfs\n      terminationGracePeriodSeconds: 30\n#      volumes:\n#      - name: rootfs\n#        hostPath:\n#          path: /\n\n</code></pre> <p>Pay attention that the line <code>terminationGracePeriodSeconds: 30</code> needs to stay uncommented and is required</p> <p>When using Wazuh via Daemonset, uncomment the lines that map the node content into the pod and edit the <code>agent.conf</code> file located on the STA's configuration bucket under <code>**wazuh-clients &gt; default &gt; agent.conf**</code> to match the new change - add the <code>/rootfs</code> prefix to all the paths in the files</p> <p></p> <p>The values for the STA_SNIFFING_DEST (if connected to NLB) and WAZUH_MANAGER environment variables should be retrieved from the \"Load Balancers\" in the EC2 console in AWS:</p> <p></p>"},{"location":"newoutput/coralogix-sta-virtual-tap/#docker-installation","title":"Docker installation","text":"<pre><code>docker run -d --name sta-virtual-tap \\\n-e \"STA_SNIFFING_DEST=&lt;REMOTE_STORAGE_CONFIGURATION|RELEVANT_INTERNAL_IP&gt;\" \\\n-e \"TAP_INTERFACE=&lt;TAP_INTERFACE&gt;\" \\\n-e \"STA_SNIFFING_FILTER=not dst port 4789\" \\\n-v /:/rootfs \\\n--privileged --net host coralogixrepo/sta-virtual-tap-docker\n</code></pre>"},{"location":"newoutput/coralogix-sta-virtual-tap/#notes","title":"Notes","text":"<ul> <li> <p>In case you don't want to install <code>Wazuh</code>, make sure to add environment variable <code>STA_DISABLE_WAZUH</code> and set it to <code>TRUE</code></p> </li> <li> <p>If the STA is deployed on AWS, the mirroring process will attempt to connect directly to the STA using its IP (extracted from the bucket's configuration). All traffic is mirrored at NO COST, but the Virtual Tap's process itself might require your machine to have more resources.     In case there was an issue with connecting directly to the STA, it will cascade the connection to the STA's NLB - this will apply mirroring costs, and can be reduced via defining mirroring filter.     To validate how the mirroring is handled you can check from within your machine where the Virtual Tap in installed by running the following command (You can determine what connection was established by looking at the output):  </p> </li> </ul> <pre><code>docker logs &lt;VIRTUAL_TAP_CONTAINER_ID&gt;\n</code></pre> <ul> <li>Additional steps are required if Azure is used as a cloud provider, and the STA is using different resource group or virtual network than your other mirrored machines - peering between those networks is required - please read this article for more information.</li> </ul> <p>To function properly, the instance hosting this docker should have an IAM role attached to it (or the AWS credentials provided to default profile) with the following permissions:</p> <ol> <li> <p>S3:GetObject</p> </li> <li> <p>S3:ListBucket</p> </li> </ol>"},{"location":"newoutput/coralogix-sta-virtual-tap/#advanced-installation","title":"Advanced Installation","text":"<p>It is possible to control many aspects of the behavior of the Coralogix STA Virtual Tap by setting the following environment variables (in addition to STA_SNIFFING_NLB and WAZUH_MANAGER mentioned above):</p> <p>STA_DISABLE_TAP - Can be set to \"TRUE\" to disable the tap feature of the Coralogix STA Virtual Tap, essentially making it work just as a Wazuh agent. By default this value is set to \"FALSE\"</p> <p>STA_DISABLE_WAZUH - Can be set to \"TRUE\" to disable the Wazuh agent STA_DISABLE_TAP - Can be set to \"TRUE\" to disable the tap feature of the Coralogix STA Virtual Tap, essentially making it work just as a virtual tap. By default this value is set to \"FALSE\".</p> <p>TAP_INTERFACE - Can be set to a specific network interface that its traffic you wish to mirror to the STA. By default this value is set to \"any\" which will mirror traffic from all of the instance's network interface. It's recommended to set a specific name depending on the machine's network interface.</p> <p>STA_SNIFFING_DEST - Indicates which STA cluster the mirrored traffic will be sent to. you can specify this by the following options:</p> <ol> <li> <p>Remote storage (AWS S3 bucket, Azure storage, etc) - will need relevant read permissions. (seamlessly will read from storage and make the relevant connection).</p> </li> <li> <p>Internal IP of dedicated STA instance's NIC VXLAN sniffer.</p> </li> <li> <p>Internal IP of the STA's NLB - on AWS this option is not recommended as you pay on the mirrored traffic.</p> </li> </ol> <p>STA_SNIFFING_FILTER - Can be set to a BPF filter that will determine which traffic will be mirrored to the STA. By default this value is empty which will mirror all types of traffic seen.</p> <p>WAZUH_GROUP - The Wazuh agent group to use for the built in agent in the Coralogix STA Virtual Tap. By default this value is set to \"default\".</p>"},{"location":"newoutput/coralogix-sta-virtual-tap/#post-installation","title":"Post-Installation","text":"<p>After installing this container on your environment, the STA should start receiving traffic and Wazuh events from your environment and display them on the Coralogix UI and, if needed, alert you upon security related issues found.</p>"},{"location":"newoutput/coralogix-sta-virtual-tap/#vpc-traffic-mirroring-vs-coralogix-sta-virtual-tap","title":"VPC Traffic Mirroring vs. Coralogix STA Virtual Tap","text":"<p>Although it is possible to install Coralogix STA Virtual Tap instead of using the AWS VPC Traffic Mirroring, we recommend that you will consider the following notes:</p> <ol> <li> <p>VPC Traffic Mirroring is handled by the AWS virtualization hypervisor which means it cannot impact the performance of the EC2 instances. A virtual tap, runs inside your EC2 instance (as a container) and therefore, can potentially have some effect on the instance's performance.</p> </li> <li> <p>VPC Traffic Mirroring is handled by the AWS virtualization hypervisor which means it cannot be detected and evaded by a potential attacker that has a foothold in that EC2 instance. A virtual tap, runs inside your EC2 instance (as a container) and therefore, can potentially be evaded or tampered with.</p> </li> <li> <p>AWS charges for every traffic mirroring session a certain fee (in addition to what they charge for the traffic that is being mirrored). The virtual tap doesn't incur such charges.</p> </li> </ol>"},{"location":"newoutput/coralogix-sta-virtual-tap/#sample-values-for-sta_sniffing_filter","title":"Sample values for STA_SNIFFING_FILTER","text":"<p>The Virtual Tap allows you to specify an eBPF filter for limiting the amount of traffic that will be mirrored to the STA. Here are some examples of what you can set it to (by default it will mirror everything):</p> <ul> <li> <p>The essential filter: <code>portrange 1-7</code>9 or icmp     This filter will mirror the most important protocols that generate the lowest amounts of traffic (in our view). That means you'll get the best value for money while trading off the ability to get the full details of what went on your network.</p> </li> <li> <p>The moderate filter: <code>icmp or (not portrange 443-445 and not port 80)</code>     This filter will exclude from mirroring protocols that tend to be noisy in most environments. It is important that you'll not exclude protocols that are potentially hazardous for your instances.</p> </li> </ul> <p>You can consult this document for more information: https://biot.com/capstats/bpf.html For more information about selecting the best mirroring strategy: https://coralogixstg.wpengine.com/docs/aws-traffic-mirroring/</p>"},{"location":"newoutput/coralogix-sta-vs-others/","title":"Coralogix STA vs. Others","text":"<p>A successful cloud and container-native security posture is the goal that organizations are striving towards but achieving it can seem lofty when faced with mounting complexities and a lack of expert security resources. Setting up a proactive monitoring solution is daunting in and of itself and creating one without causing analyst fatigue and avoiding prohibitive data costs can make it seem nearly impossible.</p> <p>Security logs like Cloudtrail, VPC Flow logs, GuardDuty, and Auditbeat without network packet data, are not enough to paint the full picture of what\u2019s being transmitted, and by whom, allowing attackers to circumvent safeguards, as well as not having the information needed to perform deep enough investigations. However, setting up, processing, and storing packet data can be laborious and cost-prohibitive.</p> <p>The Coralogix Cloud Security solution brings visibility and threat insights to SOC and DevOps teams within minutes, instead of months. In order to deliver reliable alerts with actionable context, Coralogix uniquely correlates contextual log data and combines it with network packet data.</p> <p>Here\u2019s a full comparison between the STA and all the other methods in AWS:</p> <p></p> <p>(1) Will be added soon in upcoming versions (2) Since CloudTrail logs are basically an auditing mechanism for AWS, they provide a different type of context data that is also very valuable for forensic investigations (3) Allows the user to create new rules (4) The VPC Firewall contains some protocol analyzers for analyzing the packets passed through it but they are not as extensive and extensible as the Zeek platform embedded in the STA. The Zeek platform contains protocol analyzers for hundreds of different protocols (5) Can detect threats at the perimeter only and only based on its suricata\u2019s signatures list (6) The offered metrics are very basic like dropped/passed packets. The user can add one dimension to the metrics called \u201cCustomAction\u201d. See more here: https://docs.aws.amazon.com/network-firewall/latest/developerguide/ monitoring-cloudwatch.html (7) The VPC firewall can only be installed at the perimeter level, as an inline device, which prevents it from being exposed to the inter-vpc traffic</p>"},{"location":"newoutput/coralogix-team-id/","title":"Team ID","text":"<p>This section shows how to access your Coralogix Team ID, also referred to as your Coralogix Company ID.</p>"},{"location":"newoutput/coralogix-team-id/#accessing-your-coralogix-team-id","title":"Accessing your Coralogix Team ID","text":"<p>1. In your Coralogix dashboard, click on your personal account settings in the upper right-hand corner of the toolbar &gt; Settings.</p> <p></p> <p>2. In the left-hand sidebar, select Send Your Data. Your Team Name and Company ID (also known as Team ID) will appear in the upper right-hand corner of your dashboard.</p> <p></p>"},{"location":"newoutput/coralogix-user-defined-alerts/","title":"Standard User-Defined Alerts","text":"<p>Use the Coralogix Standard Alerts feature to monitor system performance, get notified when there are changes to your logs, and instantly pinpoint potential causes.</p>"},{"location":"newoutput/coralogix-user-defined-alerts/#feature","title":"Feature","text":"<p>Standard alerts are alerts that are triggered by changes to your logs. These alerts are useful when trying to measure the number of occurrences of a particular incident - when a user arrives at your website, for example, or when an error occurs.</p> <p>With the Standard Alerts feature, you can do the following:</p> <ul> <li> <p>Monitor system performance in real-time. Obtain real-time insights based on the criteria of your choice.</p> </li> <li> <p>Construct the perfect query for your specific needs. Define a query that will capture the logs you wish to inspect, and make it more specific when you filter by application, subsystem, and severity. Then, select the range of conditions to trigger an alert. For example, you may set that more than 10 logs over minutes will trigger your alert.</p> </li> <li> <p>Enjoy a machine learning-based approach. Using this setting, the Coralogix platform profiles your data and automatically detects abnormal behavior.</p> </li> <li> <p>Receive personalized notifications. Receive real-time push notifications to your preferred communication channel.</p> </li> </ul> <p>Standard alerts are the simplest alerts that Coralogix offers. Their simplicity is their strength. We strongly recommend that you cover your most obvious use cases with standard alerting to build a strong foundation in your observability system.</p>"},{"location":"newoutput/coralogix-user-defined-alerts/#create-an-alert","title":"Create an Alert","text":"<p>STEP 1. Perform a query to filter the logs that will be returned as part of the alert.</p> <ul> <li> <p>Click on Logs in the navigation pane.</p> </li> <li> <p>Perform a query to filter the logs that will be returned as part of the alert. Use a combination of the query input or the filters on the left-hand panel.</p> </li> <li> <p>Click CREATE ALERT in the upper right-hand corner.</p> </li> </ul> <p>To create an alert without a predefined query, click on Alerts &gt; Alert Management in the Coralogix toolbar. Click NEW ALERT on the upper right-hand corner of your dashboard.</p>"},{"location":"newoutput/coralogix-user-defined-alerts/#alert-details","title":"Alert Details","text":"<p>STEP 2. Define Alert Details.</p> <ul> <li> <p>Define:</p> <ul> <li> <p>Alert Name.</p> </li> <li> <p>Description.</p> </li> <li> <p>Severity. Choose from one of four options: info, warning, error, critical.</p> </li> <li> <p>Labels. Define a new label or choose from an existing one. Nest a label using <code>key:value</code>.</p> </li> </ul> </li> </ul>"},{"location":"newoutput/coralogix-user-defined-alerts/#alert-type","title":"Alert Type","text":"<p>STEP 3. Select STANDARD Alert Type.</p>"},{"location":"newoutput/coralogix-user-defined-alerts/#query","title":"Query","text":"<p>STEP 4. Define Query.</p> <ul> <li> <p>If you created an alert from your Logs screen, your query will appear. Click EDIT to modify.</p> </li> <li> <p>If you created an alert from your Alerts screen, input a new query. Using the available RegEx cheat sheet for support.</p> </li> <li> <p>Filter by Application, Subsystem and Severity.</p> </li> </ul>"},{"location":"newoutput/coralogix-user-defined-alerts/#conditions","title":"Conditions","text":"<p>STEP 5. Set the Conditions for triggering an alert.</p> <p></p>"},{"location":"newoutput/coralogix-user-defined-alerts/#alert-when","title":"Alert When","text":"<p>Select whether to trigger the alert immediately, or define a rule based on the number of occurrences within a specified time window or using our Dynamic Alerts anomaly detecting option.</p> <ul> <li> <p>Notify Immediately: Immediate notification followed by silence for 1 minute. Hit count will present 1 in immediate alert, as we notify you of the first log that matches.</p> </li> <li> <p>More/Less Than:\u00a0Alert will trigger when the count of the entries matching the alert definition will be more/less than the chosen threshold. Hit count will present the actual number of entries that matched within the selected time window.</p> </li> <li> <p>More Than Usual.\u00a0This Dynamic Alerts setting enables you to detect abnormal behavior automatically \u2013 without setting fixed threshold values. Set minimum threshold only. Note: It takes one week for algorithms to learn the traffic pattern</p> </li> </ul>"},{"location":"newoutput/coralogix-user-defined-alerts/#evaluation-window","title":"Evaluation Window","text":"<p>The Evaluation Window is the period of time that is periodically queried for the alert query and parameters. When the alert is set to More than for a Standard Alert, you can select the Evaluation Window type from the following options:</p> <ul> <li> <p>Rolling Window. This is the default setting for new alerts. The Rolling Window is a fixed period of time (i.e. 10 minutes) and does not change, regardless of matching data and any alerts triggered as a result of the query.</p> </li> <li> <p>Dynamic Duration. The Dynamic Duration evaluation window changes the queried period when data matching the query triggers an alert.</p> </li> </ul> <p></p>"},{"location":"newoutput/coralogix-user-defined-alerts/#group-by","title":"Group By","text":"<p>Group your alerts using one or more aggregated values into a histogram.</p> <ul> <li> <p>An alert is triggered whenever the condition threshold is met for a specific aggregated value within the specified timeframe.</p> </li> <li> <p>New! If using 2 values for Group By, matching logs will first be aggregated by the parent field (ie. region), then by the child field (ie. pod_name). An alert will fire when the threshold meets the unique combination of both parent and child. Only logs that include the Group By fields will be included in the count.</p> </li> </ul>"},{"location":"newoutput/coralogix-user-defined-alerts/#notifications","title":"Notifications","text":"<p>STEP 6. Define Notification settings.</p> <p>In the notification settings, you have different options, depending on whether or not you are using the Group By condition.</p>"},{"location":"newoutput/coralogix-user-defined-alerts/#with-group-by","title":"With Group By","text":"<p>When using Group By conditions, you will see the following options:</p> <ul> <li> <p>Trigger a single alert when at least one combination of the group by values meets the condition. A single notification, aggregating all values matching an alert query and conditions, will be sent to your Coralogix Incidents screen.</p> </li> <li> <p>Trigger a separate alert for each combination that meets the condition. Multiple individual notifications for each Group By field value may be sent to your Coralogix Incidents screen when query conditions are met. Select one or more Keys - consisting of a subset of the fields selected in the alert conditions - in the drop-down menu. A separate notification will be sent for each Key selected.</p> </li> <li> <p>The number of Group By permutations is limited to 1000. If there are more permutations, then only the first 1000 are tracked.</p> </li> </ul>"},{"location":"newoutput/coralogix-user-defined-alerts/#without-group-by","title":"Without Group By","text":"<p>When not using the Group By condition, a single alert will be triggered and sent to your Incidents Screen when the query meets the condition.</p> <p>You can define additional alert recipient(s) and notification channels in both cases by clicking + ADD WEBHOOK. Once you add a webhook, you can choose the parameters of your notification:</p> <ul> <li> <p>Notify Every. Sets the alert cadence. After an alert is triggered and a notification is sent, the alert will continue to work, but notifications will be suppressed for the duration of the suppression period.</p> </li> <li> <p>Notify when resolved. Activate to receive an automatic update once an alert has ceased.</p> </li> </ul>"},{"location":"newoutput/coralogix-user-defined-alerts/#advanced-notifications","title":"Advanced Notifications","text":"<p>Once you add a webhook to the notification group, a toggle appears which enables you to move to Advanced Mode. Advanced mode enables you to set the notify every &amp; notify when resolved settings for each webhook individually. Note that the toggle affects all notification groups, and when you turn the toggle on in one notification group, it will be turned on in all notification groups.</p>"},{"location":"newoutput/coralogix-user-defined-alerts/#schedule","title":"Schedule","text":"<p>STEP 7. Set a Schedule.</p> <p>Limit triggering to specific days and times.</p>"},{"location":"newoutput/coralogix-user-defined-alerts/#notification-content","title":"Notification Content","text":"<p>STEP 8. Define Notification Content.</p> <ul> <li> <p>Choose a specific JSON key or keys to include in the alert notification.</p> </li> <li> <p>Leave blank to view the full log text.</p> </li> </ul> <p></p>"},{"location":"newoutput/coralogix-user-defined-alerts/#verify-alert","title":"Verify Alert","text":"<p>STEP 9. Verify your alert.</p> <p>Click VERIFY to view how many times the alert matched the criteria in the last 24 hours.</p> <p></p> <p>STEP 10. View your History.</p> <p>View which user performed a change in the alert and when.</p> <p>STEP 11. Create your alert.</p> <p>Click CREATE ALERT on the upper-right side of the screen.</p>"},{"location":"newoutput/coralogix-user-defined-alerts/#view-alerts","title":"View Alerts","text":"<p>Navigate to Alerts &gt; Incidents from you Coralogix toolbar.</p> <p>Bundled as incidents, triggered alert events are presented in your\u00a0Incidents Screen\u00a0according to the\u00a0Group By tags\u00a0and notification settings\u00a0set in your alert definition.</p> <p>Manage, edit, and snooze alerts. Find out more here.</p>"},{"location":"newoutput/coralogix-user-defined-alerts/#limitations","title":"Limitations","text":"<p>Standard Alerts do not support arrays in queries.</p>"},{"location":"newoutput/coralogix-user-defined-alerts/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/core-web-vitals/","title":"Core Web Vitals","text":"<p>Explore the transformative world of Core Web Vitals \u2014 a pivotal Coralogix feature designed to elevate your web pages' performance and user experience. Discover how these essential metrics, including loading speed, interactivity, and visual stability, can be optimized to ensure your audience's seamless and responsive online environment.</p> <p></p>"},{"location":"newoutput/core-web-vitals/#overview","title":"Overview","text":"<p>Google's Core Web Vitals\u00a0is a set of metrics that measure real-world user experience for loading performance, interactivity, and visual stability of the page. We highly recommend that site owners achieve good Core Web Vitals to ensure a great user experience. This and other Real User Monitoring features provide unparalleled visibility into your application's frontend performance.</p>"},{"location":"newoutput/core-web-vitals/#core-web-vitals-metrics","title":"Core Web Vitals Metrics","text":""},{"location":"newoutput/core-web-vitals/#metric-definitions","title":"Metric Definitions","text":"<ul> <li> <p>Largest Contentful Paint (LCP). Measures how quickly the main content of a web page is loaded. In other words, it measures the time from when the user initiates loading the page until the largest image or text block is rendered within the viewport. To provide a good user experience, strive to have\u00a0LCP occur within the first 2.5 seconds\u00a0of the page starting to load.</p> </li> <li> <p>First Input Delay (FID). Measures the time from when a user first interacts with your page to when the browser responds to that interaction. This measurement is taken from whatever interactive element that the user first clicks. To provide a good user experience, strive for an\u00a0FID of less than 100 milliseconds. Starting March 2024, Interaction to Next Paint (INP)\u00a0will replace FID as a Core Web Vital.</p> </li> <li> <p>Interaction to Next Paint (INP). Measures a page's responsiveness to user interactions by observing the time it takes for the page to respond to all click, tap, and keyboard interactions that occur throughout a user's visit to a page. To provide a good user experience, strive for an INP of up to 200 milliseconds.</p> </li> <li> <p>Cumulative Layout Shift (CLS). Measures the total of all individual layout shift scores for every unexpected layout shift that occurs during the entire lifespan of the page. The score is zero to any positive number, where zero means no shifting, and the larger the number, the more layout shifts on the page. To provide a good user experience, strive for a\u00a0CLS score of less than 0.1.</p> </li> <li> <p>First Contentful Paint (FCP). Measures the time it takes for the first piece of content to be rendered on the screen. To provide a good user experience, strive for an FCP score of less than 1.8 seconds.</p> </li> <li> <p>Time to First Byte (TTFB). Measures the time it takes for a browser to receive the first byte of a response from the server. To provide a good user experience, strive for an FCP score of less than 800 milliseconds.</p> </li> <li> <p>Total Blocking Time (TBT). Measures the total amount of time between the FCP and the TTI, where the browser's main thread is blocked and unable to respond to user input. To provide a good user experience, strive for a TBT score of under 200 milliseconds.</p> </li> </ul>"},{"location":"newoutput/core-web-vitals/#status-definitions","title":"Status Definitions","text":"<p>Each metric has a performance range that may operate as good, needs improvement, or poor. Coralogix provides these default values. Performance ranges can be adjusted in your UI to conform to your specific needs.</p> <p>Here are the performance ranges for each status:</p> Metric Good Needs Improvement Poor LCP &lt;=2.5s &lt;=4s &gt;4s FID &lt;=100ms &lt;=300ms &gt;300ms INP &lt;=200ms &lt;=500ms &gt;500ms CLS &lt;=0.1s &lt;=0.25 &gt;0.25 FCP &lt;=1.8s &lt;=3 &gt;3s TTFB &lt;=800ms &lt;=1800 &gt;1800ms TBT &lt;=200ms &lt;=600ms &gt;600ms"},{"location":"newoutput/core-web-vitals/#get-started","title":"Get Started","text":"<p>Once you have\u00a0configured and installed our browser SDK, in your Coralogix toolbar, navigate to\u00a0RUM\u00a0&gt;\u00a0Web Vitals to access the Core Web Vitals screen.</p>"},{"location":"newoutput/core-web-vitals/#core-web-vitals-screen","title":"Core Web Vitals Screen","text":""},{"location":"newoutput/core-web-vitals/#score","title":"Score","text":"<p>View all of your Core Web Vitals metrics and general key performance indicators: total pages, total users, and loading time.</p>"},{"location":"newoutput/core-web-vitals/#visuals","title":"Visuals","text":"<p>Enjoy a visual representation of various components contributing to the overall performance of a website using a line chart, stack bar chart, and histogram.</p> <ul> <li> <p>A line chart illustrates a Core Web Vitals metric, such as \"First Input Delay\" (FID), which depicts the metric's performance levels as it evolves over a specified timeframe. The x-axis represents time intervals, while the y-axis indicates the values of the chosen metric.</p> </li> <li> <p>A stacked bar chart representing a Core Web Vitals metric, such as \"Largest Contentful Paint\" (LCP), categorizes each bar into segments representing different performance levels: poor, moderate, and good. The x-axis typically represents individual pages or categories, while the y-axis denotes the cumulative measure of the chosen metric.</p> </li> <li> <p>A histogram representing a Core Web Vitals metric, like \"Cumulative Layout Shift\" (CLS), organizes data into intervals or \"buckets\" to display the distribution of metric values. Unlike line charts, histograms don't show trends over time; instead, they provide a snapshot of how often certain ranges of values occur. The x-axis represents different ranges or intervals of the Core Web Vitals metric, such as CLS values. At the same time, the y-axis indicates the frequency or count of occurrences within each range.</p> </li> </ul>"},{"location":"newoutput/core-web-vitals/#core-web-vitals-across-urls","title":"Core Web Vitals Across URLs","text":"<p>View Core Web Vital metrics for each page within your application.</p> <p></p> <p>Adjust the grid to present URL performance in ascending or descending order, to allow for efficient inspection and monitoring.</p> <p></p>"},{"location":"newoutput/core-web-vitals/#url-vitals-screen","title":"URL Vitals Screen","text":"<p>If you want insight into a critical page \u2014 such as a landing page, you can see a breakdown of what's contributing to its Core Web Vitals metrics. Use this information to understand how your end users are experiencing those areas of your site. For example, if you observe a consistent trend of suboptimal performance on a particular browser, you can tailor any forthcoming enhancements to serve that user segment better.</p> <p>For the URL of interest, view the following:</p> <ul> <li> <p>General KPIs, including page views, total users, and loading time</p> </li> <li> <p>Core Web Vitals Metrics presented numerically</p> </li> </ul> <p></p> <p>Select a Core Web Vitals metric to investigate further.</p> <ul> <li> <p>Metric Tab. For each metric, you are presented with the number of instances per metric distributed by performance score: Good, Fair, and Poor</p> </li> <li> <p>Impacted Elements. View the code underlying the element or object affecting the metric performance for the page.</p> </li> </ul> <p></p> <ul> <li>Metric Visualizations. View each Core Web Vitals metric as a line chart or stacked bar chart, with scores distributed by day. This allows you to identify problematic days or hours for metric performance immediately. A histogram representing a Core Web Vitals metric organizes data into intervals or \"buckets\" to display the distribution of metric values, providing a snapshot of how often certain ranges of values occur.</li> </ul> <p></p> <ul> <li>Grid. View performance metrics for each instance, allowing you to understand how diverse users encountered a specific metric and identify the object that most affected it. Information for each instance encompasses user ID, email, browser, operating system, device, country, and metric score.</li> </ul> <p></p>"},{"location":"newoutput/core-web-vitals/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/create-alerting-email-templates-with-coralogix/","title":"Create Alerting Email Templates with Coralogix","text":"<p>With Coralogix, you can easily create alerting email templates using our custom alert webhooks. These templates allow you to embed details from a log, metric, trace, or security event that triggered an event into your email subject or body, resulting in easier and more informative email notifications.</p>"},{"location":"newoutput/create-alerting-email-templates-with-coralogix/#overview","title":"Overview","text":"<p>This tutorial provides step-by-step instructions on how to set up an alerting email template with Coralogix, using Zapier as a use case. It walks you through the process of setting up a Zapier account, creating a Zap with a trigger and an action, and configuring the webhook as a trigger. It then demonstrates how to catch a webhook from Coralogix and send an email as a response.</p>"},{"location":"newoutput/create-alerting-email-templates-with-coralogix/#set-up-an-alerting-email-template","title":"Set Up an Alerting Email Template","text":""},{"location":"newoutput/create-alerting-email-templates-with-coralogix/#create-a-zap","title":"Create a Zap","text":"<p>STEP 1. Set up a Zapier Premium account.</p> <p>STEP 3. From your Zapier dashboard, click + Create a Zap in the upper right-hand corner.</p> <p></p> <p>You will be asked to configure your Trigger and Action.</p> <p></p>"},{"location":"newoutput/create-alerting-email-templates-with-coralogix/#configure-the-trigger","title":"Configure the Trigger","text":"<p>STEP 1. Configure the Coralogix webhook as a Trigger.</p> <ul> <li> <p>Click Trigger.</p> </li> <li> <p>Select Webhook in the bottom right-hand corner.</p> </li> <li> <p>Choose an event. Select Catch Hook from the drop-down menu.</p> </li> <li> <p>Click Continue.</p> </li> </ul> <p>STEP 2. Click Test to test the trigger.</p> <p>STEP 3. You will be presented with a custom webhook URL. Click Copy to copy it to your clipboard.</p>"},{"location":"newoutput/create-alerting-email-templates-with-coralogix/#configure-the-custom-webhook","title":"Configure the Custom Webhook","text":"<p>STEP 1. In your Coralogix toolbar, navigate to DataFlow &gt; Webhooks.</p> <p>STEP 2. In the upper right hand corner, click + ADD NEW. In the dropdown menu, select Webhook (generic).</p> <p></p> <p>STEP 3. Define the webhook settings. Paste the webhook URL in the URL field.</p> <p></p> <p>STEP 4. Click SAVE.</p>"},{"location":"newoutput/create-alerting-email-templates-with-coralogix/#configure-the-action","title":"Configure the Action","text":"<p>STEP 1. Return to your Zapier account to define the Action. The action in this case is sending an email.</p> <ul> <li> <p>Click Action.</p> </li> <li> <p>Select the Action of choice. In this example, the user has chosen Gmail.</p> </li> <li> <p>In the dropdown event menu, select Send an Email.</p> </li> <li> <p>Click Continue.</p> </li> </ul> <p>STEP 2. Validate that the selected email is correct. Click Continue.</p> <p>STEP 3. Define the Action settings.</p> <ul> <li>In order for the Data fields to appear, revert back to your Coralogix webhook and click TEST CONFIG. Then, in your Zapier account, click TEST TRIGGER. Within seconds, you should receive confirmation of the webhook catch with all of the data from your customized alert.</li> </ul> <p></p> <ul> <li> <p>Click Send Email in Gmail. Fill in the fields.</p> <ul> <li> <p>To. Enter text or data. Use a parameter from the webhook or input a fixed addressee for the webhook.</p> </li> <li> <p>From. Select the email that you are using in this configuration.</p> </li> <li> <p>Subject (required). Input the parameters of your webhook. In the example below, service name has an alert with field keys, including team / application / subsystem / severity, etc. Note that you have the option of formatting the data.</p> </li> <li> <p>Body type. Select plain.</p> </li> <li> <p>Body (required). In the example below, error, trigger, alert URL, and service name are used. Text has been added as well.</p> </li> <li> <p>Attachment. If relevant, attach a Runbook, for example.</p> </li> </ul> </li> </ul> <p></p> <p></p> <ul> <li>Click Continue.</li> </ul> <p>STEP 4. Click Test Step.</p> <p>STEP 5. Revert back to your saved Coralogix webhook. Click TEST CONFIG. Verify whether that webhook addressee has received an email.</p>"},{"location":"newoutput/create-alerting-email-templates-with-coralogix/#additional-resources","title":"Additional Resources","text":"DocumentationGet Started with Coralogix AlertsCustom Alert Webhooks"},{"location":"newoutput/create-alerting-email-templates-with-coralogix/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/create-annotations/","title":"Create Annotations","text":"<p>Add Annotations in Custom Dashboards to provide rich contextual information about single points in time.</p> <p></p>"},{"location":"newoutput/create-annotations/#overview","title":"Overview","text":"<p>Annotations in Custom Dashboards allow you to spotlight crucial events by adding an agnostic query that runs parallel to the query underlying your widget. Represented as vertical lines cutting across your line and bar charts, you may hover over an annotation to uncover event details and tags, adding an extra layer of context. The text message field even lets you embed dynamic label values for comprehensive insights. When two or more events occur simultaneously on an annotation, toggle between events on the same timestamp.</p> <p>Utilize annotations strategically \u2014 mark the deployment time of a new software release for instant correlation with system changes, or, during an incident response, create annotations to pinpoint critical events revealed by logs, metrics, or spans. This creates a precise chronological record of investigative steps for swift and effective resolution.</p> <p>When an annotation is created, it applies to all time-based widgets in your dashboard.</p> <p>Easily edit or delete the annotation at any time.</p>"},{"location":"newoutput/create-annotations/#create-an-annotation","title":"Create an Annotation","text":"<p>STEP 1. Click the annotation icon in the upper right-hand corner of your dashboard to add an annotation.</p> <p></p> <p>STEP 2. Enter the annotation details.</p> <p></p>"},{"location":"newoutput/create-annotations/#annotation-name","title":"Annotation Name","text":"<p>Select a name that will be easily recognizable.</p>"},{"location":"newoutput/create-annotations/#data-source","title":"Data Source","text":"<p>Annotations can be extracted from any of the following data sources:</p> <ul> <li> <p>Logs</p> </li> <li> <p>Spans</p> </li> <li> <p>Metrics. When you select metrics, you may choose a predefined metric in the Metric to Annotation field or use a free-text PromQL query in the Query field.</p> </li> <li> <p>DataPrime. This option allows logs and spans to be sourced within a DataPrime syntax query.</p> </li> </ul> <p>Annotations can be extracted from metrics. When you select metrics, you may choose a predefined metric in the Metric to Annotation field or use a free-text PromQL query in the Query field.</p>"},{"location":"newoutput/create-annotations/#query","title":"Query","text":"<p>If you haven\u2019t chosen to use a DataPrime syntax query, logs and traces are queried with Lucene and metrics with PromQL. An annotation will be automatically generated on your widgets when query results are met.</p>"},{"location":"newoutput/create-annotations/#annotation-type","title":"Annotation Type","text":"<p>Select a singular event (Single) or a time range (Range) with a start and end time.</p> <ul> <li> <p>Single. From the drop-down menu, choose the field that represents the start timestamp.</p> </li> <li> <p>Range. From the drop-down menu, choose the fields that represent the start and end timestamps. Alternatively, select the field representing the timestamp duration from the available options.</p> </li> </ul>"},{"location":"newoutput/create-annotations/#settings","title":"Settings","text":"<ul> <li> <p>Text Message. Use free text with dynamic field placeholders enclosed in curly brackets, which become visible upon hovering over the annotation. For instance: 'Service {{service name}} experienced deployment at {{deployment.timestamp}}'. Replace the field placeholders accordingly.</p> </li> <li> <p>Labels. Select label keys - such as pod name, node name, or environment - to be displayed in the annotation.</p> </li> </ul> <p>STEP 3. If you are satisfied with the annotation displayed in the Preview, click SAVE.</p>"},{"location":"newoutput/create-annotations/#hiding-annotations","title":"Hiding Annotations","text":"<p>When an annotation is created, it applies to all time-based widgets in your dashboard. Toggle your annotation on and off for all widgets in the dashboard toolbar.</p> <p>Toggle an annotation on and off in a specific widget by clicking on the annotation name in the widget legend.</p>"},{"location":"newoutput/create-annotations/#modify-delete-annotations","title":"Modify &amp; Delete Annotations","text":"<p>STEP 1. Click on an annotation name to be rerouted to the annotation setup.</p> <p>STEP 2. Modify the configuration settings, and click SAVE.</p> <p>To delete the annotation, click on the trash icon in the lower left-hand corner of your settings box.</p>"},{"location":"newoutput/create-annotations/#additional-resources","title":"Additional Resources","text":"DocumentationCustom Dashboards"},{"location":"newoutput/create-annotations/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/create-lambda-function-metrics-dashboard/","title":"Setting Up Your Lambda Function Metrics Dashboard","text":"<p>Interested in using the Coralogix AWS Lambda Telemetry Exporter v.0.2.0 to collect and send us Lambda function metrics? This tutorial provides two methods for setting up a dashboard for monitoring a single Lambda function:</p> <ul> <li> <p>Import a pre-built dashboard to Granfana (recommended)</p> </li> <li> <p>Manually create a dashboard that fits your specific needs</p> </li> </ul>"},{"location":"newoutput/create-lambda-function-metrics-dashboard/#import-a-pre-built-dashboard","title":"Import a Pre-Built Dashboard","text":"<p>STEP 1. Download the attached JSON file.</p> <p>STEP 2. Import the file to Grafana as a pre-built dashboard.</p>"},{"location":"newoutput/create-lambda-function-metrics-dashboard/#create-a-dashboard","title":"Create a Dashboard","text":"<p>Use the example presented here to create a dashboard that fits your specific needs.</p> <p></p> <p>STEP 1. Create a new Coralogix - Grafana dashboard.</p> <ul> <li>Select Grafana in the upper right-hand menu of your Coralogix dashboard.</li> </ul> <p></p> <ul> <li>Click Dashboard settings</li> </ul> <p></p> <p>STEP 2. Create a new variable</p> <ul> <li>Select Variables in the left-hand menu &gt; Click Add variable</li> </ul> <p></p> <ul> <li> <p>Create a new variable with the input below &gt; Click update</p> <ul> <li> <p>Name: <code>faas_name</code></p> </li> <li> <p>Type: <code>Constant</code></p> </li> <li> <p>Value: Input the Lambda function that you wish to monitor. (Using a constant will make it easy to replicate the dashboard for other functions in the future.)</p> </li> </ul> </li> </ul> <p>STEP 3. Create panels.</p> <ul> <li>Create an \u2018Invocations\u2019 panel. Click Add panel &gt; Add empty panel</li> </ul> <p></p> <ul> <li> <p>Set options:</p> <ul> <li> <p>Panel options &gt; Title = <code>Invocations</code></p> </li> <li> <p>Legend &gt; Legend mode = <code>Hidden</code></p> </li> <li> <p>Standard options &gt; Unit = <code>requests/sec (rps)</code></p> </li> <li> <p>Standard options &gt; Min = <code>0</code></p> </li> </ul> </li> <li> <p>Add queries:</p> <ul> <li><code>sum(increase(faas_executions_total{faas_name=\"$faas_name\"}[1m])) OR on() vector(0)</code></li> </ul> </li> <li> <p>Create a \u2018Durations\u2019 panel (per invocation). Click Add panel &gt; Add empty panel</p> <ul> <li> <p>Set options:</p> <ul> <li> <p>Panel options \u2192 Title = <code>Durations (per invocation)</code></p> </li> <li> <p>Standard options \u2192 Unit = <code>milliseconds (ms)</code></p> </li> <li> <p>Standard options \u2192 Min = <code>0</code></p> </li> </ul> </li> <li> <p>Add queries:</p> <ul> <li> <p><code>sum(increase(faas_init_duration_ms_sum{faas_name=\"$faas_name\"}[1m])) / sum(increase(faas_invoke_duration_ms_count{faas_name=\"$faas_name\"}[1m]))</code> with Legend = <code>Initialisation duration (averaged over all invocations)</code></p> </li> <li> <p><code>sum(increase(faas_restore_duration_ms_sum{faas_name=\"$faas_name\"}[1m])) / sum(increase(faas_invoke_duration_ms_count{faas_name=\"$faas_name\"}[1m]))</code> with Legend = <code>Restoration duration (averaged over all invocations)</code></p> </li> <li> <p><code>sum(increase(faas_invoke_duration_ms_sum{faas_name=\"$faas_name\"}[1m])) / sum(increase(faas_invoke_duration_ms_count{faas_name=\"$faas_name\"}[1m]))</code> with Legend = <code>Average invoke duration</code></p> </li> <li> <p><code>sum(increase(faas_overhead_duration_ms_sum{cx_subsystem_name=\"lambda-telemetry-test\"}[1m])) / sum(increase(faas_invoke_duration_ms_count{cx_subsystem_name=\"lambda-telemetry-test\"}[1m]))</code> with Legend = <code>Average overhead (runtime after invoke) duration</code></p> </li> </ul> </li> </ul> </li> <li> <p>Create an \u2018Errors/Timeouts\u2019 panel. Click Add panel &gt; Add empty panel</p> <ul> <li> <p>Set options:</p> <ul> <li> <p>Panel options &gt; Title = <code>Errors/Timeouts</code></p> </li> <li> <p>Standard options &gt; Unit = <code>requests/sec (rps)</code></p> </li> <li> <p>Standard options &gt; Min = <code>0</code></p> </li> </ul> </li> <li> <p>Add queries:</p> <ul> <li> <p><code>sum(increase(faas_errors_total{faas_name=\"$faas_name\"}[1m])) OR on() vector(0)</code> with Label = <code>errors</code></p> </li> <li> <p><code>sum(increase(faas_timeouts_total{faas_name=\"$faas_name\"}[1m])) OR on() vector(0)</code> with Label = <code>timeouts</code></p> </li> </ul> </li> </ul> </li> <li> <p>Create a \u2018Memory Consumption\u2019 panel. Click Add panel &gt; Add empty panel</p> <ul> <li> <p>Set options:</p> <ul> <li> <p>Panel options &gt; Title = <code>Memory Consumption</code></p> </li> <li> <p>Standard options &gt; Unit = <code>megabytes</code></p> </li> <li> <p>Standard options &gt; Min = <code>0</code></p> </li> </ul> </li> <li> <p>Add queries:</p> <ul> <li> <p><code>max(faas_mem_usage_MB{faas_name=\"$faas_name\"})</code> with Label = <code>maximum memory consumption</code></p> </li> <li> <p><code>max(faas_mem_limit_MB{faas_name=\"$faas_name\"})</code> with Label = <code>available memory</code></p> </li> </ul> </li> </ul> </li> <li> <p>Create a \u2018Cold Invocations\u2019 panel. Click Add panel &gt; Add empty panel</p> <ul> <li> <p>Set options:</p> <ul> <li> <p>Panel options &gt; Title = <code>Cold Invocations</code></p> </li> <li> <p>Standard options &gt; Unit = <code>requests/sec (rps)</code></p> </li> <li> <p>Standard options &gt; Min = <code>0</code></p> </li> </ul> </li> <li> <p>Add queries:</p> <ul> <li> <p><code>sum(increase(faas_init_duration_ms_count{faas_name=\"$faas_name\"}[1m]))</code> with Label = <code>Cold invocations</code></p> </li> <li> <p><code>sum(increase(faas_restore_duration_ms_count{faas_name=\"$faas_name\"}[1m]))</code> with Label = <code>Restorations</code></p> </li> </ul> </li> </ul> </li> <li> <p>Create an \u2018Initialization / Restoration Duration\u2019 panel. Click Add panel &gt; Add empty panel</p> <ul> <li> <p>Set options:</p> <ul> <li> <p>Panel options &gt; Title = <code>Initialization / Restoration Duration</code></p> </li> <li> <p>Standard options &gt; Unit = <code>milliseconds (ms)</code></p> </li> <li> <p>Standard options &gt; Min = <code>0</code></p> </li> </ul> </li> <li> <p>Add queries:</p> <ul> <li> <p><code>sum(increase(faas_init_duration_ms_sum{faas_name=\"$faas_name\"}[1m])) / sum(increase(faas_init_duration_ms_count{faas_name=\"$faas_name\"}[1m]))</code> with Label = <code>Cold invocations</code></p> </li> <li> <p><code>sum(increase(faas_restore_duration_ms_sum{faas_name=\"$faas_name\"}[1m])) / sum(increase(faas_restore_duration_ms_count{faas_name=\"$faas_name\"}[1m]))</code> with Label = <code>Average restoration duration</code></p> </li> </ul> </li> </ul> </li> </ul> <p>STEP 4. Organize panels.</p> <ul> <li> <p>Click Add panel &gt; Add new row</p> </li> <li> <p>Set Title: <code>Cold start</code></p> </li> <li> <p>Adjust the panels as displayed in the image below</p> </li> </ul> <p></p> <p>STEP 5. Enjoy your Lambda Function Dashboard!</p>"},{"location":"newoutput/create-lambda-function-metrics-dashboard/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/create-manage-saved-views/","title":"Create & Manage Saved Views","text":"<p>Create and manage Saved Views using Folders in the Coralogix Explore Screen.</p>"},{"location":"newoutput/create-manage-saved-views/#create-saved-views","title":"Create Saved Views","text":"<p>STEP 1. Click on the ellipsis next to Unsaved View in the upper left-hand corner of your Explore Screen.</p> <p>STEP 2. You may add a view name for the particular view, decide whether to save query and filters and set the view as your default dashboard. You may also determine the view\u2019s privacy settings: private or shared.</p> <p>Note: To save a query as part of a saved view, you must run the query first.</p>"},{"location":"newoutput/create-manage-saved-views/#manage-saved-views-with-folders","title":"Manage Saved Views with Folders","text":"<p>Manage your Saved Views in a more organized way using\u00a0Folders.</p> <p>STEP 1. Search for an existing folder to store your saved view or create a new one by clicking the folder icon.</p> <p>STEP 2. Drag and drop your saved views into the relevant folder.</p> <p>Saved views will appear in the right-hand column as either private or shared in their respective folders. Drag and drop views between folders.</p>"},{"location":"newoutput/create-manage-saved-views/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogix.com.</p>"},{"location":"newoutput/create-metric-from-custom-dashboard-widget/","title":"Create Metrics From Your Custom Dashboard Widget","text":"<p>When you first create a dashboard, the data you\u2019re using may be raw data. However, creating visualizations often helps you refine the information you seek from those logs or metrics. To enable you to take advantage of this refined data, Coralogix supports creating metrics from custom dashboard widgets.</p> <p>When creating a metric from a custom dashboard widget, the process depends on whether you are creating a metric from a metric, in which case you will create the new metric using a recording rule, or a log or span, in which case you will create the new metric using Events2Metric.</p>"},{"location":"newoutput/create-metric-from-custom-dashboard-widget/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create one or more Custom Dashboard widgets.</li> </ul>"},{"location":"newoutput/create-metric-from-custom-dashboard-widget/#creating-a-new-metric-from-a-logs-or-spans-widget","title":"Creating a New Metric from a Logs or Spans Widget","text":"<p>STEP 1. From your Coralogix toolbar, go to Dashboards &gt; Custom Dashboards.</p> <p>STEP 2. Open an existing custom dashboard or create a new one create a new one.</p> <p>STEP 3. Click the three-dot menu in the top right-hand corner of the widget from which you want to create a metric.</p> <p></p> <p>STEP 4. Click CREATE METRIC.</p> <p>The New Event2Metric pane opens with the fields already populated according to the widget data.</p> <p></p> <p>STEP 5. Enter a name for your metric.</p> <p>STEP 6. [Optional] Fill out any other fields you want to customize in the Event2Metric pane, as described in the Events2Metrics documentation.</p> <p>STEP 7. Click Create Logs2Metric or Create Spans2Metric (the button changes depending on whether your selected metric came from a log widget or a spans widget).</p>"},{"location":"newoutput/create-metric-from-custom-dashboard-widget/#creating-a-new-metric-from-a-metric-widget","title":"Creating a New Metric from a Metric Widget","text":"<p>STEP 1. From your Coralogix toolbar, go to Dashboards &gt; Custom Dashboards.</p> <p>STEP 2. Open an existing custom dashboard or create a new one (learn more about creating Custom Dashboards).</p> <p>STEP 3. Click the three dot menu in the top right hand corner of the widget from which you want to create a metric.</p> <p></p> <p>STEP 4. Click CREATE METRIC.</p> <p>The New Recording Rule pane opens with the New Rule and Expression fields already populated according to the widget data.</p> <p></p> <p>STEP 5. Select a rule set from the existing rule sets or click +ADD NEW SET to create a new rule set.</p> <p>STEP 6. Enter a name for your recording rule.</p> <p>STEP 7. Enter a name for your metric.</p> <p>STEP 8. [Optional] Edit any other fields you want to customize in the New Recording Rule pane as described in the Recording Rules documentation.</p> <p>STEP 9. Click Save Rule Group.</p>"},{"location":"newoutput/create-metric-from-custom-dashboard-widget/#additional-resources","title":"Additional Resources","text":"DocumentationCustom DashboardsLine ChartsData TablesGaugesPie ChartsVertical Bar ChartsHorizontal Bar Charts"},{"location":"newoutput/create-metric-from-custom-dashboard-widget/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/creating-alerts-from-custom-dashboard-widgets/","title":"Create Alerts From Custom Dashboard Widgets","text":"<p>Streamline your work process by creating alerts directly from your Custom Dashboard line chart widgets. Use the thresholds, variables, and filters embedded in your widget to create alerts easily without ever leaving your custom dashboard.</p>"},{"location":"newoutput/creating-alerts-from-custom-dashboard-widgets/#overview","title":"Overview","text":"<p>Once you\u2019ve created a line chart to display the data related to the system or application being monitored, Coralogix allows you to create an alert directly from your custom dashboard. Define it using the thresholds, variables, and filters you\u2019ve used to build your visualization.</p>"},{"location":"newoutput/creating-alerts-from-custom-dashboard-widgets/#thresholds","title":"Thresholds","text":"<p>Observing Custom Dashboard visualizations over time allows users to establish patterns and normal behavior for the system or application. This normal behavior serves as a baseline for comparison. Users can then create thresholds - predetermined limits based on desirable performance levels for specific metrics - to determine when abnormal events occur, such as sudden spikes or drops in performance, deviations from established patterns, or any unexpected behavior. For example, you might set a threshold for CPU usage at 80%, indicating that if the CPU usage exceeds this value, it is considered high and needs attention.</p>"},{"location":"newoutput/creating-alerts-from-custom-dashboard-widgets/#alerts","title":"Alerts","text":"<p>You can create standard alerts for logs and metric alerts for metrics, with your line chart parameters.</p>"},{"location":"newoutput/creating-alerts-from-custom-dashboard-widgets/#use-case","title":"Use-Case","text":"<p>A DevOps engineer has a line chart widget with many spikes, but one spike is significantly higher than the others. By creating an alert directly from the dashboard, the engineer can easily put the alert threshold at a point that is above the normal spikes, but below the unusually high one, helping avoid alert fatigue. Once the threshold is defined, the query is brought into the alert along with any variables and filters used, and the DevOps engineer can create the rest of the alert simply and easily.</p>"},{"location":"newoutput/creating-alerts-from-custom-dashboard-widgets/#configuration","title":"Configuration","text":"<p>There are two ways to create alerts from your Custom Dashboard line chart.</p> <ul> <li> <p>Create an alert using all the data in the widget.</p> </li> <li> <p>Create an alert using a single time series from the legend.</p> </li> </ul>"},{"location":"newoutput/creating-alerts-from-custom-dashboard-widgets/#create-an-alert-from-the-entire-widget","title":"Create an Alert From the Entire Widget","text":"<p>STEP 1. From your Coralogix toolbar, navigate to Dashboards &gt; Custom Dashboards.</p> <p>STEP 2. Select an existing custom dashboard or create a new one.</p> <p>STEP 3. Right-click on the three dots on the upper right-hand side of the line chart widget for which you want to create an alert. Click CREATE ALERT, causing the widget to be expanded and a Create new alert popup box to appear.</p> <p></p> <p>STEP 4. Select the threshold for your new alert. You can do this either by using the mouse to adjust the threshold on the widget, or by entering a numeric value in the threshold field in the popup box.</p> <p></p> <p>STEP 5. Select whether the trigger should be More Than or Less Than the threshold in the Create New Alert box.</p> <p>STEP 6. Click NEXT, causing the New Alert overlay to appear.</p> <p>The Query parameter will be automatically populated with the widget\u2019s query, including filters and variables. The Conditions parameter will be automatically populated with the widget\u2019s threshold, appearing as Value.</p> <p></p> <p>STEP 7. Fill out the rest of the alert details and notification group info as described in Standard User-Defined Alerts (for Log queries) or Metric Alerts (for Metric queries).</p> <p>STEP 8. Toggle Enable Alert. Click Create Alert.</p>"},{"location":"newoutput/creating-alerts-from-custom-dashboard-widgets/#create-an-alert-for-a-single-time-series-from-the-widget-legend","title":"Create an Alert for a Single Time Series From the Widget Legend","text":"<p>STEP 1. From your Coralogix toolbar, navigate to Dashboards &gt; Custom Dashboards.</p> <p>STEP 2. Select an existing custom dashboard or create a new one.</p> <p>STEP 3. Right-click on the time series for which you want to create an alert. Click CREATE ALERT.</p> <p></p> <p>This will cause the widget to be expanded, displaying only the time series selected. A Create new alert popup box will appear.</p> <p></p> <p>STEP 4. Select the threshold for your new alert. You can do this either by using the mouse to adjust the threshold on the widget, or by entering in a numeric value in the threshold field in the popup box.</p> <p>STEP 5. Select whether the trigger should be More Than or Less Than the threshold in the Create New Alert box.</p> <p>STEP 6. Click NEXT, causing the New Alert overlay to appear.</p> <p>The Query parameter will be automatically populated with the widget\u2019s query, including filters and variables. The Conditions parameter will be automatically populated with the widget\u2019s threshold, appearing as Value.</p> <p></p> <p>STEP 7. Fill out the rest of the alert details and notification group info as described in Standard User-Defined Alerts (for Log queries) or Metric Alerts (for Metric queries).</p> <p>STEP 8. Toggle Enable Rule. Click Create Alert.</p>"},{"location":"newoutput/creating-alerts-from-custom-dashboard-widgets/#additional-resources","title":"Additional Resources","text":"DocumentationCustom DashboardsLine ChartsData TablesGaugesPie ChartsBar Charts"},{"location":"newoutput/creating-alerts-from-custom-dashboard-widgets/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/crowdstrike-falcon/","title":"CrowdStrike Falcon","text":"<p>Coralogix provides seamless integration with\u00a0CrowdStrike Falcon, allowing you\u00a0to correlate security-related events with your application and infrastructure logs and detect and respond to security incidents more effectively.</p> <p>While this integration allows you to choose your preferred log shipper, we strongly recommend using OpenTelemetry as a best practice. Other available shippers can be found here.</p>"},{"location":"newoutput/crowdstrike-falcon/#configuration","title":"Configuration","text":"<p>The following is an example configuration.</p> <p>STEP 1.\u00a0Install the Crowdstrike Falcon SIEM connector.</p> <p>STEP 2.\u00a0Configure it to stream CrowdStrike events into a local file. By default the SIEM connector stores its data in /var/log/crowdstrike/falconhoseclient/. Change the default data storage location if necessary.</p> <p>STEP 3.\u00a0Download OpenTelemetry on the SIEM connector host. Get started here.</p>"},{"location":"newoutput/crowdstrike-falcon/#example","title":"Example","text":"<p>Use the example below as a basis for shipping your logs, which adopt a multiline logs pattern.</p> <p>Replace the <code>private_key</code> with your Coralogix Send-Your-Data API key and <code>domain</code> with your Coralogix domain.</p> <pre><code>receivers:\n  filelog:\n    start_at: beginning\n    include:\n      - /var/log/crowdstrike/falconhoseclient/output\n    multiline:\n      line_start_pattern: \"^{\"\n    operators:\n      - type: json_parser\n        parse_to: body\nexporters:\n  coralogix:\n    domain: \"coralogixstg.wpengine.com\"\n    private_key: \"your Send-Your-Data API key\"\n    application_name: \"open-test-app\"\n    subsystem_name: \"CrowdStrike-Falcon\"\n    timeout: 30s\nservice:\n  pipelines:\n    logs:\n      receivers: [ filelog ]\n      exporters: [ coralogix ]\n\n\n</code></pre>"},{"location":"newoutput/crowdstrike-falcon/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/custom-dashboards/","title":"Custom Dashboards","text":"<p>Coralogix now offers the ability to create unlimited, personalized custom dashboards. Use six new visualizations - Data Table, Line Chart, Gauge, Pie Chart, Vertical Bar Chart, and Horizontal Bar Chart - each of which supports all three data types - logs, metrics, and spans - to define and create a dashboard catered to your specific observability needs. Then, query across your widgets using our new Filter and Variable capabilities.</p>"},{"location":"newoutput/custom-dashboards/#create-a-custom-dashboard","title":"Create a Custom Dashboard","text":"<p>STEP 1. In your navigation bar, click Dashboard &gt; Custom Dashboards.</p> <p>STEP 2. Create a new dashboard by clicking +NEW DASHBOARD.</p> <p>STEP 3. Drag a widget into your new dashboard: Data Table, Line Chart, Gauge, Pie Chart, Vertical Bar Chart, or Horizontal Bar Chart. Set the definitions.</p> <p></p> <p>STEP 4. Hover over SAVE, then click SAVE AS and enter a name for your new dashboard.</p> <p></p> <p>STEP 5. Hover over your new dashboard in the dashboard panel, and three dots will appear to the right. Click on the three dots and select from several options in the drop-down menu: STAR, SET AS DEFAULT, RENAME, DELETE, SAVE AS.</p> <p>Notes:</p> <ul> <li> <p>Your entire team will have access to all dashboards. Those with editing permissions can edit all dashboards.</p> </li> <li> <p>Log filters are applied to log-based widgets.</p> </li> <li> <p>Variables defined on a dashboard are accessible from all widgets.</p> </li> <li> <p>When editing a widget, the widget you are currently editing is highlighted, and all other widgets are faded.</p> </li> </ul>"},{"location":"newoutput/custom-dashboards/#select-dashboard-timeframe","title":"Select Dashboard Timeframe","text":"<p>By default, custom dashboards show data from the last 15 minutes. This can be modified using the timeframe dropdown at the upper right-hand side of the dashboard.</p> <p></p> <p>You can choose between different timeframes by selecting one of the quick timeframes or by using the custom tab and selecting two dates between which you want to show data.</p>"},{"location":"newoutput/custom-dashboards/#90-day-timeframe","title":"90-Day Timeframe","text":"<p>Coralogix supports selecting a timeframe of up to 90 days for your custom dashboards. This lets you easily view metric widgets with long retention periods within your custom dashboards.</p> <p>Notes:</p> <ul> <li> <p>Timeframe limitations may apply depending on the data source and the data origin.</p> </li> <li> <p>When selecting more than seven days, the results will appear based on the specific settings of each data source, similar to the Explore screen time limitation.</p> </li> <li> <p>For example, if your logs in the Explore screen are limited by up to 8 days timeframe, this will be the case also for the custom dashboard, and logs data will appear for only the last eight days, as opposed to metrics, which can show up to 90 days of data in the same dashboard.</p> </li> <li> <p>The query range and retention period are each marked with an annotation line. Hovering over the line shows an explanation.</p> </li> <li> <p>If the retention period is, for example, 21 days, the user can see that limit on the time series widget, and selecting an older 7-day time query within those 21 days will still return results.</p> </li> </ul> <p></p>"},{"location":"newoutput/custom-dashboards/#refreshing-your-dashboard","title":"Refreshing Your Dashboard","text":"<p>Manually refresh your dashboard by clicking the refresh icon in the upper right-hand corner. Hover over the icon to see when the last refresh occurred.</p> <p>You may also rely on the auto-refresh capability. Choose your auto-refresh interval in the time-picker for Frequent Search, high-priority data. Select from the following settings: off, 2m, or 5m. Monitoring data extracted from the archive will auto-refresh at a frequency of triple this rate.</p>"},{"location":"newoutput/custom-dashboards/#view-modify-a-custom-dashboard","title":"View &amp; Modify a Custom Dashboard","text":"<p>Widgets are always in edit mode and may be edited at any time.</p> <p>STEP 1. In your dashboard, navigate to the Dashboard tab &gt; Custom Dashboards.</p> <p>STEP 2. In the left-hand sidebar, view all available dashboards.</p> <p>STEP 3. Click on the dashboard you wish to view and modify as desired.</p> <ul> <li> <p>When you click on the SAVE dropdown menu, you can select either SAVE or SAVE AS. SAVE changes your current dashboard; SAVE AS clones your dashboard and saves the changes while maintaining the original as is.</p> </li> <li> <p>If you choose to SAVE AS, a popup requires you to name your cloned dashboard.</p> </li> </ul> <p>STEP 4. Once you have made changes to a widget, click SAVE. This saves your entire dashboard. Click RESET to revert your changes and view the previously saved dashboard.</p> <p>Notes:</p> <ul> <li> <p>Three dots can be found in the upper right-hand corner of every widget. Click on this and select from the menu options DELETE or CLONE. For line charts, you also have the option to HIDE LEGEND.</p> </li> <li> <p>To expand a specific widget within a dashboard, click on the Expand icon in the top right corner of the widget. This temporarily expands the widget to fill the entire dashboard.</p> </li> </ul> <p></p> <ul> <li> <p>Hover over a specific dashboard name; three dots will appear to the right. Click on the three dots and select from several options in the drop-down menu: STAR, SET AS DEFAULT, RENAME, DELETE.</p> </li> <li> <p>STAR and DEFAULT settings are unique for each user. All other settings apply to all team members.</p> </li> <li> <p>Right-clicking on a value in the legend of a Line chart opens a context menu with the following options:</p> <ul> <li> <p>Exclude. Exclude this value from the current filter.</p> </li> <li> <p>Include. Include only this value in the filter.</p> </li> <li> <p>Explore. Opens the query in the Explore screen for further analysis.</p> </li> <li> <p>Create Alert. Create an alert directly from the custom dashboard (see Creating Alerts from Custom Dashboard Widgets ).</p> </li> <li> <p>Copy Name. Lets you copy the exact time series name for use in queries or searches.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"newoutput/custom-dashboards/#create-manage-folders","title":"Create &amp; Manage Folders","text":"<p>Manage your respective dashboards in a more organized way using\u00a0folders. Our folder\u00a0system makes it easy to view and manage all of the dashboards you created or shared in\u00a0one place. Create new folders and subfolders to keep your dashboard\u00a0organized.</p> <p>STEP 1. Click NEW. Choose CREATE FOLDER and assign a name to the folder.</p> <p></p> <p>You can create subfolders by clicking on the subfolder icon, which is visible when hovering over a folder.</p> <p></p> <p>STEP 2. Drag and drop your dashboards into the appropriate folders and subfolders. You can move subfolders between folders or transform them into subfolders by dragging and dropping them into another folder.</p>"},{"location":"newoutput/custom-dashboards/#import-export-custom-dashboards","title":"Import &amp; Export Custom Dashboards","text":"<p>Effortlessly share your custom dashboards within your organization by importing and exporting them, eliminating the need to recreate them and minimizing overhead. Find out more here.</p>"},{"location":"newoutput/custom-dashboards/#create-variables","title":"Create Variables","text":"<p>STEP 1. Click on the variable icon { } in the upper right-hand corner of your dashboard.</p> <p>STEP 2. Define your Variable by defining Type, Field, Variable Name, and Display Name.</p> <p></p> <p>STEP 3. Click SAVE.</p> <p>STEP 4. View the variables per Field in the upper right-hand corner of your screen.</p> <p></p> <p>STEP 5. Use the <code>{{ variable_name }}</code> in the widgets query language.</p> <p>Notes:</p> <ul> <li> <p>Input the Value in the Lucene or PromQL query. When the Value is changed, all widgets are affected.</p> </li> <li> <p>To use the current dashboard timeframe in your query, use the <code>${__range}</code> variable. This variable is always available and does not need to be defined before you use it. You can also use the ${__range_s}\u00a0 and\u00a0${__range_ms} variants of this variable to get the range in seconds or milliseconds. For example, you could use the query <code>sum(last_over_time(request_count[${__range}])) by (service)</code> to show the last reported request_count metric value over the entire time frame grouped by service.</p> </li> </ul> <p></p>"},{"location":"newoutput/custom-dashboards/#create-filters","title":"Create Filters","text":"<p>STEP 1. Click on Filters View in your left-hand sidebar.</p> <p>STEP 2. Click +ADD FILTER.</p> <p>Note: Application and subsystem filters are created by default; you can create new filters.</p> <p>STEP 3. Define the parameters of your Filter: Source, Field, and Operator.</p> <p></p> <p>STEP 4. Click SAVE.</p> <p>Notes:</p> <ul> <li> <p>When a filter is enabled, only log-based widgets are filtered.</p> </li> <li> <p>Disable a filter by clicking on the toggle next to the filter name.</p> </li> </ul>"},{"location":"newoutput/custom-dashboards/#create-annotations","title":"Create Annotations","text":"<p>Add Annotations to your dashboards to provide rich contextual information about single points in time.</p> <p>Annotations allow you to spotlight crucial events by adding an agnostic query that runs in parallel to the query underlying your widget. Represented as vertical lines cutting across your line and bar charts, you may hover over an annotation to uncover event details and tags, adding an extra layer of context. The text message field even lets you embed dynamic label values for comprehensive insights.</p> <p>Find out more here.</p>"},{"location":"newoutput/custom-dashboards/#metrics-exploration","title":"Metrics Exploration","text":"<p>When you select metrics as the source for your Custom Dashboards widget, use free text in the Query section to search for a metric of your choice. As you do so, all relevant metrics will appear. Hover over any metric to view its system-generated metadata labels. Hover over a label to see its values.</p> <p></p>"},{"location":"newoutput/custom-dashboards/#additional-resources","title":"Additional Resources","text":"DocumentationLine ChartsData TablesGaugesPie ChartsVertical Bar ChartsHorizontal Bar Charts"},{"location":"newoutput/custom-dashboards/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/custom-dashboards-bar-charts/","title":"Vertical Bar Charts","text":"<p>The vertical bar chart widget in Coralogix offers an additional way to view your data. Vertical bar charts sort your data alphabetically by column name by default. In horizontal bar charts, data is usually sorted by value.</p>"},{"location":"newoutput/custom-dashboards-bar-charts/#create-a-vertical-bar-chart","title":"Create a Vertical Bar Chart","text":"<p>Create a customized vertical bar chart visualization.</p> <p>STEP 1. Drag and drop the Vertical Bar Chart widget from the left-hand side bar to get started.</p> <p></p> <p>STEP 2. Set the definitions for your Bar Chart in the right-hand sidebar.</p> <ul> <li> <p>Name &amp; Description. Create a name and description.</p> </li> <li> <p>Load data from. Select whether to load data from Frequent Search or Monitoring.</p> </li> </ul> <p></p> <ul> <li> <p>Source. Select a data type.</p> <ul> <li> <p>If the data type chosen is metrics, specify the metric or desired PromQL in the Query field. Use free text to search for a metric of your choice. As you do so, all relevant metrics will appear. Hover over any metric to view its system-generated metadata labels. Hover over a label to see its values.</p> <ul> <li>When creating a bar chart with metrics as the data type, the categories specified in the PromQL query appear in the Group By field automatically. Within the Group By field it is possible to reorder the categories by dragging and dropping.Drag and drop categories from the Category field into the Stacking field, to stack by a particular category.</li> </ul> </li> <li> <p>ADD FILTER. [Optional] Add a filter to your bar chart.</p> <ul> <li> <p>As opposed to the dashboard filter in the left-hand sidebar which affects the entire dashboard, this filter only affects the widget.</p> </li> <li> <p>The widget and dashboard filters operate in parallel to one another and intersect. If they negate one another, dashboard filters override widget filters.</p> </li> </ul> </li> </ul> </li> <li> <p>Group By. Select the fields to group by from the dropdown menu.</p> </li> </ul> <p></p> <ul> <li> <p>X AXIS. Select whether the X axis of the chart should be grouped by value or by time. If you group by value, the chart is grouped by a field or label (or several fields or labels), and if you group by time, the main grouping is based on periods of time (time buckets).</p> <ul> <li> <p>If you select Value, enter the value to show (for example, Country).</p> </li> <li> <p>If you select Time, select Auto or Manual time buckets, and if Manual, the length of the time bucket.</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Aggregation: Aggregate by Count, Count Distinct, Sum, Min, Max, and/or Average.</p> <ul> <li> <p>In Bar charts, changing the aggregation type will change the type of data you see.</p> </li> <li> <p>For example, aggregating by Count, might show you the number of people in a country. On the other hand when aggregating by Average, for example, you need to provide additional parameters, such as height, which will give you a bar chart displaying the average height by country.</p> </li> </ul> </li> <li> <p>Stack By. [Optional] Select a field by which to stack the chart. This shows you a second layer of data on the chart.</p> </li> </ul> <p></p> <ul> <li> <p>Advanced. Select from the following advanced options.</p> <ul> <li> <p>Legend Colors By. Select whether you want your legend colors to be by group or by stack.</p> </li> <li> <p>Scale. Select whether you want the scale of the bar chart to be Logarithmic or Linear. The default setting is linear, however if you have large differences between the different values, it can be helpful to show the logarithmic scale instead. For example, if the majority of your values are under 1k and one value is 10k, using the logarithmic scale will show you an easier to read bar chart than the linear scale.</p> </li> <li> <p>Sort By. Select whether to sort the chart by column name or by value. By default the vertical bar chart is sorted alphabetically by name.</p> </li> <li> <p>Color Scheme. Select the color scheme for your chart. Note that when Legend Colors By Group is selected, color scheme is disabled.</p> </li> <li> <p></p> </li> <li> <p>Max Bars Per Graph. Select the maximum number of bars you want to show per graph.</p> </li> <li> <p>Group Name. Customize the displayed group name.</p> </li> <li> <p>Unit. Select the unit type to use in your bar chart.</p> </li> </ul> </li> </ul> <p></p> <p>STEP 3. [Optional] If you want to save your dashboard for future use, click SAVE in the upper right hand corner.</p>"},{"location":"newoutput/custom-dashboards-bar-charts/#additional-resources","title":"Additional Resources","text":"DocumentationCustom DashboardsLine ChartsData TablesGaugesPie ChartsHorizontal Bar Charts"},{"location":"newoutput/custom-dashboards-bar-charts/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/custom-dashboards-data-tables/","title":"Data Tables","text":"<p>Create a Data Table in your custom dashboard in Events or Aggregated View.</p>"},{"location":"newoutput/custom-dashboards-data-tables/#overview","title":"Overview","text":"<p>Create a customized data table and visualize logs or metrics using Events or Aggregated View.</p>"},{"location":"newoutput/custom-dashboards-data-tables/#events-view","title":"Events View","text":"<p>Events View is supported for logs and spans.</p>"},{"location":"newoutput/custom-dashboards-data-tables/#aggregated-view","title":"Aggregated View","text":"<p>Aggregated View is supported for logs, spans and metrics.</p> <p>For logs and spans, you need to select a column by which you want to aggregate the data, then select which columns you want to see.</p> <p>For metrics, the data is auto-generated from within the metric. Two different calculations based on a PromQL query exist:</p> <ul> <li> <p>In case\u00a0instant vector selectors\u00a0are used without\u00a0@ modifier, it is possible to run PromQL aggregation functions to return precise results.</p> </li> <li> <p>In case of a more complex query, aggregations are calculated based on sampled values and are approximations.</p> </li> </ul>"},{"location":"newoutput/custom-dashboards-data-tables/#create-a-data-table","title":"Create a Data Table","text":"<p>Create a customized data table and visualize logs or metrics.</p> <p>STEP 1. In a custom dashboard, drag and drop the Data Table widget from your left-hand side bar to get started.</p> <p>STEP 2. Set the definitions for your Data Table in the right-hand sidebar.</p> <ul> <li> <p>Name &amp; Description. Create a name and description for your data table.</p> </li> <li> <p>Load data from. Select whether to load data from Frequent Search or Monitoring.</p> </li> </ul> <p></p> <ul> <li> <p>Source. Select a data source.</p> <ul> <li> <p>Coralogix supports metrics, logs and spans as the data source for the data table widget. The metrics data source provides you with the ability to investigate any metric easily despite the volume of information given. It enables you to view all the permutations of each label for a given query. Each permutation gives you the values for the selected timespan including Last, Min, Max, Avg, and Sum. It includes a view that lets you see the values at any given time in the selected timespan.</p> </li> <li> <p>If the Source chosen is metrics, specify the metric or desired PromQL in the Query field. Use free text to search for a metric of your choice. As you do so, all relevant metrics will appear. Hover over any metric to view its system-generated metadata labels. Hover over a label to see its values.</p> </li> </ul> </li> <li> <p>Table Type. When using logs or spans as the data source, you can choose between the Events view and the Aggregated view. Metrics are automatically generated in aggregated view.</p> <ul> <li> <p>Event tables show a table with a list of logs or spans ordered by time (you can change this to be sorted by other columns later) with additional metadata and labels relevant to those events, similar to the view you see in the Explore screen.</p> </li> <li> <p>Aggregation tables creates a table of values, grouped by a field or fields you specify. Each column represents a different value depending on the chosen group. Values could be a simple count or an average of the values for the specific field, for each of the defined group-by values.</p> </li> <li> <p>You can add and edit columns and filters for the aggregation table using the right-hand side bar.</p> </li> <li> <p>Note: Traces are currently limited to a single aggregation column per table widget.</p> </li> </ul> </li> </ul> <p></p> <ul> <li>Columns. Manage columns by selecting one or more relevant fields.</li> </ul> <p></p> <ul> <li>Advanced: Select the number of results to be displayed per page.</li> </ul> <p>STEP 4. In the bottom Query bar, add a Lucene or PromQL query to filter specific information.</p> <p>STEP 5. If you wish to save your dashboard with the new widget, click SAVE in the upper right hand corner.</p>"},{"location":"newoutput/custom-dashboards-data-tables/#additional-resources","title":"Additional Resources","text":"DocumentationCustom DashboardsLine ChartsGaugesPie ChartsBar Charts"},{"location":"newoutput/custom-dashboards-data-tables/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/custom-dashboards-gauges/","title":"Gauges","text":"<p>Create a customized gauge visualization in Custom Dashboards.</p>"},{"location":"newoutput/custom-dashboards-gauges/#create-a-gauge","title":"Create a Gauge","text":"<p>STEP 1. In a custom dashboard, drag and drop the Gauge widget from your left-hand sidebar to get started.</p> <p>STEP 2. Set the definitions for your Gauge in the right-hand sidebar.</p> <ul> <li> <p>Name &amp; Description. Create a name and description.</p> </li> <li> <p>Load data from. Select whether to load data from Frequent Search or Monitoring.</p> </li> </ul> <p></p> <ul> <li> <p>Source. Select a data type.</p> <ul> <li> <p>If the data type chosen is metrics, specify the metric or desired PromQL in the Query field. Use free text to search for a metric of your choice. As you do so, all relevant metrics will appear. Hover over any metric to view its system-generated metadata labels. Hover over a label to see its values.</p> <ul> <li>Calculation: Determines the values for your gauge. Instant, Last, Avg, Sum, Min and Max are possible parameters.<ul> <li>Selecting the parameter Last will provide the last data point in a time series within the selected time frame, whereas Instant will provide the final value at the end of the time frame. Avg, Sum, Min, and Max calculate the value by applying the selected aggregation function to the time series data points within the time frame.<ul> <li>For example, if the time is 03:25:00, fetching the time series with hour steps returns 00:00:00 - 1, 01:00:00 - 3, 02:00:00 - 15, 03:00:00 - 60, therefore the Last value would be 60. However, if we get the value for the exact time of 03:25:00 using Instant, the value returned will be taken from the exact time in the timeline, resulting in a value of 120.</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>If the data type chosen is logs or spans, you will be directed to select an Aggregation.</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Add Filter. [Optional] Add a filter to your gauge.</p> <ul> <li> <p>As opposed to the dashboard filter in the left-hand sidebar which affects the entire dashboard, this filter only affects the widget.</p> </li> <li> <p>The widget and dashboard filters operate in parallel to one another and intersect. If they negate one another, dashboard filters override widget filters.</p> </li> </ul> </li> <li> <p>Visuality.</p> <ul> <li> <p>Unit. Choose to display a % symbol alongside your results.</p> </li> <li> <p>Enable or disable the Inner arc and Outer arc. The inner arc will show actual value. The outer arc will show the thresholds. If you choose not to enable either arc, you will be left only with a number in your gauge.</p> </li> <li> <p>Thresholds. Choose the base Thresholds - that is, when the gauge should appear green and red. Add additional thresholds - yellow and orange - if necessary.</p> </li> </ul> </li> </ul> <p>STEP 3. Click SAVE in the upper right-hand corner.</p> <p>Note: The gauge widget currently works on a single time series.</p>"},{"location":"newoutput/custom-dashboards-gauges/#additional-resources","title":"Additional Resources","text":"DocumentationCustom DashboardsLine ChartsData TablesPie ChartsBar Charts"},{"location":"newoutput/custom-dashboards-gauges/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/custom-dashboards-line-charts/","title":"Line Charts","text":"<p>Create a customized line chart and visualize logs or metrics in Custom Dashboards.</p>"},{"location":"newoutput/custom-dashboards-line-charts/#create-a-line-chart","title":"Create a Line Chart","text":"<p>STEP 1. In a custom dashboard, drag and drop the Line Chart widget from the left-hand sidebar to get started.</p> <p>STEP 2. Set the definitions for your Line Chart in the right-hand sidebar.</p> <ul> <li> <p>Name &amp; Description. Create a name and description for your line chart.</p> </li> <li> <p>Load data from. Select whether to load data from Frequent Search or Monitoring.</p> </li> </ul> <p></p> <ul> <li> <p>Source. Select a data type.</p> <ul> <li> <p>If the data type chosen is metrics, specify the metric or desired PromQL in the Query field. Use free text to search for a metric of your choice. As you do so, all relevant metrics will appear. Hover over any metric to view its system-generated metadata labels. Hover over a label to see its values.</p> </li> <li> <p>If the data type chosen is logs or spans, you will be directed to select the following fields:</p> <ul> <li> <p>Aggregation: Aggregate by Count, Count Distinct, Sum, Min, Max, and/or Average</p> </li> <li> <p>Group By: Select any field from the dropdown menu</p> </li> </ul> </li> </ul> </li> <li> <p>+ ADD FILTER. [Optional] Add a widget filter.</p> <ul> <li> <p>As opposed to the dashboard filter in the left-hand sidebar which affects the entire dashboard, this filter only affects the widget.</p> </li> <li> <p>The widget and dashboard filters operate in parallel to one another and intersect. If they negate one another, dashboard filters override widget filters.</p> </li> </ul> </li> <li> <p>Advanced: Select from the following advanced options.</p> <ul> <li> <p>Scale. Select whether you want the scale of the line chart to be Logarithmic or Linear. The default setting is linear, however if you have large differences between the different values, it can be helpful to show the logarithmic scale instead. For example, if the majority of your values are under 1k and one value is 10k, using the logarithmic scale will show you an easier to read line chart than the linear scale.</p> </li> <li> <p>Series Per Query.</p> </li> <li> <p>Series Name. Create a series name</p> </li> <li> <p>Unit. Select the unit type to use in your line chart.</p> </li> <li> <p>Tooltip. Select whether to see all time series when hovering over them, or to see only the single time series you hover over.</p> </li> <li> <p></p> </li> <li> <p>Legend Values. Customize values to be displayed in your legend: Min, Max, Sum, Avg, Last.</p> </li> <li> <p>Color Scheme. Select the color scheme for your chart. Note that when Legend Colors By Group is selected, color scheme is disabled.</p> </li> </ul> </li> </ul> <p></p> <p>STEP 3. In the bottom Query bar, add a Lucene or PromQL query to query specific information.</p> <ul> <li>New! Coralogix is bringing Full Stack Observability into your Custom Dashboards! With the new multiple query feature available in custom dashboard widgets, you can view queries from multiple data types in a single chart. For more information on multiple queries, see Multiple Queries in Custom Dashboard Widgets.</li> </ul> <p></p> <p>STEP 4. [Optional] If you want to save your dashboard for future use, click SAVE in the upper right-hand corner.</p>"},{"location":"newoutput/custom-dashboards-line-charts/#additional-resources","title":"Additional Resources","text":"DocumentationCustom DashboardsData TablesGaugesPie ChartsBar Charts"},{"location":"newoutput/custom-dashboards-line-charts/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/custom-dashboards-markdown-widget/","title":"Markdown Widget","text":"<p>As part of our Custom Dashboard capabilities, Coralogix offers a Markdown Widget. Add tables, lists, reference guides, links, code snippets, images, and more to customize, contextualize, and optimize your dashboards using Markdown syntax.</p>"},{"location":"newoutput/custom-dashboards-markdown-widget/#overview","title":"Overview","text":"<p>Markdown is a lightweight markup language that allows users to format and structure plain text documents using a simple and intuitive syntax. By adding the Markdown Widget to your custom dashboards, you can easily add personalized context to your dashboards using straightforward symbols and conventions.</p> <ul> <li> <p>Bookmark Coralogix features to drill deeper and find more insights.</p> </li> <li> <p>Add tables, lists, reference guides, links, code-snippets, images and more.</p> </li> <li> <p>Explain complicated syntax queries.</p> </li> </ul> <p>No prior knowledge of Markdown is required. You have the option to edit in Markdown or take advantage of the simplified WYSIWYG editor that converts your input to Markdown.</p>"},{"location":"newoutput/custom-dashboards-markdown-widget/#create-a-markdown-widget","title":"Create a Markdown Widget","text":"<p>STEP 1. In a custom dashboard, drag and drop the Markdown Widget from your left-hand side bar to get started.</p> <p></p> <p>STEP 2. Enter your Markdown content in the Markdown Widget.</p> <p>The widget has two sections. The lower section allows you to edit in Markdown and contains a simplified WYSIWYG editor that is converted to Markdown.</p> <p></p> <p>The WYSIWYG editor includes the following options for editing Markdown:</p> <ul> <li> <p>Bold</p> </li> <li> <p>Italics</p> </li> <li> <p>Strikethrough</p> </li> <li> <p>Heading/Normal text</p> </li> <li> <p>Bulleted list</p> </li> <li> <p>Numbered list</p> </li> <li> <p>Checkbox list</p> </li> <li> <p>Add Link</p> </li> <li> <p>Quote</p> </li> <li> <p>Code</p> </li> <li> <p>Code Block</p> </li> <li> <p>Add table</p> </li> <li> <p>Add divider</p> </li> </ul> <p>STEP 3. [Optional] Add any text to the tooltip to be shown when you hover over the widget title bar.</p> <p></p> <p>STEP 4. [Optional] If you want to save your dashboard with the new widget, click SAVE in the upper right hand corner.</p>"},{"location":"newoutput/custom-dashboards-markdown-widget/#additional-resources","title":"Additional Resources","text":"DocumentationCustom DashboardsLine ChartsData TablesGaugesPie ChartsVertical Bar ChartsHorizontal Bar Charts"},{"location":"newoutput/custom-dashboards-markdown-widget/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/custom-dashboards-pie-charts/","title":"Pie Charts","text":"<p>Create a customized pie chart visualization in Custom Dashboards.</p>"},{"location":"newoutput/custom-dashboards-pie-charts/#create-a-pie-chart","title":"Create a Pie Chart","text":"<p>STEP 1. Drag and drop the Pie Chart widget from the left-hand sidebar to get started.</p> <p>STEP 2. Set the definitions for your Pie Chart in the right-hand sidebar.</p> <ul> <li> <p>Name &amp; Description. Create a name and description.</p> </li> <li> <p>Load data from. Select whether to load data from Frequent Search or Monitoring.</p> </li> </ul> <p></p> <ul> <li> <p>Source. Select a data type.</p> <ul> <li> <p>If the data type chosen is metrics, specify the metric or desired PromQL in the Query field. Use free text to search for a metric of your choice. As you do so, all relevant metrics will appear. Hover over any metric to view its system-generated metadata labels. Hover over a label to see its values.</p> <ul> <li> <p>When creating a pie chart with metrics as the data type, the categories specified in the PromQL query appear automatically in the Group By field. Within the Group By field, it is possible to reorder the categories by dragging and dropping.</p> </li> <li> <p>Drag and drop categories from the Category field into the Stacking field, to stack by a particular category.</p> </li> </ul> </li> <li> <p>If the data type chosen is logs or spans, you will be directed to select an Aggregation.</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>+ ADD FILTER. [Optional] Add a filter to your pie chart.</p> <ul> <li> <p>As opposed to the dashboard filter in the left-hand sidebar which affects the entire dashboard, this filter only affects the widget.</p> </li> <li> <p>The widget and dashboard filters operate in parallel to one another and intersect. If they negate one another, dashboard filters override widget filters.</p> </li> </ul> </li> <li> <p>Group By. Select up to two fields from the dropdown menu.</p> </li> <li> <p>Stack By. Optionally select a field to stack the chart by. This shows you a second layer of data on the table.</p> </li> <li> <p>Advanced. Select from the following advanced options:</p> <ul> <li> <p>Max slices in the chart. Select the maximum number of slices to show in the pie chart.</p> </li> <li> <p>Minimum % for size display. Set the minimum percentage required to display a slice</p> </li> <li> <p>Unit. Select the units to display in the pie chart.</p> </li> <li> <p>Color Scheme. Select the color scheme for your chart. You can choose between seven different color schemes.</p> </li> <li> <p></p> </li> <li> <p>Show Label. Select whether or not to show labels and which labels to show</p> </li> <li> <p>Group name. Customize the group name</p> </li> <li> <p>Show legend. Select whether or not to show the legend.</p> </li> </ul> </li> </ul> <p></p> <p>STEP 3. [Optional] If you want to save your dashboard for future use, click SAVE in the upper right-hand corner.</p>"},{"location":"newoutput/custom-dashboards-pie-charts/#additional-resources","title":"Additional Resources","text":"DocumentationCustom DashboardsLine ChartsData TablesGaugesBar Charts"},{"location":"newoutput/custom-dashboards-pie-charts/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/custom-metrics/","title":"Custom Metrics","text":"<p>Coralogix provides a scalable Prometheus-compatible managed service for time-series data.</p>"},{"location":"newoutput/custom-metrics/#overview","title":"Overview","text":"<p>Coralogix supports ingesting metrics in multiple ways. Our most common integrations are Prometheus &amp; OpenTelemetry, and we also have metrics integrations such as Cloudwatch metrics and AWS Kinesis firehose.</p> <p>This tutorial presents a series of use cases employing our custom metric endpoint, referred to here as the Otel endpoint, including serverless computing and quick cURL-like calls to send counters and gauges to Coralogix.</p> <p>View this GitHub repo for SDK examples of the custom metrics endpoint for grpcurl, Java, and Go.</p>"},{"location":"newoutput/custom-metrics/#data-model","title":"Data Model","text":"<p>Coralogix metrics follow the Prometheus data model, and metrics can be one of the following: Counter, Gauge, and Histogram (Read more about Prometheus data model).</p> <p>The Custom Metric API implementation is based on the stable OpenTelemetry Metric Spec.</p> <p>Here's a sample of both a counter and a gauge:</p> <pre><code>{\n  \"resource_metrics\": {\n    \"scope_metrics\": {\n      \"metrics\": [{\n        \"name\": \"grpc_sample_gauge1\",\n        \"gauge\": {\n          \"data_points\": [{\n            \"as_double\": 0.8,\n            \"attributes\": [{\n              \"key\": \"service.name\",\n              \"value\": {\n                \"string_value\": \"test-service\"\n              }\n              }],\n              \"start_time_unix_nano\": 1657079957000000000,\n              \"time_unix_nano\": 1657079957000000000\n          }]\n        }\n      },{\n        \"name\": \"grpc_sample_counter1\",\n        \"gauge\": {\n          \"data_points\": [{\n            \"as_int\": 100,\n            \"attributes\": [{\n              \"key\": \"service.name\",\n              \"value\": {\n                \"string_value\": \"test-service\"\n              }\n              }],\n              \"start_time_unix_nano\": 1657079957000000000,\n              \"time_unix_nano\": 1657079957000000000\n          }]\n        }\n      }]\n    }\n  }\n}\n</code></pre> <p>* Currently both timestamps as well as <code>service.name</code> are mandatory.</p>"},{"location":"newoutput/custom-metrics/#sending-data-with-grpcurl","title":"Sending Data with grpcurl","text":"<p>gRPC is a modern way of calling APIs on top of HTTP/2. Similar to cURL, grpcurl is a command-line tool used to communicate with gRPC services.</p> <p>Coralogix currently supports gRPC for its custom metrics endpoint.</p> <p>Assuming the example in the data model is saved as \"sample.json\", the following command will send it to Coralogix:</p> <pre><code>grpcurl -v -d @ -H 'Authorization: Bearer e0cxxxx-xxxx-xxxx-xxxx-xxxa08b' &lt;custom-metrics-endpoint&gt; opentelemetry.proto.collector.metrics.v1.MetricsService/Export &lt;sample.json\n</code></pre> <ul> <li> <p>For <code>&lt;custom-metrics-endpoint&gt;</code>, input the Coralogix OpenTelemetry endpoint associated with your Coralogix domain.</p> </li> <li> <p>For the Authorization key, input your Coralogix Send-Your-Data API key.</p> </li> </ul>"},{"location":"newoutput/custom-metrics/#using-java","title":"Using Java","text":"<p>While there are many OpenTelemetry SDKs in this tutorial we will be using Java.</p> <p>In order to get started quickly with OpenTelemetry SDK and Coralogix, follow this sample project.</p> <p>Add to your maven pom.xml the following libraries:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n  &lt;artifactId&gt;opentelemetry-sdk-metrics&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n  &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n  &lt;artifactId&gt;opentelemetry-exporter-otlp&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Code snippet to generate a counter and a gauge:</p> <pre><code>SdkMeterProvider meterProvider = \n      SdkMeterProvider.builder()\n        .registerMetricReader(\n          PeriodicMetricReader.builder(\n            OtlpGrpcMetricExporter.builder()\n              .setEndpoint(\"&lt;custom-metrics-endpoint&gt;\")\n              .addHeader(\"Authorization\", \"Bearer e0cxxxx-xxxx-xxxx-xxxx-xxxa08b\")\n          .build())\n        .build())\n      .build();\n\n    Meter meter = meterProvider.meterBuilder(\"test\").build();\n\n    LongCounter counter = meter\n      .counterBuilder(\"otlp_test_counter1\")\n      .setDescription(\"Processed jobs\")\n      .build();\n\n    counter.add(\n      100l, \n      Attributes.of(AttributeKey.stringKey(\"service.name\"), \"my-test-service\")\n    );\n\n    meter\n      .gaugeBuilder(\"otlp_test_gauge1\")\n      .buildWithCallback(measurement -&gt; {\n        measurement.record(0.8, Attributes.of(AttributeKey.stringKey(\"service.name\"), \"my-test-service\"));\n      });\n\n    meterProvider.forceFlush();\n</code></pre> <p>* Currently the <code>service.name</code> attribute is mandatory on each metric.</p>"},{"location":"newoutput/custom-metrics/#limits-quotas","title":"Limits &amp; Quotas","text":"<p>Coralogix places the following limits on endpoints:</p> <ul> <li> <p>A hard limit of 10MB of data to our OpenTelemetry endpoint, with a recommendation of 2MB</p> </li> <li> <p>A hard limit of 2411724 bytes of data to our Prometheus RemoteWrite endpoint, with a recommendation for any amount less than this limit</p> </li> </ul> <p>Limits apply to single requests, regardless of timespan.</p>"},{"location":"newoutput/custom-metrics/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix EndpointsGitHubGitHub Repo"},{"location":"newoutput/custom-metrics/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/custom-syslog/","title":"Custom Syslog","text":"<p>Seamlessly send Coralogix your logs using a syslog template with a custom format.</p>"},{"location":"newoutput/custom-syslog/#overview","title":"Overview","text":"<p>Syslog is a protocol that computer systems use to send event data logs to an external syslog server for storage. Several applications and SaaS services have the ability to send logs using syslog.</p> <p>For using this kind of integration, use a syslog template with a custom format.</p> <p></p>"},{"location":"newoutput/custom-syslog/#general","title":"General","text":"<p>Private Key: Your Send-Your-Data API key is a unique ID that represents your company.</p> <p>Company ID: A unique number that represents your company. Access your Company ID from the settings tab in your Coralogix dashboard.</p> <p>Application Name: The name of your main application. For example, the \"SuperData\" company might insert the \u201cSuperData\u201d string parameter. If it wants to debug its test environment, it might choose \u201cSuperData\u2013 Test\u201d.</p> <p>SubSystem Name: The name of your subsystem(s), such as \"backend servers\", \"middleware\", \"frontend servers,\" etc. This is required in order to organize and query your data.</p> <p>SyslogEndpoint: Select one of the following Syslog endpoints on the basis of your Coralogix domain.</p>"},{"location":"newoutput/custom-syslog/#syslog-template","title":"Syslog Template","text":"<p>Create a Syslog template with the following variables modified: PRIVATE_KEY, COMPANY_ID, APPLICATION_NAME, and SUBSYSTEM_NAME.</p> <pre><code>{\"fields\": {\"private_key\":\"PRIVATE_KEY\",\"company_id\":\"COMPANY_ID\",\"app_name\":\"APPLICATION_NAME\",\"subsystem_name\":\"SUBSYSTEM_NAME\"},\"message\": {\"message\":\"%msg%\"}}\n\n</code></pre>"},{"location":"newoutput/custom-syslog/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/custom-webhooks-metric-alerts/","title":"Custom Webhooks: Metric Alerts","text":"<p>Set up one or more custom webhooks to define notification settings for your metric alerts.</p>"},{"location":"newoutput/custom-webhooks-metric-alerts/#set-up-a-custom-webhook-for-your-metric-alert","title":"Set Up a Custom Webhook for Your Metric Alert","text":""},{"location":"newoutput/custom-webhooks-metric-alerts/#create-a-custom-webhook","title":"Create a Custom Webhook","text":"<p>STEP 1. Create a generic outbound webhook or other custom webhook.</p>"},{"location":"newoutput/custom-webhooks-metric-alerts/#define-your-metric-alert-notification-settings","title":"Define Your Metric Alert Notification Settings","text":"<p>As you create a metric alert, you must define your alert's Notifications settings. You have different options, depending on whether or not you are using the\u00a0Group By\u00a0condition.</p> <p></p>"},{"location":"newoutput/custom-webhooks-metric-alerts/#using-group-by","title":"Using Group By","text":"<p>When using\u00a0Group By\u00a0conditions, you will see the following options:</p> <ul> <li> <p>Trigger a single alert when at least one combination of\u00a0the group by values meets the condition. A single notification, aggregating all values matching an alert query and conditions, will be sent to your Coralogix Incidents screen.</p> </li> <li> <p>Trigger a separate alert for each combination that meets the condition. Multiple individual notifications for each Group By field value may be sent to your Coralogix\u00a0Incidents Screen\u00a0when query conditions are met. Select one or more keys \u2013 consisting of a subset of the fields selected in the alert conditions \u2013 in the drop-down menu. A separate notification will be sent for each key selected.</p> </li> <li> <p>When grouping by a given Group By field, you must group the metric by this field to allow the data to propagate to the $GROUP_BY_VALUE_1 parameter.</p> </li> <li> <p>The number of\u00a0Group By\u00a0permutations is limited to 1000. If there are more permutations, then only the first 1000 are tracked.</p> </li> </ul> <p>When not using the\u00a0Group By\u00a0condition,\u00a0a single alert will be triggered\u00a0and sent to your\u00a0Incidents Screen\u00a0when the query meets the condition.</p>"},{"location":"newoutput/custom-webhooks-metric-alerts/#define-notification-parameters","title":"Define Notification Parameters","text":"<ul> <li> <p>Notify every. Sets the alert cadence. After an alert is triggered and a notification is sent, the alert will continue to work, but notifications will be suppressed for the duration of the suppression period.</p> </li> <li> <p>Notify when resolved. Activate to receive an automatic update once an alert has ceased.</p> </li> <li> <p>Define additional alert recipient(s) and notification channels by clicking\u00a0+ ADD WEBHOOK.</p> </li> </ul>"},{"location":"newoutput/custom-webhooks-metric-alerts/#additional-resources","title":"Additional Resources","text":"DocumentationMetric Alerts"},{"location":"newoutput/custom-webhooks-metric-alerts/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/dashboard-widgets/","title":"Coralogix Dashboard and Widgets","text":"<p>Coralogix is all about making your life simple. That\u2019s why we put a strong emphasis on a simple and intuitive user experience that provides insights at a glance. An essential part of a great user experience is dashboards and widgets. The right widgets and dashboards are perfect for detecting abnormal activity or keeping an eye on your business KPIs.\u00a0Widgets can be saved personally or for the entire team. They can be pinned to your dashboard, or to your Coralogix \u2018Tags\u2018 view for maximum control over your system\u2019s performance.</p>"},{"location":"newoutput/dashboard-widgets/#built-in-graphs","title":"Built-in Graphs","text":"<p>Coralogix comes with some graphs built in, which can be filtered on application and subsystem. In cases where some of the data sources (e.g Tracing, Metrics or Security) have not been configured, you can choose which graphs will be displayed on the dashboard.</p> <p></p> <p>Here is the list of built-in graphs:</p> <ul> <li>Volume of logs, grouped by severity:</li> </ul> <p></p> <p>This UI enables you to view your logging volumes, to better understand activity in your system and also highlight when your error rates are increasing.</p> <ul> <li>High severity errors and errored traces:</li> </ul> <p></p> <p>This view helps you to zoom in on the error activity in both your logs and your tracing data. Sudden changes in this view may indicate a breaking change or other system impacting event.</p> <ul> <li>Metrics Series used:</li> </ul> <p></p> <ul> <li>High Severity Failed Tests count:</li> </ul> <p></p> <ul> <li>Triggered Alerts statistics on number of occurrences and overall percentage by their severity:</li> </ul> <p></p> <ul> <li>Anomalies on number of occurrences and overall percentage by their severity:</li> </ul> <p></p> <ul> <li>Triggered Alerts information, where you also have the option of snoozing an alert:</li> </ul> <p></p> <ul> <li>Top 3 Abnormal Errors that occurred at above their usual occurrence rates or occurred for the first time in this time frame:</li> </ul> <p></p> <ul> <li>Top Error rate subsystems where the highest error rates are grouped by application and subsystem:</li> </ul> <p></p> <ul> <li>Tracing section where the graphs breakdown is by actions with the longest duration, how many spans the service produces and errors per service:</li> </ul> <p></p>"},{"location":"newoutput/dashboard-widgets/#filtering-and-querying-data","title":"Filtering and Querying Data","text":"<p>You can filter the data brought up within our default graphs and show only the relevant information to you, by using the Applications and Subsystems filter list:</p> <p></p> <p>In addition, Coralogix provides a simple and intuitive interface for creating your dashboard widgets, on queries, patterns, or even parameters within log patterns.</p> <p>You can\u00a0 also select a timeframe from the UI, to further filter your data:</p> <p></p>"},{"location":"newoutput/dashboard-widgets/#add-new-widgets","title":"Add New Widgets","text":""},{"location":"newoutput/dashboard-widgets/#query-based-widgets","title":"Query-Based Widgets","text":"<p>To define a new query-based widget, follow these 3 simple steps:</p> <p>1) Click the Explore icon on your dashboard:</p> <p></p> <p>2) Run a query according to the graph you want to create (e.g IP_geoip.postal_code:\"20149\" AND coralogix.metadata.severity:5):</p> <p></p> <p>3) Click Settings - Pin Graph, then give your widget a name and description. Choose whether to submit your widget straight to your Main Dashboard or the Tag Reports, and choose the timeline. Click Create. That's it!</p> <p></p>"},{"location":"newoutput/dashboard-widgets/#pattern-based-widgets","title":"Pattern-Based Widgets","text":"<p>You can use Coralogix\u00a0Templates\u00a0to define powerful widgets on specific log patterns in just seconds:</p> <p>1) Run a query according to the filters you want to apply and click the \"Templates\" tab:</p> <p></p> <p>2) Click the number in the \"Occurrences\" column, this is the number of occurrences for a pattern in the query timeframe:</p> <p></p> <p>*Notice that you have two values here, 1 (green) is the number of occurrences for that pattern per minute in the current query timeframe and the second (gray) is the normal behavior of that pattern for that day and hour as learned by Coralogix for the past 2 weeks.</p> <p>3) Click \"Pin Graph\" in the top left corner, to give your new widget a name, description, timeframe, and the ability to submit it to your Main Dashboard:</p> <p></p>"},{"location":"newoutput/dashboard-widgets/#parameter-based-widgets","title":"Parameter-Based Widgets","text":"<p>1) Run a query according to the filters you want to apply and click the \"Templates\" tab:</p> <p></p> <p>2) Click a parameter within a log pattern (parameters can be categorical, numeric or free, determined by Coralogix\u2019s algorithm)</p> <p></p> <p>3) Click \"Pin Graph\" in the top left corner, to give your new widget a name, description, timeframe, and submit it to your Main Dashboard:</p> <p></p>"},{"location":"newoutput/dashboard-widgets/#json-value-widget","title":"JSON Value Widget","text":"<p>Coralogix automatically parses JSON format logs to allow you an easier view, smart filtering, and super simple widgets definition.</p> <p>1) Click to select a JSON formatted log, and press\u00a0the 'space' bar.\u00a0</p> <p></p> <p>2) Hover over a JSON value and click it. Then select \"Show Graph for Key\":\u00a0</p> <p></p> <p>3) Click \"Pin Graph\" on the top left of the graph to send this field visualization to your Main Dashboard:\u00a0</p> <p></p> <p>To make the best of Coralogix, you are welcome to schedule your demo and we\u2019ll walk you through all you need to know, step by step, to make you a production monster.</p>"},{"location":"newoutput/data-contained-in-sta-event-types/","title":"Data Contained in STA Event Types","text":"<p>The Coralogix STA is a tool built by Coralogix that can analyze raw traffic packets seen by it by using traffic mirroring (in AWS it's called VPC Traffic Mirroring), collect information about hosts from the hosts themselves by using a Wazuh agent. In addition, Wazuh can collect information from the cloud provider such as AWS from services such as AWS Inspector, AWS CloudTrail and others.</p> <p>As such a broad solution, it produces a variety of different event types, each one with it's own set of fields. In this article I will go over the most common data types that the STA produces, their meaning and important fields by the Subsystem Name they use. If you come across an event type that is not mentioned here please let us know and we'll add it:</p> <ol> <li>zeek_conn - These events provide information about every connection that the Zeek engine of the STA has seen. It will include information about the source and destination IP addresses and ports, MAC addresses, protocol used, bytes sent and received, if the connection was tunnelled, it will contain a linking ID that allows the user to cross reference several logs. This is the list of fields that are included in each such event (fields written in red are core fields and therefore will appear in most of the other events too):<ol> <li>security.event_type - A string that indicates the type of data that is included in the event. Used to create the subsystem name value.</li> <li>security.uid - A unique identifier of the connection. Generated by Zeek automatically.</li> <li>security.source_ip - The IP address of the connection initiator.</li> <li>security.source_geo - The geographical information (both lat/lon coordinates and textual information about the relevant country, city, etc.</li> <li>security.source_port - The source port used by this connection.</li> <li>security.destination_ip - The IP address of the connection responder.</li> <li>security.destination_geo - The geographical information (both lat/lon coordinates and textual information about the relevant country, city, etc.</li> <li>security.destination_port - The destination port used by this connection.</li> <li>security.protocol - The protocol used (e.g. tcp, udp, icmp)</li> <li>security.hostname - A string that indicates the STA instance itself.</li> <li>security.source_filename - The name of the file on the STA that contained the raw information in this event.</li> <li>security.source_filename_pos - The position in the file security.source_filename which this event was read from</li> <li>security.logshipper_before_filters_timestamp - This field is used by Coralogix to help detect and investigate logs shipping issues in the STA.</li> <li>security._write_ts - This field is used by Coralogix to help detect and investigate logs shipping issues in the STA.</li> <li>security.logshipper_before_output_timestamp - This field is used by Coralogix to help detect and investigate logs shipping issues in the STA.</li> <li>security.community_id - This field contains a hash of the source IP, source port, destination IP and destination port. You can use this field to pivot to other logs (such as from a zeek_conn log to a suricata_flow or suricata_alert) about the same connection, for example - see this screenshot: </li> <li>security.security.local_orig - A boolean indicating whether the source IP address is a private IP.</li> <li>security.local_resp - A boolean indicating whether the destination IP address is a private IP.</li> <li>security.duration - The duration of the connection in seconds</li> <li>security.connection_state - The state of the connection according to the following list:<ol> <li>S0: Connection attempt seen, no reply.</li> <li>S1: Connection established, not terminated.</li> <li>SF: Normal establishment and termination. Note that this is the same symbol as for state S1. You can tell the two apart because for S1 there will not be any byte counts in the summary, while for SF there will be.</li> <li>REJ: Connection attempt rejected.</li> <li>S2: Connection established and close attempt by originator seen (but no reply from responder).</li> <li>S3: Connection established and close attempt by responder seen (but no reply from originator).</li> <li>RSTO: Connection established, originator aborted (sent a RST).</li> <li>RSTR: Responder sent a RST.</li> <li>RSTOS0: Originator sent a SYN followed by a RST, we never saw a SYN-ACK from the responder.</li> <li>RSTRH: Responder sent a SYN ACK followed by a RST, we never saw a SYN from the (purported) originator.</li> <li>SH: Originator sent a SYN followed by a FIN, we never saw a SYN ACK from the responder (hence the connection was \u201chalf\u201d open).</li> <li>SHR: Responder sent a SYN ACK followed by a FIN, we never saw a SYN from the originator.</li> <li>OTH: No SYN seen, just midstream traffic (one example of this is a \u201cpartial connection\u201d that was not later closed).</li> </ol> </li> <li>security.history - A combination of the following letters and signs indicating the connection flow between the connected parties:<ol> <li>s - a SYN w/o the ACK bit set</li> <li>h - a SYN+ACK (\u201chandshake\u201d)</li> <li>a - a pure ACK</li> <li>d - packet with payload (\u201cdata\u201d)</li> <li>f - packet with FIN bit set</li> <li>r - packet with RST bit set</li> <li>c - packet with a bad checksum (applies to UDP too)</li> <li>g - a content gap</li> <li>t - packet with retransmitted payload</li> <li>w - packet with a zero window advertisement</li> <li>i - inconsistent packet (e.g. FIN+RST bits set)</li> <li>q - multi-flag packet (SYN+FIN or SYN+RST bits set)</li> <li>^ - connection direction was flipped by Zeek\u2019s heuristic</li> </ol> </li> <li>security.tunnel_parents - If this connection was tunnelled, this list will contain the value of the security.uid field of the events related to the tunnel connection.</li> <li>security.orig_ip_bytes - The bytes sent during this connection by the originator</li> <li>security.resp_ip_bytes - The bytes sent during this connection by the responder</li> <li>security.orig_l2_addr - The MAC address of the connection originator</li> <li>security.resp_l2_addr - The MAC address of the responder</li> </ol> </li> <li>suricata_flow - These events provide information about every connection that the Suricata engine of the STA has seen. It will include information about the source and destination IP addresses and ports, MAC addresses, protocol used, bytes sent and received, if the connection was tunnelled, it will contain a linking ID that allows the user to cross reference several logs. This is the list of fields that are included in each such event (fields written in red are core fields and therefore will appear in most of the other events too):<ol> <li>security.connection_state - The connection state</li> <li>security.connection_started - The timestamp at which the connection has started</li> <li>security.connection_ended - The timestamp at which the connection has ended.</li> <li>security.flow_id - The flow id of the connection. Can be used to locate other Suricata logs related to this connection.</li> <li>security.connection_state_description - The description for the current connection state. E.g. \"timeout\"</li> <li>security.nids_alerted - Whether Suricata has generated an alert about this connection.</li> <li>security.icmp_type - In ICMP connections, this field will indicate the exact type of ICMP request that has been sent.</li> <li>security.app_proto - The application level protocol used in this connection if Suricata could detect it (e.g. http, dns, ssh etc.)</li> <li>security.tcp_flags - TCP flags</li> <li>security.tcp_flags_tc - TCP flags sent to the client</li> <li>security.tcp_flags_ts - TCP flags sent to the server</li> <li>security.tcp_syn - Whether a TCP SYN flag was seen in this connection.</li> <li>security.tcp_ack - Whether a TCP ACK flag was seen in this connection.</li> <li>security.tcp_psh - Whether a TCP PUSH flag was seen in this connection.</li> <li>security.tcp_fin - Whether a TCP FIN flag was seen in this connection.</li> </ol> </li> <li>zeek_dns - These events provide information about DNS requests observed by the Zeek engine of the STA. It will include information about DNS queries performed and the answers provided by the DNS server. In addition, such events may also include some additional fields which contain the results of some enrichment services in the STA (marked in blue). The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.query - The string that was sent to the DNS server as a query.</li> <li>security.query_class - The query class (numeric)</li> <li>security.query_class_name - The query class name (e.g. C_Internet)</li> <li>security.query_type - The requested record type (numeric)</li> <li>security.query_type_name - The requested record type name (e.g. A, PTR, AAAA, TXT, etc.)</li> <li>security.AA - Authoritative answer. Is the response came from a name server that is an authority for the queried domain name.</li> <li>security.TC - Truncated. Has the message been truncated.</li> <li>security.RD - Recursion Desired. Whether the client indicated that the server will handle the DNS recursion for it.</li> <li>security.Z - Reserved flag bit in the DNS request. Usually set to zero.</li> <li>security.RA - Recursion Available. Whether the server allows clients to run recursive queries.</li> <li>security.rtt - Round trip time. The time from the request was sent to the time the response was seen. Can be used to detect delays in the DNS traffic.</li> <li>security.rejected - Whether the query was rejected by the DNS server.</li> <li>security.rcode - The response code (numeric)</li> <li>security.rcode_name - The response code (e.g. NOERROR, NXDOMAIN)</li> <li>security.highest_registered_domain - Domain name parsing enrichment. The STA will attempt to parse the domain name to a TLD, a parent domain name, and subdomains. This part will contain the parent domain name with the TLD - e.g. amazon.com. (from the query tp.47cf2c8c9-frontier.amazon.com)</li> <li>security.parent_domain - Domain name parsing enrichment. See security.highest_registered_domain for more details. This field will contain the parent domain name - e.g. amazon (from the query tp.47cf2c8c9-frontier.amazon.com)</li> <li>security.subdomain - Domain name parsing enrichment. See security.highest_registered_domain for more details. This field will contain the subdomain as parsed from the query - e.g. tp.47cf2c8c9-frontier (from the query tp.47cf2c8c9-frontier.amazon.com)</li> <li>security.top_level_domain - Domain name parsing enrichment. See security.highest_registered_domain for more details. This field will contain the top level domain name as parsed from the query - e.g. com (from the query tp.47cf2c8c9-frontier.amazon.com)</li> <li>security.query_length - Domain name parsing enrichment. See security.highest_registered_domain for more details. This field contains the characters length of the entire query.</li> <li>security.parent_domain_length - Domain name parsing enrichment. See security.highest_registered_domain for more details. This field contains the characters length of the parent domain. This value is very useful in detecting malicious domain names which tend to be quite long.</li> <li>security.subdomain_length - Domain name parsing enrichment. See security.highest_registered_domain for more details. This field contains the characters length of the subdomain.</li> <li>security.info_type - Domain name parsing enrichment. This field contain a value indicating the type of data that was sent in the query - IPv4, IPv6, Domain or Malformed.</li> <li>security.creation_date - Domain stats enrichment. The creation date of the queried domain name. This field can be quite useful in detecting connections (even encrypted ones!!!) to \"baby domains\" i.e. domains that were registered in the past three months or less and therefore are often associated with malicious activities.</li> <li>security.expiration_date - Domain stats enrichment. The expiration date of the queried domain name.</li> <li>security.domain_frequency_score - Frequency score enrichment. This is an NLP score calculated over the queried domain and can assist in determining whether the queried domain name is human generated (value above five) or machine generated (value below five).</li> <li>security.parent_domain_frequency_score - Frequency score enrichment. This is an NLP score calculated over the parent domain queried and can assist in determining whether the queried domain name is human generated (value above five) or machine generated (value below five).</li> <li>security.subdomain_frequency_score - Frequency score enrichment. This is an NLP score calculated over the subdomain queried and can assist in determining whether the queried domain name is human generated (value above five) or machine generated (value below five).</li> <li>security.transaction_id - A random 16 bit ID given to the DNS query by the program that has initiated the DNS request. This value is also returned by the server to help in matching the DNS response to the appropriate request.</li> <li>security.answers - The answers given by the DNS server in the order they were provided by the DNS server.</li> <li>security.TTLs - The TTL (Time To Live) for every record returned by the DNS server.</li> </ol> </li> <li>suricata_dns - These events provide information about DNS requests observed by the Suricata engine of the STA. It will include information about DNS queries performed and the answers provided by the DNS server. In addition, such events may also include some additional fields which contain the results of some enrichment services in the STA (marked in blue). The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.flags - DNS flags as a number.</li> <li>security.qr - An indication if this event represents a DNS query.</li> <li>security.flow_id - An ID used by Suricata to identify the connection. Can be used to pivot to other Suricata logs related to this connection.</li> <li>security.type - Whether that event represents a DNS request or a DNS answer.</li> </ol> </li> <li>zeek_tunnel - These events provide information about tunnels as observed by the Zeek engine in the STA. Note that in AWS VPC traffic mirroring all the mirrored traffic is being wrapped in a VXLAN tunnel on port 4789 by AWS design. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.action - The action performed on the tunnel.</li> <li>security.tunnel_type - The type of tunnel used (e.g. Tunnel::VXLAN)</li> </ol> </li> <li>suricata_alert - These events are sent whenever the Suricata engine in the STA detects a traffic that matches one of its signatures. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.payload - The binary payload that matched the signature, encoded as a Base64 string.</li> <li>security.packet - The raw packet that matched the signature, encoded as a Base64 string.</li> <li>security.classification - The signature's classification (e.g. privilege escalation, data leakage)</li> <li>security.alert - The textual description of the alert</li> <li>security.signature_id - The Suricata's signature ID. If the number is higher than 9,000,000, the signature is a custom signature.</li> <li>security.gid - The Suricata's signature group ID.</li> <li>security.rev - The Suricata's signature revision number.</li> <li>security.priority - The Suricata's signature priority.</li> <li>security.payload_printable - The printable strings found in the payload that matched the signature. This field can be used to create more sophisticated alerts based on the raw Suricata signature.</li> </ol> </li> <li>zeek_notice - These events are sent whenever the Zeek engine in the STA detects a traffic pattern that matches a known traffic pattern or behavior that is known to be malicious or one that should not be permitted in a work environment. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.msg - The notice message. e.g. \"Unique\u00a0queries\u00a0(55q,\u00a0&lt;\u00a01.0\u00a0hr)\u00a0to\u00a0domain:\u00a0compute.internal\u00a0exceeded\u00a0threshold.\"</li> <li>security.sub_msg - The notice type. e.g. AnomalousDNS::Domain_Query_Limit</li> <li>security.suppress_for - The number of seconds this event will be suppressed for.</li> </ol> </li> <li>zeek_weird - These events are similar to zeek_notice but are alerting you about things that might be a problem for your organization but might also be benign. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.name - The name of the anomaly found. E.g \"empty_http_request\"</li> <li>security.notice - Whether this event also generated a zeek_notice event.</li> <li>security.source - What type of data this event is about.</li> </ol> </li> <li>zeek_software - The Zeek engine in the STA continuously attempts to detect the software packages on the servers based on the traffic signatures that they generate. These events contain all the details about all the software packages that have been detected. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.unparsed_version - The software package name as it has been detected before parsing.</li> <li>security.name - The software package name.</li> <li>security.version_major - The major version of the software package.</li> <li>security.version_minor - The minor version of the software package.</li> <li>security.version_additional - Additional version information found in the software name.</li> <li>security.software_type - The type/category of the software that has been detected. E.g. HTTP::BROWSER</li> <li>security.nist_info.CPEs - NIST enrichment. Based on the detected software information, the STA will automatically attempt to correlate that data to data from the NIST NVD. When such a match is found this field will contain the matching CPEs (Common Product Enumerations)</li> <li>security.nist_info.CVEs - NIST enrichment. Based on the detected software information, the STA will automatically attempt to correlate that data to data from the NIST NVD. When such a match is found this field will contain the matching CVEs (Common Vulnerabilities Enumerations) and links to the NIST website about them.</li> </ol> </li> <li>zeek_ntp - These events contain information about NTP (Network Time Protocol) sessions seen in the traffic. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.mode - The NTP mode being used. Possible values are:<ol> <li>symmetric active</li> <li>symmetric passive</li> <li>client</li> <li>server</li> <li>broadcast</li> <li>NTP control message</li> <li>reserved for private use</li> </ol> </li> <li>security.ref_time - Reference timestamp. Time when the system clock was last set or correct.</li> <li>security.root_delay - The total round-trip delay to the reference clock.</li> <li>security.root_disp - Root Dispersion. The total dispersion to the reference clock.</li> <li>security.poll - The maximum interval between successive messages.</li> <li>security.rec_time - Reference timestamp. Time when the system clock was last set or correct.</li> <li>security.precision - The precision of the system clock.</li> <li>security.version - The NTP version number (1, 2, 3, 4).</li> <li>security.num_exts - Number of extension fields (which are not currently parsed).</li> <li>security.xmt_time - Transmit timestamp. Time at the server when the response departed</li> <li>security.ref_id - Reference ID. For stratum 1, this is the ID assigned to the reference clock by IANA. For example: GOES, GPS, GAL, etc. (see RFC 5905)</li> <li>security.stratum - This value mainly identifies the type of server (primary server, secondary server, etc.). Possible values, as in RFC 5905, are:<ul> <li>0 -&gt; unspecified or invalid</li> <li>1 -&gt; primary server (e.g., equipped with a GPS receiver)</li> <li>2-15 -&gt; secondary server (via NTP)</li> <li>16 -&gt; unsynchronized</li> <li>17-255 -&gt; reserved</li> </ul> </li> <li>security.org_time - Origin timestamp. Time at the client when the request departed for the NTP server.</li> <li>security.kiss_code - For stratum 0, four-character ASCII string used for debugging and monitoring. Values are defined in RFC 1345.</li> <li>security.ref_addr - Above stratum 1, when using IPv4, the IP address of the reference clock. Note that the NTP protocol did not originally specify a large enough field to represent IPv6 addresses, so they use the first four bytes of the MD5 hash of the reference clock\u2019s IPv6 address (i.e. an IPv4 address here is not necessarily IPv4).</li> <li>security.key_id - Key used to designate a secret MD5 key.</li> <li>security.digest - MD5 hash computed over the key followed by the NTP packet header and extension fields.</li> </ol> </li> <li>suricata_ssh - These events include information about SSH sessions that were detected in the traffic. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.ssh.server.hassh.string - SSH server fingerprinting string. You can read more about it here: https://github.com/salesforce/hassh.</li> <li>security.ssh.server.hassh.hash - A hash of the SSH server fingerprint string.</li> <li>security.ssh.server.proto_version - SSH protocol version on the server.</li> <li>security.ssh.server.software_version - The server's SSH application</li> <li>security.ssh.client.hassh.string - SSH client fingerprinting string. You can read more about it here: https://github.com/salesforce/hassh.</li> <li>security.ssh.client.hassh.hash - A hash of the SSH client fingerprint string.</li> <li>security.ssh.client.proto_version - SSH protocol version on the client.</li> <li>security.ssh.client.software_version - The client's SSH application.</li> </ol> </li> <li>zeek_ssh - These events include information about SSH sessions that were detected in the traffic. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.mac_alg - The signing (MAC) algorithm in use</li> <li>security.kex_alg - The key exchange algorithm in use</li> <li>security.auth_attempts - The number of authentication attemps we observed. There\u2019s always at least one, since some servers might support no authentication at all. It\u2019s important to note that not all of these are failures, since some servers require two-factor auth (e.g. password AND pubkey)</li> <li>security.version - SSH major version (1, 2, or unset). The version can be unset if the client and server version strings are unset, malformed or incompatible so no common version can be extracted. If no version can be extracted even though both client and server versions are set a weird will be generated.</li> <li>security.client - The client\u2019s version string</li> <li>security.server - The server\u2019s version string</li> <li>security.host_key_alg - The server host key\u2019s algorithm</li> <li>security.cshka - Client host key algorithms</li> <li>security.host_key - The server\u2019s key fingerprint</li> <li>security.sshka - Server host key algorithms</li> <li>security.direction - Direction of the connection. If the client was a local host logging into an external host, this would be OUTBOUND. INBOUND would be set for the opposite situation.</li> <li>security.compression_alg - Compression algorithm preferences</li> <li>security.hasshAlgorithms - SSH client fingerprinting string. You can read more about it here: https://github.com/salesforce/hassh.</li> <li>security.hasshServerAlgorithms - SSH server fingerprinting string. You can read more about it here: https://github.com/salesforce/hassh.</li> <li>security.hasshVersion - The version of the HASSH algorithm used to fingerprint the server and client.</li> <li>security.cipher_alg - The encryption algorithm in use</li> <li>security.hasshServer - A hash of the SSH server fingerprint string.</li> <li>security.hassh - A hash of the SSH client fingerprint string.</li> </ol> </li> <li>suricata_http - These events contain information about HTTP sessions (and HTTPS if decrypted in the STA) as detected by Suricata. Note that many fields here appear also under the DNS information because the STA enriches the HTTP virtualhost field in a very similar way to the enrichment performed on the DNS query. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.virtualhost - The host name specified in URL on the client side.</li> <li>security.virtualhost_length - The length of the virtual host name. Can be used to detect very long domain names which are often used in attack campaigns. Also can be used to help reduce false positives caused by the NLP based score.</li> <li>security.response_body_length - The length of the HTTP response. Very long response body can be an indication of a data leak.</li> <li>security.http_http_user_agent - The user-agent used. Unusual values here can indicate an on going attack campaign.</li> <li>security.uri - The HTTP URI used by the client.</li> <li>security.method - The HTTP method used by the client. (e.g. GET, POST, PUT, DELETE, etc.)</li> <li>security.tx_id - A per-flow incrementing \u201ctx_id\u201d value.</li> </ol> </li> <li>zeek_http - These events contain information about HTTP sessions (and HTTPS if decrypted in the STA) as detected by Zeek. Note that many fields here appear also under the DNS information because the STA enriches the HTTP virtualhost field in a very similar way to the enrichment performed on the DNS query. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.status_code - The numeric status code returned from the server for that HTTP request. E.g. 400, 500, 301, 200, etc.</li> <li>security.status_message - The textual status message returned from the server for that HTTP request. E.g. \"Not Modified\"</li> <li>security.tags_list - A set of indicators of various attributes discovered and related to a particular request/response pair.</li> <li>security.trans_depth - Represents the pipelined depth into the connection of this request/response transaction.</li> <li>security.version - The HTTP version used.</li> </ol> </li> <li>zeek_ssl - These events contain information about TLS and SSL sessions as detected by Zeek. Note that many fields here appear also under the DNS information because the STA enriches the TLS/SSL server_name field in a very similar way to the enrichment performed on the DNS query. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.curve - Elliptic curve the server chose when using ECDH/ECDHE.</li> <li>security.ja3s - A hash that is used as a fingerprint of the server's SSL/TLS protocol stack. See more here: https://github.com/salesforce/ja3.</li> <li>security.ja3 - A hash that is used as a fingerprint of the client's SSL/TLS protocol stack. See more here: https://github.com/salesforce/ja3.</li> <li>security.ssl_history - SSL history showing which types of packets we received in which order. Letters have the following meaning with client-sent letters being capitalized:<ul> <li>H hello_request</li> <li>C client_hello</li> <li>S server_hello</li> <li>V hello_verify_request</li> <li>T NewSessionTicket</li> <li>X certificate</li> <li>K server_key_exchange</li> <li>R certificate_request</li> <li>N server_hello_done</li> <li>Y certificate_verify</li> <li>G client_key_exchange</li> <li>F finished</li> <li>W certificate_url</li> <li>U certificate_status</li> <li>A supplemental_data</li> <li>Z unassigned_handshake_type</li> <li>I change_cipher_spec</li> <li>B heartbeat</li> <li>D application_data</li> <li>E end_of_early_data</li> <li>O encrypted_extensions</li> <li>P key_update</li> <li>M message_hash</li> <li>J hello_retry_request</li> <li>L alert</li> <li>Q unknown_content_type</li> </ul> </li> <li>security.version - SSL/TLS version that the server chose.</li> <li>security.server_version - Numeric version of the server in the server hello</li> <li>security.client_version - Numeric version of the client in the client hello</li> <li>security.cipher - SSL/TLS cipher suite that the server chose.</li> <li>security.resumed - Flag to indicate if the session was resumed reusing the key material exchanged in an earlier connection.</li> <li>security.established - Flag to indicate if this ssl session has been established successfully, or if it was aborted during the handshake.</li> <li>security.server_name - Value of the Server Name Indicator SSL/TLS extension. It indicates the server name that the client was requesting.</li> <li>security.validation_status - Result of certificate validation for this connection.</li> <li>security.valid_ct_logs - Number of different Logs for which valid SCTs were encountered in the connection.</li> <li>security.valid_ct_operators - Number of different Log operators of which valid SCTs were encountered in the connection.</li> <li>security.cert_chain_fps - Chain of certificates (file IDs that can be searched in zeek_files logs) offered by the server to validate its complete signing chain.</li> <li>security.client_cert_chain_fps - Chain of certificates (file IDs that can be searched in zeek_files logs) offered by the client to validate its complete signing chain.</li> <li>security.sni_matches_cert - Set to true if the hostname sent in the SNI matches the certificate. Set to false if they do not match. Unset if the client did not send an SNI.</li> <li>security.subject - Subject of the X.509 certificate offered by the server.</li> <li>security.issuer - Issuer of the signer of the X.509 certificate offered by the server.</li> <li>security.client_subject - Subject of the X.509 certificate offered by the client.</li> <li>security.client_issuer - Subject of the signer of the X.509 certificate offered by the client.</li> <li>security.server_depth - Current number of certificates seen from either side. Used to create file handles.</li> <li>security.oscp_status - Result of ocsp validation for this connection.</li> </ol> </li> <li>suricata_tls - These events contain information about TLS and SSL sessions as detected by Suricata. Note that many fields here appear also under the DNS information because the STA enriches the TLS/SSL server_name field in a very similar way to the enrichment performed on the DNS query. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.tls_notbefore - The NotBefore field from the TLS certificate</li> <li>security.tls_notafter - The NotAfter field from the TLS certificate</li> <li>security.tls_fingerprint - The (SHA1) fingerprint of the TLS certificate</li> <li>security.tls_serial - The serial number of the TLS certificate</li> <li>security.certificate_subject_CN - The common name from the subject of the TLS certificate</li> <li>security.certificate_subject_L - The location registered on the subject of the TLS certificate.</li> <li>security.certificate_subject_O - The organization field from the subject of the TLS certificate</li> <li>security.certificate_subject - The subject field from the TLS certificate</li> <li>security.certificate_issuer_CN - The common name from the subject of the issuer's certificate</li> <li>security.certificate_issuer_O - The organization from the subject of the issuer's certificate</li> <li>security.certificate_issuer_C - The country code from the subject of the issuer's certificate</li> <li>security.certificate_issuer - The issuer field from the TLS certificate</li> </ol> </li> <li>zeek_files - These events contain information about files that have traversed the network as detected and analyzed by Zeek. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.depth - A value to represent the depth of this file in relation to its source. In SMTP, it is the depth of the MIME attachment on the message. In HTTP, it is the depth of the request within the TCP connection.</li> <li>security.analyzers - A set of analysis types done during the file analysis.</li> <li>security.mime_type - A mime type provided by the strongest file magic signature match against the\u00a0bof_buffer\u00a0field, or in the cases where no buffering of the beginning of file occurs, an initial guess of the mime type based on the first data seen.</li> <li>security.is_orig - If the source of this file is a network connection, this field indicates if the file is being sent by the originator of the connection or the responder.</li> <li>security.local_orig - If the source of this file is a network connection, this field indicates if the data originated from the local network or not</li> <li>security.sha1 - A SHA1 digest of the file contents.</li> <li>security.source - An identification of the source of the file data. E.g. it may be a network protocol over which it was transferred, or a local file path which was read, or some other input source.</li> <li>security.timedout - Whether the file analysis timed out at least once for the file.</li> <li>security.uids - Connection UIDs over which the file was transferred.</li> <li>security.fuid - An identifier associated with a single file.</li> <li>security.missing_bytes - The number of bytes in the file stream that were completely missed during the process of analysis e.g. due to dropped packets.</li> <li>security.overflow_bytes - The number of bytes in the file stream that were not delivered to stream file analyzers. This could be overlapping bytes or bytes that couldn\u2019t be reassembled.</li> <li>security.md5 - An MD5 digest of the file contents.</li> <li>security.seen_bytes - Number of bytes provided to the file analysis engine for the file.</li> <li>security.extracted - Local filename of extracted file. (will be uploaded to the S3 bucket that has been configured to hold the exported packets)</li> <li>security.parent_fuid - Identifier associated with a container file from which this one was extracted as part of the file analysis.</li> </ol> </li> <li>suricata_fileinfo - These events contain information about files that have traversed the network as detected and analyzed by Suricata. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.description - A textual description of the file</li> <li>fileinfo_state - The state of the file analysis</li> </ol> </li> <li>zeek_x509 - These events contain information about X.509 certificates that have traversed the network as detected and analyzed by Zeek. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.certificate_issuer - The certificate issuer string</li> <li>security.certificate_issuer_CN - The certificate issuer's common name</li> <li>security.certificate_issuer_CN_length - The certificate issuer's common name length</li> <li>security.certificate_issuer_O - The certificate issuer's organization</li> <li>security.certificate.key_type - The asymmetric key algorithm (e.g. rsa)</li> <li>security.certificate_key_length - The length of the certificate's key</li> <li>security.certificate.not_valid_after - The date which beyond it the certificate will become invalid</li> <li>security.certificate.not_valid_before - The date that before it the certificate is not yet valid</li> <li>security.certificate.version - The version of the X.509 standard used</li> <li>security.certificate.exponent - The exponent part of the certificate</li> <li>security.certificate_subject - The raw subject string of the certificate</li> <li>security.certificate_subject_CN - The common name of the the certificate</li> <li>security.certificate_subject_CN_length - The length of the common name on the certificate</li> <li>security.certificate_subject_CN_frequency_score - Frequency score enrichment. This is an NLP score calculated over the certificate's common name and can assist in determining whether the certificate common name is human generated (value above five) or machine generated (value below five).</li> <li>security.certificate_subject_O - The organization specified on the certificate's subject</li> <li>security.certificate_subject_C - The country code specified on the certificate's subject</li> <li>security.basic_constraints.ca - Basic constraints extension of the certificate.</li> <li>security.certificate.serial - The certificate's serial number</li> <li>security.fingerprint - The fingerprint of the certificate</li> <li>security.certificate_key_algorithm - The key generation algorithm specified in the certificate.</li> <li>security.certificate_signing_algorithm - The signing algorithm specified in the certificate.</li> <li>security.client_cert - Indicates if this certificate was sent from the client</li> <li>security.host_cert - Indicates if this certificate was a end-host certificate, or sent as part of a chain</li> <li>security.san.dns - Subject alternative name extension of the certificate.</li> </ol> </li> <li>zeek_ocsp - These events contain information about OCSP (Online Certificate Status Protocol) sessions as decoded and parsed by Zeek. These sessions are used by TLS clients to verify that a certificate they have received hasn't been revoked. The following fields are included in the event (fields that are also included in previously mentioned event types are not listed here):<ol> <li>security.hashAlgorithm - Hash algorithm used to generate issuerNameHash and issuerKeyHash.</li> <li>security.thisUpdate - The time at which the status being shows is known to have been correct.</li> <li>security.nextUpdate - The latest time at which new information about the status of the certificate will be available.</li> <li>security.revoketime - Time at which the certificate was revoked.</li> <li>security.revokereason - Reason for which the certificate was revoked.</li> <li>security.certStatus - Status of the affected certificate.</li> <li>security.issuerNameHash - Hash of the issuer\u2019s distingueshed name.</li> <li>security.issuerKeyHash - Hash of the issuer\u2019s public key.</li> <li>security.serialNumber - Serial number of the affected certificate.</li> <li>security.id - File id of the OCSP reply.</li> </ol> </li> <li>wazuh - These events contain information that arrived from your Wazuh agents that you have deployed in your network. These events can contain information about the operations conducted by the Wazuh manager (where security.source_filename == \"/coralogix/sta/logs/wazuh/ossec.log\") or information about the instances that have the Wazuh agent installed and configured (where security.source_filename == \"/coralogix/sta/logs/wazuh/alerts/alerts.json\"). These events are very detailed so this list will contain the most commonly used fields. If you came across a field that you don't know what it is for please let us know and we'll add it here:<ol> <li>security.wazuh_rule - Information about the rule that has triggered this event, its level (severity), the times it has fired in the past, the security regulations it is related to (e.g. GDPR, PCI DSS, TSC, etc.), the description assigned to it and its classification group.</li> <li>security.location - The \"part\" of Wazuh that has sent this event.</li> <li>security.agent - Details about the agent from which the event was sent: Its name, IP address and ID.</li> <li>security.manager - Details about the Wazuh manager that was contacted.</li> <li>security.data.vulnerability - Information about a vulnerability that has been detected during a vulnerability scan. The CVE number, its name, information from common attack frameworks such as CVSS, MITRE and others, references to online information about the vulnerability detected, its severity, the date it became public and more.</li> <li>security.data.sca - Details about a SCA tests conducted and their results.</li> </ol> </li> <li>zeek_broker - The Zeek engine in the STA is comprised of multiple processes that communicate internally. These logs are used by Coralogix personnel to monitor the health of the STA and to diagnose its current status in case there's a problem. The data here doesn't concern the security of your environment.</li> <li>zeek_cluster - The Zeek engine in the STA is comprised of multiple processes that communicate internally. These logs are used by Coralogix personnel to monitor the health of the STA and to diagnose its current status in case there's a problem. The data here doesn't concern the security of your environment.</li> <li>syslog - These logs contain information about the overall health of the STA instance and its services. These logs are used by Coralogix personnel to monitor the health of the STA and to diagnose its current status in case there's a problem. The data here doesn't concern the security of your environment. If you see a sudden steep rise in the number of logs of this type it can indicate a problem in the STA.</li> </ol>"},{"location":"newoutput/data-enrichment-coralogix-sta/","title":"Data Enrichment in Coralogix STA","text":"<p>The raw packet data that is passed through Coralogix is enriched with the following fields, in addition to protocol specific fields, to facilitate the creation of effective and powerful alerting rules that will accurately detect security related threats:</p> <p>Connection state (security.connection_state, security.connection_state_description) - Allows you to filter, in alerts as well as in searches, only for successful or unsuccessful connections or connections at a certain stage. For example, by using this field it is possible to detect SYN flooding or SYN based reconnaissance.</p> <p>Whether the connection is established (security.established) - Allows you to filter, in alerts as well as in searches, only for connections that have succeeded or failed. For example, you might want to measure, in a dashboard, the ratio between established and not established connections to your front end servers to detect connectivity issues to them that may result from or be an indication of DDoS attack that is in progress.</p> <p>Connection duration (security.duration) - Indicates the connection\u2019s duration. Each protocol has a typical connection length, for example HTTP connections are relatively short since it\u2019s a connectionless protocol while SSH connections can be much longer. By identifying the correct threshold for your organization for common protocols you can detect attacks such as slow post attacks which are essentially non-volumetric denial of service attacks.</p> <p>Private/Public Source/Destination IP indications (security.local_orig, security.local_respond) - Makes it easier to create rules that apply only to outbound or inbound connections.</p> <p>Bytes transferred from each connected party (security.original_ip_bytes, security.respond_ip_bytes) - Makes it easier to detect large data leakage issues, unusual high number of stale connections and many other important issues. Source &amp; Destination geographic location details (security.geo. city_name, security._geo.continent_code, security._geo.country_code2, security._geo.country_code3, security._geo.country_name, security._geo.location) - Allows you to create whitelist or blacklist based alerts that will fire when a connection from/to an unexpected geographic location is detected.</p> <p>NLP based score for domain names (security.frequency_scores, security.highest_registered_domain_frequency_score) - Many types of attacks nowadays rely on a technique known as Domain Generation Algorithm, Coralogix automatically calculates a score for each domain, domain part certificate names and much more. This score is based on the frequency of letter combinations in the text and the expected letter combinations in English. By using this score, in alert rules it would be possible to detect DGA usage.</p> <p>Domain creation date (security.creation_date) - Coralogix will enrich domain names with the date at which they were created. This is since that younger domains are often involved in malicious activity such as DGAs and phishing attempts. By using this field you can easily create an alert that alerts you if a computer is trying to access young domains.</p> <p>Domain parts (security.highest_registered_domain, security.parent_domain, security.subdomain, security.tld.subdomain, security.top_level_domain) - Allows you to filter for specific types of domains without having to rely on complex regular expressions.</p> <p>Domain parts lengths (security.parent_domain_length, security.subdomain_length) - Since DGA tends to be quite long, the length of parts of the domain can be used to create alerts that will help in the detection of such cases.</p> <p>Source &amp; Destination ASN (Autonomous System Number) details - allows you to detect any suspicious activity originating from certain IP addresses associated with malicious intent and create alerts based on it.</p> <pre><code>{\n    \"security\": {\n\n        ...\n\n        \"source_ip_as_number\": {\n            \"as_number\": 15169,\n            \"as_number_country_code\": \"US\",\n            \"as_number_description\": \"GOOGLE\"\n        }\n\n        ...\n\n        \"destination_ip_as_number\": {\n            \"as_number\": 396982,\n            \"as_number_country_code\": \"US\",\n            \"as_number_description\": \"GOOGLE-CLOUD-PLATFORM\"\n        }\n\n        ...\n\n    }\n}\n</code></pre>"},{"location":"newoutput/data-usage/","title":"Data Usage","text":"<p>View your Data Usage Report to receive an overview of your account usage for the previous 90 days.</p> <p>View your Detailed Data Usage Report to receive an overview of all data sent, per policy, for either the current month or retroactively 30 or 90 days.</p> <p>Access both by clicking Settings &gt; Data Usage in your navigation pane.</p>"},{"location":"newoutput/data-usage/#data-usage-report","title":"Data Usage Report","text":""},{"location":"newoutput/data-usage/#overview","title":"Overview","text":"<p>The Data Usage Report includes:</p> <ul> <li> <p>Usage chart. This includes two usage columns, with the green column presenting the actual amount of data sent per day and the purple column presenting the units for which you've been charged per day in conformity with your TCO Optimizer settings.</p> </li> <li> <p>DAILY MAX. The day in which the largest amount of data was sent.</p> </li> <li> <p>TOTAL. The sum of all days.</p> </li> <li> <p>MAX SAVED. Total data sent minus the total amount of counted units for a given period - 90 days, 30 days, or the current month.</p> </li> </ul> <p>Hover over any day to view a tooltip that includes information on the actual GB sent for each type of priority category and how many counted units it is worth.</p> <p></p> <p>Notes:</p> <ul> <li>Units available per day are equal to your team\u2019s daily quota or maximum daily usage (if all your data is assigned as HIGH priority). This is the default setting for any set of logs or traces unless it is modified in the TCO Optimizer.</li> </ul> Logs PrioritySent dataUnitsHIGH (Frequent search)1 GB0.75MEDIUM (Monitoring)1 GB0.32LOW (Compliance)1 GB0.12BLOCK1 GB0.065METRICS30 GB1 Traces PrioritySent dataUnitsHIGH (Frequent search)1 GB0.5MEDIUM (Monitoring)1 GB0.25LOW (Compliance)1 GB0.1 <ul> <li> <p>In the example above, 185.80 units were sent, but the customer - whose plan quota is 300 units - is only charged for 62.75 units given his / her TCO Optimizer settings.</p> </li> <li> <p>Data blocked is also presented as \"counted units\u201d, with the charge 8% - the cost of the network traffic ingest.</p> </li> <li> <p>Current day information is not updated live.</p> </li> </ul>"},{"location":"newoutput/data-usage/#export-your-data-usage-report","title":"Export Your Data Usage Report","text":"<p>Export the Data Usage Report as a CSV file.</p> <p>STEP 1. In the navigation bar, click Settings &gt; Data Usage.</p> <p>STEP 2. Click Export as CSV.</p> <p>STEP 3. Select the Data Usage Overview Report type.</p> <p>STEP 4. Select the time range (either Current Month, Last 30 days, or Last 90 days).</p> <p>STEP 5. Click EXPORT.</p>"},{"location":"newoutput/data-usage/#detailed-data-usage-report","title":"Detailed Data Usage Report","text":""},{"location":"newoutput/data-usage/#overview_1","title":"Overview","text":"<p>The Detailed Data Usage Report shows you all data sent, per policy, for either the current month or retroactively 30 or 90 days. Use our Data Usage Service API in support of this feature.</p> <p>When exported as a CSV file, it includes the following details:</p> <ul> <li> <p>Date</p> </li> <li> <p>Application</p> </li> <li> <p>Subsystem</p> </li> <li> <p>Severity</p> </li> <li> <p>TCO Priority (if applicable)</p> </li> <li> <p>Type (logs, spans or metrics)</p> </li> <li> <p>GB sent</p> </li> <li> <p>Units sent</p> </li> </ul>   ![](images/Data-Usage-CSV.png)    Example of an exported CSV file including logs, metrics and traces (spans).   <p>This enables you to see for each policy on a daily basis which TCO priority it was sent to, how much data was ingested, and how many units were used in practice for that policy. The report also contains the daily metric and traces usage for the policy, which is sent once a day.</p>"},{"location":"newoutput/data-usage/#export-your-detailed-data-usage-report","title":"Export Your Detailed Data Usage Report","text":"<p>Export the Detailed Data Usage Report as a CSV file.</p> <p>STEP 1. In the navigation bar, click Settings &gt; Data Usage.</p> <p></p> <p>STEP 2. Click Export as CSV.</p> <p>STEP 3. Select the Detailed Usage Report type.</p> <p>STEP 4. Select the time range (either Current Month, Last 30 days, or Last 90 days).</p> <p>STEP 5. Click EXPORT.</p>"},{"location":"newoutput/data-usage/#enabling-data-usage-metrics","title":"Enabling Data Usage Metrics","text":"<p>Enable Data Usage Metrics for an added layer of granularity in your data usage overview. Use this feature to create custom dashboards, insights, alerts, and useful summaries of your data. Find out more here.</p> <p></p>"},{"location":"newoutput/data-usage/#additional-resources","title":"Additional Resources","text":"DocumentationData Usage MetricsAPIData Usage Service API"},{"location":"newoutput/data-usage/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/data-usage-metrics/","title":"Data Usage Metrics","text":"<p>Enable Data Usage Metrics for an added layer of granularity in your data usage overview. Use this feature to create custom dashboards, insights, alerts, and useful summaries of your data.</p> <p></p>"},{"location":"newoutput/data-usage-metrics/#overview","title":"Overview","text":"<p>The Data Usage Metrics feature creates three new metrics: GB Sent, Counted Units, and Daily Quota. These metrics are counted towards your team\u2019s daily metrics quota. With these new metrics, you will be able to create custom dashboards, insights, alerts, and an overview of your data.</p>"},{"location":"newoutput/data-usage-metrics/#enabling-data-usage-metrics","title":"Enabling Data Usage Metrics","text":"<p>Enable the data usage metrics feature after having configured your s3 metrics bucket.</p> <p>Once enabled, Coralogix begins an auto-populate process to provide data for the past 24 hours. This process may take up to two hours.</p> <p>STEP 1. In the navigation bar, click Settings &gt; Data Usage.</p> <p></p> <p>STEP 2. On the Data Usage page, toggle Enable data usage metrics.</p> <ul> <li>If you do not have a required S3 metrics bucket configured, the toggle will be disabled.</li> </ul> <p>STEP 3. View your new data usage metrics using our custom dashboards. Metric names will appear as:</p> <ul> <li> <p><code>cx_data_usage_units</code> </p> </li> <li> <p><code>cx_data_usage_bytes_total</code></p> </li> <li> <p><code>cx_data_plan_units_per_day</code></p> </li> </ul> <p>Step 4. Use the data usage metrics to define useful alerts to monitor your data consumption and quota.</p>"},{"location":"newoutput/data-usage-metrics/#labels","title":"Labels","text":"<p>The information gathered on the new metrics includes the following labels:</p> LabelDescription<code>pillar</code>Type of traffic (logs, spans, metrics) in each data point<code>subsystem_name</code>Subsystem that generated the traffic (used by the logs and spans pillars)<code>application_name</code>Application that generated the traffic (used by the logs and spans pillars)<code>priority</code>TCO priority (high, medium, low, blocked)<code>severity</code>Log severity (critical, error, warn, info, debug, trace) used by the logs pillar<code>blocking_reason</code>_typeThe reason why traffic was blocked. The value can be either <code>tco_policy</code>\u00a0or <code>parsing_rule</code>.<code>blocking_reason_name</code>The name of the TCO policy or parsing rule which is blocking traffic."},{"location":"newoutput/data-usage-metrics/#additional-resources","title":"Additional Resources","text":"APIData UsageData Usage Service API"},{"location":"newoutput/data-usage-metrics/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/data-usage-service-api/","title":"Data Usage Service API","text":"<p>Coralogix provides an API in support of our Detailed Data Usage Report, which presents you with all data sent, per policy, for either the current month or retroactively 30 or 90 days. The API allows you to query your data consumption in given a time period.</p>"},{"location":"newoutput/data-usage-service-api/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Coralogix Alerts, Rules and Tags AP Key. Access this in your navigation pane by clicking Data Flow &gt; API Keys.</p> </li> <li> <p>Management API Endpoint</p> </li> </ul>"},{"location":"newoutput/data-usage-service-api/#fetch-detailed-data-usage-group-by-application-and-subsystem-name","title":"Fetch Detailed Data Usage Group by Application and Subsystem Name","text":"<pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.datausage.v2.DataUsageService/GetTeamDetailedDataUsage &lt;&lt;EOF\n{\n  \"resolution\": \"6h\",\n  \"date_range\": {\n    \"from_date\": \"2023-03-20T01:30:15.01Z\",\n      \"to_date\": \"2023-03-21T01:30:15.01Z\"\n  }\n}\nEOF\n\n</code></pre>"},{"location":"newoutput/data-usage-service-api/#request-args","title":"Request args","text":"Field Description Resolution Describes the precision by which to group your data. In this example, the response contains the data usage per Application and Subsystem every 6h Team Id ID of the team you are seeking Date Range Date range of the requested data in ISO 8601 format"},{"location":"newoutput/data-usage-service-api/#response","title":"Response","text":"<p>The response will be a list of data, such as the following:</p> <pre><code>[{\n  \"timestamp\": \"2023-03-20T16:00:00Z\",\n  \"sizeGb\": 0.000011989847,\n  \"units\": 0.000004795939,\n  \"dimensions\": [\n    {\n      \"tier\": \"TCO_TIER_HIGH\"\n    },\n    {\n      \"genericDimension\": {\n        \"key\": \"subsystem_name\",\n        \"value\": \"vzmgr-server\"\n      }\n    },\n    {\n      \"pillar\": \"PILLAR_LOGS\"\n    },\n    {\n      \"severity\": \"SEVERITY_CRITICAL\"\n    },\n    {\n      \"genericDimension\": {\n        \"key\": \"application_name\",\n        \"value\": \"staging\"\n      }\n    }\n  ]\n}]\n\n</code></pre> Field Description Field Description timestamp Date of the sample sizeGb Size in GB of the processed data units Amount in units dimension List of dimensions by which data has been grouped genericDimension Generic label of the data. Example: application_name and subsystem_name. tier Data priority label: TCO_TIER_HIGH, TCO_TIER_MEDIUM, TCO_TIER_LOW, TCO_TIER_BLOCKED pillar Pillar information: PILLAR_LOGS, PILLAR_METRICS, PILLAR_SPANS severity Severity just for PILLAR_LOGS: SEVERITY_UNSPECIFIED, SEVERITY_DEBUG, SEVERITY_VERBOSE, SEVERITY_INFO, SEVERITY_WARNING,SEVERITY_ERROR,SEVERITY_CRITICAL"},{"location":"newoutput/data-usage-service-api/#additional-resources","title":"Additional Resources","text":"<p>Data Usage</p>"},{"location":"newoutput/data-usage-service-api/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/datamap/","title":"DataMap","text":"<p>DataMap is a new way to create a visual layer for your data, whether logs, metrics, or security. This enables you to combine infrastructure metrics, business metrics, and log data, and then compose maps that represent the structure and quality of your business.</p> <p>DataMaps are unlimited. Any metric, label, or log key can be utilized to represent a different way for you to understand your data.\u00a0</p> <p>By creating multiple POVs, based on different perspectives and goals, you can unlock more dimensions of the same log or metric records.\u00a0</p> <p>Example #1: </p> <p>A host can represent a dimension you want to monitor with criteria based on infrastructure metrics, but it is also the home of your k8s pods and can be used as a way to group them under a cloud resource.\u00a0</p> <p>Example #2: </p> <p>A log key representing 'country' can be used to monitor the performance that your users are experiencing based on their location. It can also be used as the drill-down layer when you want to check which countries are affected by a specific problematic k8s pod.\u00a0</p> <p>DataMap consists of a base metric and groups/labels associated with them which define the map hierarchy.</p> <p>It allows you to enrich any structure with a health status by applying metric or log-based thresholds to each hierarchy level in your map.</p>"},{"location":"newoutput/datamap/#setup","title":"Setup","text":"<ul> <li> <p>Under the Explore section - Click \u201cDataMap\u201d\u00a0</p> </li> <li> <p>Click on \u201c+New Group\u201d and choose the base Metric / Log criteria that will be used to define the structure of the map.</p> </li> </ul> <p></p> <ul> <li> <p>Clicking\u00a0 \u201cGenerate groups\u201d will automatically retrieve, choose &amp; arrange default labels from the chosen base metric or log criteria.  </p> <p>You can also customize the groups by choosing different labels and changing the order according to the map you want to build.</p> </li> </ul> <p></p> <p></p>"},{"location":"newoutput/datamap/#data-source","title":"Data source","text":"<p>Group Name - You can set any group name</p> <p>Sort By - You can sort the metric values Alphanumeric, Direct Children Count, or Status</p> <p>Limit - Limits the hierarchy level shown</p>"},{"location":"newoutput/datamap/#group-status-settings","title":"Group Status Settings","text":"<p>Metric Name - you can define the separate metric for the group</p> <p>Aggregation - choose one of aggregations: Average, Maximum, Minimum, Count, Sum</p> <p>Rate - set a rate in minutes</p>"},{"location":"newoutput/datamap/#thresholds","title":"Thresholds","text":"<p>Preview - When you manage a particular metric in the edit window, you can preview actual values so you can decide what thresholds to use by viewing actual values.</p> <p>Value - set a threshold value and choose the scale if needed. Instead of writing 1000000000 you can put 1 and choose Giga as its scale. Scale options: None, Nano, Micro, Milli, Kilo, Mega, Giga, Tera, Peta</p> <p>Each entity on the map can have 7 different colors based on thresholds and definitions:\u00a0</p> <ul> <li> <p>Dark Grey - No threshold defined\u00a0</p> </li> <li> <p>Dark Red - Highest/Lowest threshold met (Negative)</p> </li> <li> <p>Dark Orange\u00a0</p> </li> <li> <p>Orange\u00a0</p> </li> <li> <p>Green - Highest/Lowest threshold met (Positive)\u00a0</p> </li> <li> <p>Grey - No threshold met\u00a0</p> </li> <li> <p>Stripes - No data for the selected timeframe\u00a0</p> </li> </ul> <p>These thresholds can be set for independent criteria for each group.</p> <p>For over X % Last Y Minutes - for over X% of the values should be over/under the threshold in the last Y minutes</p> <p>Replace missing values with 0 - if you want to take null values into the threshold calculations then you need to replace them with 0 - check this parameter.</p> <p>Min. X % of timeframe must have values - Specify the percentage how much data must have values (Note: If there is no enough data then you get No Data results. This is why you might consider to enable Replace missing values with 0)</p> <p>After you add thresholds to your groups, the map colors each of the groups separately and provides a top-down view of the data structure you have created.\u00a0</p> <p></p> <p>Below your map, you will have an interactive legend that allows you to:</p> <ul> <li> <p>Choose the top-level hierarchy\u00a0</p> </li> <li> <p>Understand the base metric/log criteria by which the group was generated</p> </li> <li> <p>View the threshold defined per group</p> </li> <li> <p>Filter into specific statuses (cmd + click allows multi-selection)</p> </li> </ul> <p></p> <p></p> <p></p> <p>Hovering over the title will display additional information.</p> <p></p>"},{"location":"newoutput/datamap/#drill-down","title":"Drill Down","text":"<p>Each element on the map can be zoomed into, investigated and compared to other cells:</p> <p></p> <p></p> <p></p> <p>When investigating a metric criteria, you will receive a 24H graph of the metric, with the thresholds marked on the graph.\u00a0</p> <p></p> <p>When \"Compare to others\" is clicked then you will receive a 24H graph with metrics so\u00a0you can compare the chosen cell to a random 10 series from the same group, and see how it differs from the rest.</p> <p>Save view - With save view you can share or back up a copy of your views.</p> <p></p> <p>Load view - allows you to import a view from another team or upload a backup copy</p> <p></p>"},{"location":"newoutput/datamap/#datamap-label-mapping","title":"DataMap Label Mapping","text":"<p>When to use DataMap Label Mapping?</p> <p>The DataMap Label Mapping feature has been introduced to address the following scenario:  </p> <p>1. Metrics have been defined by one or several teams for the same system (for example a Kubernetes cluster).  </p> <p>2. At least one (or more) labels used to define the metrics use different names for the same information.</p> <p>3. Using DataMap Label Mapping we can \u201cjoin\u201d those metrics, which would otherwise appear to be unrelated in a DataMap.</p> <p>Let\u2019s illustrate this feature with the following example.</p> <p>Please consider the following set of metrics: 1. elasticsearch_os_load15 (partial columns included in the screenshot):  </p> <p>2. elastic_client_proxy_client_failures (partial columns included in the screenshot):</p> <p>Please note how both CX_LEVEL and CX_ENV_ID contain the same information.</p> <p>3. Now let\u2019s look at a portion of the DataMap definition:Please note how it indicates: \u201c1 labels were renamed\u201d.</p> <p>4. If we drill down into this by clicking \u201cEDIT\u201d, we will see how the Label Mapping was defined:</p> <p>Now if you need to do the same for your DataMap, please follow these steps:</p> <ol> <li> <p>From the \u201cGroup Status Settings\u201d of the label you would like to create the Data Mapping for, please click the 3 dots on the right side:</p> </li> <li> <p>Select \u201cLABEL MAPPING\u201d:</p> </li> <li> <p>Define the Mapping matching the correct labels:</p> </li> <li> <p>Click \u201cSAVE\u201d on the button right of the UI:</p> </li> </ol> <p>That\u2019s it; you have successfully created your DataMap Label Mapping.</p> <p>Are you interested in using the DataMap feature?</p>"},{"location":"newoutput/dataprime-cheat-sheet/","title":"DataPrime Cheat Sheet","text":"<p>Use our innovative DataPrime language not only to query your data, but transform it using a series of operations in a meaningful manner for you.</p> <p>This Cheat Sheet will enable you to hit the ground running using DataPrime. A complete glossary of all DataPrime operators and expressions can be found here.</p>"},{"location":"newoutput/dataprime-cheat-sheet/#language-basics","title":"Language Basics","text":"<p>DataPrime supports data and helper types, expressions, and operators.</p>"},{"location":"newoutput/dataprime-cheat-sheet/#data-helper-types","title":"Data &amp; Helper Types","text":"<p>DataPrime supports the following data types:</p> <ul> <li> <p><code>boolean</code>: &gt;, &lt;, =,\u00a0<code>true</code>\u00a0or\u00a0<code>false</code></p> </li> <li> <p><code>number</code>/<code>num</code>: 42</p> </li> <li> <p><code>string</code>: \u201cfoo\u201d</p> </li> <li> <p><code>timestamp</code>: 10-10-2021:11:11</p> </li> </ul> <p>It also supports several helper types, used as arguments for functions or operators, including:</p> <ul> <li> <p><code>Interval</code>: duration of time</p> </li> <li> <p><code>Regexp</code>: standard regular expressions</p> </li> </ul>"},{"location":"newoutput/dataprime-cheat-sheet/#expressions","title":"Expressions","text":"<p>Expressions consist of <code>literals</code>, <code>functions</code> / <code>methods</code>, <code>operators</code>, <code>cast</code>, and / or <code>groupings</code> that return data.</p> <ul> <li> <p><code>literals</code>: 42</p> </li> <li> <p><code>functions</code>: length($.foo)</p> </li> <li> <p><code>methods</code> :$d.foo.length() //just alternative syntax for functions</p> </li> <li> <p><code>operator expressions</code>: 1 + $d.foo</p> </li> <li> <p><code>cast</code>: $d.foo:string</p> </li> <li> <p><code>groupings</code>: (...)</p> </li> </ul>"},{"location":"newoutput/dataprime-cheat-sheet/#operators","title":"Operators","text":"<p>Use operators as commands that transform unstructured streams of JSON data and creates a new stream for a specified time period. Examples include: <code>filter</code>, <code>extract</code>, <code>sortby</code>, <code>groupby</code>, <code>orderby</code>, <code>find</code>, <code>choose</code>.</p> <p>Find out more about the language building blocks here.</p>"},{"location":"newoutput/dataprime-cheat-sheet/#format","title":"Format","text":"<p>Query format is as follows:</p> <pre><code>source logs | operator1 ... | operator2 ... | operator3 | ...\n\n\n</code></pre> <p>Any whitespace between operators is ignored, allowing to write queries as multiline queries:</p> <pre><code>source logs\n  | operator1 ....\n  | operator2 ....\n  | operator3 ....\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#most-frequently-used-operators-examples","title":"Most Frequently Used Operators: Examples","text":"<p>The following section provides use-cases of most frequently used operators, so you can hit the ground running. View a full glossary of all operators and expressions here.</p> <p>Example input data:</p> <pre><code># Input Examples\n{ region: \"us-east-1\", az: \"us-east-1a\", duration_ms: 231, result: 'success' }\n{ region: \"us-east-1\", az: \"us-east-1b\", duration_ms: 2222, result: 'failure' }\n{ region: \"eu-west-1\", az: \"eu-west-1a\", duration_ms: 501, result: 'success' }\n{ region: \"eu-west-2\", az: \"eu-west-2a\", duration_ms: 23, result: 'success' }\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#filter","title":"<code>filter</code>","text":"<p>Get only logs which signify <code>success</code>:</p> <pre><code>source logs\n| filter result == 'success'\n\n# Result - Full raw logs\n\n</code></pre> <p>Get only logs which signify <code>failure</code> in the <code>eu-west-1</code> region:</p> <pre><code># Option 1\nsource logs\n| filter result == 'failure' &amp;&amp; region == 'eu-west-1'\n\n# Option 2\nsource logs\n| filter result == 'failure'\n| filter region == 'eu-west-1'\n\n# Result - Full raw logs\n\n</code></pre> <p>Get only logs which have a region that starts with <code>eu-</code> :</p> <pre><code>source logs\n| filter region.startsWith('eu-')\n\n# Result - Full raw logs\n\n</code></pre> <p>Get only logs which have a duration larger than 2 seconds:</p> <pre><code>source logs\n| filter duration_ms / 1000 &gt; 2\n\n# Result - Full raw logs\n\n</code></pre> <p>Get all the logs except the ones which have a <code>ap-southeast-1</code> region (3 options):</p> <pre><code># Option 1\nsource logs\n| filter region != 'ap-southeast-1'\n\n# Option 2\nsource logs\n| filter !(region == 'ap-southeast-1')\n\n# Option 3\nsource logs\n| block region == 'ap-southeast-1'\n\n</code></pre> <p>Get 10 success logs which have a duration larger than 2 seconds:</p> <pre><code>source logs\n| filter result == 'success' &amp;&amp; duration_ms / 1000 &gt; 2\n| limit 10\n\n# Result - Full raw logs\n\n</code></pre> <p>Order the success logs by descending duration fetching the top-most 10 logs:</p> <pre><code>source logs\n| filter result == 'success'\n| orderby duration_ms desc\n| limit 10\n\n# Result - Full raw logs\n\n</code></pre> <p>Do a free-text search on the <code>msg</code> field, returning only logs which have the word \u201cstarted\u201d in them. Combine the free-text search with another filter (the <code>stream</code> field has the value <code>stdout</code>):</p> <pre><code># Query 1 - Using the find operator for finding the text in msg, and then filtering using the filter operator\nsource logs\n| find 'started' in msg\n| filter stream == 'stdout'\n\n# Results 1 - Full raw logs\n\n# Query 2 - Using the ~ predicate and combining the free-text search and the filter into one expression that is passed to filter\nsource logs\n| msg ~ 'started' &amp;&amp; filter stream == 'stdout'\n\n# Result - Full raw logs\n\n</code></pre> <p>Perform a wild-search of text inside the entire log:</p> <pre><code># Option 1 - Using wildfind operator\nsource logs\n| wildfind 'coralogix'\n\n# Option 2 - Inside an expression\nsource logs\n| filter $d ~~ 'coralogix'\n\n# Result - Full raw logs\n\n</code></pre> <p>Use a full Lucene query to filter results:</p> <pre><code>source logs\n| lucene 'region:\"us-east-1\" AND \"coralogix\"'\n\n</code></pre> <p>Convert the data type of a key: Get the logs whose <code>version</code> field contains a value greater than 32.</p> <pre><code>Input Data Example:\n{ \"version\" : \"12\", ... }\n{ \"version\": \"17\", ... }\n{ \"version\": \"65\", ... }\n\n# Option 1 - By casting\nsource logs\n| filter version:number &gt; 32\n\n# Option 2 - Using convert operator\nsource logs\n| convert version:number\n| filter version &gt; 32\n\n</code></pre> <p>Get success logs, but choose only <code>result</code> and <code>duration</code> fields for the output:</p> <pre><code># Option 1 - Using the choose operator\nsource logs\n| filter result == 'success'\n| choose result, duration_ms\n\n# Option 2 - Using the select operator, which is just an alias for choose\nsource logs\n| filter result == 'success'\n| select result, duration_ms\n\n# Result - Only result and duration keys will remain in each event\n\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#choose","title":"<code>choose</code>","text":"<p>Get success logs, but choose only <code>result</code> and <code>duration</code> fields for the output.</p> <pre><code># Option 1 - Using the choose operator\nsource logs\n| filter result == 'success'\n| choose result, duration_ms\n\n# Result - Only result and duration keys will remain in each event\n\n</code></pre> <p>Construct a new object using <code>choose</code>. The output fields will be as follows:</p> <ul> <li> <p><code>outcome</code> which will contains the value of the <code>result</code> field from the original log</p> </li> <li> <p><code>duration_seconds</code> which will contain the original <code>duration_ms</code> divided by 1000 in order to convert it to seconds</p> </li> <li> <p>A new field called <code>meaning_of_life</code> which will contain the value 42</p> </li> </ul> <pre><code>source logs\n| choose result as outcome, duration_ms / 1000 as duration_seconds, 42 as meaning_of_life\n\n# Result - Notice the key names have been changed according to the \"as X\" parts\n{ \"outcome\": \"success\", \"duration_seconds\": 2.54, \"meaning_of_life\": 42 }\n{ \"outcome\": \"failure\", \"duration_seconds\": 0.233, \"meaning_of_life\": 42 }\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#count-countby","title":"<code>count</code> / <code>countby</code>","text":"<p>Count all the success logs:</p> <pre><code>source logs\n| filter result == 'success'\n| count\n\n# Result - Total number of logs after filtering\n\n</code></pre> <p>Count logs, grouped by success/failure:</p> <pre><code>source logs\n| ountby result\n\n# Result - Number of logs per result value\n{ \"result\": \"success\", \"_count\": 847 }\n{ \"result\": \"failure\", \"_count\": 22 }\n\n</code></pre> <p>Count logs, grouped success/failure per region, with the results in a new field named <code>request_count</code>:</p> <pre><code>source logs\n| countby region,result into request_count\n\n# Result - Notice that the count keyname is set to request_count because of \"into request_count\"\n{ \"region\": \"eu-west-1\", \"result\": \"success\", \"request_count\": 287 }\n{ \"region\": \"eu-west-1\", \"result\": \"failure\", \"request_count\": 2 }\n{ \"region\": \"eu-west-2\", \"result\": \"success\", \"request_count\": 2000 }\n{ \"region\": \"eu-west-3\", \"result\": \"success\", \"request_count\": 54 }\n{ \"region\": \"eu-west-3\", \"result\": \"failure\", \"request_count\": 2 }\n\n</code></pre> <p>Count events in each region, and return the top 3 regions:</p> <pre><code>source logs\n| top 3 region by count()\n\n# Result - 3 rows, each containing a region and a count of logs\n\n</code></pre> <p>Average the duration in seconds for each region, and return the lowest (bottom) 3 regions:</p> <pre><code>source logs\n| bottom 3 regions by avg(duration_ms)\n\n# Result - 3 rows, each containing a region and an average duration\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#groupby","title":"<code>groupby</code>","text":"<p>Get the average and maximum durations for successes/failures:</p> <pre><code># Option 1 - Output keypaths are named automatically\nsource logs\n| groupby result calc avg(duration_ms),max(duration_ms)\n\n# Result 1\n{ \"result\": \"success\", \"_avg\": 23.4, \"_max\": 287 }\n{ \"result\": \"failure\", \"_avg\": 980.1, \"_max\": 1000.2 }\n\n# Option 2 - Using \"as X\" to name the output keypaths\nsource logs\n| groupby result calc avg(duration_ms) as avg_duration,max(duration_ms) as max_duration\n\n# Result 2\n{ \"result\": \"success\", \"average_duration\": 23.4, \"max_duration\": 287 }\n{ \"result\": \"failure\", \"average_duration\": 980.1, \"max_duration\": 1000.2 }\n\n\n</code></pre> <p>When querying with the <code>groupby</code> operator, you can now apply an aggregation function (such as <code>avg</code>, <code>max</code>, <code>sum</code>) to the bucket of results. This feature gives you the power to manipulate an aggregation expression inside the expression itself, allowing you to calculate and manipulate your data simultaneously. Examples of DataPrime expressions in aggregations can be found here.</p>"},{"location":"newoutput/dataprime-cheat-sheet/#distinct","title":"<code>distinct</code>","text":"<p>Get distinct regions from the data, grouping logs by region name without any aggregations.</p> <pre><code># Input Examples:\n{ \"region\": \"us-east-1\", ... }\n{ \"region\": \"us-east-1\", ... }\n{ \"region\": \"eu-west-1\", ... }\n\n# Query 1 - Get distinct regions from the data\nsource logs\n| distinct region\n\n# Results 1 - distinct region names\n{ \"region\": \"us-east-1\" }\n{ \"region\": \"eu-west-1\" }\n\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#enrich","title":"<code>enrich</code>","text":"<p>Enrich and filter your logs using additional context from a lookup table. For example, enrich user activity logs with the user\u2019s department and then retrieve logs of all users in the Finance department.</p> <p>First, upload the lookup table:</p> <p>Go to Data Flow &gt; Data Enrichment page &gt; Custom Enrichment section, and add Custom Enrichment. For more details see the Custom Enrichment documentation.</p> <p>There are two possible ways to enrich your logs:</p> <ul> <li> <p>Select log key to look up for a key value and enrich the logs automatically during ingestion. The logs are saved with the enriched fields. The advantages of this mode:</p> <ul> <li> <p>Logs are automatically enriched.</p> </li> <li> <p>The logs themselves include the enrichment data, which makes it easier to consume everywhere (by any query, and also by third-party products that read the logs from the S3 bucket).</p> </li> </ul> </li> <li> <p>Use the DataPrime <code>enrich</code> query to look up for a value in this table and enrich the log dynamically for the purpose of the query. The advantages of this mode:</p> <ul> <li> <p>It allows you to enrich old logs already ingested into Coralogix.</p> </li> <li> <p>The enrichment does not increase the size of the stored logs, as the enrichment is done dynamically, only for the query results.</p> </li> </ul> </li> </ul> <p>The syntax:</p> <pre><code>enrich &lt;value_to_lookup&gt; into &lt;enriched_key&gt; using &lt;lookup_table&gt;\n\n</code></pre> <p>The <code>&lt;value_to_lookup&gt;</code> (name of a log key or the actual value) will be looked up in the Custom Enrichment <code>&lt;lookup_table&gt;</code> and a key called <code>&lt;enriched_key&gt;</code> will be added to the log, containing all table columns as sub-keys. If the <code>&lt;value_to_lookup&gt;</code> is not found in the <code>&lt;lookup_table&gt;</code>, the <code>&lt;enriched_key&gt;</code> will still be added but with \u201cnull\u201d values, in order to preserve the same structure for all result logs. You can then filter the results using the DataPrime capabilities, such as filtering logs by specific value in the enriched field.</p> <p>Example</p> <p>The original log:</p> <pre><code>{\n    \"userid\": \"111\",\n    ...\n}\n\n</code></pre> <p>The Custom Enrichment lookup table called \u201cmy_users\u201d:</p> ID Name Department 111 John Finance 222 Emily IT <p>Running the following query:</p> <pre><code>enrich $d.userid into $d.user_enriched using my_users\n\n</code></pre> <p>Gives the following enriched log:</p> <pre><code>{\n    \"userid\": \"111\",\n    \"user_enriched\": {\n        \"ID: \"111\",\n        \"Name\": \"John\",\n        \"Department\": \"Finance\"\n    },\n    ...\n}\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>Run the DataPrime query\u00a0<code>source &lt;lookup_table&gt;</code>\u00a0to view the enrichment table.</p> </li> <li> <p>If the original log already contains the enriched key:</p> <ul> <li> <p>If <code>&lt;value_to_lookup&gt;</code> exists in the <code>&lt;lookup_table&gt;</code>, the sub-keys will be updated with the new value. If the <code>&lt;value_to_lookup&gt;</code> does not exist, their current value will remain.</p> </li> <li> <p>Any other sub-keys which are not columns in the <code>&lt;lookup_table&gt;</code> will remain with their existing values.</p> </li> </ul> </li> <li> <p>All values in the <code>&lt;lookup_table&gt;</code>\u00a0are considered to be strings. This means that:</p> <ul> <li> <p>The <code>&lt;value_to_lookup&gt;</code> must be in a string format.</p> </li> <li> <p>All values are enriched in a string format. You may then convert them to your preferred format (e.g. JSON, timestamp) using the appropriate functions.</p> </li> </ul> </li> </ul>"},{"location":"newoutput/dataprime-cheat-sheet/#extract","title":"<code>extract</code>","text":"<p><code>extract</code> allows you to take some semi-structured text, and extract meaningful data out of it. There are multiple methods to extract this data:</p> <ul> <li> <p><code>regexp</code> - Extract using regular expression capture-groups</p> </li> <li> <p><code>jsonobject</code> - Take a stringified JSON and extract it to a real object</p> </li> <li> <p><code>kv</code> - Extract key=value pairs from a string</p> </li> </ul> <p>The extracted values can also be converted to their real data type as part of the extraction. This is done by adding <code>datatypes</code> clause that contains the required conversions (same syntax as the <code>convert</code> operator).</p> <p>Examples</p> <p>Extract information from a text field using a regular expression:</p> <pre><code># Input Data Examples:\n{ \"msg\": \"... Query_time: 2.32 Lock_time: 0.05487 ...\" }\n{ \"msg\": \"... Query_time: 0.1222 Lock_time: 0.0002 ...\" }\n...\n\n# Example 1\n\n# Query 1\nsource logs \n// Filter the relevant logs using lucene\n| lucene '\"Query_time:\"' \n// Extract duration and lock_time strings from the msg field\n| extract msg into stats using regexp(\n  /# Query_time: (?&lt;duration&gt;.*?) Lock_time: (?&lt;lock_time&gt;.*?) /) \n// Choose to leave only the stats object that the extraction has created\n| choose stats\n\n# Results 1 - Output contains strings\n{ \"stats\": { \"duration\": \"0.08273\" , \"lock_time\": \"0.00121\" } }\n{ \"stats\": { \"duration\": \"0.12\" , \"lock_time\": \"0.001\" } }\n{ \"stats\": { \"duration\": \"3.121\" , \"lock_time\": \"0.83322\" } }\n...\n\n# Query 2 - Added datatypes clause, so the extracted values will be numbers instead of strings\nsource logs \n| lucene '\"Query_time:\"' \n| extract msg into stats using regexp(\n  /# Query_time: (?&lt;duration&gt;.*?) Lock_time: (?&lt;lock_time&gt;.*?) /) \n  datatypes duration:number,lock_time:number\n| choose stats\n\n# Results 1 - Output contains real numbers and not strings (see above example)\n{ \"stats\": { \"duration\": 0.08273 , \"lock_time\": 0.00121 } }\n{ \"stats\": { \"duration\": 0.12 , \"lock_time\": 0.001 } }\n{ \"stats\": { \"duration\": 3.121 , \"lock_time\": 0.83322 } }\n...\n\n# Query 3 - Use the extracted values in a later operator, in this case a filter\nsource logs \n| lucene '\"Query_time:\"' \n| extract msg into stats using regexp(\n  /# Query_time: (?&lt;duration&gt;.*?) Lock_time: (?&lt;lock_time&gt;.*?) /) \n  datatypes duration:number,lock_time:number\n| choose stats\n// Filter for only the logs which contain a lock_time which is above 0.5\n| filter stats.lock_time &gt; 0.5\n\n# Results 1 - Output contains real numbers\n{ \"stats\": { \"duration\": 3.121 , \"lock_time\": 0.83322 } }\n...\n\n</code></pre> <p>Extract a JSON object stored in a string:</p> <pre><code>Input Data Examples:\n{\"my_json\": \"{\\\\\"x\\\\\": 100, \\\\\"y\\\\\": 200, \\\\\"z\\\\\": {\\\\\"a\\\\\": 300}}\" , \"some_value\": 1}\n{\"my_json\": \"{\\\\\"x\\\\\": 400, \\\\\"y\\\\\": 500, \\\\\"z\\\\\": {\\\\\"a\\\\\": 600}}\" , \"some_value\": 2}\n...\n\n# Query 1\nsource logs\n| **extract my_json into my_data using jsonobject()**\n\n# Results 1\n{ \n  \"my_json\": \"...\"\n  \"my_data\": {\n    \"x\": 100,\n    \"y\": 200,\n    \"z\": 300\n  }\n  \"some_value\": 1\n}\n{\n  \"my_json\": \"...\"\n  \"my_data\": {\n    \"x\": 400,\n    \"y\": 500,\n    \"z\": 600\n  }\n  \"some_value\": 2\n}\n\n# Query 2 - Additional filtering on the resulting object\nsource logs\n| extract my_json into my_data using jsonobject()\n| **filter my_data.x = 100**\n\n# Results 2 - Only the object containing x=100 is returned\n\n\n</code></pre> <p>Extract key=value data from a string. Notice that the <code>kv</code> extraction honors quoted values.</p> <pre><code># Example data for Query 1\n{ \"log\": \"country=Japan city=\\\\\"Tokyo\\\\\"\" , ... }\n{ \"log\": \"country=Israel city=\\\\\"Tel Aviv\\\\\"\" , ... }\n...\n\n# Query 1\nsource logs\n| extract log into my_data using kv()\n\n# Results 1\n{ \n  \"log\": \"...\"\n  \"my_data\": {\n    \"country\": \"Japan\"\n    \"city\": \"tokyo\"\n  }\n  ...\n}\n{\n  \"log\": \"...\"\n  \"my_data\": {\n    \"country\": \"Israel\"\n    \"city\": \"Tel Aviv\"\n  }\n  ...\n}\n\n# Example Data for Query 2 - Key/Value delimiter is \":\" and not \"=\"\n{ \"log\": \"country:Japan city:\\\\\"Tokyo\\\\\"\" , ... }\n{ \"log\": \"country:Israel city:\\\\\"Tel Aviv\\\\\"\" , ... }\n...\n\n# Query 2\nsource logs\n| extract log into my_data using kv(':')\n\n# Results 2 - Same results as query 1\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#orderbysortby","title":"<code>orderby</code>/<code>sortby</code>","text":"<p>Order the successes by descending duration fetching the top-most 10 logs:</p> <pre><code>source logs\n| filter result == 'success'\n| orderby duration_ms desc\n| limit 10\n\n# Result - Full raw logs\n\n</code></pre> <p>Numerically order a string field which effectively contains numbers:</p> <pre><code># Input Data Examples:\n{ \"error_code\": \"23\" }\n{ \"error_code\": \"12\" }\n{ \"error_code\": \"4\" }\n{ \"error_code\": \"1\" }\n\n# Query 1\n\nsource logs\n| orderby error_code:number\n\n# Results 1 - Ordered by numeric value\n{ \"error_code\": \"1\" }\n{ \"error_code\": \"4\" }\n{ \"error_code\": \"12\" }\n{ \"error_code\": \"23\" }\n\n# Query 2 - By using the convert operator\nsource logs\n| convert error_code:number\n| orderby error_code\n\n# Results 2 - Same results\n\n</code></pre> <p>Order by alphabetical order of multiple fields. [Note: Ordering is case-sensitive; A-Z will be ordered before a-z.]</p> <pre><code># Example Input Data:\n{ \"last_name\": \"musk\" , \"first_name\": \"elon\" }\n{ \"last_name\": \"jobs\", \"first_name\": \"steve\" }\n...\n\n# Query\nsource logs\n| orderby last_name,first_name\n\n</code></pre> <p>Same example but with case-insensitive ordering:</p> <pre><code>source logs\n| orderby toLowerCase(last_name),toLowerCase(first_name)\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#orderby","title":"<code>orderby</code>","text":"<p>Create a new keypath value:</p> <pre><code># Example Input Data\n{ \"country\": \"Japan\", \"city\": \"Tokyo\" }\n{ \"country\": \"Israel\", \"city\": \"Jerusalem\" }\n...\n\n# Query 1\nsource logs\n| create default_temperature from 32.5\n\n# Results 1 - Each log contains the new field, with the same value\n{ \"country\": \"Japan\", \"city\": \"Tokyo\", \"default_temperature\": 32.5 }\n{ \"country\": \"Israel\", \"city\": \"Jerusalem\", \"default_temperature\": 32.5 }\n...\n\n# Query 2 - Create a new field which contains the first three letters of the country, converted to uppercase\nsource logs\n| create country_initials from toUpperCase(substr(country,1,3))\n\n# Results 2\n{ \"country\": \"Japan\", \"city\": \"Tokyo\", \"country_initials\": \"JAP\" }\n{ \"country\": \"Israel\", \"city\": \"Jerusalem\", \"country_initials\": \"ISR\" }\n\n# Input Examples for Query 3\n{ ... , \"temp_in_fahrenheit\": 87.2 }\n{ ... , \"temp_in_fahrenheit\": 32 }\n...\n\n# Query 3\nsource logs\n| create temp_in_celcius from (temp_in_fahrenheit - 32) * 5 / 9\n\n# Results 3\n{ ... , \"temp_in_fahrenheit\": 87.2, \"temp_in_celcius\": 30.66666 }\n{ ... , \"temp_in_fahrenheit\": 32, \"temp_in_celcius\": 0.0 }\n...\n\n# Query 3 - Create a new field containing &lt;country&gt;/&lt;city&gt; as a string. Uses string-interpolation syntax\nsource logs\n| create country_and_city from `{country}/{city}`\n\n# Results 3\n{ \"country\": \"Japan\", \"city\": \"Tokyo\", \"country_and_city\": \"Japan/Tokyo\" }\n{ \"country\": \"Israel\", \"city\": \"Jerusalem\", \"country_and_city\": \"Israel/Jerusalem\" }\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#move","title":"<code>move</code>","text":"<p>This operator can be used to move a source keypath to a target keypath, including entire subtrees. It can also be used for renaming a keypath.</p> <p>For nested source keypaths, only the actual key is moved, merging the target keypath with any other objects or keys which already exist in the data.</p> <p>When moving an entire subtree, the target keypath will serve as the root of the new subtree.</p> <p>Examples</p> <p>Move a key to a target location:</p> <pre><code>Input Data Example:\n{ \"query_id\": \"AAAA\", \"stats\": { \"total_duration\": 23.3 , \"total_rows: 500 }}\n...\n\n# Query 1 - Rename a keypath\nsource logs\n| move query_id to query_identifier\n\n# Results 1 - Keypath renamed\n{ \"query_identifier\": \"AAAA\", \"stats\": { \"total_duration\": 23.3 , \"total_rows: 500 } }\n...\n\n# Query 2 - Move a key to an existing subtree\nsource logs\n| move query_id to stats.query_id\n\n# Results 2 - query_id moved below \"stats\"\n{ \"stats\": { \"total_duration\": 23.3 , \"total_rows: 500, \"query_id\": \"AAAA\" } }\n...\n\n</code></pre> <p>Move subtree to another location:</p> <pre><code># Query 1 - Rename subtree\nsource logs\n| move stats to execution_data\n\n# Results 1 - Rename subtree\n{ \"query_identifier\": \"AAAA\", \"execution_data\": { \"total_duration\": 23.3 , \"total_rows: 500 } }\n...\n\n# Query 2 - Move subtree to root\nsource logs\n| move stats to $d\n\n# Results 2\n{ \"query_id\": \"AAAA\", \"total_duration\": 23.3 , \"total_rows: 500 }\n...\n\n# Input Data Examples for Query 3\n{ \"request\": { \"id\": \"1000\" } , \"user\": { \"name\": \"james\", \"id\": 50 } }\n...\n\n# Query 3 - Move subtree to another subtree\nsource logs\n| move user to request.user_info\n\n# Results 3 - Entire user subtree moved below request.user_info\n{ \"request\": { \"id\": \"1000\", \"user_info\": { \"name\": \"james\", \"id\": 50 } } }\n...\n\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#replace","title":"<code>replace</code>","text":"<p>Replace the value in an existing keypath:</p> <pre><code># Input Data Examples:\n{ \"user\": { \"id\": \"1000\" , \"name\": \"James\", \"email\": \"james@coralogixstg.wpengine.com\" } }\n{ \"user\": { \"id\": \"2000\" , \"name\": \"John\", \"email\": \"john@coralogixstg.wpengine.com\" } }\n...\n\n# Example 1\nreplace user.name with 'anyone'\n\n# Results 1\n{ \"user\": { \"id\": \"1000\" , \"name\": \"anyone\", \"email\": \"james@coralogixstg.wpengine.com\" } }\n{ \"user\": { \"id\": \"2000\" , \"name\": \"anyone\", \"email\": \"john@coralogixstg.wpengine.com\" } }\n...\n\n# Example 2\nreplace user.name with user.email\n\n# Results 2\n{ \"user\": { \"id\": \"1000\" , \"name\": \"james@coralogixstg.wpengine.com\", \"email\": \"james@coralogixstg.wpengine.com\" } }\n{ \"user\": { \"id\": \"2000\" , \"name\": \"john@coralogixstg.wpengine.com\", \"email\": \"john@coralogixstg.wpengine.com\" } }\n\n# Example 3\nreplace user.name with `UserName={user.id}`\n\n# Results 3\n{ \"user\": { \"id\": \"1000\" , \"name\": \"UserName=1000\", \"email\": \"james@coralogixstg.wpengine.com\" } }\n{ \"user\": { \"id\": \"2000\" , \"name\": \"UserName=2000\", \"email\": \"john@coralogixstg.wpengine.com\" } }\n...\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#remove","title":"<code>remove</code>","text":"<p>The negation of <code>choose</code>.</p> <p>Examples</p> <pre><code># Input Data Examples:\n{ \n  \"stats\": { \n    \"duration_ms\": 2.34, \n    \"rows_scanned\": 501, \n    \"message\": \"Operation has taken 2.34 seconds\" \n  }, \n  \"some_value\": 1000 \n}\n...\n\n# Query 1 - Remove the message keypath\nsource logs\n| remove stats.message\n\n# Results 1\n{ \n  \"stats\": { \n    \"duration_ms\": 2.34, \n    \"rows_scanned\": 501 \n  }, \n  \"some_value\": 1000 \n}\n...\n\n# Query 2 - Remove the entire stats subtree\nsource logs\n| remove stats\n\n# Results 2\n{ \n  \"some_value\": 1000 \n}\n...\n\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#redact","title":"<code>redact</code>","text":"<p>Examples</p> <pre><code>Input Examples:\n{ \"serverIp\": \"ip-172-30-20-12.eu-west-1.compute.internal\", ... }\n{ \"serverIp\": \"ip-172-82-121-1.eu-west-2.compute.internal\", ... }\n{ \"serverIp\": \"ip-172-99-72-187.us-east-1.compute.internal\", ... }\n...\n\n# Query 1 - Redact all parts containing the string '.computer.internal'\nsource logs\n| redact serverIp matching 'compute.internal' to ''\n\n# Results 1\n{ \"serverIp\": \"ip-172-30-20-12.eu-west-1\", ... }\n{ \"serverIp\": \"ip-172-82-121-1.eu-west-2\", ... }\n{ \"serverIp\": \"ip-172-99-72-187.us-east-1\", ... }\n\n# Query 2 - Redact all digits before aggregation using regexp\nsource logs\n| redact serverIp matching /[0-9]+/ to 'X'\n| countby serverIp\n\n# Results 2\n{ \"serverIp\": \"ip-X-X-X-X.eu-west-X.compute.internal\", \"_count\": 2323 }\n{ \"serverIp\": \"ip-X-X-X-X.us-east-X.compute.internal\", \"_count\": 827 }\n...\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#source","title":"<code>source</code>","text":"<p>Set the data source that your DataPrime query is based on.</p> <p>Syntax</p> <pre><code>source &lt;data_store&gt;\n\n</code></pre> <p>Where <code>&lt;data_store&gt;</code> can be either:</p> <ul> <li> <p><code>logs</code></p> </li> <li> <p><code>spans</code></p> </li> <li> <p>The name of the custom enrichment. In this case, the command will display the custom enrichment table.</p> </li> </ul> <p>Example</p> <pre><code>source logs\n\n</code></pre>"},{"location":"newoutput/dataprime-cheat-sheet/#additional-resources","title":"Additional Resources","text":"DataPrime Quick-Start GuideDataPrime Query LanguageGlossary: DataPrime Operators &amp; Expressions"},{"location":"newoutput/dataprime-query-language/","title":"DataPrime Query Language","text":"<p>DataPrime is Coralogix's next-generation query and data discovery language. It's a piped language that provides users with a simple yet powerful way to describe event transformations and aggregations. The balance between simplicity and power is achieved by having a rather small set of idioms that encapsulate event structure transformation while supporting the use of standard JavaScript expressions to describe value transformations.</p> <p>DataPrime is currently enabled to 'Explore' your logs in archive mode. To query your archive with DataPrime, enable the CX-Data format bucket.</p>"},{"location":"newoutput/dataprime-query-language/#overview","title":"Overview","text":"<p>Use our innovative DataPrime syntax language not only to query your data, but transform it using a series of operations in a manner that it meaningful for you.</p> <p>Use DataPrime to:</p> <ul> <li> <p>Calculate. Take a set of data and filter on top of it.</p> </li> <li> <p>Extract. Apply logic to unstructured data and transform it into calculable numbers.</p> </li> <li> <p>Aggregate. Generate new analytics for your business.</p> </li> <li> <p>Transform Data. Transform data with functions creating new fields or replacing existing ones.</p> </li> </ul> <p>The language operates in a manner similar to\u00a0the bash command line\u00a0in Linux, allowing\u00a0the user\u00a0to compose a set of\u00a0small\u00a0processes to achieve a particular goal.</p>"},{"location":"newoutput/dataprime-query-language/#query-format","title":"Query\u00a0Format","text":"<p>Query\u00a0format is as\u00a0follows:</p> <pre><code>source logs | operator ... | operator ... | operator | ...\n\n</code></pre> <p>Any\u00a0whitespace\u00a0between\u00a0operators is ignored, allowing\u00a0you\u00a0to write queries as readable, multiline queries. For example:</p> <pre><code>source logs\n  | operator1 ....\n  | operator2 ....\n  | ...\n\n</code></pre>"},{"location":"newoutput/dataprime-query-language/#data-types","title":"Data\u00a0Types","text":"<p>These\u00a0are the data types currently supported:</p> <ul> <li> <p><code>string</code></p> </li> <li> <p><code>number</code>/<code>num</code>\u00a0- A number (double or integer)</p> </li> <li> <p><code>boolean</code>\u00a0- A boolean type, with\u00a0<code>true</code>\u00a0or\u00a0<code>false</code>\u00a0values</p> </li> <li> <p><code>null</code>\u00a0- A null value</p> </li> <li> <p><code>timestamp</code>\u00a0- A UTC timestamp in\u00a0nanoseconds</p> </li> <li> <p><code>interval</code>\u00a0- A time span in nanoseconds</p> </li> </ul>"},{"location":"newoutput/dataprime-query-language/#language-constructs","title":"Language Constructs","text":"<p>All language constructs that are supported:</p> <ul> <li> <p>Constants: strings, numbers, booleans, regular expressions,\u00a0<code>null</code></p> </li> <li> <p>Nested\u00a0field access</p> </li> <li> <p>Basic\u00a0math operations:\u00a0<code>+</code>,\u00a0<code>-</code>,\u00a0<code>*</code>,\u00a0<code>\\</code>,\u00a0<code>%</code></p> </li> <li> <p>Boolean operations:\u00a0<code>&amp;&amp;</code>,\u00a0<code>||</code>,\u00a0<code>!</code></p> </li> <li> <p>Equality\u00a0and comparison:\u00a0<code>==</code>,\u00a0<code>!=</code>,\u00a0<code>&lt;</code>,\u00a0<code>&lt;=</code>,\u00a0<code>&gt;</code>,\u00a0<code>&gt;=</code></p> </li> <li> <p>Text\u00a0search:\u00a0<code>~</code>,\u00a0<code>~~</code></p> </li> <li> <p>String\u00a0interpolation</p> </li> <li> <p>Timestamp\u00a0expressions\u00a0and\u00a0interval literals</p> </li> <li> <p>Casting\u00a0an expression to a desired data type: e.g.\u00a0<code>$d.temperature:number</code>. Type inference is automatically applied when\u00a0possible\u00a0to reduce the need for casting.</p> </li> </ul>"},{"location":"newoutput/dataprime-query-language/#field-access","title":"Field\u00a0Access","text":"<p>Accessing\u00a0nested data is done by using a keypath, similar to any programming language or json tool. Keys with special characters can\u00a0be\u00a0accessed using a map-like syntax, with the key string as the map index, e.g.\u00a0<code>$d.my_superkey['my_field_with_a_special/character']</code>.</p> <pre><code>$m.timestamp\n$d.my_superkey.myfield\n$d.my_superkey['my_field_with_a_special/character']\n$l.applicationname\n\n</code></pre>"},{"location":"newoutput/dataprime-query-language/#string-interpolation","title":"String\u00a0Interpolation","text":"<ul> <li> <p><code>`this is an interpolated {$d.some_keypath} string`</code>\u00a0-\u00a0<code>{$d.some_keypath}</code>\u00a0will be replaced with the evaluated\u00a0expression\u00a0that is wrapped by the brackets</p> </li> <li> <p><code>`this is how you escape \\{ and \\} and \\``</code>\u00a0- Backward slash (<code>\\</code>) is used to escape characters like\u00a0<code>{</code>,\u00a0<code>}</code>\u00a0that are used\u00a0for\u00a0keypaths.</p> </li> </ul>"},{"location":"newoutput/dataprime-query-language/#text-search","title":"Text\u00a0Search","text":"<p>Boolean\u00a0expressions\u00a0for text search:</p> <ul> <li> <p><code>$d.field ~ 'text phrase'</code>\u00a0- case-insensitive search for a text phrase in a specific field.</p> </li> <li> <p><code>$d ~~ 'text phrase'</code>\u00a0- case-insensitive search for a text phrase in\u00a0<code>$d</code>.</p> </li> </ul>"},{"location":"newoutput/dataprime-query-language/#timestamp-expressions","title":"Timestamp Expressions","text":"<p>Expressions prefixed by\u00a0<code>@</code>\u00a0are timestamp expressions and always return a timestamp. They can be either literals (<code>@number</code>\u00a0or\u00a0<code>@'string'</code>) which are validated at query compilation time, or dynamic expressions (<code>@expression</code>) which is evaluated at query runtime based on the expression's data type.</p> <ul> <li> <p>Number\u00a0timestamp literals:</p> <ul> <li> <p>Seconds\u00a0(10 digits), e.g.\u00a0<code>@1234567890</code></p> </li> <li> <p>Milliseconds (13 digits), e.g.\u00a0<code>@1234567890123</code></p> </li> <li> <p>Microseconds (16 digits), e.g.\u00a0<code>@1234567890123456</code></p> </li> <li> <p>Nanoseconds\u00a0(19 digits), e.g.\u00a0<code>@1234567890123456789</code></p> </li> </ul> </li> <li> <p>String\u00a0timestamp\u00a0literals:</p> <ul> <li> <p>ISO\u00a08601 dates, e.g. `@'2023-08-07'</p> </li> <li> <p>ISO\u00a08601 date/time, e.g.\u00a0<code>@'2023-08-07T19:06:42'</code></p> </li> <li> <p>ISO\u00a08601 date/time with time zone, e.g.\u00a0<code>@'2023-08-07T19:06:42+03:00'</code></p> </li> </ul> </li> <li> <p>Dynamic\u00a0expressions:</p> <ul> <li> <p>Numbers\u00a0are interpreted as nanoseconds, e.g.\u00a0<code>@($d.ts_millis * 1000000)</code>.</p> </li> <li> <p>Strings\u00a0are parsed to a timestamp on a best-effort basis, e.g.\u00a0<code>@`2023-08-{$d.day}`</code>. For extended and customizable\u00a0timestamp\u00a0parsing, see\u00a0parseTimestamp.</p> </li> <li> <p>A\u00a0timestamp\u00a0expression of any other data type returns\u00a0<code>null</code>.</p> </li> </ul> </li> </ul>"},{"location":"newoutput/dataprime-query-language/#interval-literals","title":"Interval\u00a0Literals","text":"<p>An\u00a0interval\u00a0literal\u00a0represents a span of time in a normalized and human-readable format,\u00a0<code>NdNhNmNsNmsNusNns</code>\u00a0where\u00a0<code>N</code>\u00a0is the amount of\u00a0each\u00a0time unit. The following rules apply:</p> <ul> <li> <p>It consists of time unit components - a non-negative integer followed by the short time unit name. Supported time units are:\u00a0<code>d</code>,\u00a0<code>h</code>,\u00a0<code>m</code>,\u00a0<code>s</code>,\u00a0<code>ms</code>,\u00a0<code>us</code>,\u00a0<code>ns</code>.</p> </li> <li> <p>There must be at least one time unit component.</p> </li> <li> <p>The same time unit cannot appear more than once.</p> </li> <li> <p>Components must be decreasing in time unit order - from days to nanoseconds.</p> </li> <li> <p>It can start with\u00a0<code>-</code>\u00a0to represent negative intervals.</p> </li> </ul>"},{"location":"newoutput/dataprime-query-language/#timestamp-math","title":"Timestamp\u00a0Math","text":"<p>In addition\u00a0to\u00a0timestamp expressions and interval literals, Dataprime supports math operations between them:</p> <ul> <li> <p><code>timestamp + interval</code>: adds an interval to a timestamp</p> </li> <li> <p><code>timestamp - interval</code>: subtracts an interval from a timestamp</p> </li> <li> <p><code>timestamp - timestamp</code>: calculates the interval between two timestamps</p> </li> <li> <p><code>timestamp / interval</code>: rounds a timestamp to the nearest interval</p> </li> <li> <p><code>interval + interval</code>: adds two intervals together</p> </li> <li> <p><code>interval - interval</code>: subtracts one interval from another</p> </li> <li> <p><code>interval * number</code>: multiplies an interval by a numeric factor</p> </li> </ul>"},{"location":"newoutput/dataprime-query-language/#scalar-functions","title":"Scalar\u00a0Functions","text":"<p>Various\u00a0functions can\u00a0be\u00a0used\u00a0to\u00a0transform\u00a0values. All functions can be called as methods as well, e.g.\u00a0<code>$d.msg.contains('x')</code>\u00a0is equivalent to\u00a0<code>contains($d.msg,'x')</code>.</p> <p>Returns\u00a0the IP\u00a0prefix\u00a0of a given ip_address with subnetSize bits (e.g.:\u00a0<code>192.128.0.0/9</code>).</p>"},{"location":"newoutput/dataprime-query-language/#uuid-functions","title":"UUID\u00a0Functions","text":""},{"location":"newoutput/dataprime-query-language/#isuuid","title":"isUuid","text":"<p><code>isUuid(uuid: string): bool</code></p> <ul> <li> <p><code>interval</code>\u00a0(required) - the interval to\u00a0format.</p> </li> <li> <p><code>scale</code>\u00a0(optional) - the largest\u00a0time unit\u00a0of\u00a0the\u00a0interval to show. Defaults to\u00a0<code>nano</code>.</p> </li> </ul> <pre><code># Example:\nlimit 3 | choose formatInterval(now() - $m.timestamp, 's') as i\n# Results:\n{ \"i\": \"122s261ms466us27ns\"  }\n{ \"i\": \"122s359ms197us227ns\" }\n{ \"i\": \"122s359ms197us227ns\" }\n</code></pre>"},{"location":"newoutput/dataprime-query-language/#formattimestamp","title":"formatTimestamp","text":"<p><code>formatTimestamp(timestamp: timestamp, format: string?, tz: string?): string</code></p> <p>Function parameters:</p> <ul> <li> <p><code>timestamp</code>\u00a0(required) - the timestamp to format.</p> </li> <li> <p><code>format</code>\u00a0(optional) -\u00a0a\u00a0date/time format specification for\u00a0parsing\u00a0timestamps. The following format options are supported:</p> <ul> <li> <p><code>'%Y-%m-%d'</code>\u00a0- print the date only, e.g.\u00a0<code>'2023-04-05'</code></p> </li> <li> <p><code>'%H:%M:%S'</code>\u00a0- print the time only, e.g.\u00a0<code>'16:07:33'</code></p> </li> <li> <p><code>'%F %H:%M:%S'</code>\u00a0- print both date and time, e.g.\u00a0<code>'2023-04-05 16:07:33'</code></p> </li> <li> <p><code>'iso8601'</code>\u00a0- print a timestamp in ISO 8601 format, e.g.\u00a0<code>'2023-04-05T16:07:33.123Z'</code></p> </li> <li> <p><code>'timestamp_milli'</code>\u00a0- print a timestamp in milliseconds (13 digits), e.g.\u00a0<code>'1680710853123'</code></p> </li> </ul> </li> <li> <p><code>tz</code>\u00a0(optional) - the destination\u00a0time zone\u00a0to convert the timestamp before formatting</p> </li> </ul> <pre><code># Example 1: print a timestamp with default format and +5h offset\nlimit 1 | choose $m.timestamp.formatTimestamp(tz='+05') as ts\n# Result 1:\n{ \"ts\": \"2023-08-29T19:08:37.405937400+0500\" }\n\n# Example 2: print only the year and month\nlimit 1 | choose $m.timestamp.formatTimestamp('%Y-%m') as ym\n# Result 2:\n{ \"ym\": \"2023-08\" }\n\n# Example 3: print only the hours and minutes\nlimit 1 | choose $m.timestamp.formatTimestamp('%H:%M') as hm\n# Result 3:\n{ \"hm\": \"14:11\" }\n\n# Example 4: print a timestamp in milliseconds (13 digits)\nlimit 1 | choose $m.timestamp.formatTimestamp('timestamp_milli') as ms\n# Result 4:\n{ \"ms\": \"1693318678696\" }\n</code></pre>"},{"location":"newoutput/dataprime-query-language/#parseinterval","title":"parseInterval","text":"<p><code>parseInterval(string: string): interval</code></p> <p>Parses an interval from a\u00a0<code>string</code>\u00a0with format\u00a0<code>NdNhNmNsNmsNusNns</code>\u00a0where\u00a0<code>N</code>\u00a0is the amount of each time unit. Returns\u00a0<code>null</code>\u00a0when the input does not match the expected\u00a0format.</p> <pre><code># Example 1: parse a zero interval\nlimit 1 | choose '0s'.parseInterval() as i\n# Result 1:\n{ \"i\": \"0ns\" }\n\n# Example 2: parse a positive interval\nlimit 1 | choose '1d48h0m'.parseInterval() as i\n# Result 2:\n{ \"i\": \"3d\" }\n\n# Example 3: parse a negative interval\nlimit 1 | choose '-5m45s'.parseInterval() as i\n# Result 3:\n{ \"i\": \"-5m45s\" }\n</code></pre> <p>Function parameters:</p> <ul> <li> <p><code>string</code>\u00a0(required) - the input from which the timestamp will be extracted.</p> </li> <li> <p><code>format</code>\u00a0(optional) - a date/time format specification for parsing timestamps. The following format options are supported:</p> <ul> <li> <p><code>'auto'</code>\u00a0(default) - attempt to parse a timestamp on a best-effort basis</p> </li> <li> <p><code>'iso8601'</code>\u00a0/\u00a0<code>'iso8601bare'</code>\u00a0- ISO 8601 format with / without a time zone resp.</p> </li> <li> <p><code>'timestamp_second'</code>\u00a0/\u00a0<code>'timestamp_milli'</code>\u00a0/\u00a0<code>'timestamp_micro'</code>\u00a0/\u00a0<code>'timestamp_nano'</code>\u00a0- timestamp in seconds /\u00a0milliseconds\u00a0/ microseconds / nanoseconds (10/13/16/19 digits) resp.</p> </li> <li> <p>Custom\u00a0timestamp formats</p> </li> <li> <p><code>'format1|format2|...'</code>\u00a0- a cascade of formats to attempt in sequence</p> </li> </ul> </li> <li> <p><code>tz</code>\u00a0(optional) - a\u00a0time zone\u00a0override to convert the timestamp while parsing. This parameter will override any time zone\u00a0present\u00a0in the input. A time zone can be extracted from the string by using an appropriate format and omitting this parameter.</p> </li> </ul> <pre><code># Example 1: parse a date with the default format\nlimit 1 | choose '2023-04-05'.parseTimestamp() as ts\n# Result 1:\n{ \"ts\": 1680652800000000000 }\n\n# Example 2: parse a date in US format\nlimit 1 | choose '04/05/23'.parseTimestamp('%D') as ts\n# Result 2:\n{ \"ts\": 1680652800000000000 }\n\n# Example 3: parse date and time with units\nlimit 1 | choose '2023-04-05 16h07m'.parseTimestamp('%F %Hh%Mm') as ts\n# Result 3:\n{ \"ts\": 1680710820000000000 }\n\n# Example 4: parse a timestamp in seconds (10 digits)\nlimit 1 | choose '1680710853'.parseTimestamp('timestamp_second') as ts\n# Result 4:\n{ \"ts\": 1680710853000000000 }\n</code></pre> <p>Case\u00a0expressions are special constructs in the language that\u00a0allow\u00a0choosing between multiple options in an easy manner and in a readable way. They can be\u00a0wherever\u00a0an\u00a0expression\u00a0is\u00a0expected.</p>"},{"location":"newoutput/dataprime-query-language/#getting-started","title":"Getting Started","text":"<p>Find a list of namespaces, example expressions, operator syntax, and more in our DataPrime Quick-Start Guide.  </p> <p>[NEW] DataPrime now supports Data Aggregation, for more information and examples please refer to the DataPrime Cheat Sheet.</p> <p>DataPrime and Lucene are both optional for querying your Archive and Logs (Under \"Explore\"). You should click the currently active language label toggle between the two languages, Clicking &lt;&gt;Lucene would switch to &lt;&gt;DataPrime and vice versa.</p> <p></p> <p></p> <p>While in DataPrime mode, 2 additional buttons are enabled: - Cheat sheet: A detailed sheet that includes all the schemes and language basics with examples - Query History: For reusing your historical DataPrime queries</p> <p></p>"},{"location":"newoutput/dataprime-query-language/#main-concepts","title":"Main Concepts","text":""},{"location":"newoutput/dataprime-query-language/#stages","title":"Stages","text":"<p>A query is composed of multiple stages, e.g. (Do X and then do Y and then...). The syntax is essentially based on bash-like pipes where each stage's output is piped into the next one.</p>"},{"location":"newoutput/dataprime-query-language/#keypaths","title":"Keypaths","text":"<p>DataPrime can handle fully-nested data. Nested keys are written as 'keypaths', (i.e. <code>key.subkey.subkey</code>) and are handled in a granular way, meaning that operations happen only on the relevant keys, leaving other nested keys intact.</p> <p>For example, creating a new keypath <code>stats.mykey</code> will either create a new key called <code>mykey</code> in an existing <code>stats</code> superkey, or create the entire path - a top-level object called <code>stats</code> and within it, a subkey called <code>mykey</code>.</p>"},{"location":"newoutput/dataprime-query-language/#expressions","title":"Expressions","text":"<p>The language contains a small set of idioms for structure transformation. A large part of its power comes from the ability to use JavaScript-like expressions in various places throughout the language. This allows for describing rich value transformations without resorting to special language-constructs, or to actual code.</p> <p>Several predefined scopes/namespaces are available for expressions. The main ones are the following:</p> <pre><code>$d / $data\n</code></pre> <p>The user-data. For raw data, it's the event data itself, but after aggregations, this could be the aggregation results</p> <pre><code>$m / $metadata\n</code></pre> <p>Engine-related event metadata, such as the <code>timestamp</code> and the <code>logid</code></p> <pre><code>$l / $labels\n</code></pre> <p>User-managed event labels. Flat, key/values (strings only)</p>"},{"location":"newoutput/dataprime-query-language/#example-expressions","title":"Example expressions","text":"<p>Refer to the <code>my_text</code> field in the input:</p> <pre><code>$d.my_text\n</code></pre> <p>Refer to the key <code>key</code> inside the key <code>stats</code>:</p> <pre><code>$d.stats.key\n</code></pre> <p>The result of multiplying the value of the <code>radius</code> key and 8:</p> <pre><code>$d.radius * 8\n</code></pre> <p>The logical timestamp of the event:</p> <pre><code>$m.timestamp\n</code></pre> <p>The application name of the event:</p> <pre><code>$l.applicationName\n</code></pre> <p>Evaluated expressions have a dynamic data type, similar to any javascript code. It's the job of DataPrime to track these data types when they're applied as values of keys.</p>"},{"location":"newoutput/dataprime-query-language/#extractions","title":"Extractions","text":"<p>Data extractions are natively supported by the language, and are extendable, meaning that multiple types of extractions are supported, and new ones can be added without changing the structure of the language.</p> <p>Examples of extraction types:</p> <p>Extract a string into a new object containing captured data from the string:</p> <pre><code>regexp\n</code></pre> <p>Extracting key-value pairs from a string into a new object:</p> <pre><code>kv\n</code></pre> <p>Creating a new object from a json encoded as a string:</p> <pre><code>jsonobject\n</code></pre> <p>Splitting a string into a new array of native elements:</p> <pre><code>split\n</code></pre>"},{"location":"newoutput/dataprime-query-language/#store","title":"Store","text":"<p>A <code>Store</code> is the definition of some storage mechanism for data. This could be a Kafka topic or an S3 location, for example, and includes metadata about the content structure, schema, and primary key (used for enrichments).</p>"},{"location":"newoutput/dataprime-query-language/#limitations","title":"Limitations","text":""},{"location":"newoutput/dataprime-query-language/#tokenized-form","title":"Tokenized Form","text":"<p>In high tier, Coralogix saves text fields longer than 256 symbols only in tokenized form, without special characters and\u00a0stop words. DataPrime functions that operate on such string fields return no results.</p>"},{"location":"newoutput/dataprime-query-language/#no-keypath-adjustments","title":"No Keypath Adjustments","text":"<p>Dataprime does not have keypath adjustments. If a keypath contains dots, you are required to\u00a0use bracket access syntax to\u00a0refer to this keypath\u00a0in archive mode.</p>"},{"location":"newoutput/dataprime-query-language/#additional-resources","title":"Additional Resources","text":"DataPrime Quick-Start GuideGlossary: DataPrime Operators &amp; ExpressionsDataPrime Cheat Sheet"},{"location":"newoutput/dataprime-quick-start-guide/","title":"DataPrime Quick-Start Guide","text":""},{"location":"newoutput/dataprime-quick-start-guide/#namespaces","title":"Namespaces","text":"<p>The user-data (JSON):</p> <pre><code>$d / $data\n</code></pre> <p>Engine-related event metadata. Ex - \"timestamp\", \"severity\", \"logid\", \"priorityclass\":</p> <pre><code>$m / $metadata\n</code></pre> <p>User-managed event labels. Flat, key/values (strings only) Known labels: \"applicationname\", \"subsystemname\", \"category\", \"classname\", \"computername\", \"methodname\", \"threadid\", \"ipaddress\":</p> <pre><code>$l / $labels\n</code></pre>"},{"location":"newoutput/dataprime-quick-start-guide/#example-keypaths","title":"Example keypaths","text":"<pre><code>$d.kubernetes.pod_name\n</code></pre> <pre><code>$l.applicationName\n</code></pre>"},{"location":"newoutput/dataprime-quick-start-guide/#example-expressions","title":"Example expressions","text":"<p>Refer to the key <code>key</code> inside the key <code>stats</code> and apply lowercase function to it:</p> <pre><code>$d.stats.key.toLowerCase()\n</code></pre> <p>The result of multiplying the value of 8 and the <code>radius</code> key casted to number (does not work now will be fixed soon):</p> <pre><code>$d.radius:num * 8\n</code></pre> <p>The logical timestamp of the event (any keypath is valid expression):</p> <pre><code>$m.timestamp\n</code></pre>"},{"location":"newoutput/dataprime-quick-start-guide/#operators-syntax","title":"Operators syntax","text":"<p>Filter data matching expression-predicate:</p> <pre><code>filter &lt;expression&gt;\n</code></pre> <ul> <li>Ex:</li> </ul> <pre><code>filter $d.k8s.pod_name == 'pod1'\n</code></pre> <p>Find entries containing search-string:</p> <pre><code>wildfind '&lt;search-string&gt;\u2019\n</code></pre> <ul> <li>Ex:</li> </ul> <pre><code>wildfind 'foo'\n</code></pre> <p>Find entries matching lucene-query:</p> <pre><code>lucene '&lt;lucene-query&gt;\u2019\n</code></pre> <ul> <li>Ex:</li> </ul> <pre><code>lucene 'hello -world'\n</code></pre> <p>Find entries containing search-string in given keypath:</p> <pre><code>find '&lt;search-string&gt;' in &lt;key-path&gt;\n</code></pre> <ul> <li>Ex:</li> </ul> <pre><code>find 'west' in $d.kubernetes.labels.CX_REGION\n</code></pre> <p>****Order entries by given expression:</p> <pre><code>order by &lt;expression&gt; \n</code></pre> <ul> <li>Ex:</li> </ul> <pre><code>order by $d.priority * -1\n</code></pre> <p>Take first N entries:</p> <pre><code>limit &lt;N&gt;\n</code></pre> <ul> <li>Ex:</li> </ul> <pre><code>limit 10\n</code></pre> <p>Leave only the keypaths provided, discarding all other keys from an entry:</p> <pre><code>choose &lt;keypath&gt;, &lt;keypath&gt; \u2026, &lt;keypath&gt;\n</code></pre> <p>Cast any expression to one of the following types [bool, num, string]:</p> <pre><code>: (cast)\n</code></pre> <ul> <li>Ex:</li> </ul> <pre><code>filter $d.x:num &gt; 3\n</code></pre> <p>Extract parts of one keypath into new keypath using extractor-function:</p> <pre><code>extract &lt;keypath&gt; into &lt;keypath&gt; using &lt;extractor-function&gt;\n</code></pre> <ul> <li>Ex:<ul> <li>**Creates field <code>\"y\"</code>of shape: <code>{\"name\" : \"foo\" , \"id\" : \"42\"}</code> given <code>x:\"Name:foo Id:42\u201d</code></li> </ul> </li> </ul> <pre><code>extract $d.x into $d.y using regexp(e=/Name:(?&lt;name&gt;[\\\\w\\\\s]+) Id:(?&lt;id&gt;\\\\d+)/) \n</code></pre> <ul> <li>Ex:<ul> <li>**Creates field <code>\"y\"</code>of shape: <code>{\"a\" : \"42\", \"b\" : \"11\"}</code> given <code>x: \"a=42 b=11\"</code></li> </ul> </li> </ul> <pre><code>extract $d.x into $d.y using kv(pair_delimiter=' ', key_delimiter='=') \n</code></pre> <ul> <li>Ex:<ul> <li>**Creates field <code>\"y\"</code> of shape: <code>{\"a\": 1, \"b\": true}</code> given <code>x:\"{\\\\\"a\\\\\": 1, \\\\\"b\\\\\": true}\"</code> (stringified json object)</li> </ul> </li> </ul> <pre><code>extract $d.x into $d.y using jsonobject()\n</code></pre>"},{"location":"newoutput/dataprime-quick-start-guide/#example-queries","title":"Example queries","text":"<p>Select the 10 \u2018successful\u2019 logs ordered by department_id:</p> <pre><code>source logs\n| find 'success' in $d.result\n| order by $d.department_id\n| limit 10\n\n</code></pre> <p>Find cx-cluster logs (without knowing the log structure):</p> <pre><code>source logs | wildfind 'cx-cluster'\n\n</code></pre> <p>Select 100 log messages along with 'processed\u2019 statuses from \u2018enrichment-ingest\u2019 service where processed \u2260 0:</p> <pre><code>source logs\n| lucene 'NOT log:\"stderr F\"'\n| lucene 'log:\"stdout F\"'\n| filter $d.kubernetes.labels.CX_SERVICE_NAME != 'enrichment-ingest'\n| extract $d.log into $d.stats using regexp(e=/.*T?(?&lt;processed&gt;\\\\d+:\\\\d+:\\\\d+[.,]\\\\d+).*/)\n| filter $d.stats.processed != '0'\n| limit 100\n\n</code></pre> <p>[NEW] DataPrime now supports Data Aggregation, for more information and examples please refer to the DataPrime Cheat Sheet.</p>"},{"location":"newoutput/dataprime-specifying-timestamp-formats/","title":"DataPrime: Specifying Timestamp Formats","text":"<p>The DataPrime functions <code>parseTimestamp</code> and <code>formatTimestamp</code> accept a <code>format</code> argument that can be used to specify the format for parsing, respectively printing a timestamp to a string. The syntax is based on the <code>strftime</code> function from programming languages such as Python, C or Rust. It can be any valid string with embedded format specifiers as detailed in the table below. Other parts of the string which are not format specifiers are reproduced verbatim.</p> Specifier Example Meaning Dates %Y 2023 Full year since BCE, 4 digits %G 2023 Week-based full year, 4 digits %C 20 Century since BCE (first 2 digits of %Y) %y 23 Short year since BCE (last 2 digits %Y) %g 23 Week based short year (last 2 digits of %G) %m 08 Month number (01-12), zero-padded to 2 digits %B August Full month name %b Aug Abbreviated month name, 3 letters %h Aug Alias for %b %d 07 Day of the month (01-31), zero-padded to 2 digits %e 7 Alias for %_d (space-padded) %j 219 Day of the year (001-366), zero-padded to 3 digits %u 1 Day of the week number, one-based (1-7), ISO 8601 %w 0 Day of the week number, zero-based (0-6) %A Monday Full day of the week %a Mon Abbreviated day of the week, 3 letters %U 32 Week number starting on Sunday (00-53), zero-padded to 2 digits %W 32 Week number starting on Monday (00-53), zero-padded to 2 digits %V 32 Week number according to ISO 8601 (01-53), zero-padded to 2 digits %D 08/07/23 Alias for %m/%d/%y (\u201dmonth / day / year\u201d) %F 2023-08-01 Alias for %Y-%m-%d (\u201dyear - month - day\u201d), ISO 8601 %v 7-Aug-2023 Alias for %e-%b-%Y (\u201dday - month - year\u201d) Time %H 19 Hours, 24-based (00-23), zero-padded to 2 digits %k 19 Alias for %_H (space-padded) %I 07 Hours, 12-based (01-12), zero-padded to 2 digits %l 7 Alias for %_I (space-padded) %P pm am or pm %p PM AM or PM %M 06 Minutes (00-59), zero-padded to 2 digits %S 42 Seconds (00-60), 60 on leap seconds, zero-padded to 2 digits %f 82259081 Nanoseconds since the last whole second %3f 082 Milliseconds since the last whole second, zero-padded to 3 digits %6f 082259 Microseconds since the last whole second, zero-padded to 6 digits %9f 082259081 Nanoseconds since the last whole second, zero-padded to 9 digits %.f .082259081 Fractional part of a second (3, 6 or 9 decimals) %.3f .082 Fractional part of a second, 3 decimals (milliseconds) %.6f .082259 Fractional part of a second, 6 decimals (microseconds) %.9f .082259081 Fractional part of a second, 9 decimals (nanoseconds) %R 19:06 Alias for %H:%M (\u201dhours : minutes\u201d) %T 19:06:42 Alias for %H:%M:%S (\u201dhours : minutes : seconds\u201d) Time Zones %Z Europe/Athens Time zone identifier (only when parsing), IANA %z +0300 Offset from UTC in hours and minutes, zero-padded to 4 digits %:z +03:00 Offset from UTC in \u201chours : minutes\u201d format %::z +03:00:00 Offset from UTC in \u201chours : minutes : seconds\u201d format %:::z +03 Offset from UTC in hours, zero-padded to 2 digits Escaping %t \\t Literal tab character %n \\n Literal new line character %% % Literal percent sign Modifiers %-? %-I \u2192 7 Disables padding %_? %_M \u2192 6 Pads with spaces %0? %0e \u2192 07 Pads with zeroes"},{"location":"newoutput/dataprime-specifying-timestamp-formats/#additional-resources","title":"Additional Resources","text":"DocumentationDataPrime Query LanguageDataPrime Cheat SheetGlossary: DataPrime Operators &amp; Expressions"},{"location":"newoutput/dataprime-specifying-timestamp-formats/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/dataprime-widget/","title":"DataPrime Widget","text":"<p>Leverage the capabilities of DataPrime queries in Custom Dashboards using the DataPrime Widget.</p>"},{"location":"newoutput/dataprime-widget/#overview","title":"Overview","text":"<p>Coralogix's innovative DataPrime language empowers you to query your data and transform it through various operations tailored to your specific needs, such as calculation, extraction, and aggregation.</p> <p>In Custom Dashboards, the DataPrime Widget enables you to harness the capabilities of DataPrime queries. As your query evolves, this feature automatically generates optimal visualizations to display your data and suggests modifying your query for additional visualizations.</p>"},{"location":"newoutput/dataprime-widget/#create-a-dataprime-widget","title":"Create a DataPrime Widget","text":"<p>STEP 1. In a custom dashboard, drag and drop the DataPrime Widget from your left-hand sidebar to get started.</p> <p></p> <p>A data table will appear in the top panel of the widget, while the DataPrime query will appear in the bottom panel.</p> <p>STEP 2. Construct your DataPrime query. As your query evolves, select from a list of visualizations supporting your query results - horizontal or vertical bar chart, line chart, gauge, or pie chart - in the right-hand column. Hover over faded visualizations to receive suggestions on modifying your query, enabling you to achieve the desired visualization.</p> <p>STEP 3. Fill in the remaining fields in the right-hand column.</p> <p>STEP 4. [Optional] If you want to save your dashboard with the new widget, click SAVE in the upper right-hand corner.</p> <p>At any stage, view the raw data of the query by clicking RAW DATA on the toolbar of the query pane.</p> DocumentationCustom DashboardsDataPrime Cheat Sheet"},{"location":"newoutput/dataprime-widget/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/diagnostic-data-microsoft-azure-resource-manager-arm/","title":"Diagnostic Data: Microsoft Azure Resource Manager (ARM)","text":"<p>Coralogix provides a seamless integration with Azure cloud, allowing you to send your logs from anywhere and parse them according to your needs.</p> <p>The Azure Diagnostic Data integration allows processing of logs and metrics submitted to an Event Hub using the resource diagnostic settings configuration.</p>"},{"location":"newoutput/diagnostic-data-microsoft-azure-resource-manager-arm/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure account with an active subscription</li> </ul>"},{"location":"newoutput/diagnostic-data-microsoft-azure-resource-manager-arm/#azure-resource-manager-template-deployment","title":"Azure Resource Manager Template Deployment","text":"<p>Sign in to your Azure account and deploy the Diagnostic Data integration by clicking here.</p> <p></p> <p>Notes:</p> <ul> <li> <p>Newly configured diagnostic settings can take up to 90 minutes to begin submission of data to the configured Event Hub.</p> </li> <li> <p>Due to the varying formats of metrics generated by Azure, some metrics may not be supported.</p> </li> </ul>"},{"location":"newoutput/diagnostic-data-microsoft-azure-resource-manager-arm/#fields","title":"Fields","text":"FieldDescriptionSubscriptionAzure subscription for which you wish to deploy the integration. Must be the same as the monitored storage account.Resource GroupResource group into which you wish to deploy the integrationCoralogix RegionRegion associated with your\u00a0Coralogix domainCustom URLCustom URL associated with your Coralogix account. Ignore if you do not have a custom URL.Coralogix Private KeyCoralogix\u00a0Send-Your-Data API keyCoralogix ApplicationMandatory\u00a0metadata field\u00a0sent with each log and helps to classify itCoralogix SubsystemMandatory\u00a0metadata field\u00a0sent with each log and helps to classify itEvent Hub Resource GroupName of the resource group that contains the Event HubEvent Hub Instance NameName of the Event Hub instance to be monitoredEvent Hub Shared Access Policy NameName of a shared access policy of the Event Hub namespaceFunction App Service Plan TypeType of service plan to use for the integrationConsumption is cheapest with support for 'public' Event Hubs.Use Premium if you need to use VNet to configure access to restricted EventHubs."},{"location":"newoutput/diagnostic-data-microsoft-azure-resource-manager-arm/#optional-configuration-options","title":"Optional Configuration Options","text":"<p>If your Event Hub has restricted access, review our optional configuration documentation to learn about VNet Support options.</p>"},{"location":"newoutput/diagnostic-data-microsoft-azure-resource-manager-arm/#additional-resources","title":"Additional Resources","text":"Microsoft Azure Functions Manual IntegrationsEvent HubQueue StorageBlob Storage"},{"location":"newoutput/diagnostic-data-microsoft-azure-resource-manager-arm/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/direct-query-http-api/","title":"Direct Archive Query HTTP API","text":"<p>Use our Direct Archive Query HTTP API to run DataPrime or Lucene queries of your indexed and archived logs without the need to access your Coralogix UI.</p>"},{"location":"newoutput/direct-query-http-api/#api-reference","title":"API Reference","text":"<p>The Direct Archive Query HTTP API is of an HTTP-style method.</p> <p>Our API has predictable resource-oriented URLs, accepts POST JSON-formatted request bodies, returns\u00a0Newline Delimited or NDJSON-formatted responses, and uses standard HTTP response codes, authentication, and verbs.</p>"},{"location":"newoutput/direct-query-http-api/#prerequisites","title":"Prerequisites","text":"<p>Requests must be sent using TLS.</p>"},{"location":"newoutput/direct-query-http-api/#base-url","title":"Base URL","text":"<p>Use the Management Endpoint that matches your Coralogix domain.</p> <p>When sending an HTTPS POST, you are required to present your arguments as JSON.</p> HTTP MethodPOSTContent-TypeJSON"},{"location":"newoutput/direct-query-http-api/#authentication","title":"Authentication","text":""},{"location":"newoutput/direct-query-http-api/#authorization-header","title":"Authorization Header","text":"<p>To authenticate, add an\u00a0<code>Authorization Bearer &lt;key&gt;</code>\u00a0header to your API request. It contains a\u00a0Bearer Token, which identifies a single user, bot user, or workspace-application relationship</p> <p>for authentication.</p> KeyAuthorizationValueBearer &lt;Your Coralogix Logs Query Key&gt;"},{"location":"newoutput/direct-query-http-api/#api-key","title":"API Key","text":"<p>The Direct Archive Query HTTP API uses\u00a0your Logs Query API Key\u00a0to authenticate requests. To access this API key in your Coralogix navigation pane, click Data Flow &gt; API Keys &gt; Logs Query API Key.</p> <p>All API requests must be made over\u00a0HTTPS. Calls made over plain HTTP will fail. API requests without authentication will also fail.</p>"},{"location":"newoutput/direct-query-http-api/#requests","title":"Requests","text":""},{"location":"newoutput/direct-query-http-api/#host","title":"Host","text":"<p>Use the Management Endpoint that matches your Coralogix domain.</p> <p>All requests must be made over HTTPS. The API does not support HTTP.</p>"},{"location":"newoutput/direct-query-http-api/#authorization-header_1","title":"Authorization Header","text":"<p>You must provide an authorization header as described in\u00a0Authentication.</p>"},{"location":"newoutput/direct-query-http-api/#formatting-your-request","title":"Formatting Your Request","text":"<p>For complete guidance on how to structure requests and responses, view this swagger.md, a human-readable document derived from a swagger.json file.</p> <p>Download swagger.json</p> <p>We recommend using OpenAPI Editor or a similar technology for support.</p>"},{"location":"newoutput/direct-query-http-api/#request-body","title":"Request Body","text":"<p>When submitting data to a resource via\u00a0<code>POST</code>, you must submit your payload in JSON.</p> <p>You are required to include a <code>query</code> in the API body.</p> <p>All other fields are optional and form the metadata object.</p>"},{"location":"newoutput/direct-query-http-api/#body-examples","title":"Body Examples","text":""},{"location":"newoutput/direct-query-http-api/#json-object","title":"JSON Object","text":"<p>The following example consists of a JSON object that represents the request</p> <p>Minimal DataPrime Query</p> <pre><code>{\n    \"query\": \"source logs | limit 100\"\n}\n\n</code></pre> <p>Dataprime with Lucene Operator</p> <pre><code>{\n    \"query\": \"source logs | filter log_obj.subsystem_name == 'foo' | lucene 'coralogix.metadata.applicationName:bar'\"\n}\n\n</code></pre> <p>Lucene Query</p> <pre><code>{\n    \"query\": \"coralogix.metadata.applicationName:bar'\",\n    \"metadata\": {\n        \"syntax\": \"QUERY_SYNTAX_LUCENE\"\n    }\n}\n\n</code></pre> <p>Full DataPrime Query</p> <pre><code>{\n    \"query\": \"source logs | limit 100\",\n    \"metadata\": {\n        \"tier\": \"TIER_FREQUENT_SEARCH\",\n        \"syntax\": \"QUERY_SYNTAX_DATAPRIME\",\n        \"startDate\": \"2023-05-29T11:20:00.00Z\",\n        \"endDate\": \"2023-05-29T11:30:00.00Z\",\n        \"defaultSource\": \"logs\"\n    }\n}\n\n</code></pre>"},{"location":"newoutput/direct-query-http-api/#curl","title":"cURL","text":"<p>Basic Usage</p> <pre><code>curl --location '&lt;url&gt;/api/v1/dataprime/query' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;api-key&gt;' \\\n--data '{\n    \"query\": \"source logs | limit 10\"\n}'\n\n</code></pre> <p>With Metadata</p> <pre><code>curl --location '&lt;url&gt;/api/v1/dataprime/query' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;api-key&gt;' \\\n--data '{\n    \"query\": \"source logs | limit 10\",\n    \"metadata\": {\n        \"tier\": \"TIER_FREQUENT_SEARCH\",\n        \"syntax\": \"QUERY_SYNTAX_DATAPRIME\",\n        \"startDate\": \"2023-05-29T11:20:00.00Z\",\n        \"endDate\": \"2023-05-29T11:30:00.00Z\"\n    }\n}'\n\n</code></pre> <p>Notes:</p> <ul> <li> <p><code>startDate</code> and <code>endDate</code> assume Coordinated Universal Time (UTC). If the time zone of your timestamp differs, convert it to UTC or offset the timestamp to your time zone. The timestamp in Coralogix is usually presented in you local time.</p> <ul> <li> <p>For example, to start the query at 11:20 local time:</p> <ul> <li> <p>In San Fransisco (Pacific Daylight Time): 2023-05-29T11:20:00.00-07:00</p> </li> <li> <p>In India (India Standard Time): 2023-05-29T11:20:00.00+05:30</p> </li> </ul> </li> </ul> </li> <li> <p>To view or amend your the time zone settings in your Coralogix account, navigate to Account settings &gt; Preferences &gt; Change Your Time Zone Settings</p> </li> </ul> <p>With <code>defaultSource</code></p> <pre><code>curl --location '&lt;url&gt;/api/v1/dataprime/query' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;api-key&gt;' \\\n--data '{\n    \"query\": \"limit 10\",\n    \"metadata\": {\n        \"tier\": \"TIER_FREQUENT_SEARCH\",\n        \"syntax\": \"QUERY_SYNTAX_DATAPRIME\",\n                \"defaultSource\": \"logs\"\n    }\n}'\n\n</code></pre>"},{"location":"newoutput/direct-query-http-api/#responses","title":"Responses","text":"<p>The\u00a0<code>Content-Type</code>\u00a0representation header is used to indicate the original\u00a0 media type of the resource (prior to any content encoding applied for sending).</p> <p>In responses, a\u00a0<code>Content-Type</code>\u00a0header provides the client with the actual content type of the returned content.</p> <p>Correct results are returned in batches. Each batch may contain multiple rows of results, which are returned as Newline Delimited JSON or ndjson format.</p>"},{"location":"newoutput/direct-query-http-api/#example-response","title":"Example Response","text":"<p>The following is a generic example response. Responses may vary per query and user data.</p> <pre><code>{\n    \"result\": {\n        \"results\": [\n            {\n                \"metadata\": [\n                    {\n                        \"key\": \"logid\",\n                        \"value\": \"c3ca5343-88dc-4807-a8f3-82832274afb7\"\n                    }\n                ],\n                \"labels\": [\n                    {\n                        \"key\": \"applicationname\",\n                        \"value\": \"staging\"\n                    }\n                ],\n                \"userData\": \"{ ... \\\\\"log_obj\\\\\":{\\\\\"level\\\\\":\\\\\"INFO\\\\\",\\\\\"message\\\\\":\\\\\"some log message\\\\\" ... }, ...}\"\n            }\n        ]\n    }\n}\n\n</code></pre>"},{"location":"newoutput/direct-query-http-api/#status-codes","title":"Status Codes","text":"<p>Here are some of the most common status codes to expect.</p> Status Code Description 200 No Error 400 Bad Request 403 Forbidden"},{"location":"newoutput/direct-query-http-api/#failed-requests","title":"Failed Requests","text":"<p>The general format guidelines are displayed when the accompanying status code is returned.</p>"},{"location":"newoutput/direct-query-http-api/#warnings","title":"Warnings","text":"<p>The\u00a0<code>Warning</code>\u00a0response contains information about possible problems with the status of the message. More than one\u00a0<code>Warning</code>\u00a0header may appear in a response.</p> <p><code>Warning</code>\u00a0responses can be applied to any message, before or after query results.</p> Warning Type Description CompileWarning Warning of potential compilation error in your query. In the event of a compilation failure, you will receive an error response. TimeRangeWarning When the time frame for your query has been built incorrectly or exceeds internal limits NumberOfResultsLimitWarning When the number of query results exceeds internal limits BytesScannedLimitWarning When the number of bytes returned in query results exceeds internal limits DeprecationWarning When a value in your query is changed or deprecated incorrectly"},{"location":"newoutput/direct-query-http-api/#limitations","title":"Limitations","text":"<p>Coralogix places certain limitations on query responses. Warnings are returned when a limit is breached.</p>"},{"location":"newoutput/direct-query-http-api/#results-returned","title":"Results Returned","text":"<p>The number of results returned in rows is limited as follows:</p> <ul> <li> <p>12k for S3 Archive queries</p> </li> <li> <p>12k for OpenSearch queries</p> </li> </ul>"},{"location":"newoutput/direct-query-http-api/#bytes-scanned-limit","title":"Bytes Scanned Limit","text":"<p>The number of bytes scanned (for high-tier data) is limited as follows:</p> <ul> <li>100MB for for OpenSearch queries</li> </ul> <p>This limitation is placed on fetching 100 MB of high-tier data. It does not limit the scanning within the database storage.</p>"},{"location":"newoutput/direct-query-http-api/#rate-limiting","title":"Rate Limiting","text":"<p>The number of requests per minute is limited as follows:</p> <ul> <li>10 requests per minute</li> </ul>"},{"location":"newoutput/direct-query-http-api/#additional-resources","title":"Additional Resources","text":"DocumentationComplete Guidance on Structuring Requests &amp; Responses (Swagger)DataPrime SyntaxLucene SyntaxS3 Archive"},{"location":"newoutput/direct-query-http-api/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/distributed-tracing/","title":"Distributed Tracing","text":"<p>Coralogix has created a unique tracing experience, allowing you to observe and gain instant insights into your modern micro-services infrastructure. Use our updated\u00a0Tracing\u00a0function, alongside\u00a0Logs\u00a0and\u00a0Templates, to enjoy powerful data visualization of your traces and spans.</p> <p>Enjoy distributed tracing using our Tracing\u00a0function. Optimize performance, troubleshoot bottlenecks, monitor latency, and link traces to logs in an improved visual format.</p>"},{"location":"newoutput/distributed-tracing/#what-is-tracing","title":"What is Tracing?","text":"<p>Tracing refers to the ability to trace the flow of a request or transaction through a system. Tracing provides a detailed view of how a request moves through various components of a system, helping to identify bottlenecks and performance issues and understand the overall system behavior.</p> <p>Traces are made up of spans. A span represents a unit of work in a trace. It encapsulates information about a specific operation within the system, such as a method call or an HTTP request. Spans have a start time and duration and may contain additional metadata.</p>"},{"location":"newoutput/distributed-tracing/#query-traces-spans","title":"Query Traces &amp; Spans","text":"<p>To access our Tracing\u00a0function:</p> <p>STEP 1. From the Coralogix navigation toolbar, click Explore &gt;\u00a0Tracing.</p> <p>STEP 2. Choose whether you want to view spans or traces by clicking the SPANS/TRACES button in the toolbar. View only Frequent Search (high priority) data or data of all priority levels (Frequent Search, Monitoring, and Archive).</p> <p></p> <p>STEP 3. Search for traces/spans of interest with the relevant filters. Use Lucene or DataPrime to query your spans.</p> <p>Notes:</p> <ul> <li> <p>When viewing traces, we will present the 50 latest traces that meet your specified filter criteria. Each displayed row is a trace, which may have multiple spans within it.</p> </li> <li> <p>When viewing spans,\u00a015,000\u00a0rows are displayed in either ascending or descending order based on the timestamp. Each row is a single span.</p> </li> <li> <p>Hover over the trace or span number and click the ellipsis that appears to visualize the data, copy the ID, view the raw span, or export as a JSON or CSV file.</p> </li> </ul>"},{"location":"newoutput/distributed-tracing/#filter","title":"Filter","text":"<p>On the left-hand sidebar, you will find the following default filters:</p> <ul> <li> <p>Application &amp; Subsystem</p> </li> <li> <p>Action</p> </li> <li> <p>Service</p> </li> <li> <p>Duration. The Duration\u00a0filter allows you to find traces that last a long time or are within a certain min-max range that you can easily define, either by adjusting the double range slider from each side or by inputting the exact time in milliseconds in the start-end boxes above the slider.</p> </li> <li> <p>Teams. When a user is a member of multiple teams, they can search for a trace across multiple teams in a single action. The list of teams will be displayed under the\u00a0Teams\u00a0filter.</p> </li> </ul> <p>Once you have added all of the relevant filters, click APPLY.</p>"},{"location":"newoutput/distributed-tracing/#save-your-traces-view","title":"Save Your Traces View","text":"<p>If you would like to use the same filters and column order in the future:</p> <p>STEP 1. Click SAVE VIEW at the top of the grid.</p> <p></p> <p>STEP 2. Enter a name for your new view.</p> <p>STEP 3. Select whether you want to keep the view private or share it with your team.</p> <p>STEP 4. Select whether you would also like to save the query parameters.</p> <p>STEP 5. Select if you want to set this as your default view.</p> <p>STEP 6. To access your saved views or your team\u2019s public views, click SAVE VIEW. Once the View Menu pops up, scroll down and click on the name of your view. You can toggle between \u2018All views\u2019/\u2019My views\u2019, or search views by text.</p> <p>Note: When you save the view, you save the view for the Logs and Tracing tabs at the same time.</p>"},{"location":"newoutput/distributed-tracing/#aggregation-function","title":"Aggregation Function","text":"<p>In the tracing screen, the graphs in the top section give you the ability to calculate statistics using any of the supported arithmetic options: Count, AVG, MIN, MAX SUM and Percentiles (50th / 95th / 99th). You have the option of Grouping by: Application, Subsystem, Service and Action.</p> <p>The tracing screen has 3 default graphs:</p> <ul> <li>Max Duration\u00a0grouped by\u00a0Action:</li> </ul> <p></p> <ul> <li>Spans\u00a0(count) grouped by\u00a0Application:</li> </ul> <p></p> <ul> <li>Errors (count) grouped by Service:</li> </ul> <p></p> <p>Choosing any aggregation other than Count will change the Y scale units of the graph into milliseconds.</p> <p></p>"},{"location":"newoutput/distributed-tracing/#visualize-spans","title":"Visualize Spans","text":"<p>Click on a trace of interest to view its underlying spans.</p> <p>Select your preferred visualization mode\u2014Dependencies, Flame, or Gantt view\u2014to explore varied views of the span data.</p> <ul> <li> <p>The top toolbar of the trace shows details of the number of services represented, the span depth, total spans, the number of events that occurred, and the date and total length of the span.</p> </li> <li> <p>Hovering over SERVICES shows you the execution time for each service. This lets you see which services are taking up more or less time within the trace.</p> </li> <li> <p>The right-hand pane presents:</p> <ul> <li> <p>Span Drill-Down. Access the span name, operation name, application, service, duration, subsystem, and status in the\u00a0Span pane. If your span contains actions (such as an HTTP request or DB action), an overview of these actions will also appear in this pane.</p> </li> <li> <p>Events. View all span events in this pane.</p> </li> <li> <p>Resources. View Kubernetes information here.</p> </li> </ul> </li> </ul>"},{"location":"newoutput/distributed-tracing/#dependencies-view","title":"Dependencies View","text":"<p>Choose Dependencies View to view a mapping of span trajectory for your trace.</p> <p></p> <p>Hover over a span action to view the service name, along with its operation name, application, duration, and subsystem.</p> <p></p> <p>Click on a span link to view all of the occurrences and their respective durations.</p> <p>Select a tag to group by from the GROUP BY menu in the bottom left-hand corner of the screen. When you select tags to group by, you must also select the aggregation type (min, max, avg, sum).</p>"},{"location":"newoutput/distributed-tracing/#flame-view","title":"Flame View","text":"<p>Flame view displays you trace\u2019s spans as horizontally stacked rectangles, visually representing their duration and relationships within a trace.</p> <p></p>"},{"location":"newoutput/distributed-tracing/#gantt-view","title":"GANTT View","text":"<p>Gantt view presents spans as horizontal bars along a timeline, visually representing their timing and relationships within a trace.</p> <p></p>"},{"location":"newoutput/distributed-tracing/#apm-features","title":"APM Features","text":"<p>Choose RELATED DATA in your Overview Pane to access our APM features: related logs, events, pod, and host.</p> <p></p> <p></p> <p>Pairing Spans with Related Logs</p> <p>Define the mapping between the trace spans to the related logs by accessing the RELATED LOGS sub-screen and clicking Setup Correlation. This will open a menu where you will be able to add the relevant field containing the span ID.</p> <p></p> <p></p> <p>Use the POD feature to enjoy additional visualized information.</p> <p></p> <p>Use the HOST feature to explore host performance.</p> <p></p> <p>There is also the option to be redirected to the relevant logs by clicking on Open Logs Query. This will direct you to a Logs screen, showing you the log containing the correlated span ID.</p> <p></p>"},{"location":"newoutput/distributed-tracing/#additional-resources","title":"Additional Resources","text":"DocumentationApplication Performance Monitoring (APM)OpenTelemetry"},{"location":"newoutput/distributed-tracing/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/downloading-your-security-report/","title":"Downloading Your Security Report","text":"<p>This guide demonstrates how to download your Coralogix security report via API.</p>"},{"location":"newoutput/downloading-your-security-report/#prerequisites","title":"Prerequisites","text":"<ul> <li>Cloud Security Posture Management (CSPM) set up</li> </ul>"},{"location":"newoutput/downloading-your-security-report/#configuration","title":"Configuration","text":"<p>STEP 1. Access your API URL based on the domain associated with your account.</p> .comIrelandng-api-http.coralogixstg.wpengine.com.app.eu2.coralogixstg.wpengine.comStockholmng-api-http.eu2.coralogixstg.wpengine.com.app.coralogixsg.comSingaporeng-api-http.coralogixsg.com.inMumbaing-api-http.app.coralogix.in.usUnited Statesng-api-http.coralogix.us <p>STEP 2. Access your Coralogix Logs Query Key.</p> <ul> <li>On your Coralogix dashboard, click Data Flow &gt; select API Keys.</li> </ul> <p></p> <ul> <li>Copy your Logs Query Key.</li> </ul> <p></p> <p>STEP 3. Create the API.</p> URLhttps:// {{Coralogix domain}}/xdr/get-reportHTTP MethodPOSTContent Typeapplication/jsonAuthorizationBearer {{Logs Query key}} <p>STEP 4. Schema</p> <p>Request schema.</p> <pre><code>{\n    \"executionId\": string(uuid), // in case it's not provided using the last scan id\n    \"filter\": {\n        \"region\": string[],\n        \"account\": string[],\n        \"complianceFramework\": string[],\n        \"provider\": string[], // \"aws\", \"gcp\", \"azure\", \"github\", etc...\n    \"service\": string[], // \"RDS\", \"BIG QUERY\", \"S3\", etc\n        \"testName\": string[], // sort name of the security rule (testIdentity)\n        \"result\": string[] // (enum) \"Passed\", \"Failed\"\n        \"severity\": int[], // (enum) 1 - Low, 2 - Medium, 3 - High, 4 - Critical \n        \"active\": string[] // (enum): \"Enabled\", \"Disabled\"\n    }\n}\n</code></pre> <p>Note: Every field in the request payload is optional. Passing a <code>null</code> value or ignoring that field is the same as passing an empty list.</p> <p>Compliance frameworks and short names:</p> Snowbit snowbit CIS AWS 1.4.0 cis_aws HIPAA hipaa ISO-27001 iso_27001 PCI DSS 3.1.0 pci_dss SOC 2 soc2 <p>Response schema:</p> <pre><code>{\n    \"executionId\": string // uuid v4 format\n    \"data\":[\n    {\n      \"region\": string,\n      \"account\": string,\n      \"complianceFrameworks\": string[],\n      \"provider\": string,\n            \"category\": string, // From the category view eg: \"Database\", \"Storage\", \"Identity Management\", etc\n      \"service\": string, // \"RDS\", \"BIG QUERY\", \"S3\", etc\n      \"testName\": string,\n      \"severity\": int, // enum: 1 - Low, 2 - Medium, 3 - High, 4 - Critical \n      \"resourceName\": string,\n      \"resourceId\": string,\n      \"passed\": boolean,\n      \"active\": boolean\n    }\n  ]\n}\n</code></pre>"},{"location":"newoutput/downloading-your-security-report/#additional-resources","title":"Additional Resources","text":"DocumentationCloud Security Posture Management (CSPM)"},{"location":"newoutput/downloading-your-security-report/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/duo-security/","title":"Duo Security","text":"<p>This tutorial demonstrates how to seamlessly send your Duo Security authentication and administrative logs to Coralogix.</p>"},{"location":"newoutput/duo-security/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Duo Security account</p> </li> <li> <p>Server capable of fetching data from Duo Security (e.g. EC2, VM Instance, etc.)</p> </li> </ul>"},{"location":"newoutput/duo-security/#admin-api-setup","title":"Admin API Setup","text":"<p>Add the Duo Security Admin API to your Duo instance.</p> <p>STEP 1. Log in to the Duo Security admin portal, the storage place for your authentication and administrative logs.</p> <p>STEP 2. Click on Applications &gt; Protect an Application.</p> <p>STEP 3. Select the Admin API.</p> <p></p> <p>STEP 4. Once the Duo Admin API application is created, copy the host name and key values to use in the Duo Security log sync configuration. Use the integration key (ikey), secret key (skey), and API hostname (hostname) values here to populate the configuration script.</p> <p></p>"},{"location":"newoutput/duo-security/#log-sync-setup-configuration","title":"Log Sync Setup &amp; Configuration","text":"<p>Full instructions for this section can be found here.</p> <p>STEP 1. Install Python3 on the server if you haven't already done so.</p> <p>STEP 2. Clone the duo_log_sync repo.</p> <p>STEP 3. Create a temp directory at <code>c:\\temp</code> to store your log files.</p> <ul> <li>Those using Linux operating systems will already have a <code>/tmp</code> directory that can be used.</li> </ul> <p>STEP 4. Create a file called <code>config.yml</code> inside <code>...\\duologsync\\config.yml</code>.</p> <ul> <li> <p>Those using Windows operating systems will need to escape the directory references and put the full path, as in the example below.</p> </li> <li> <p>Input the skey, ikey, and hostname values from your Admin API application.</p> </li> </ul> <p>Linux config.yml file:</p> <pre><code>duoclient:\n  skey: \"ENTER-SECRET-KEY-HERE\"\n  ikey: \"ENTER-INTEGRATION-KEY-HERE\"\n  host: \"ENTER-API-HOSTNAME-HERE\"\n\nlogs:\n  logDir: \"/tmp\"\n  endpoints:\n    enabled: [\"auth\", \"telephony\", \"adminaction\"]\n  polling:\n    duration: 5\n    daysinpast: 1\n  checkpointDir: \"/tmp\"\n\ntransport:\n  protocol: \"TCP\"\n  host: \"localhost\"\n  port: 8877\n  certFileDir: \"/tmp\"\n  certFileName: \"selfsigned.cert\"\n\nrecoverFromCheckpoint:\n  enabled: FalseWindows config.yml\n</code></pre> <p>STEP 5. Save the file.</p> <p>STEP 6. Create a self-signed certificate and place it in the <code>tmp</code> directory.</p> <p>STEP 7. Inside the <code>duo_log_sync</code> directory use the command-line to type:</p> <pre><code>python3 setup.py install\n</code></pre> <p>STEP 8. Once the application is running, deploy the Fluentd log shipper to intercept the traffic. Send your logs to Coralogix on port 8877.</p>"},{"location":"newoutput/duo-security/#additional-resources","title":"Additional Resources","text":"DocumentationFluentdExternalDuo Security GitHub Repository"},{"location":"newoutput/duo-security/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/dynamic-tco-app/","title":"Dynamic TCO App","text":"<p>TCO pipelines are one of the key features in Coralogix. Customers are allowed to define in which way their logs are distributed across 3 distinct use cases.</p> <p>One of our customers had a Coralogix quota alarm set up to send an email to them when the quota reached 90%. When the alert was triggered, they logged into Coralogix and changed the distribution of their logs using our TCO Optimizer. This way they could address the spike in their log volume without reaching or surpassing the account limits.</p> <p>This is a great solution for ensuring that there are no billing overages or data loss, but ideally, the process would be automated. So we started thinking of a solution for them (and the rest of our customers).</p> <p>We developed a Lambda function that will, upon a POST Request (API Gateway), set up the \u201cEmergency\u201d TCO Policy and Overrides to the account. At 00:00 UTC, when the daily quota resets, it will revert back to the \u201cOriginal\u201d TCO Policy and Overrides.</p> <p>Now, you can automatically handle a volume increase without reaching your quota limit.</p> <p>View the full Dynamic TCO App installation instructions.</p> <p>To start, we wanted the user to be able to easily change their \u201cEmergency TCO Policy and Overrides\u201d when needed, so we followed the same syntax that the TCO Optimizer API uses for Creating Policy. This way the body of our request will be an array of \u201cTCO Policy\u201d.</p> <p>This means that you can pick one of the items in the below body example and use it as the body of the TCO Optimizer API Call, and it will successfully create the corresponding Policy.</p> <p>The most important thing is to define which TCO Rules we will want to apply once we reach quota usage warning.</p> <p>Here is the structure of the Dynamic TCO POST:</p> <p>Header:</p> <pre><code>{\n  \"Content-Type\": \"application/json\",\n  \"Function-Key\": \"thisismysecretkey\"\n}\n</code></pre> <p>And Body:</p> <pre><code>[\n  {\n    \"name\": \"All low\",\n    \"priority\": \"low\",\n    \"severities\": [\n      4,\n      5,\n      6\n    ]\n  },\n  {\n    \"name\": \"Policy Creation test new\",\n    \"priority\": \"medium\",\n    \"severities\": [\n      1,\n      2,\n      3\n    ]\n  }\n]\n</code></pre> <p>With this setup, we are only missing a way we can trigger this. For that, we are going to use Coralogix\u2019s alerting mechanism and Audit Account.</p> <p>When the quota warning threshold is surpassed a new log will be written in the Audit Account. We can then configure an alert on this event that will send a webhook to our Lambda Function with the body and headers requirements.</p> <p>This way as soon as your quota reaches 90%, the alert will automatically trigger the function to change your TCO Policy and Overrides so you can cope with the high volume of logs.</p> <p>At 00:00 UTC, a Cron event (A scheduled trigger) will trigger the Lambda to restore your original TCO Policy and Overrides.</p> <p>For traceability, the Lambda function will log the changes in your Coralogix account and save them to the S3 bucket configured in your TCO Policy and Overrides configuration.</p> <p>This way you can not only control how your logs are categorized on arrival with our normal TCO Pipelines but also have emergency TCO Pipelines setup for handling log volume bursts.\u00a0</p> <p>View the full Dynamic TCO App installation instructions.</p> <p>If you have any questions, feel free to reach out to our 24/7 support team via our in-app chat!</p>"},{"location":"newoutput/email-group-outbound-webhooks/","title":"Email Group Outbound Webhooks","text":"<p>Enhance your observability workflows by sending real-time event notifications and log data to an Email group, a pre-defined group of email recipients. With this outbound webhook, you can easily automate responses to critical events, optimizing your organization's incident management and alerting processes.</p>"},{"location":"newoutput/email-group-outbound-webhooks/#create-an-email-groups-webhook","title":"Create an Email Groups Webhook","text":"<p>STEP 1. From the Coralogix toolbar, navigate to Data Flow &gt; Outbound Webhooks.</p> <p>STEP 2. In the Outbound Webhooks section, click EMAIL GROUPS WEBHOOK.</p> <p></p> <p>STEP 3. Click + ADD NEW.</p> <p></p> <p>STEP 4. Enter the following details for your webhook:</p> <ul> <li> <p>Webhook Name. Enter a name for your webhook that will enable you to easily identify this webhook later when attaching it to one of your alerts.</p> </li> <li> <p>Add Emails. Enter the emails you want to add to your email group. Alternatively, upload a CSV file with the emails.</p> </li> </ul> <p>STEP 5. Click CONTINUE.</p> <p>STEP 6. Once the webhook is created, select the alert(s) for which this webhook will be used, once an alert is triggered.</p> <p></p> <p>STEP 7. Click ADD next to the alert(s) of choice. The Alert Notifications panel will appear.</p> <p></p> <p>STEP 8. In the Notification Groups section, the new webhook will appear. Click + Add WEBHOOK and select the newly created webhook. Click SAVE CHANGES. You can assign the webhook to more than one notification group.</p> <p>STEP 9. Click FINISH.</p> <p></p>"},{"location":"newoutput/email-group-outbound-webhooks/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Integration Packages"},{"location":"newoutput/email-group-outbound-webhooks/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/error-analytics/","title":"Error Analytics","text":"<p>Error Analytics in Real User Monitoring (RUM) offers a powerful, at-a-glance view of error data, providing users with comprehensive insights into error occurrences within their web applications. By breaking down error data into different dimensions such as device, operating system, browser, URL, users affected, app version, and custom log labels, Error Analytics allows you to quickly identify the sources of issues and effectively prioritize your efforts. These visualizations not only streamline the troubleshooting process but also enable you to improve user experience, make data-driven decisions, and take proactive measures to minimize errors.</p>"},{"location":"newoutput/error-analytics/#benefits","title":"Benefits","text":"<p>Use Error Analytics to enjoy these benefits:</p> <ul> <li> <p>Granular Error Insights. Error Analytics provides detailed visualizations that break down error data into various dimensions, such as device, OS, browser, URL, users affected, app version, and log labels. This granular level of insight allows users to pinpoint the specific factors contributing to errors.</p> </li> <li> <p>Faster Troubleshooting. By categorizing errors based on these dimensions, users can quickly identify the common denominators associated with errors. This accelerates the troubleshooting process, making it easier to isolate the root causes and resolve issues promptly.</p> </li> <li> <p>Optimized User Experience. Understanding which devices, browsers, or OS versions are most susceptible to errors helps in optimizing the user experience. Users can prioritize development efforts to ensure compatibility with the most commonly used platforms.</p> </li> <li> <p>User-Centric Insights. Error data broken down by users affected provides a user-centric perspective. This enables teams to focus on issues impacting a larger number of users, enhancing user satisfaction and loyalty.</p> </li> </ul>"},{"location":"newoutput/error-analytics/#get-started","title":"Get Started","text":"<p>STEP 1. Once you have installed our Browser SDK, in your Coralogix toolbar, navigate to RUM &gt; Browser Error Tracking.</p> <p>STEP 2. Access Error Analytics by clicking on ANALYTICS on the top of your Error Screen.</p> <p></p> <p>Alternatively, view analytics in Error Template View, where similar errors with shared attributes are grouped into a single template for swift and easy analysis. Here you will find analytics only for a particular template.</p>"},{"location":"newoutput/error-analytics/#error-analytics-screen","title":"Error Analytics Screen","text":"<p>Error Analytics presents detailed visualizations that break down error data into various dimensions, such as device, OS, browser, URL, users affected, app version, and log labels. These visualizations are presented for all front-end errors or those from a particular error template.</p> <p>The Error Analytics screen includes:</p> <ul> <li>KPIs. Key performance indicators, including number of errors and users affected.</li> </ul> <p></p> <ul> <li> <p>Device Pie Chart. View a break down of errors per device by hovering over the inner part of the pie chart. Hover over the outer section to view errors per device version.</p> </li> <li> <p>OS Pie Chart. View a break down of errors per operating system by hovering over the inner part of the pie chart. Hover over the outer section to view errors per OS version.</p> </li> <li> <p>Browser Pie Chart. View a break down of errors per browser by hovering over the inner part of the pie chart. Hover over the outer section to view errors per browser version.</p> </li> </ul> <p></p> <ul> <li> <p>URL Grid. View a break down of errors per URL.</p> </li> <li> <p>App Version Line Chart. View a break down of errors per app version.</p> </li> </ul> <p></p> <ul> <li>Users Affected Grid. This grid presents you with comprehensive data of the users affected by errors, including user ID and email, last error encountered, number of errors, and percentage of total errors encountered by the user.</li> </ul> <p></p> <ul> <li>Labels Bar Graph. Customize your analytics by selecting up to six log labels by which to organize errors. Labels may include application and subsystem labels, for example.</li> </ul>"},{"location":"newoutput/error-analytics/#additional-resources","title":"Additional Resources","text":"DocumentationError Tracking"},{"location":"newoutput/error-analytics/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/error-template-view/","title":"Error Template View","text":"<p>As part of our multi-faceted Error Tracking, take advantage of the Error Template View, where similar errors with shared attributes are grouped into a single template for swift and easy analysis.</p> <p></p>"},{"location":"newoutput/error-template-view/#overview","title":"Overview","text":"<p>Grouping errors as part of Real User Monitoring is a strategic approach to managing and improving the quality of your digital services. It enhances your ability to diagnose issues, prioritize fixes, optimize performance, and ultimately deliver a better user experience. By understanding the nature and impact of errors, you can make informed decisions to enhance the reliability and performance of your website or application.</p> <p>Error Template View provides you with an overview of all the errors generated from your browser, grouped by templates on the basis of the error source. View the error type, URL, last appearance, number of users the error affects, and the number of times the error occurred. Filter according to your needs.</p>"},{"location":"newoutput/error-template-view/#access-error-template-view","title":"Access Error Template View","text":"<p>Access Error Template View by clicking on the drop-down menu in the upper-right corner of your Error Tracking UI. Move between between ERROR TEMPLATING (grouped errors) and ERROR TRACKING (individual errors).</p> <p></p>"},{"location":"newoutput/error-template-view/#error-template-view-components","title":"Error Template View Components","text":"<p>Error Template View includes:</p> <ul> <li> <p>Error Overall Graph and KPIs. This graph presents the error trend in the application, in addition to the overall number of errors and users affected.</p> </li> <li> <p>Template Grid. Use the error template grid to view error template information, row by row. Modify columns and the information presented based on your specific interests. Clicking on a specific template will take you to the analytics view, where you can see the distribution of the error.</p> </li> <li> <p>Filters. Refine your view by filtering templates according to particular error attributes.</p> </li> <li> <p>Alerts. Create alerts directly from this screen by clicking CREATE ALERT in the upper right-hand corner.</p> </li> </ul> <p></p>"},{"location":"newoutput/error-template-view/#error-attributes","title":"Error Attributes","text":"<p>Front-end errors are collected in groups on the basis of their source. For each source, particular attributes are displayed.</p> SOURCE TYPE ERROR ATTRIBUTES <code>Exception</code> \u2022 Error Type \u2022 Error Message (log message) \u2022 Page URL (fragment) \u2022 Application <code>Custom</code> \u2022 Error Message (log message) \u2022 Page URL (fragment) \u2022 Application <code>Network</code> \u2022 Network Request \u2022 Status Code \u2022 Page URL (fragment) \u2022 Application <p>Source type error attributes are explained below.</p> ATTRIBUTES DESCRIPTION Error Type Describes the general category or class of the error, helping to group similar errors together, e.g., database connection error, file not found, etc. Error Message A concise description of the specific error instance, often including details like error codes or messages that provide context for the issue. Page URL This is a portion of the URL associated with the web page or application where the error occurred, helping to pinpoint the location of the problem. Network Request Pertains to the type of network request (e.g., HTTP GET, POST) that triggered the error, aiding in identifying the source of network-related issues. Status Code Indicates the HTTP status code associated with the network error (e.g., 404 for \u201cNot Found\u201d or 500 for \u201cInternal Server Error\u201d). Application Specifies the particular application or component within your system that generated the error, helping to isolate the source of the issue in a multi-application environment. <p>For example, you might have a template where all the errors have <code>TypeError</code>, with message r.match is not a function, in URL example.com/yourproduct/. Resolving this template would enable you to remove the entire group of errors from the system.</p>"},{"location":"newoutput/error-template-view/#additional-resources","title":"Additional Resources","text":"DocumentationError Tracking: User Manual"},{"location":"newoutput/error-template-view/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/error-tracking/","title":"Error Tracking","text":"<p>The health of your system relies on your ability to monitor and troubleshoot errors experienced by users on your website and applications consistently. This can be challenging, however, especially when dealing with complex systems. Errors may be difficult to reproduce, and diagnosing them can consume time and resources. Without real-time notifications, developers may only become aware of error patterns when users report them, resulting in lost revenue and lower customer satisfaction.</p> <p>As part of our Real User Monitoring (RUM) toolkit, Coralogix offers multi-faceted Error Tracking. Designed to help web application owners and developers gain insights into errors occurring within their users' browsers, this tool allows you to effectively capture and analyze frontend errors to optimize application performance and enhance the overall user experience. What\u2019s more, Error Tracking is powered by our\u00a0Streama\u00a9 technology, allowing your data to run on the Coralogix monitoring pipeline at a third of the cost, without prior indexing.</p> <p>Error Tracking is available for Monitoring and Compliance priority-level data, in addition to Frequent Search.</p>"},{"location":"newoutput/error-tracking/#what-is-error-tracking","title":"What is Error Tracking?","text":"<p>Coralogix Error Tracking was designed to help web application owners and developers gain insights into errors occurring within their users' browsers. Providing a Browser SDK and a user-friendly interface, it allows you to effectively capture and analyze frontend errors to optimize application performance and enhance the user experience.</p> <p>We offer a comprehensive view of your captured frontend errors. Errors are categorized into different types, including application, exception, and network errors. Within their Coralogix UI, users can explore detailed error reports, such as error messages, stack traces, associated URLs, and timestamps. They can also access analytics and visualizations to identify error trends, apply filters, search for specific errors, and receive notifications or alerts for critical issues.</p> <p>Use Error Tracking to:</p> <ul> <li> <p>Stay informed about any fatal issues that may occur by setting alerts on Error Tracking events.</p> </li> <li> <p>Identify important errors and reduce noise by grouping similar errors into issues.</p> </li> <li> <p>Keep track of issues over time to determine when they first started, if they are still ongoing, and how often they are occurring.</p> </li> <li> <p>Collect all necessary context in one place for optimized troubleshooting.</p> </li> </ul>"},{"location":"newoutput/error-tracking/#how-can-i-use-error-tracking","title":"How Can I Use Error Tracking?","text":"<p>Take a look at these use-cases to get a feel for the many ways that Coralogix\u00a0Error Tracking\u00a0can serve you.</p>"},{"location":"newoutput/error-tracking/#investigating-an-exception-error","title":"Investigating an Exception Error","text":"<p>A DevOps engineer accesses the RUM Error Screen in his Coralogix UI.</p> <p></p> <p>He filters according to TypeError (exception error) and finds that the amount of errors is greater than expected.</p> <p></p> <p>He then clicks on the first error of this kind to drill-down and investigate. He is presented with the error specifics: time of occurrence, browser, source, location, application page, and associated user information.</p> <p>The user is presented with other similar errors and a graph displaying the concentration of errors over time.</p> <p></p> <p>He clicks on the STACK TRACE tab to understand the source of the error in the source code, what the error looks like and its evolution.</p> <p></p>"},{"location":"newoutput/error-tracking/#investigating-a-network-error","title":"Investigating a Network Error","text":"<p>A performance engineer accesses the RUM Error Screen in her Coralogix UI. She filters according to network-request (network error) and is presented with many errors. She notices that there was a spike in errors at approximately 10:30.</p> <p>She chooses to drill-down into an internal server error, viewing the associated status code, URL, method, etc.</p> <p></p> <p>She then navigates to the Coralogix DEPENDENCIES VIEW to investigate the server on which the network error occurred. She sees the full trace, originating in the front-end application. With this visualization, she is able to easily locate where in the trace the error occurred.</p> <p></p>"},{"location":"newoutput/error-tracking/#get-started","title":"Get Started","text":""},{"location":"newoutput/error-tracking/#configure-install-our-browser-sdk","title":"Configure &amp; Install our Browser SDK","text":"<p>Coralogix offers a lightweight code tool integrated into the front end of web applications as a prerequisite for Error Tracking. It detects and captures errors that arise within users' browsers, including JavaScript runtime errors, unhandled exceptions, network errors, and application (custom logic) errors. The SDK collects essential error information and additional contextual data, such as browser details and URLs, and securely sent it to our platform through logs for further analysis.</p> <p>If you haven't already done so using our RUM integration package, configure and install our Browser SDK to collect essential error information and associated data and sent it securely to Coralogix.</p>"},{"location":"newoutput/error-tracking/#track-errors","title":"Track Errors","text":"<p>Track your errors and engage with our user-friendly UI using our User Manual.</p>"},{"location":"newoutput/error-tracking/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/error-tracking-user-manual/","title":"Error Tracking: User Manual","text":"<p>As part of our Real User Monitoring (RUM) toolkit, Coralogix offers multi-faceted Error Tracking. Designed to help web application owners and developers gain insights into errors occurring within their users' browsers, this tool allows you to effectively capture and analyze frontend errors to optimize application performance and enhance the overall user experience. Get an overview of all errors generated from your browser or view related errors in a consolidated manner. You can view errors as individual errors (Error Screen) or aggregated into groups (Error Template View).</p> <p>This user manual demonstrates how to engage with our RUM UI to get the most out of our Error Tracking features.</p>"},{"location":"newoutput/error-tracking-user-manual/#get-started","title":"Get Started","text":"<p>Once you have configured and installed our browser SDK, in your Coralogix toolbar, navigate to RUM &gt; Browser Error Tracking.</p>"},{"location":"newoutput/error-tracking-user-manual/#error-screen","title":"Error Screen","text":"<p>Get an overview of all of the errors generated by your browser. View the number of errors and users affected by each, and filter according to your needs. In addition, create alerts directly from this screen by clicking CREATE ALERT in the upper right-hand corner.</p>"},{"location":"newoutput/error-tracking-user-manual/#display","title":"Display","text":"<p>The Error Screen includes:</p> <ol> <li> <p>Filters. Refine your view by filtering errors according to particular attributes.</p> </li> <li> <p>Error Overall Graph and KPIs. This graph presents the error trend in the application and the overall number of errors and users affected.</p> </li> <li> <p>Error Grid. Use the error table grid to view error information, row by row. Modify columns and the information presented based on your specific interests.</p> </li> </ol> <p></p>"},{"location":"newoutput/error-tracking-user-manual/#group-related-errors-error-template-view","title":"Group Related Errors (Error Template View)","text":"<p>In addition to a granular view of all errors, take advantage of our Error Template View, where similar errors with shared attributes are grouped into a single template for quick and easy analysis. Find out more here.</p> <p></p>"},{"location":"newoutput/error-tracking-user-manual/#investigate-specific-errors","title":"Investigate Specific Errors","text":"<p>Drill down and investigate a specific error by clicking on an error of interest. The Specific Error Screen includes:</p> <ol> <li> <p>Error Header. A header bar with information about the error selected.</p> </li> <li> <p>Error Graph. This time series graph presents the frequency of the specific error that has been chosen.</p> </li> <li> <p>Information Grid.</p> <ul> <li> <p>Click on the OVERVIEW tab to acquire additional information surrounding the error, including event ID, type, group by, error status, browser, device type, language, country, and labels.</p> </li> <li> <p>Depending on the error type, you will presented with a STACK TRACE tab, a DEPENDENCIES VIEW tab, or a RELATED LOGS tab.</p> <ul> <li> <p>STACK TRACE allows you to drill down into a specific stack trace tab. It is presented when the error is an unhandled exception - an error in a computer program or application when the code has no appropriate handling exceptions.</p> </li> <li> <p>DEPENDENCIES VIEW allows you to visualize all the services in the error flow and pinpoint specific servers that may have caused the error. It is presented when a network error has occurred.</p> </li> <li> <p>CORRELATE LOGS enables users to view detailed logs associated with errors, providing additional contextual information to aid debugging.</p> </li> </ul> </li> </ul> </li> <li> <p>Similar Errors Grid. View and easily move between identical errors received in a selected time period.</p> </li> </ol> <p></p>"},{"location":"newoutput/error-tracking-user-manual/#stack-trace","title":"Stack Trace","text":"<p>Drill-down and investigate the stack trace of an error by clicking on the STACK TRACE tab.</p> <p></p>"},{"location":"newoutput/error-tracking-user-manual/#dependencies-view","title":"Dependencies View","text":"<p>Visualize all the services in the error flow and pinpoint specific servers that may have caused the error by clicking on the DEPENDENCIES VIEW tab.</p> <p></p>"},{"location":"newoutput/error-tracking-user-manual/#correlate-logs","title":"Correlate Logs","text":"<p>View detailed logs associated with errors, providing additional contextual information to aid debugging by clicking on the CORRELATE LOGS tab.</p> <p></p>"},{"location":"newoutput/error-tracking-user-manual/#add-an-alert-to-track-an-error","title":"Add an Alert to Track an Error","text":"<p>Alerting refers to the practice of setting up automated notifications or alarms that trigger when certain predefined conditions or thresholds are met. These conditions can be related to the performance, health, or behavior of the systems, applications, or infrastructure being monitored. The main purpose of alerting is to promptly notify DevOps teams and developers \u2013 when something unusual or problematic occurs, so you can take appropriate actions to resolve the issue before it escalates.</p> <p>For more detailed information on Coralogix Alerts, see Getting Started with Coralogix Alerts.</p> <p>Coralogix enables you to create alerts directly from the Error Tracking screen, with several different available options for alert creation.</p> <ul> <li>Create a custom alert directly from the error tracking screen. Click the CREATE ALERT button on the right-hand side of the screen. The blank alert creation panel appears on the right-hand side of the screen for you to create your alert.</li> </ul> <p></p> <ul> <li>Create an alert from a specific error. Click on the three dots on the left-hand side of the error number and select CREATE ALERT. The alert creation panel appears with the specified error details already in the query.</li> </ul> <p></p> <ul> <li>Create an alert from a specific template. Click on the three dots on the left-hand side of the template number and select CREATE ALERT. The alert creation panel appears with the specified template details already in the query.</li> </ul> <p></p> <ul> <li>Create an alert from a template or error drill-down page. Click on the CREATE ALERT button on the right-hand side of the screen. The alert creation panel appears with the specified error or template details already in the query.</li> </ul> <p></p>"},{"location":"newoutput/error-tracking-user-manual/#additional-resources","title":"Additional Resources","text":"DocumentationReal User MonitoringError TrackingError Template ViewBrowser SDK Installation Guide"},{"location":"newoutput/error-tracking-user-manual/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/events2metrics/","title":"Events2Metrics","text":"<p>Coralogix Events2Metrics enables you to generate metrics from your spans and logs to optimize storage without sacrificing important data.</p> <p>Define a query and Coralogix will execute it every minute and store different data aggregations in a long-term index. Metrics start to gather from the point in time in which they were defined. The available query time range for your Events2Metrics indices is 90 days. With Events2Metrics activated, you can create up to 30 metric rules and set the retention period to any length, providing ample opportunity for data analysis without retention restrictions. Each organization is limited to a quota of 10M total metric permutations per day.</p>"},{"location":"newoutput/events2metrics/#activation","title":"Activation","text":"<p>STEP 1. In your navigation pane, click Data Flow &gt; Events2Metrics. Click NEW METRIC.</p> <p></p> <p>STEP 2. Input Details.</p> <p></p> <ul> <li> <p>Name. The name chosen will appear in the field representing this metric in the long-term index.</p> </li> <li> <p>Description. Describe your metric.</p> </li> </ul> <p>STEP 3. Select your Event Source. Choose either Logs or Spans.</p> <p></p> <p>STEP 4. Define your Query.</p> <p></p> <ul> <li>Use a written text query or a Lucene type of query.</li> </ul> <p>STEP 5. Define Scope.</p> <ul> <li>For spans:<ul> <li>Filter by Applications, Subsystems, and Service. For Actions, filter particular HTTP requests and their associated services.</li> </ul> </li> </ul> <p></p> <ul> <li>For logs:<ul> <li>Filter by Applications, Subsystems, and Severities.</li> </ul> </li> </ul> <p></p> <p>STEP 6. Define Metric Fields (optional).</p> <p></p> <ul> <li> <p>Notes:</p> <ul> <li> <p>Define up to 10 fields for which the metrics will be collected.</p> </li> <li> <p>For each metric, you can choose an aggregation function that will aggregate the stream of the data and calculate Max/Min/Count/Avg/Sum/Histogram/Samples in 1 minute granularity.</p> </li> <li> <p>A Histogram bucket is a range of values within a histogram. When you create a histogram, you specify the ranges of the values that will be used to group the data. Each bucket in the histogram represents the number of observations that fall within the range of values for that bucket. More granular buckets provide more accurate percentiles during querying, but will increase the number of time series and storage.</p> </li> <li> <p>When selecting Histogram aggregation, you will need to provide the buckets that represent the distribution of the data.</p> </li> <li> <p>An example Histogram bucket from a CPU metric looks as follows:\u00a0<code>0, 10, 30, 45, 50, 60, 70, 85, 90, 100</code> - numbers separated by commas.</p> </li> <li> <p>When selecting Samples, you will also need to select either Min or Max aggregation. Coralogix collects 4 samples per minute, with each sample representing a quarter of a minute. When we receive data from the same quarter, we take the minimum or the maximum (depending on the selection) from the old and new data.</p> </li> </ul> </li> </ul> <p>STEP 7. Define Labels.</p> <p></p> <ul> <li>Create up to six labels. Contact our support staff should you need more.</li> </ul> <p>STEP 8. Choose the maximum amount of Metric Permutations allowed for this metric.</p> <p>Notes:</p> <ul> <li> <p>Each organization is limited to a quota of 10M total metric permutations per day.</p> </li> <li> <p>Each metric rule has a particular permutation quota. It appears in grey in the input field.</p> </li> <li> <p>Metric rules are blocked when a particular quota or the overall organization quota has been met, whichever comes first.</p> </li> <li> <p>Given the set Alert conditions above, the graph displays the number of permutations per day that would have theoretically resulted in the previous 7 days for this metric rule for high and medium priority logs. Use this information to determine the maximum amount of permutations you would like to allow for this metric moving forward, based on the theoretical usage. Choose a number that is high enough to allow average daily permutations, but not so high that it will cause you to pass your overall daily quota.</p> </li> </ul> <p></p>"},{"location":"newoutput/events2metrics/#default-metrics","title":"Default Metrics","text":"<p>With every Events2Metrics created in Coralogix, a default metric is created. This metric counts the total logs or spans that matches the query and the scope that was provided in the Events2Metrics setup. The metric name will be\u00a0<code>&lt;Events2Metrics Name&gt;_cx_docs_total</code> , and it can be queried via Grafana, as with any other metric. This default metric can be grouped by the labels that were defined in the Labels section.</p>"},{"location":"newoutput/events2metrics/#usage","title":"Usage","text":"<p>A Count metric is enabled for tracking the number of spans or logs that match the Events2Metrics filters.</p> <ul> <li> <p>In Grafana, explore the number of spans or logs meeting the Events2Metrics conditions using Query, Applications, Subsystems, and Severities.</p> </li> <li> <p>You may also utilize PromQL while creating a Metric Alert and query the default metric.</p> </li> </ul>"},{"location":"newoutput/events2metrics/#additional-resources","title":"Additional Resources","text":"<p>Check out this video tutorial.</p>"},{"location":"newoutput/events2metrics/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/example/","title":"ExampleMayaTammy","text":"<p>dddd</p>"},{"location":"newoutput/explore-screen/","title":"Explore Screen: Logs","text":"<p>The Coralogix Explore Screen is a powerful and user-friendly interface designed to provide a comprehensive view of your telemetry data. With its intuitive layout and well-organized presentation, users can effortlessly navigate through their data, gaining valuable insights into their system's behavior.</p> <p>This tutorial walks you through exploring logs in your Explore Screen.</p> <p></p>"},{"location":"newoutput/explore-screen/#overview","title":"Overview","text":"<p>Use the Explore screen to:</p> <ul> <li> <p>Filter and query your telemetry data to pinpoint specific data points.</p> </li> <li> <p>View, filter, and data of all TCO priorities.</p> </li> <li> <p>Correlate logs and traces to identify cause-and-effect relationships, track the flow of events, and recognize how various system components interact.</p> </li> <li> <p>Create new alerts for your data directly from the Explore screen.</p> </li> </ul>"},{"location":"newoutput/explore-screen/#filter-your-data","title":"Filter Your Data","text":""},{"location":"newoutput/explore-screen/#filter-using-predefined-parameters","title":"Filter Using Predefined Parameters","text":"<p>In the left-hand sidebar, filter your data by any of the following parameters:</p> <ul> <li> <p>teams</p> </li> <li> <p>application and subsystem</p> </li> <li> <p>severity</p> </li> </ul> <p>The Filters section shows the count-per-filter value at the side of each filter option. This shows how many results will appear when each filter is applied. It also provides a way to generate a count distribution graph for the filtered key using the graph icon that appears when you hover over a heading.</p>"},{"location":"newoutput/explore-screen/#filter-using-keywords","title":"Filter Using Keywords","text":"<p>Add filters using keyword fields by clicking ADD FILTER at the top of the filter section or by clicking on a JSON field and selecting ADD TO FILTER LIST in the dropdown menu.</p> <p></p>"},{"location":"newoutput/explore-screen/#time-selection","title":"Time Selection","text":"<p>Select the time range for your query using the data range dropdown on the upper right-hand side of the browser window.</p> <p></p> <p>The default time period is the last 15 minutes. It can be modified and filtered using the following:</p> <ul> <li> <p>Tags: Allow you to search for an\u00a0application tag\u00a0and find all logs associated with it</p> </li> <li> <p>Custom: Choose an absolute date/time range to filter.</p> </li> <li> <p>Relative: Choose a time window between (current time \u2013 t1) and (current time \u2013 t2). Intervals are set in hours, minutes, and seconds.</p> </li> <li> <p>Quick: Select a predefined query interval with one click. Intervals are set in minutes, hours, and days.</p> </li> </ul> <p>Dragging the mouse across a section of the logs flow graph zooms into the previous query time range and sets a new time interval.</p>"},{"location":"newoutput/explore-screen/#query-your-data","title":"Query Your Data","text":"<p>Easily query your logs with our state-of-the-art functionalities.</p>"},{"location":"newoutput/explore-screen/#frequent-search-all-logs","title":"Frequent Search / All Logs","text":"<p>In the left-hand panel, you can execute a default query of your frequent search (high-priority) logs or query all logs - compliance (low priority), monitoring (medium priority), and frequent search (high priority) logs.</p> <p></p>"},{"location":"newoutput/explore-screen/#logs-query","title":"Logs Query","text":"<p>Query your logs using the \u201cSearch logs\u201d entry field with the support of the logs screen dropdown menu that helps you build queries based on the log fields using Lucene or DataPrime query languages.</p> <p>When clicking on a JSON field value, a drop-down menu will open:</p> <p></p> <p>In this example, clicking on the option INCLUDE IN QUERY adds the expression \u2018<code>_*exists_*: \"resource.attributes.k8s.container.name\"</code>\u2019 to the query. Choosing the EXCLUDE FROM QUERY option adds the expression \u2018<code>NOT_*exists_*: \"resource.attributes.k8s.container.name\"</code>\u2019 to the query.</p> <p>You can also copy the complete JSON path to the clipboard, which can help in building query expressions.</p> <p></p>"},{"location":"newoutput/explore-screen/#results-displayed","title":"Results Displayed","text":"<p>For every query, 15,000 logs are displayed. The logs can be displayed in either ascending or descending order based on the timestamp.</p>"},{"location":"newoutput/explore-screen/#clear-a-query","title":"Clear a Query","text":"<p>Clicking on the x to the right of the query string deletes the query string but does not affect the rest of the query parameters (filters and time window).</p>"},{"location":"newoutput/explore-screen/#logs-and-visualizations","title":"Logs and Visualizations","text":""},{"location":"newoutput/explore-screen/#data-types-formatting","title":"Data Types &amp; Formatting","text":"<p>There are four tabs to the top left in this section:</p> <ul> <li> <p>Logs \u2013 Log query results.</p> </li> <li> <p>Templates \u2013\u00a0Loggregation templates are logs automatically identified and clustered into the same type/origin.</p> </li> <li> <p>Tracing \u2013 View\u00a0distributed tracing\u00a0results.</p> </li> <li> <p>DataMap (BETA)\u00a0\u2013 View and compose maps combining infrastructure, business metrics, and log data.</p> </li> </ul> <p>On the top right panel, there are five options:</p> <ul> <li> <p>Reset \u2013 Clears all query parameters (including filters).</p> </li> <li> <p>Create Alert \u2013 This button brings up the Create Alert prompt, for creating\u00a0User-Defined Alerts.</p> </li> <li> <p>Row Formatting \u2013 Change the format of how the logs are displayed.</p> <ul> <li> <p>1-Line \u2013 logs are condensed into one line.</p> </li> <li> <p>2-Line \u2013 logs are condensed into two lines.</p> </li> <li> <p>Condensed \u2013 the entire log is visible in wrapped format.</p> </li> <li> <p>JSON \u2013 the default view where JSON objects are parsed.</p> </li> <li> <p>List \u2013 presents log data in an easy-to-read list of key-value pairs.</p> </li> </ul> </li> <li> <p>Columns \u2013 see explanation of Columns button below.</p> </li> <li> <p>Settings \u2013 change the settings of the Explore page.</p> </li> </ul>"},{"location":"newoutput/explore-screen/#custom-and-saved-views","title":"Custom and Saved Views","text":"<p>Custom views help organize specific, relevant log information, as well as views that help other users work and retrieve important data more efficiently. You can also save your custom views for use at a future time. For more information, see Explore Screen - Custom Views.</p>"},{"location":"newoutput/explore-screen/#expand-log-text","title":"Expand Log Text","text":"<p>If the log is in a view that can be expanded, clicking the arrow \u2018&gt;\u2019 icon or double-clicking on the log expands and shows the entire log message.</p> <p></p> <p></p>"},{"location":"newoutput/explore-screen/#view-surrounding-logs","title":"View Surrounding Logs","text":"<p>In some cases, you may want to view the logs that occurred before and after a particular log to get additional context to better understand the log.</p> <p>To view the logs surrounding a specific log:</p> <p>STEP 1. Hover over the log number and click the three dots that appear.</p> <p>STEP 2. Click VIEW SURROUNDING LOGS.</p> <p></p> <p>STEP 3. Select the time window for which to view the surrounding logs (5 seconds, 30 seconds, 1 minute, 5 minutes, or 10 minutes).</p> <p>Notes:</p> <ul> <li> <p>When surrounding logs are displayed, the log of interest is highlighted amongst all the results returned.</p> </li> <li> <p>However, if there are more than 15,000 results, and the timeline's log distribution means your specific log isn't within the first or last 15,000, it won't be highlighted since it's not part of the currently displayed results.</p> </li> </ul>"},{"location":"newoutput/explore-screen/#manage-columns","title":"Manage Columns","text":"<p>Clicking on the COLUMNS button opens the Manage Columns window. There, fields can be arranged between two lists by dragging them. The APPLY button updates the columns on the logs screen. The Manage Columns window can also be accessed from the SETTINGS menu.</p> <p></p>"},{"location":"newoutput/explore-screen/#manage-keys","title":"Manage Keys","text":"<p>Use our\u00a0Manage Keys\u00a0feature to personalize the JSON layout with fields, by clicking the column icon next to the Column header. This opens the Manage Keys box, allowing you to simplify the way you arrange your columns to view data. Select the keys you wish to view, pin and sort them, and save your view. Find out more\u00a0here.</p> <p></p>"},{"location":"newoutput/explore-screen/#content-column","title":"CONTENT Column","text":"<p>Improve visibility for your log\u2019s most important data by using the\u00a0CONTENT\u00a0column, which displays selected keys from your logs. Find out more\u00a0here.</p> <p></p>"},{"location":"newoutput/explore-screen/#apm-service-drill-down","title":"APM Service Drill-Down","text":"<p>Smoothly transition from reviewing logs to exploring the service responsible for generating them in the APM Service Drill-Down.</p> <p>Within the Service Drill-Down, you'll find a concise overview of the service and its associated metrics.</p> <p>To access this feature, hover over the row number corresponding to the log you're interested in. Then, click on the ellipsis that appears and choose APM SERVICE DRILL-DOWN from the drop-down menu.</p> <p></p> <p>We recommend that your\u00a0subsystem\u00a0name reflect the service or application producing your logs, metrics, and traces. Without this naming in place, you will be rerouted to the Service Catalog page.</p>"},{"location":"newoutput/explore-screen/#additional-resources","title":"Additional Resources","text":"DocumentationManage KeysContent ColumnCustom Views"},{"location":"newoutput/explore-screen/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/explore-screen-content-column/","title":"Content Column","text":"<p>The CONTENT column in the Explore Screen displays selected keys from your logs, improving visibility for your log\u2019s most important data.</p> <p></p>"},{"location":"newoutput/explore-screen-content-column/#content-column","title":"Content Column","text":"<p>Manage the values displayed in the CONTENT column by clicking on the column settings icon by the CONTENT header. This will open a menu that allows you to select keys known as Content Keys, used to detect the content in your logs. Use the keys available or add to this list.</p> <p></p> <p>Content Keys should contain the string value of one of the existing keys in your logs (i.e. <code>message</code>, <code>msg</code>, <code>log</code>, <code>k8s.log</code>). Content for each log will be extracted according to a predefined ordered list of Content Keys that you create.</p> <p>Once you have inputted your Content Keys, click APPLY. Edit this list as necessary.</p>"},{"location":"newoutput/explore-screen-content-column/#additional-resources","title":"Additional Resources","text":"Documentation Explore Screen"},{"location":"newoutput/explore-screen-content-column/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/explore-screen-custom-views/","title":"Custom Views","text":""},{"location":"newoutput/explore-screen-custom-views/#custom-views","title":"Custom Views","text":"<p>The Explore Screen\u2019s Custom views feature helps organize specific, relevant log information, as well as views that help other users work and retrieve important data more efficiently.</p> <p>A View is defined as a\u00a0query\u00a0that creates the initial logs set that the view starts with, and columns that define the log data that is visible. \u201cText\u201d is a special column (or field) that holds the entire log (including metadata). Every field can be removed or added to a view.</p>"},{"location":"newoutput/explore-screen-custom-views/#save-your-current-view","title":"Save Your Current View","text":"<p>The SAVE VIEW button saves the current settings and customizations of the Explore Screen for use at a later time. This opens a window with options to give the view a name, as well as save the query and filter settings. There is also an option to save the current view as default.</p> <p></p> <p>The Private/Shared option allows the view to be kept private or shared between team members.</p>"},{"location":"newoutput/explore-screen-custom-views/#additional-resources","title":"Additional Resources","text":"DocumentationExplore Screen"},{"location":"newoutput/explore-screen-custom-views/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/explore-screen-manage-columns/","title":"Manage Keys","text":"<p>Personalize the columns presenting your log data using our Manage Keys feature.</p>"},{"location":"newoutput/explore-screen-manage-columns/#manage-keys","title":"Manage Keys","text":"<p>The Manage Keys feature in the Explore Screen can be used to personalize the JSON keys layout, by clicking the column icon next to the column header. This will open the Manage Keys box, allowing you to simplify the way you arrange your keys to view data. Select the keys you wish to view, pin and sort them, and save your view.</p> <p></p> <ul> <li> <p>Sort. Sort unpinned keys as in JSON format (that is, the order they appear in the JSON that appears in the column) or in ascending or descending alphabetical order.</p> </li> <li> <p>Search. Select the keys you wish to view by searching key type and / or using one of the following parameters: STARTS WITH, INCLUDES, or EQUALS.</p> </li> <li> <p>PINNED / INCLUDED / EXCLUDED. Select as many keys as you want and set them to be pinned to the top of the list of keys for easy access.</p> <ul> <li> <p>Hovering over a key will give you the option to include or exclude it from the list of keys to view (depending on its current state), and also the option to pin a key to the top of the list for easy access.</p> </li> <li> <p>You also have the option to include or exclude all keys using the EXCLUDE ALL KEYS or INCLUDE ALL KEYS buttons.</p> </li> <li> <p>For example, in the image below, there are three pinned keys, which can be accessed from above the list of included keys without the need to search for them. There are also two excluded keys which are shown towards the bottom of the list.</p> </li> </ul> </li> </ul> <p></p> <p>It is also possible to pin keys directly from the grid, by clicking on the key, opening the context menu, and then selecting Pin from the menu.</p> <p></p>"},{"location":"newoutput/explore-screen-manage-columns/#additional-resources","title":"Additional Resources","text":"Documentation Explore Screen"},{"location":"newoutput/explore-screen-manage-columns/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/external-labels/","title":"External Labels","text":"<p>Send Coralogix your metrics using external labels, maximizing query performance and adding an extra layer of granularity in your data indexing.</p>"},{"location":"newoutput/external-labels/#feature","title":"Feature","text":"<p>When you send us your data using Prometheus or another agent, you now have the option of using external labels with your metrics.</p> <p>External labels mark label names that already exist in your data as a partition key. Using these labels, they differentiate metrics from different environments, allowing the creation of a separate index for each data source producing metrics. When you query your data from a particular environment, our system will only load the index associated with that environment, maximizing query performance.</p>"},{"location":"newoutput/external-labels/#example","title":"Example","text":"<p>If using, for instance, <code>external_labels=cluster</code> , Coralogix will extract the <code>cluster</code> label from your ingested metrics and then put all metrics with a particular timeseries value <code>cluster=cluster_eu</code> in one index group and another timeseries value <code>cluster=cluster_us</code> in another index group.</p>"},{"location":"newoutput/external-labels/#configuration","title":"Configuration","text":"<p>The following is an example configuration for one external label using the Prometheus remote_write URL.</p> <pre><code>remote_write:\n- url:&lt;https://ingress.coralogixstg.wpengine.com/prometheus/v1?external_labels=cluster&gt;\n  name: '&lt;customer_name&gt;'\n  remote_timeout: 120s\n  bearer_token: '&lt;Send_Your_Data_private_key&gt;'\n</code></pre> <p>The following is an example configuration for two external labels using the Prometheus remote_write URL.</p> <pre><code>remote_write:\n- url:&lt;https://ingress.coralogixstg.wpengine.com/prometheus/v1?external_labels=region&amp;external_labels=cluster&gt;\n  name: '&lt;customer_name&gt;'\n  remote_timeout: 120s\n  bearer_token: '&lt;Send_Your_Data_private_key&gt;'\n</code></pre> <p>Notes:</p> <ul> <li> <p>External labels are attached to the URL.</p> </li> <li> <p>The configuration requires that you input your Coralogix Send-Your-Data API key.</p> </li> </ul>"},{"location":"newoutput/external-labels/#best-practices","title":"Best Practices","text":"<p>Follow these guidelines as best practices for using external labels:</p> <ul> <li>Use external labels to refer to the name of a data source, rather than a particular value.</li> </ul> <p>For example, if <code>external_labels=cluster</code>, label values found in the timeseries may be <code>{cluster: prod}</code> and <code>{cluster: dev}</code>. The Coralogix pipeline will partition data by the <code>cluster</code> value.</p> <ul> <li>Refrain from using external labels with many values.</li> </ul> <p>For every value of your external label, a new index is created. We suggest using a maximum of 20 values per label to ensure optimal query performance.</p>"},{"location":"newoutput/external-labels/#additional-resources","title":"Additional Resources","text":"DocsPrometheus"},{"location":"newoutput/external-labels/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/faqs-kubernetes-observability-using-opentelemetry/","title":"FAQs","text":"<p>Check out these frequently asked questions regarding Kubernetes Observability using OpenTelemetry.</p>"},{"location":"newoutput/faqs-kubernetes-observability-using-opentelemetry/#how-do-i-upgrade-the-otel-integration-chart-to-its-latest-version","title":"How do I upgrade the Otel integration chart to its latest version?","text":"<p>STEP 1. Ensure that your existing Values YAML file is aligned with the latest values.yaml file. This is to avoid configuration issues that may arise.</p> <p>STEP 2. Update the chart repository listing, assuming the OpenTelemetry Integration chart is named <code>otel-integration</code>.</p> <pre><code>helm repo update coralogix\n\n</code></pre> <p>STEP 3. Compare the latest chart version with the Helm installed chart version.</p> <pre><code>helm search repo coralogix | grep otel-integration\nhelm list -n $NAMESPACE\n\n</code></pre> <p>STEP 4. Upgrade to the latest with the following command:</p> <pre><code>helm upgrade otel-integration coralogix/otel-integration -f values.yaml -n $NAMESPACE\n\n</code></pre>"},{"location":"newoutput/faqs-kubernetes-observability-using-opentelemetry/#how-do-i-prevent-telemetry-from-being-exported-to-coralogix","title":"How do I prevent telemetry from being exported to Coralogix?","text":"<p>In cases where telemetry data (such as logs, metrics, or traces) and their corresponding metadata are no longer needed for observability in Coralogix, the Coralogix Exporter can be replaced with the Debug Exporter. This ensures that the telemetry data is not sent to Coralogix. It is important to note that at least one exporter is required, otherwise the Debug Exporter will fail.</p> <p>Below is an example that demonstrates the definition and configuration of the Debug Exporter for traces.</p> <pre><code>opentelemetry-agent:\n  config:    \n    exporters:\n      debug: # Define the debug exporter here\n        verbosity: detailed\n      coralogix:\n        timeout: \"30s\"\n        private_key: \"${CORALOGIX_PRIVATE_KEY}\"\n        domain: \"{{ .Values.global.domain }}\"\n        application_name: \"{{ .Values.global.defaultApplicationName }}\"\n        subsystem_name: \"{{ .Values.global.defaultSubsystemName }}\"\n    service:\n      pipelines:\n        metrics:\n          exporters:\n            - coralogix\n          processors:\n            - batch\n          receivers:\n            - otlp\n        traces:\n          exporters: \n            - debug # Enable only the Debug Exporter for Traces\n          processors: \n            - batch\n          receivers:\n            - otlp\n        logs:\n          exporters:\n            - coralogix\n          processors:\n            - batch\n          receivers:\n            - otlp\n</code></pre>"},{"location":"newoutput/faqs-kubernetes-observability-using-opentelemetry/#how-do-i-fix-multi-line-logs-using-recombine-filelog-operator","title":"How do I fix multi-line logs using recombine FileLog Operator?","text":"<p>In the case of multi-line logs, a single logical log entry may exist in multiple lines, resulting in multiple log records by default. The OpenTelemetry Integration Helm Chart includes a commented section in the values.yaml file to address this issue. Navigate to <code>presets</code> &gt; <code>logsCollection</code> &gt; <code>extraFilelogOperators</code>.</p> <pre><code>presets:\n    metadata:\n      enabled: true\n      clusterName: \"{{.Values.global.clusterName}}\"\n      integrationName: \"coralogix-integration-helm\"\n    logsCollection:\n      enabled: true\n      storeCheckpoints: true\n      maxRecombineLogSize: 1048576\n      extraFilelogOperators: []\n#     - type: recombine\n#       combine_field: body\n#       source_identifier: attributes[\"log.file.path\"]\n#       is_first_entry: body matches \"^(YOUR-LOGS-REGEX)\"\n\n</code></pre> <p>The <code>extraFilelogOperators</code> is an array where we can define any additional Filelog Operators that we want to leverage when shipping logs.</p> <p>Uncomment the example with the recombine operator. This operator is used to recombine log entries from a single log file, as defined by the <code>source_identifier</code> looking at the <code>log.file.path</code> attribute, in cases where multi-line has occurred. To achieve this, a regex is defined for the first line of the log using the <code>is_first_entry</code> field.</p> <p>A working example of a values.yaml file can be seen below:</p> <pre><code>global:\n  domain: \"eu2.coralogixstg.wpengine.com\"\n  clusterName: \"coralogix-cluster\"\n\nopentelemetry-agent:\n  presets:\n    logsCollection:\n      extraFilelogOperators:\n        - type: recombine\n          combine_field: body\n          combine_with: \"\\n\"\n          source_identifier: attributes[\"log.file.path\"]\n          is_first_entry: body matches \"(\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{3}\\\\s\\\\[\\\\w+\\\\]\\\\s\\\\w+\\\\s|\\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{6}Z\\\\s\\\\w+\\\\s)\"\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>The example seeks 2 different regex patterns to recognize the start of the log entry.</p> </li> <li> <p>Double-escape all special characters to match the GoLang standard patterns.</p> </li> </ul> <pre><code>\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{3}\\\\s\\\\[\\\\w+\\\\]\\\\s\\\\w+\\\\s\nAND\n\\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\\\\.\\\\d{6}Z\\\\s\\\\w+\\\\s\n\n</code></pre>"},{"location":"newoutput/faqs-kubernetes-observability-using-opentelemetry/#how-do-i-optimize-batch-sizing","title":"How do I optimize batch sizing?","text":"<p>Coralogix recommends the default otel-integration chart settings for batch processors in all collectors:</p> <pre><code>  batch:\n    send_batch_size: 1024\n    send_batch_max_size: 2048\n    timeout: \"1s\"\n</code></pre> <p>This sizing ensures that the telemetry sent to the Coralogix backend is batched into larger requests, reducing networking overhead and improving performance.</p> <p>These settings impose a hard limit of 2048 units (spans, metrics, logs) on the batch size, balancing the recommended batch size and networking overhead.</p> <p>While you can adjust these settings to suit your requirements, considering the size limits enforced by Coralogix endpoints, currently set to a max of 10 MB after decompression, is essential.</p> <p>Find out more here about configuring your batch processor.</p>"},{"location":"newoutput/faqs-kubernetes-observability-using-opentelemetry/#support","title":"Support","text":"<p>Need help?</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing support@coralogix.com.</p>"},{"location":"newoutput/firehose-cloudtrail-logs/","title":"AWS CloudTrail Logs via Firehose","text":"<p>Streamline the process of ingesting and analyzing logs from your AWS resources using our automated CloudTrail Logs using AWS Kinesis Data Firehose integration package.</p>"},{"location":"newoutput/firehose-cloudtrail-logs/#overview","title":"Overview","text":"<p>AWS CloudTrail logs are comprehensive records of actions performed within your Amazon Web Services (AWS) account, encompassing details like API calls, configuration changes, and user activities. These logs play a pivotal role in fortifying security, ensuring compliance, and enhancing operational transparency. By sending CloudTrail logs to AWS Kinesis Data Firehose, you streamline the process of ingesting and transforming this data, setting the stage for its efficient delivery to external services like Coralogix.</p>"},{"location":"newoutput/firehose-cloudtrail-logs/#benefits","title":"Benefits","text":"<p>CloudTrail logs, when processed through Kinesis Data Firehose and sent to Coralogix, enable real-time monitoring, advanced analytics, and proactive alerting. This combined approach equips you with the tools to seamlessly analyze and visualize your AWS audit data, thereby strengthening security, simplifying compliance endeavors, identifying anomalies, and refining operational efficiency. By harnessing the capabilities of both AWS Kinesis Data Firehose and Coralogix, your CloudTrail logs become actionable insights that safeguard your AWS environment and streamline compliance reporting, thus optimizing the overall value of your CloudTrail data.</p>"},{"location":"newoutput/firehose-cloudtrail-logs/#configuration","title":"Configuration","text":"<p>STEP 1. From your Coralogix toolbar, navigate to Data Flow &gt; Integrations.</p> <p>STEP 2. In the Integrations section, select AWS Cloudtrail Logs via Firehose.</p> <p></p> <p>STEP 3. Click ADD NEW.</p> <p>STEP 4. Input your integration details.</p> <p></p> <ul> <li> <p>Integration Name. Enter a name for your integration. This will be used as a stack name in CloudFormation.</p> </li> <li> <p>API Key. Enter your Send-Your-Data API key or click CREATE A NEW KEY to create a new API key for the integration.</p> </li> <li> <p>Application Name. Enter an application name. The default name is AWS.</p> </li> <li> <p>Subsystem Name. Enter a subsystem name. The default name is Firehose.</p> </li> <li> <p>Enable Dynamic Metadata. When enabled, this sets the <code>applicationName</code> and <code>subsystemName</code> dynamically. When enabled, this feature dynamically sets the <code>applicationName</code> based on the name of the CloudWatch log group.</p> </li> <li> <p>Kinesis Stream ARN. [Optional] Enter the ARN of the Kinesis stream if using Amazon Kinesis Data Streams as a source for logs.</p> </li> <li> <p>AWS Region. Select your AWS region from the dropdown menu.</p> </li> <li> <p>AWS PrivateLink (Advanced Settings). Enabling the use of AWS PrivateLink is recommended in order to ensure a secure and private connection between your VPCs and AWS services. Find out more here.</p> </li> </ul> <p>STEP 6. Click NEXT.</p> <p>STEP 7. Review the instructions for your integration. Click\u00a0CREATE CLOUDFORMATION.</p> <p></p> <p>STEP 8. You will be rerouted to the AWS website. Verify that all of the auto pre-populated values are correct. Click Create Stack.</p> <p>STEP 9. Return to the Coralogix application where you will be presented with instructions on how to create a subscription filter inside your CloudWatch log group .</p> <p></p> <p>Notes:</p> <p>If you provide a Kinesis Stream ARN, Coralogix assumes that the data is in the stream and does not provide any additional instructions. It is the user\u2019s responsibility to deliver data to the stream. Instead of the instructions you will see a message that prompts the user to confirm the integration.</p> <p>STEP 10. Click\u00a0COMPLETE\u00a0to close the module.</p> <p>STEP 11. [Optional] Deploy the\u00a0extension package\u00a0of your choice to complement your integration needs. We offer the following extensions for data originating from CloudTrail and WAF:</p> <ul> <li> <p>AWS CloudTrail</p> </li> <li> <p>AWS WAF</p> </li> </ul> <p>STEP 12. View the logs by navigating to Explore &gt; Logs in your Coralogix toolbar. Find out more here.</p>"},{"location":"newoutput/firehose-cloudtrail-logs/#additional-resources","title":"Additional Resources","text":"DocumentationGetting Started with Coralogix"},{"location":"newoutput/firehose-cloudtrail-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/firehose-cloudwatch-logs/","title":"AWS CloudWatch Logs via Firehose","text":"<p>Streamline the process of ingesting and analyzing logs from your AWS resources using our automated CloudWatch Logs using AWS Kinesis Data Firehose integration package.</p>"},{"location":"newoutput/firehose-cloudwatch-logs/#overview","title":"Overview","text":"<p>AWS CloudWatch logs are records of events and data generated by various AWS resources, applications, and services. These logs play a crucial role in monitoring and troubleshooting your AWS environment. They provide insights into system behavior, errors, and operational performance, allowing you to detect issues, analyze trends, and ensure the reliability and security of your applications.</p>"},{"location":"newoutput/firehose-cloudwatch-logs/#benefits","title":"Benefits","text":"<ul> <li> <p>Gain deeper insights, conduct advanced analytics, set up proactive alerting, and monitor your AWS CloudWatch logs in real time, enhancing your ability to troubleshoot and maintain optimal system performance.</p> </li> <li> <p>Using AWS Kinesis Data Firehose streamlines the data delivery process by providing a reliable and scalable data transfer mechanism. It ensures that your log data reaches Coralogix with minimal latency and no loss of data.</p> </li> </ul>"},{"location":"newoutput/firehose-cloudwatch-logs/#configuration","title":"Configuration","text":"<p>STEP 1. From your Coralogix toolbar, navigate to Data Flow &gt; Integrations.</p> <p>STEP 2. In the Integrations section, select AWS CloudWatch Logs via Firehose.</p> <p></p> <p>STEP 3. Click ADD NEW.</p> <p>STEP 4. Input your integration details.</p> <ul> <li> <p>Integration Name. Enter a name for your integration. This will be used as a stack name in CloudFormation.</p> </li> <li> <p>API Key. Enter your Send-Your-Data API key or click CREATE A NEW KEY to create a new API key for the integration.</p> </li> <li> <p>Application Name. Enter an application name. The default name is AWS.</p> </li> <li> <p>Subsystem Name. Enter a subsystem name. The default name is Firehose.</p> </li> <li> <p>Kinesis Stream ARN. [Optional] Enter the ARN of the Kinesis stream if using Amazon Kinesis Data Streams as source for logs.</p> </li> <li> <p>AWS Region. Select your AWS region from the dropdown menu.</p> </li> <li> <p>AWS PrivateLink (Advanced Settings). Enabling AWS PrivateLink is recommended to ensure a secure and private connection between your VPCs and AWS services. Find out more here.</p> </li> </ul> <p>STEP 6. Click NEXT.</p> <p>STEP 7. Review the instructions for your integration. Click\u00a0CREATE CLOUDFORMATION.</p> <p>STEP 8. You will be rerouted to the AWS website. Verify that all of the auto pre-populated values are correct. Click Create Stack.</p> <p>STEP 9. Return to the Coralogix application, where you will be given instructions on creating a subscription filter inside your CloudWatch log group.</p> <p></p> <p>Notes:</p> <p>If you provide a Kinesis Stream ARN, Coralogix assumes that the data is in the stream and does not provide any additional instructions. It is the user\u2019s responsibility to deliver data to the stream. Instead of the instructions, you will see a message that prompts the user to confirm the integration.</p> <p>STEP 10. Click\u00a0COMPLETE\u00a0to close the module.</p> <p>STEP 11. [Optional] Deploy the\u00a0extension package\u00a0of your choice to complement your integration needs. We offer the following extensions for data originating from CloudTrail and WAF:</p> <ul> <li> <p>AWS CloudTrail</p> </li> <li> <p>AWS WAF</p> </li> </ul> <p>STEP 12. View the logs by navigating to Explore &gt; Logs in your Coralogix toolbar. Find out more here.</p>"},{"location":"newoutput/firehose-cloudwatch-logs/#additional-resources","title":"Additional Resources","text":"DocumentationGetting Started with Coralogix"},{"location":"newoutput/firehose-cloudwatch-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/firehose-fargate-logs/","title":"AWS EKS Fargate Logs via Firehose","text":"<p>Streamline the process of ingesting and analyzing logs from your AWS resources using our automated EKS Fargate Logs using AWS Kinesis Data Firehose integration package.</p>"},{"location":"newoutput/firehose-fargate-logs/#overview","title":"Overview","text":"<p>Amazon Elastic Kubernetes Service (EKS) Fargate logs are the records of events, activities, and information generated by containers running within the EKS Fargate environment. These logs provide essential insights into the operation and performance of your containerized applications. They encompass data related to application behavior, errors, resource usage, and security events. The value of EKS Fargate logs lies in their ability to help you monitor, troubleshoot, and optimize your containerized workloads within EKS. These logs are instrumental for identifying issues, tracking resource utilization, ensuring application reliability, and enhancing security by detecting potential threats.</p>"},{"location":"newoutput/firehose-fargate-logs/#benefits","title":"Benefits","text":"<ul> <li> <p>Ensure effective monitoring, troubleshooting, and management of your containerized applications within EKS.</p> </li> <li> <p>Using AWS Kinesis Data Firehose streamlines the data delivery process by providing a reliable and scalable data transfer mechanism. It ensures that your log data reaches Coralogix with minimal latency and no loss of data.</p> </li> </ul>"},{"location":"newoutput/firehose-fargate-logs/#configuration","title":"Configuration","text":"<p>STEP 1. From your Coralogix toolbar, navigate to Data Flow &gt; Integrations.</p> <p>STEP 2. In the Integrations section, select AWS EKS Fargate Logs via Firehose.</p> <p></p> <p>STEP 3. Click ADD NEW.</p> <p>STEP 4. Input your integration details.</p> <p></p> <ul> <li> <p>Integration Name. Enter a name for your integration. This will be used as a stack name in CloudFormation.</p> </li> <li> <p>API Key. Enter your Send-Your-Data API key or click CREATE A NEW KEY to create a new API key for the integration.</p> </li> <li> <p>Application Name. Enter an application name. The default name is AWS.</p> </li> <li> <p>Subsystem Name. Enter a subsystem name. The default name is Firehose.</p> </li> <li> <p>Enable Dynamic Metadata. When enabled, this sets the <code>applicationName</code> and <code>subsystemName</code> dynamically. When enabled, this feature dynamically sets the <code>applicationName</code> based on the name of the <code>kubernetes.namespace_name</code> field and the <code>subsystemName</code> based on the <code>kubernetes.container_name</code> field.</p> </li> <li> <p>Kinesis Stream ARN. [Optional] Enter the ARN of the Kinesis stream if using Amazon Kinesis Data Streams as a source for logs.</p> </li> <li> <p>AWS Region. Select your AWS region from the dropdown menu.</p> </li> <li> <p>AWS PrivateLink (Advanced Settings). Enabling the use of AWS PrivateLink is recommended to ensure a secure and private connection between your VPCs and AWS services. Find out more here.</p> </li> </ul> <p>STEP 6. Click NEXT.</p> <p>STEP 7. Review the instructions for your integration. Click\u00a0CREATE CLOUDFORMATION.</p> <p></p> <p>STEP 8. You will be rerouted to the AWS website. Verify that all of the auto pre-populated values are correct. Click Create Stack.</p> <p>STEP 9. Return to the Coralogix application where you will be presented with instructions on how to configure <code>log_router</code> built into Fargate Kubelet to route your application logs to the Kinesis Data Firehose stream.</p> <p></p> <p>Notes:</p> <p>If you provide a Kinesis Stream ARN, Coralogix assumes that the data is in the stream and does not provide any additional instructions. It is the user\u2019s responsibility to deliver data to the stream. Instead of the instructions you will see a message that prompts the user to confirm the integration.</p> <p>STEP 10. Click\u00a0COMPLETE\u00a0to close the module.</p> <p>STEP 11. [Optional] Deploy the\u00a0extension package\u00a0of your choice to complement your integration needs. We offer the following extensions for data originating from CloudTrail and WAF:</p> <ul> <li> <p>AWS CloudTrail</p> </li> <li> <p>AWS WAF</p> </li> </ul> <p>STEP 12. View the logs by navigating to Explore &gt; Logs in your Coralogix toolbar. Find out more here.</p>"},{"location":"newoutput/firehose-fargate-logs/#additional-resources","title":"Additional Resources","text":"DocumentationGetting Started with Coralogix"},{"location":"newoutput/firehose-fargate-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/firehose-metrics/","title":"CloudWatch Metrics via Firehose","text":"<p>Streamline the process of ingesting and analyzing metrics from your AWS resources using our automated CloudWatch Metrics via Firehose integration package.</p>"},{"location":"newoutput/firehose-metrics/#overview","title":"Overview","text":"<p>Enable this integration to see all of your AWS Kinesis Data Firehose metrics in Coralogix, an AWS Partner Network (APN) Advanced Technology Partner with AWS\u00a0 Competencies in DevOps. This includes metrics generated by your AWS services and applications, such as AWS Lambda, Amazon EC2, Amazon CloudWatch, and custom applications running on AWS infrastructure.</p> <p>Enjoy its many benefits:</p> <ul> <li> <p>Scalability. AWS Kinesis Data Firehose is highly scalable, accommodating growing data volumes as your applications and infrastructure expand.</p> </li> <li> <p>Centralized Monitoring. Centralizing your metrics in Coralogix provides a single location for monitoring and analyzing all your data, simplifying troubleshooting and performance optimization.</p> </li> <li> <p>Real-Time Insights. The integration allows for real-time data streaming, ensuring you have immediate access to critical metrics and can respond quickly to anomalies or issues.</p> </li> <li> <p>Customizable Data Flow. You can configure what metrics and log data to send, ensuring that you only transmit relevant information, which can help reduce noise and improve focus.</p> </li> <li> <p>Monitoring Made Easy. Coralogix offers a variety of out-of-the-box extension packages. Each tailored extension unlocks a set of predefined items \u2013 alerts, parsing rules, dashboards, saved views, actions, and more \u2013 allowing you to jumpstart Coralogix monitoring of your external-facing resources.</p> </li> </ul>"},{"location":"newoutput/firehose-metrics/#prerequisites","title":"Prerequisites","text":"<ul> <li>Metrics bucket configured.</li> </ul>"},{"location":"newoutput/firehose-metrics/#configuration","title":"Configuration","text":"<p>STEP 1. From your Coralogix toolbar, navigate to Data Flow &gt; Integrations.</p> <p>STEP 2. In the Integrations section, select CloudWatch Metrics via Firehose.</p> <p></p> <p>STEP 3. Click ADD NEW.</p> <p></p> <p>STEP 4. Input your integration details.</p> <ul> <li> <p>Integration Name. Enter a name for your integration and either enter your Send-Your-Data API key or click CREATE A NEW KEY to create a new API key for the integration.</p> </li> <li> <p>Application Name. Enter an application name. The default name is AWS.</p> </li> <li> <p>Subsystem Name. Enter a subsystem name. The default name is Firehose.</p> </li> <li> <p>CloudWatch Metric Namespaces. Select from the dropdown menu the namespaces you want to bring into the Coralogix platform.</p> </li> <li> <p>Enable Additional Statistics. Enabling additional statistics is recommended in order to receive detailed percentile data on key AWS metrics. However, enabling it may also incur additional costs from AWS. For out more about AWS pricing here.</p> <ul> <li>The configuration used for the additional statistics is as follows:</li> </ul> </li> </ul> <pre><code>[\n  {\n    \"AdditionalStatistics\": [\"p50\", \"p75\", \"p95\", \"p99\"],\n    \"IncludeMetrics\": [\n      {\"Namespace\": \"AWS/EBS\", \"MetricName\": \"VolumeTotalReadTime\"},\n      {\"Namespace\": \"AWS/EBS\", \"MetricName\": \"VolumeTotalWriteTime\"},\n      {\"Namespace\": \"AWS/ELB\", \"MetricName\": \"Latency\"},\n      {\"Namespace\": \"AWS/ELB\", \"MetricName\": \"Duration\"},\n      {\"Namespace\": \"AWS/Lambda\", \"MetricName\": \"PostRuntimeExtensionsDuration\"},\n      {\"Namespace\": \"AWS/S3\", \"MetricName\": \"FirstByteLatency\"},\n      {\"Namespace\": \"AWS/S3\", \"MetricName\": \"TotalRequestLatency\"}\n    ]\n  }\n]\n</code></pre> <ul> <li> <p>AWS Region. Select your AWS region from the dropdown menu.</p> </li> <li> <p>AWS PrivateLink. Enabling the use of AWS PrivateLink is recommended in order to ensure a secure and private connection between your VPCs and AWS services. Find out more here.</p> </li> </ul> <p>STEP 6. Click NEXT.</p> <p></p> <p>STEP 7. Review the instructions for your integration and click\u00a0CREATE CLOUDFORMATION.</p> <p>STEP 8. You will be rerouted to the AWS website. Verify that all of the auto pre-populated values are correct. Click Create Stack.</p> <p>STEP 9. Go back to the Coralogix application and click\u00a0COMPLETE\u00a0to close the module. Revert back to the integration page.</p> <p></p> <p>STEP 10. [Optional] Deploy the\u00a0extension package\u00a0of your choice to complement your integration needs.</p> <ul> <li> <p>AWS EBS</p> </li> <li> <p>AWS EC2</p> </li> <li> <p>AWS ElastiCache</p> </li> <li> <p>AWS RDS</p> </li> </ul> <p>STEP 11. View the metrics in your Coralogix dashboard, either using Custom Dashboards [recommended] or hosted Grafana View.</p>"},{"location":"newoutput/firehose-metrics/#transformation-lambda","title":"Transformation Lambda","text":"<p>The CloudWatch Metric Streams Lambda transformation function is used as part of Kinesis Firehose to enrich the metrics from CloudWatch Metric Streams with AWS resource tags. Find out more here.</p>"},{"location":"newoutput/firehose-metrics/#destination-errors","title":"Destination Errors","text":"<p>You may encounter these common destinations errors. View their possible solutions below.</p> Message Solution The delivery timed out before a response was received and will be retried. If this error persists, contact the AWS Firehose service team. None needed \u2013 no data loss Delivery to the endpoint was unsuccessful. See Troubleshooting HTTP Endpoints in the Firehose documentation for more information. Response received with status code. 502\u2026 Coralogix returned HTTP 502 error code, firehose will resend the data. None needed \u2013 no data loss"},{"location":"newoutput/firehose-metrics/#limitations","title":"Limitations","text":"<p>CloudWatch Metric Streams does not send metrics that are older than 2 hours. This means that some CloudWatch metrics are calculated at the end of a day and reported with the beginning timestamp of the same day. This includes S3 daily storage metrics and some billing metrics.</p> <p>Should you need these metrics, we recommend using Cloudwatch Exporter using Prometheus alongside our new CloudWatch integration designed to retrieve those metrics. For updated information, contact Coralogix Support.</p>"},{"location":"newoutput/firehose-metrics/#additional-resources","title":"Additional Resources","text":"DocumentationGet Started with Coralogix"},{"location":"newoutput/firehose-metrics/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/firehose-waf-logs/","title":"AWS WAF Logs via Firehose","text":"<p>Streamline the process of ingesting and analyzing logs from your AWS resources using our automated WAF Logs using AWS Kinesis Data Firehose integration package.</p>"},{"location":"newoutput/firehose-waf-logs/#overview","title":"Overview","text":"<p>Web Application Firewall (WAF) logs are detailed records of web traffic and security events generated by AWS Web Application Firewall. These logs contain critical information about incoming requests, such as IP addresses, request types, response codes, and potential security threats or attacks. The value of WAF logs lies in their role in enhancing the security and compliance of your web applications. They provide visibility into potential threats and attacks, enabling you to monitor and safeguard your applications against malicious activities and ensuring compliance with security standards and regulations.</p>"},{"location":"newoutput/firehose-waf-logs/#benefits","title":"Benefits","text":"<ul> <li> <p>Gain deeper insights, conduct advanced analytics, set up proactive alerting, and monitor your WAF logs in real time, enhancing your ability to troubleshoot and maintain optimal system performance.</p> </li> <li> <p>Using AWS Kinesis Data Firehose streamlines the data delivery process by providing a reliable and scalable data transfer mechanism. It ensures that your log data reaches Coralogix with minimal latency and no loss of data.</p> </li> </ul>"},{"location":"newoutput/firehose-waf-logs/#configuration","title":"Configuration","text":"<p>STEP 1. From your Coralogix toolbar, navigate to Data Flow &gt; Integrations.</p> <p>STEP 2. In the Integrations section, select AWS WAF Logs via Firehose.</p> <p></p> <p>STEP 3. Click ADD NEW.</p> <p>STEP 4. Input your integration details.</p> <p></p> <ul> <li> <p>Integration Name. Enter a name for your integration. This will be used as a stack name in CloudFormation.</p> </li> <li> <p>API Key. Enter your Send-Your-Data API key or click CREATE A NEW KEY to create a new API key for the integration.</p> </li> <li> <p>Application Name. Enter an application name. The default name is AWS.</p> </li> <li> <p>Subsystem Name. Enter a subsystem name. The default name is Firehose.</p> </li> <li> <p>Enable Dynamic Metadata. [Optional] When enabled, this sets the <code>applicationName</code> and <code>subsystemName</code> dynamically. When enabled, this feature dynamically sets the <code>applicationName</code> based on the web ACL name.</p> </li> <li> <p>Kinesis Stream ARN. [Optional] Enter the ARN of the Kinesis stream if using Amazon Kinesis Data Streams as a source for logs.</p> </li> <li> <p>AWS Region. Select your AWS region from the dropdown menu.</p> </li> <li> <p>AWS PrivateLink (Advanced Settings). [Optional] Enabling the use of AWS PrivateLink is recommended to ensure a secure and private connection between your VPCs and AWS services. Find out more here.</p> </li> </ul> <p>STEP 6. Click NEXT.</p> <p>STEP 7. Review the instructions for your integration. Click\u00a0CREATE CLOUDFORMATION.</p> <p></p> <p>STEP 8. You will be rerouted to the AWS website. Verify that all of the auto pre-populated values are correct. Click Create Stack.</p> <p>STEP 9. Return to the Coralogix application where you will be presented with instructions on how to start sending logs about traffic from your Web ACLs.</p> <p></p> <p>Notes:</p> <p>If you provide a Kinesis Stream ARN, Coralogix assumes that the data is in the stream and does not provide any additional instructions. It is the user\u2019s responsibility to deliver data to the stream. Instead of the instructions you will see a message that prompts the user to confirm the integration.</p> <p>STEP 10. Click\u00a0COMPLETE\u00a0to close the module.</p> <p>STEP 11. [Optional] Deploy the AWS WAF\u00a0extension package\u00a0to complement your integration needs.</p> <p>STEP 12. View the logs by navigating to Explore &gt; Logs in your Coralogix toolbar. Find out more here.</p>"},{"location":"newoutput/firehose-waf-logs/#additional-resources","title":"Additional Resources","text":"DocumentationGetting Started with Coralogix"},{"location":"newoutput/firehose-waf-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/flow-alert/","title":"Flow Alert","text":"<p>Coralogix Flow Alerts provide a revolutionary data monitoring and analysis approach, allowing you to harness the power of logs, metrics, traces, and security in a single, streamlined platform. They not only notify you of a problem in your system but also allow you to understand its cause and how to prevent it in the future.</p>"},{"location":"newoutput/flow-alert/#overview","title":"Overview","text":"<p>A Flow Alert is designed to notify you when any combination of alert events occurs in a specific sequence within a defined timeframe.</p> <p>For example, to be notified of an increase in HTTP error rate caused by high CPU utilization, a Flow Alert should be configured to trigger when a high CPU utilization alert is followed by a high HTTP error rate alert within a defined timeframe.</p>"},{"location":"newoutput/flow-alert/#benefits","title":"Benefits","text":"<p>Here are some significant benefits of using Flow Alerts:</p> <ul> <li> <p>Comprehensive Data Correlation: With Coralogix Flow Alerts, you can correlate alerting on logs, metrics, traces, and security events. This multifaceted approach provides a holistic view of your system's performance, not just isolated slices of information, ensuring you have all the data you need to make informed decisions.</p> </li> <li> <p>Advanced Root Cause Analysis: Coralogix Flow Alerts can be configured to identify the root cause of an issue. With the ability to define an alert that already pinpoints the root cause, you can promptly respond to issues, thus reducing system downtime and enhancing operational efficiency.</p> </li> <li> <p>Reduced Alert Fatigue: Traditional monitoring systems often flood users with redundant alerts, leading to alert fatigue and the potential overlooking of critical issues. Coralogix Flow Alerts significantly reduce false alerts by applying an ordered, time-bound criteria filter. This means you only get alerted when all the set conditions are met, saving you from unnecessary notification noise.</p> </li> <li> <p>Customizable Alert Sequences: Flow Alerts' unique feature allows you to define your alerting sequence with a simple drag-and-drop interface. Create a flow that triggers only if all criteria are met by order and time, providing an intuitive and custom monitoring experience.</p> </li> <li> <p>Efficient Troubleshooting: With the ability to visualize the sequence of alerts on a canvas, troubleshooting becomes easier and more efficient. You can easily identify patterns, understand the chain of events leading to an alert, and quickly act to rectify the issue.</p> </li> <li> <p>Optimized Resource Utilization: By reducing false alerts and enabling root cause identification, you'll save significant time and resources. This optimization allows your team to focus on more strategic tasks, rather than being occupied with a constant stream of false alerts.</p> </li> </ul>"},{"location":"newoutput/flow-alert/#building-blocks","title":"Building Blocks","text":"<p>Coralogix provides the Flow Builder tool to visually combine, and then chain together, the user-defined alerts that will trigger a Flow Alert. The basic building blocks of the Flow Alert are stages and groups.</p> <p>A group represents a logical combination of individual user-defined alerts. The group supports OR, AND, and NOT logical operators to combine multiple individual alerts.</p> <p></p> <p>A stage represents alert groups that need to trigger within a specified timeframe. Multiple groups can be present in a stage.</p> <p></p>"},{"location":"newoutput/flow-alert/#limitations","title":"Limitations","text":"<p>As you define a flow alert, consider the following constraints:</p> <ul> <li> <p>The Flow Alert must have a minimum of 2 stages.</p> </li> <li> <p>The first stage of a flow alert can only contain 1 group.</p> </li> <li> <p>The duration of the timeframe in all stages cannot exceed 36 hours.</p> </li> <li> <p>You can combine a maximum of 30 alerts into a single Flow alert.</p> </li> <li> <p>The following alert types do not support the NOT logical operator:</p> <ul> <li> <p>New Value alerts</p> </li> <li> <p>Unique Count alerts</p> </li> <li> <p>Notify immediately</p> </li> <li> <p>Standard alerts</p> </li> </ul> </li> </ul>"},{"location":"newoutput/flow-alert/#create-a-flow-alert","title":"Create a Flow Alert","text":"<p>STEP 1. Create a new alert. Enter the Alert Name, Description, and Severity.</p> <p></p> <p>STEP 2. Select the Alert type: Flow Alert.</p> <p></p> <p>STEP 3. Define the Alert Flow by clicking Open Flow Builder.</p> <p></p> <p>STEP 4. Drag and drop existing alerts from the left-hand panel into the Flow Builder workspace area. To view a tool-tip with the details for each alert, hover over the alert. These details include the Query, Conditions, and Group By fields.</p> <p></p> <p>STEP 5. Organize the alerts into groups and stages as explained above. Remember to set a timeframe for each stage.</p> <p></p> <p>STEP 6. Click Apply to save the Alert Flow.</p> <p>STEP 7. Select the Group By keys.</p> <p>Note that the available keys will be the intersection group between the different alerts. For example, if Alert A is grouped by <code>Region</code> and by <code>Cluster</code>, and Alert B is grouped by <code>Region</code> and by <code>Pod</code>, the Alert Flow will only be able to be grouped by <code>Region</code>, and not by <code>Cluster</code> or <code>Pod</code>, as that is the only Group by option available to both alerts in the flow. You can see which Group by options are available for each alert in the Alert Builder by hovering over the alert and viewing the Alert Description.</p> <p></p> <p>STEP 8. Define the Notification settings.</p> <p>In the notification settings, you have different options, depending on whether or not you are using the\u00a0Group By\u00a0condition.</p> <p>When using\u00a0Group By\u00a0conditions, you will see the following options:</p> <ul> <li> <p>Trigger a single alert when at least one combination of\u00a0the group by values meets the condition. A single notification, aggregating all values matching an alert query and conditions, will be sent to your Coralogix Incidents screen.</p> </li> <li> <p>Trigger a separate alert for each combination that meets the condition. Multiple individual notifications for each Group By field value may be sent to your Coralogix Incidents screen when query conditions are met. Select one or more Keys \u2013 consisting of a subset of the fields selected in the alert conditions \u2013 in the drop-down menu. A separate notification will be sent for each Key selected.</p> </li> <li> <p>The number of\u00a0Group By\u00a0permutations is limited to 1000. If there are more permutations, then only the first 1000 are tracked.</p> </li> </ul> <p>When not using the\u00a0Group By\u00a0condition,\u00a0a single alert will be triggered\u00a0and sent to your\u00a0Incidents Screen\u00a0when the query meets the condition.</p> <p>You can define additional alert recipient(s) and notification channels in both cases by clicking\u00a0+ ADD WEBHOOK. Once you add a webhook, you can choose the parameters of your notification:</p> <ul> <li> <p>Notify Every. Sets the alert cadence. After an alert is triggered and a notification is sent, the alert will continue to work, but notifications will be suppressed for the duration of the suppression period.</p> </li> <li> <p>Notify when resolved. Activate to receive an automatic update once an alert has ceased.</p> </li> </ul> <p>STEP 9. Create an Alert Schedule.</p> <p>STEP 10. Create Notification Content for the Flow Alert.</p> <p>STEP 11. Click Create Alert.</p>"},{"location":"newoutput/flow-alert/#view-your-alert","title":"View Your Alert","text":"<p>Our\u00a0Incidents\u00a0Screen\u00a0displays all of your triggered alert events within the Coralogix platform. View all those events that are currently triggered or those triggered within a specific time frame. With easy-to-use functionalities and the ability to drill down into events of interest, the feature ensures top-notch monitoring and analysis.</p>"},{"location":"newoutput/flow-alert/#example","title":"Example","text":"<p>The following example notifies you when successful orders are reduced due to a failed database cleanup task that occurred 24 hours earlier.</p> <p></p>"},{"location":"newoutput/flow-alert/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/fluent-bit/","title":"Fluent Bit","text":"<p>Coralogix provides seamless integration with\u00a0Fluent Bit, allowing you\u00a0to send your logs from anywhere and parse them according to your needs.</p> <p>Coralogix supports Fluent Bit\u00a0v2.0.7\u00a0and onwards.</p>"},{"location":"newoutput/fluent-bit/#prerequisites","title":"Prerequisites","text":"<ul> <li>Fluent Bit\u00a0installed</li> </ul>"},{"location":"newoutput/fluent-bit/#environment-variables","title":"Environment Variables","text":"<p>Unless you want to hardcode your values into their appropriate positions, you must export the following four environment variables when deploying the Fluent Bit integration.</p> Variable Description Api_Key Your\u00a0Send-Your-Data API key\u00a0is a unique ID that represents your Coralogix team. Input the key\u00a0without\u00a0quotation marks or apostrophes. Application_Name The name of your\u00a0application, as will appear in your Coralogix dashboard. For example, a company named\u00a0SuperData\u00a0might insert the\u00a0SuperData\u00a0string parameter. If SuperData wants to debug its test environment, it might use\u00a0SuperData\u2013Test. SubSystem_Name The name of your\u00a0subsystem, as will appear in your Coralogix dashboard. Applications often have multiple subsystems (ie. Backend Servers, Middleware, Frontend Servers, etc.). In order to help you examine the data you need, inserting the subsystem parameter is vital. Endpoint Find the Coralogix REST API Singles endpoint corresponding to your Coralogix domain. Enter only the FQDN portion. Example:\u00a0ingress.coralogixstg.wpengine.com"},{"location":"newoutput/fluent-bit/#configuration","title":"Configuration","text":"<p>Open your\u00a0existing\u00a0<code>Fluent-Bit</code>\u00a0configuration file and add\u00a0the following:</p> <pre><code>[FILTER]\n        Name            nest\n        Match           *\n        Operation       nest\n        Wildcard        *\n        Nest_under      text\n[FILTER]\n        Name            modify\n        Match           *\n        Add             applicationName ${Application_Name}\n        Add             subsystemName ${SubSystem_Name}\n        Add             computerName ${HOSTNAME}\n[OUTPUT]\n        Name            http\n        Match           *\n        Host            ${Endpoint}\n        Port            443\n        URI             /logs/v1/singles\n        Format          json_lines\n        TLS             On\n        Header          Authorization Bearer ${Api_Key}\n        compress        gzip\n        Retry_Limit     10\n\n</code></pre>"},{"location":"newoutput/fluent-bit/#application-and-subsystem-name","title":"Application and Subsystem Name","text":"<p>If you wish to set your\u00a0application and subsystem names\u00a0as fixed values, use\u00a0Application_Name\u00a0and\u00a0SubSystem_Name\u00a0as described above in your configuration file.</p> <p>If your input stream is a\u00a0<code>JSON</code>\u00a0object, you can extract\u00a0an Application_Name\u00a0and/or SubSystem_Name from the\u00a0<code>JSON</code>\u00a0by adding our LUA filter just before the OUTPUT:</p> <pre><code>[FILTER]\n  Name    lua\n  Match   *\n  call set_cx_keys\n  code function set_cx_keys(tag, timestamp, record) new_record = record new_record[\"applicationName\"] = record[\"&lt;your_app_key&gt;\"] new_record[\"subsystemName\"] = record[\"parent_key\"][\"&lt;your_subsystem_key&gt;\"] return 2, timestamp, new_record end\n\n</code></pre> <p>For instance, in the following example,\u00a0<code>JSON</code> <code>new_record[\"applicationName\"] = record[\"application\"]</code>\u00a0will extract\u00a0\"testApp\"\u00a0into the Coralogix applicationName.</p> <pre><code>{\n    \"application\": \"testApp\",\n    \"subsystem\": \"testSub\",\n    \"code\": \"200\",\n    \"stream\": \"stdout\",\n    \"timestamp\": \"2016-07-20T17:05:17.743Z\",\n    \"message\": \"hello_world\",\n}\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>Nested JSONs are supported.</p> </li> <li> <p>Extract nested values as your\u00a0<code>applicationName</code>\u00a0and/or\u00a0<code>subsystemName</code>.</p> </li> </ul>"},{"location":"newoutput/fluent-bit/#example","title":"Example","text":"<p>The following is an example configuration.</p> <pre><code>[SERVICE]\n    Flush           1\n    Daemon          Off\n    Log_Level       info\n    Parsers_File    parsers.conf\n\n[INPUT]\n    Name            dummy\n    Tag             dummy_input\n    Rate            1\n\n[OUTPUT]\n    Name            stdout\n    Match           *\n\n[FILTER]\n    Name            nest\n    Match           *\n    Operation       nest\n    Wildcard        *\n    Nest_under      text\n\n[FILTER]\n    Name            modify\n    Match           *\n    Add             computerName ${HOSTNAME}\n\n[FILTER]\n    Name            lua\n    Match           *\n    call            applicationNameFromEnv\n    code            function applicationNameFromEnv(tag, timestamp, record) record[\"applicationName\"] = record[\"metadata\"][\"computerName\"] or os.getenv(\"Application_Name\") return 2, timestamp, record end\n\n[FILTER]\n    Name            lua\n    Match           *\n    call            subsystemNameFromEnv\n    code            function subsystemNameFromEnv(tag, timestamp, record) record[\"subsystemName\"] = tag or os.getenv(\"SubSystem_Name\") return 2, timestamp, record end\n\n[OUTPUT]\n    Name            http\n    Match           *\n    Host            ${ENDPOINT}\n    Port            443\n    URI             /logs/v1/singles\n    Format          json_lines\n    TLS             On\n    Header          Authorization Bearer ${Api_Key}\n    compress        gzip\n    Retry_Limit     10\n\n</code></pre>"},{"location":"newoutput/fluent-bit/#additional-resources","title":"Additional Resources","text":"Documentation GitHub"},{"location":"newoutput/fluent-bit/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/fluentbit-helm-chart-for-kubernetes/","title":"Fluent Bit Helm Chart for Kubernetes","text":"<p>Use our multi-arch Helm chart to streamline your Kubernetes monitoring by creating a DaemonSet on your\u00a0Kubernetes\u00a0cluster using the\u00a0Helm\u00a0package manager.</p>"},{"location":"newoutput/fluentbit-helm-chart-for-kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Kubernetes\u00a01.20+ with Beta APIs enabled</p> </li> <li> <p>Helm\u00a02.9+ package manager installed</p> </li> </ul>"},{"location":"newoutput/fluentbit-helm-chart-for-kubernetes/#installation","title":"Installation","text":"<p>STEP 1. Create a namespace for the DaemonSet. The following example adopts the namespace coralogix-logger.</p> <pre><code>kubectl create namespace coralogix-logger\n</code></pre> <p>STEP 2. Create a secret key using your Coralogix Send-Your-Data API key.</p> <pre><code>kubectl create secret generic coralogix-keys \\\n        -n coralogix-logger \\\n        --from-literal=PRIVATE_KEY=&lt;Send-Your-Data API Key&gt;\n</code></pre> <p>STEP 3. Add the Helm Chart Repo and run an update to fetch it.</p> <pre><code>helm repo add coralogix-charts-virtual https://cgx.jfrog.io/artifactory/coralogix-charts-virtual &amp;&amp;\nhelm repo update\n</code></pre> <p>STEP 4. Create an <code>override.yaml</code> to override particular settings.</p> <ul> <li> <p><code>endpoint</code>: Input the OpenTelemetry endpoint associated with your Coralogix domain.</p> </li> <li> <p><code>dynamic_metadata</code>: You may change the dynamic field from which we extract the application and subsystem name or a static value to overwrite these names.</p> </li> </ul> <pre><code>---\n# fluentbit-override.yaml:\ndynamic_metadata:\n  app_name: kubernetes.namespace_name\n  sub_system: kubernetes.container_name\nfluent-bit:\n  endpoint: &lt;coralogix_endpoint&gt;\n  logLevel: error\n</code></pre> <pre><code>---\n# fluentbit-override.yaml:\nstatic_metadata:\n  app_name: MyApplication\n  sub_system: MySubsystem\nfluent-bit:\n  endpoint: &lt;coralogix_endpoint&gt;\n  logLevel: error\n</code></pre> <ul> <li>If a dynamic variable includes special characters, it must be declared in a different notation, such as the following. Failure to do so will result in LUA exceptions and failure to process properly.</li> </ul> <pre><code>  app_name: kubernetes.labels[\"k8s-app\"]\n  app_name: kubernetes.labels[\"app.kubernetes.io/name\"]\n</code></pre> <p>STEP 5. Deploy the Helm Chart.</p> <pre><code>helm upgrade fluent-bit-http coralogix-charts-virtual/fluent-bit-http \\\n  --install \\\n  --namespace=coralogix-logger \\\n  -f fluentbit-override.yaml\n</code></pre>"},{"location":"newoutput/fluentbit-helm-chart-for-kubernetes/#remove-the-daemonset","title":"Remove the Daemonset","text":"<p>STEP 6. Remove the DaemonSet.</p> <pre><code>helm uninstall fluent-bit-http \\\n     -n coralogix-logger\n</code></pre> <p>Notes:</p> <ul> <li> <p><code>podsecuritypolicy</code> has been deprecated for Kubernetes v1.25+.</p> </li> <li> <p>Disable this by adding the following to your override file:</p> </li> </ul> <pre><code>podSecurityPolicy:\n  create: false\n</code></pre>"},{"location":"newoutput/fluentbit-helm-chart-for-kubernetes/#additional-resources","title":"Additional Resources","text":"<p>Review and make suggestions for improvement for our Helm chart in our Integrations Repository.</p>"},{"location":"newoutput/fluentbit-helm-chart-for-kubernetes/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/fluentbit-helm-chart-for-kubernetes/#_1","title":"Fluent Bit Helm Chart for Kubernetes","text":""},{"location":"newoutput/fluentd-helm-chart-for-kubernetes/","title":"Fluentd Helm Chart for Kubernetes","text":"<p>Here at Coralogix, we love Kubernetes\u00a0and we love making things simple. To help streamline your Kubernetes monitoring, we created this chart to bootstrap our optimized Fluentd image\u00a0to create a DaemonSet on your\u00a0Kubernetes\u00a0cluster using the\u00a0Helm\u00a0Package Manager.</p> <p>Our chart also supports metrics for Fluentd itself by default and is MultiArch.</p> <p>Our Helm chart is open source and you are welcome to review and make suggestions for improvements in our Integrations repository.</p>"},{"location":"newoutput/fluentd-helm-chart-for-kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p><code>Kubernetes</code>\u00a01.20+ with Beta APIs enabled.</p> </li> <li> <p><code>Helm</code>\u00a02.9+ Package Manager installed (For installation instructions please visit\u00a0Get Helm!).</p> </li> </ul> <p>Kubernetes 1.25+ podsecuritypolicy has been deprecated for this version up. You can disable this by adding the following to your override file</p> <pre><code>podSecurityPolicy:\n  create: false\n</code></pre>"},{"location":"newoutput/fluentd-helm-chart-for-kubernetes/#installation","title":"Installation","text":"<p>Create a Namespace for the daemonset (in our example we will use: coralogix-logger):</p> <pre><code>kubectl create namespace coralogix-logger\n</code></pre>"},{"location":"newoutput/fluentd-helm-chart-for-kubernetes/#create-secret-key","title":"Create Secret Key","text":"<p>Your Send-Your-Data API key can be found in the Coralogix UI in the top of the screen under Data Flow --&gt; API Keys --&gt; Send Your Data</p> <pre><code>kubectl create secret generic coralogix-keys \\\n        -n coralogix-logger \\\n        --from-literal=PRIVATE_KEY=&lt;Send-Your-Data API key&gt;\n</code></pre>"},{"location":"newoutput/fluentd-helm-chart-for-kubernetes/#add-the-helm-chart-repo","title":"Add the Helm Chart Repo","text":"<p>(And run an update to fetch it)</p> <pre><code>helm repo add coralogix-charts-virtual https://cgx.jfrog.io/artifactory/coralogix-charts-virtual &amp;&amp;\nhelm repo update\n</code></pre>"},{"location":"newoutput/fluentd-helm-chart-for-kubernetes/#create-an-overrideyml-file","title":"Create An override.yml File","text":"<p>This is where you can override settings like the HTTP endpoint to which we send logs (we added a table of endpoints at the bottom of this manual).</p> <p>You can also change the dynamic field from which we extract the application and subsystem name or completely override the configuration.</p> <pre><code>---\n#override.yaml:\nfluentd:\n  configMapConfigs:\n  - fluentd-prometheus-conf\n  # - fluentd-systemd-conf\n  env:\n  - name: APP_NAME\n    value: namespace_name\n  - name: SUB_SYSTEM\n    value: container_name\n  - name: APP_NAME_SYSTEMD\n    value: systemd\n  - name: SUB_SYSTEM_SYSTEMD\n    value: kubelet.service\n  - name: ENDPOINT\n    value: &lt;put_your_coralogix_endpoint_here&gt;\n  - name: \"FLUENTD_CONF\"\n    value: \"../../etc/fluent/fluent.conf\"\n  - name: LOG_LEVEL\n    value: error\n  - name: K8S_NODE_NAME\n    valueFrom:\n      fieldRef:\n        fieldPath: spec.nodeName\n</code></pre>"},{"location":"newoutput/fluentd-helm-chart-for-kubernetes/#deploy-the-chart","title":"Deploy the Chart","text":"<pre><code>helm upgrade fluentd-http coralogix-charts-virtual/fluentd-http \\\n     --install --namespace=coralogix-logger \\\n     -f override.yaml\n</code></pre>"},{"location":"newoutput/fluentd-helm-chart-for-kubernetes/#remove-the-daemonset","title":"Remove the Daemonset","text":"<pre><code>helm uninstall fluentd-http \\\n     -n coralogix-logger\n</code></pre>"},{"location":"newoutput/fluentd-helm-chart-for-kubernetes/#coralogix-endpoints","title":"Coralogix Endpoints","text":"<p>For a list of up to date Coralogix Endpoints, see the Coralogix Endpoints List.</p>"},{"location":"newoutput/fluentd-helm-chart-for-kubernetes/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/fluentd-helm-chart-for-kubernetes/#_1","title":"Fluentd Helm Chart for Kubernetes","text":""},{"location":"newoutput/fortigate-traffic-logging/","title":"FortiGate","text":"<p>FortiGate traffic logs are essential records of network activity generated by Fortinet's security appliances, providing valuable insights into the traffic patterns, security events, and performance of your network. Send these logs to Coralogix to gain a comprehensive and real-time view of your network's health and security. With the power of data-driven insights, you can optimize network performance, troubleshoot issues faster, and make informed decisions to enhance your organization's overall security posture.</p> <p>This integration guides you on how to configure FortiGate with syslog using OpenTelemetry.</p>"},{"location":"newoutput/fortigate-traffic-logging/#prerequisites","title":"Prerequisites","text":"<p>To ship syslog messages from your FortiGate setup to an OpenTelemetry Collector setup, you are required to satisfy the following prerequisites:</p> <ul> <li> <p>Syslog over TCP. Configure your FortiGate device to send syslog messages using TCP as the transport protocol.</p> </li> <li> <p>Syslog Format. The syslog message format should comply with RFC 5424.</p> </li> <li> <p>Destination Address and Port. Set the destination address to the IP address where OpenTelemetry Collector is running and set the destination port to 54526, as defined in your configuration.</p> </li> <li> <p>Timestamp Configuration [optional]. We recommend synchronizing your FortiGate device time with a UTC NTP server to match the <code>location: UTC</code> defined in your OpenTelemetry setup.</p> </li> </ul>"},{"location":"newoutput/fortigate-traffic-logging/#configuration","title":"Configuration","text":"<p>Configure Fortigate with syslog using OpenTelemetry.</p>"},{"location":"newoutput/fortigate-traffic-logging/#example","title":"Example","text":"<p>The following is an example of a receiver configuration:</p> <pre><code>receivers:\n  syslog:\n    tcp:\n      listen_address: \"0.0.0.0:54526\"\n    protocol: rfc5424\n    operators:\n      - type: syslog_parser\n        protocol: rfc5424\n        parse_from: body\n        parse_to: body\n      - type: remove\n        field: attributes\n\n</code></pre> <p>Customers who have configured syslog protocol RFC 5424 and syslog over TCP should configure the parameter <code>enable_octet_counting</code>\\=True, as follows:</p> <pre><code>receivers:\n  syslog:\n    tcp:\n      listen_address: \"0.0.0.0:54526\"\n    protocol: rfc5424\n    enable_octet_counting: true\n    operators:\n      - type: syslog_parser\n        protocol: rfc5424\n        parse_from: body\n        parse_to: body\n      - type: remove\n        field: attributes\n</code></pre> <p>Find out more here.</p>"},{"location":"newoutput/fortigate-traffic-logging/#additional-resources","title":"Additional Resources","text":"DocumentationSyslog using OpenTelemetry"},{"location":"newoutput/fortigate-traffic-logging/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/gcp-getting-started/","title":"GCP - Getting Started","text":""},{"location":"newoutput/gcp-getting-started/#overview","title":"Overview","text":"<p>Coralogix offers a number of basic integrations with Google Cloud Platform. However, a prerequisite for each is that you first\u00a0Configure a Service Account. Once you have created a Service Account and its corresponding API key, access your integration-specific instructions in the table below:</p> GCP LogsGoogle Workspace UsersGCP TracesGoogle Alert CenterGCP Metrics"},{"location":"newoutput/gcp-getting-started/#configure-a-service-account","title":"Configure a Service Account","text":"<p>This should be done before any GCP integration with Coralogix. Please make sure that you have\u00a0Super admin permissions. Also, you should have an existing project within Google Cloud. Your new service account will be created there.</p> <p>On the Google Cloud side, basic Service Account configuration is done in two steps:</p> <p>Step 1. You first need to create a new Service Account to authenticate with Coralogix.</p> <p>Step 2. For this account, you will create an API key. This key will be used for the Coralogix integration.</p>"},{"location":"newoutput/gcp-getting-started/#step-1-create-a-service-account","title":"Step 1: Create a Service Account","text":"<p>1.\u00a0Sign in to Google Cloud Console and choose the project where you want to create the service account.</p> <p>2.\u00a0Go to\u00a0IAM &amp; Admin\u00a0in the menu and scroll down to\u00a0Service Accounts.</p> <p>3.\u00a0On the\u00a0Service accounts\u00a0list, click\u00a0+ CREATE SERVICE ACCOUNT.</p> <p>4.\u00a0Input your service account details: name, account ID, and description. Click\u00a0CREATE AND CONTINUE.</p> <p>5.\u00a0Based on the type of integration you are setting up, you may need to assign specific roles to the service account.</p> <ul> <li> <p>GCP Logs:\u00a0To collect logs, select the\u00a0<code>Pub/Sub Subscriber</code> role.</p> </li> <li> <p>GCP Traces:\u00a0To collect traces, select\u00a0<code>BigQuery Job User</code>\u00a0and\u00a0<code>BigQuery Data Viewer</code>\u00a0.</p> </li> <li> <p>GCP Metrics:\u00a0To collect metrics, select\u00a0<code>Compute Viewer</code>,\u00a0<code>Monitoring Viewer</code>, and\u00a0<code>Cloud Asset Viewer.</code></p> </li> <li> <p>Google Workspace Users:\u00a0You don\u2019t need to assign roles for this service account.</p> </li> <li> <p>Google Alerts Center:\u00a0You don\u2019t need to assign roles for this service account.</p> </li> </ul> <p>At the end, click\u00a0CONTINUE\u00a0and\u00a0DONE\u00a0in the step below. Your service account is now ready for use.</p> <p>6.\u00a0Find the new service account on the list. Note down the\u00a0OAuth 2 Client ID. You will need it to finish this tutorial.</p> <p></p>"},{"location":"newoutput/gcp-getting-started/#step-2-create-a-private-key","title":"Step 2: Create a Private Key","text":"<p>1.\u00a0Click the three dots in the rightmost\u00a0Actions\u00a0column and choose\u00a0Manage keys.</p> <p></p> <p>2.\u00a0Click\u00a0Add Key and\u00a0choose the JSON\u00a0Key type. Download it and store locally. You will need to upload it to your Coralogix integration.</p>"},{"location":"newoutput/gcp-getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have created a Service Account and an API key, consult our integration-specific tutorials. For these, you will need to use the Coralogix platform.</p>"},{"location":"newoutput/gcp-getting-started/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email to\u00a0support@coralogix.com.</p>"},{"location":"newoutput/gcp-log-explorer/","title":"GCP Log Explorer","text":""},{"location":"newoutput/gcp-log-explorer/#before-proceeding","title":"Before proceeding","text":"<p>GCP Log Explorer is being deprecated in favor of GCP Logs. The initial generation of the pull-based GCP Log Explorer integration exhibited performance limitations due to the nature of push-based GCP Pub/Sub subscriptions. Specifically, the lack of batching in requests caused delays in processing as log volume increased.</p> <p>With the introduction of the new integration, you can expect to receive your logs in a more reliable and timely manner.</p>"},{"location":"newoutput/gcp-log-explorer/#migration-guide","title":"Migration guide","text":"<p>To set up a GCP Logs integration using the subscription name currently utilized for the push-based integration, follow these steps:</p> <ol> <li> <p>Navigate to the configuration page of the GCP Pub/Sub subscription responsible for sending logs to Coralogix.</p> </li> <li> <p>Modify the subscription type from \"Push\" to \"Pull\".</p> </li> <li> <p>Disable the \"Exactly once delivery\" option.</p> </li> <li> <p>Adjust the \"Retry policy\" to \"Retry after exponential backoff delay\".</p> </li> </ol> <p>After completing these steps, the integration may initially appear as \"Failing\" with a message indicating, \"This method is not supported for this subscription type.\" This behavior is expected, as the integration necessitates a \"Pull\" subscription for proper functionality.</p>"},{"location":"newoutput/gcp-log-explorer/#overview","title":"Overview","text":"<p>GCP Log Explorer is one of the first tools to use to troubleshoot issues and find a solution quickly to problems you might be facing. Coralogix came up with an easy way to help you export Log Explorer logs to help you visualize and filter them and create dashboards to see trends and issues that might arise. Following the steps below will guide you through how to set up the Log Explorer integration.</p> <p>Coralogix offers a number of different approaches for collecting logs from your Google Cloud environments including using GCP Log Explorer and Google Cloud Storage.</p> <p>The tutorial describes how to configure a Logs router to send logs to a Pub/Sub topic and deliver them to Coralogix using a push subscription on the topic.</p> <p>The main advantage of using a push subscription is that it avoids running any additional software (i.e. functions) in your GCP account which can contribute to operational overhead and costs.</p>"},{"location":"newoutput/gcp-log-explorer/#requirements","title":"Requirements","text":"<ul> <li> <p>Coralogix account</p> </li> <li> <p>GCP account with permissions to configure <code>Logging</code> and <code>Pub/Sub</code> services</p> </li> </ul>"},{"location":"newoutput/gcp-log-explorer/#query-params-in-the-endpoint-url","title":"Query params in the endpoint URL","text":"<p>When you create your subscription, you will need to set the correct Endpoint URL using the following parameters:</p> <ul> <li> <p><code>**domain**</code>: domain associated with your Coralogix account</p> </li> <li> <p><code>**key**</code>: Coralogix Send-Your-Data API key</p> </li> <li> <p><code>**application**</code> (optional): overrides the default application name</p> </li> <li> <p><code>**subsystem**</code> (optional): overrides the default subsystem name</p> </li> <li> <p><code>**computer**</code> (optional): sets the computer name (otherwise there is no computer name)</p> </li> </ul>"},{"location":"newoutput/gcp-log-explorer/#configuration","title":"Configuration","text":"<p>To configure the ingestion of GCP log data to Coralogix, we will first create a new topic in Google Cloud Pub/Sub. Then we will configure the topic as a sink in the logs router and configure a subscription to push the data to Coralogix.</p> <p>STEP 1. Log in to GCP console.</p> <p>STEP 2. Go to <code>Pub/Sub</code> / <code>Topics</code> and create a Topic</p> <p>Notes:</p> <ul> <li> <p>Uncheck the \u2018Add a default subscription\u2019 checkbox.</p> </li> <li> <p>A subscription is created in a later step.</p> </li> </ul> <p>STEP 3. Go to <code>Logging</code> / <code>Logs Router</code> and create a Sink</p> <ul> <li> <p>Select sink service: Cloud Pub/Sub topic</p> </li> <li> <p>Select the Cloud Pub/Sub topic created in the previous step</p> </li> <li> <p>Choose logs to include in Sink: Leave blank. Do not denote logs that should be included or excluded.</p> </li> </ul> <p></p> <p>STEP 4. Go to <code>Pub/Sub</code> / <code>Subscriptions</code> and create a Subscription</p> <ul> <li> <p>Select the topic created in step 2 and make the following adjustments:</p> <ul> <li> <p>Set Delivery type to 'Push'.</p> </li> <li> <p>Set Endpoint URL to https://ingress./gcp/v1/logs?key=. <li> <p>If you like to pass additional attributes to Coralogix, set the endpoint to https://ingress./gcp/v1/logs?key=&amp;application=&amp;subsystem=. <li> <p>Set message retention to '1 day'.</p> </li> <li> <p>Set Retry policy to 'Retry after exponential backoff delay'. Keep the default backoff values.</p> </li> <p></p> <p></p>"},{"location":"newoutput/gcp-log-explorer/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/gcp-logs/","title":"GCP Logs","text":"<p>Google Cloud Platform offers integrated monitoring and observability tools that enable users to gather and analyze logs from their GCP resources. Send these\u00a0GCP logs\u00a0to Coralogix to search, analyze, and visualize your data. Gain insights into application behavior, identify errors, and troubleshoot problems effectively.</p>"},{"location":"newoutput/gcp-logs/#overview","title":"Overview","text":"<p>Google Cloud Platform (GCP) logs encompass all types of log entries generated across various Google Cloud Platform services and resources. These logs capture a wide range of information, including activities, events, and system operations related to your entire GCP environment. This can include logs from compute instances, databases, networking, security, access, and more.</p> <p>The integration collects logs that are published to a GCP Pub/Sub topic to which one has a subscription, referenced by a subscription name.</p>"},{"location":"newoutput/gcp-logs/#benefits","title":"Benefits","text":"<p>By integrating Coralogix seamlessly with your GCP environment, you can gain a comprehensive understanding of your system\u2019s performance, user experiences, and security. This enables you to proactively detect issues, troubleshoot effectively, and optimize your resources, ultimately improving the reliability and efficiency of your applications.</p>"},{"location":"newoutput/gcp-logs/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Super admin permissions in Google Workspace.</p> </li> <li> <p>An existing project within your Workspace.</p> </li> <li> <p>Create a pull subscription\u00a0to a Google Cloud Pub/Sub topic within your Google Cloud project.</p> <ul> <li> <p>The following property settings are mandatory:</p> <ul> <li> <p>Delivery type:\u00a0Pull</p> </li> <li> <p>Exactly once delivery: Disabled</p> </li> </ul> </li> <li> <p>All other property settings below are\u00a0recommended, but not mandatory.</p> </li> </ul> </li> </ul>"},{"location":"newoutput/gcp-logs/#setup","title":"Setup","text":"<p>1. Configure a Service Account\u00a0and API Key to facilitate automated intermediation.</p> <p>2. From your Coralogix toolbar, navigate to\u00a0Data Flow\u00a0&gt;\u00a0Integrations.</p> <p>3.\u00a0From the\u00a0Integrations\u00a0section, select\u00a0GCP Logs.</p> <p>4.\u00a0Click\u00a0+ ADD NEW.</p> <p>5.\u00a0If you haven\u2019t already done so, click\u00a0GO TO GCP ACCOUNT\u00a0and create a key file. Then, click\u00a0NEXT.</p> <p>6.\u00a0Click\u00a0SELECT FILE\u00a0and upload the key file\u00a0you previously created.</p> <p>7.\u00a0A confirmation will appear when the file is uploaded successfully. Click\u00a0NEXT.</p> <p>8.\u00a0Fill in the settings:</p> <ul> <li> <p>Integration Name:\u00a0Name your integration. This field is automatically populated, but can be changed if you want.</p> </li> <li> <p>Application Name:\u00a0(optional)\u00a0naming conventions\u00a0for production, development and / or staging environments.</p> </li> <li> <p>Subsystem Name:\u00a0(optional)\u00a0naming conventions\u00a0for production, development and / or staging environments.</p> </li> <li> <p>Subscription Name:\u00a0This is the subscription name you previously created in Prerequisites.</p> </li> </ul> <p>9.\u00a0Click\u00a0COMPLETE and finish the setup.</p> <p>Please wait a few minutes before the integration takes effect and your data is available on the platform.</p>"},{"location":"newoutput/gcp-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email to\u00a0support@coralogix.com.</p>"},{"location":"newoutput/gcp-metrics/","title":"GCP Metrics","text":"<p>Google Cloud Platform provides built-in monitoring and observability tools that allow users to collect and analyze metrics and traces from their GCP resources. Send\u00a0Google Cloud metrics\u00a0seamlessly to Coralogix. Search, analyze, and visualize your data, gaining insights into application behavior, identifying errors, and troubleshooting problems.</p>"},{"location":"newoutput/gcp-metrics/#overview","title":"Overview","text":"<p>Google Cloud Platform (GCP) logs encompass all types of log entries generated across various Google Cloud Platform services and resources. These logs capture a wide range of information, including activities, events, and system operations related to your entire GCP environment. This can include logs from compute instances, databases, networking, security, access, and more.</p> <p>The integration collects logs that are published to a GCP Pub/Sub topic to which one has a subscription, referenced by a subscription name.</p>"},{"location":"newoutput/gcp-metrics/#benefits","title":"Benefits","text":"<p>By integrating Coralogix seamlessly with your GCP environment, you can gain a comprehensive understanding of your system\u2019s performance, user experiences, and security. This enables you to proactively detect issues, troubleshoot effectively, and optimize your resources, ultimately improving the reliability and efficiency of your applications.</p>"},{"location":"newoutput/gcp-metrics/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Super admin permissions in Google Workspace.</p> </li> <li> <p>An existing project within your Workspace.</p> </li> </ul>"},{"location":"newoutput/gcp-metrics/#setup-instructions","title":"Setup Instructions","text":"<p>1. Configure a Service Account\u00a0and API Key to facilitate automated intermediation. You will need to set proper account Roles to collect metrics.</p> <p>2. From your Coralogix toolbar, navigate to\u00a0Data Flow\u00a0&gt;\u00a0Integrations.</p> <p>3.\u00a0From the\u00a0Integrations\u00a0section, select\u00a0GCP Metrics.</p> <p>4.\u00a0Click\u00a0+ ADD NEW.</p> <p>5.\u00a0If you haven\u2019t already done so, click\u00a0GO TO GCP ACCOUNT\u00a0and create a key file. Then, click\u00a0NEXT.</p> <p>6.\u00a0Click\u00a0SELECT FILE\u00a0and upload the key file\u00a0you previously created.</p> <p>7.\u00a0A confirmation will appear when the file is uploaded successfully. Click\u00a0NEXT.</p> <p>8.\u00a0Choose the metric prefixes you would like to pull into Coralogix by selecting them from the dropdown menu. To limit unnecessary API calls that fetch no data but count towards the quota limit, specify only the GCP prefixes of the metrics that actually exist and you want to pull into Coralogix.</p> <ul> <li> <p>Coralogix queries only the metrics under the selected prefixes.</p> </li> <li> <p>Metrics for which no data is available for more than 15 minutes will reduce the scraping interval from 15 minutes to 1 hour.</p> </li> <li> <p>Querying all of your metrics may result in GCP quota limits and less frequent metric updates.</p> </li> </ul> <p>9. Click\u00a0NEXT. On the next screen, fill in the settings:</p> <ul> <li> <p>Integration Name:\u00a0Name your integration. This field is automatically populated, but can be changed if you want.</p> </li> <li> <p>Application Name:\u00a0(optional)\u00a0Naming conventions\u00a0for production, development and / or staging environments.</p> </li> <li> <p>Subsystem Name:\u00a0(optional) Naming conventions\u00a0for production, development and / or staging environments.</p> </li> <li> <p>Subscription Name:\u00a0This is the name of your subscription you previously created in Prerequisites.</p> </li> </ul> <p>10.\u00a0Click\u00a0COMPLETE and finish the setup. Please wait a few minutes before the integration takes effect and your data is available on the platform.</p>"},{"location":"newoutput/gcp-metrics/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email to\u00a0support@coralogix.com.</p>"},{"location":"newoutput/gcp-status-logs/","title":"GCP Status Logs","text":"<p>Collect your GCP status logs in the Coralogix platform using our automatic Contextual Data Integration Package. The package lets you enable GCP data ingestion to allow you to see status updates on public GCP incidents.</p>"},{"location":"newoutput/gcp-status-logs/#overview","title":"Overview","text":"<p>Google Cloud Platform (GCP) is a comprehensive suite of cloud computing services offered by Google, designed to provide organizations with the tools and infrastructure necessary to build, deploy, and manage a wide range of applications and services. GCP encompasses a variety of services, including computing, storage, databases, machine learning, networking, analytics, and more, all hosted on Google's highly scalable and globally distributed infrastructure. GCP enables businesses to leverage the power of cloud computing to enhance agility, scale resources on demand, innovate with advanced technologies, and optimize operations, ultimately driving efficiency and innovation across various industries.</p> <p>Sending your Google Cloud Platform (GCP) status logs to Coralogix facilitates streamlined log aggregation, real-time monitoring, and efficient troubleshooting. By channeling GCP status logs into Coralogix, you gain a comprehensive view of your cloud infrastructure's health, enabling rapid detection of anomalies, proactive issue resolution, and data-driven decision-making. This integration empowers teams to optimize resource utilization, enhance system reliability, and maintain operational excellence by leveraging Coralogix's analytics, alerts, and visualization tools to extract valuable insights from GCP status logs.</p>"},{"location":"newoutput/gcp-status-logs/#get-started","title":"Get Started","text":"<p>STEP 1.\u00a0In your navigation pane, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 2.\u00a0In the Contextual Data section, select GCP and click\u00a0+ ADD.</p> <p></p> <p>STEP 3. Click Enable GCP Data Ingestion.</p>"},{"location":"newoutput/gcp-status-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/gcp-traces/","title":"GCP Traces","text":"<p>Google Cloud Platform provides built-in monitoring and observability tools that allow users to collect and analyze metrics and traces from their GCP resources. Send Google Cloud traces seamlessly to Coralogix. Search, analyze, and visualize your data, gaining insights into application behavior, identifying errors, and troubleshooting problems.</p> <p>Find documentation on sending us your Google Cloud metrics here.</p>"},{"location":"newoutput/gcp-traces/#overview","title":"Overview","text":"<p>This tutorial details how to send your Google Cloud traces for ingestion by Coralogix. It requires that you configure GCP to send all your traces to a BigQuery sink, then create a service account giving Coralogix access to the BigQuery table holding the trace records. The table will be scanned periodically, with traces imported to Coralogix.</p>"},{"location":"newoutput/gcp-traces/#prerequisites","title":"Prerequisites","text":"<ul> <li>Google Cloud SDK\u00a0installed</li> </ul>"},{"location":"newoutput/gcp-traces/#create-a-gcp-service-account","title":"Create a GCP Service Account","text":"<p>A prerequisite for sending your Google Cloud traces to Coralogix is creating a GCP service account.</p> <p>STEP 1. Log in to your Google Cloud Console and select the project in which you want the service account to be created.</p> <p>STEP 2. Navigate to the IAM &amp; Admin section of the console by clicking on the menu on the top left corner of the console. Select IAM &amp; Admin from the menu.</p> <p>STEP 3. Click Service accounts in the left-hand menu and then click + CREATE SERVICE ACCOUNT.</p> <p></p> <p>STEP 4. Input your service account details: name, account ID, and description. Click CREATE AND CONTINUE.</p> <p></p> <p>STEP 5. Select roles for the service account. To collect traces, the roles <code>BigQuery Job User</code> and <code>BigQuery Data Viewer</code> are required..</p> <p></p> <p>STEP 6. Click Done.</p> <p>STEP 7. An overview of all of your service accounts will appear. Find the service account you just created. Click the three dots in the left-most Action column and select Manage keys.</p> <p></p> <p>STEP 8. Click Add Key. Select JSON Key type. Store the key locally, as you will need it for the UI.</p> <p></p> <p>STEP 9. Click CREATE to create and download the key file.</p>"},{"location":"newoutput/gcp-traces/#create-a-gcp-traces-integration","title":"Create a GCP Traces Integration","text":""},{"location":"newoutput/gcp-traces/#set-up-a-bigquery-traces-sink","title":"Set Up a BigQuery Traces Sink","text":"<p>A BigQuery traces sink has to be set up in your GCP project. This a requirement for configuration. Once created, it allows you to stream data from this table to Coralogix.</p> <p>As a prerequisite, set the following environment variables based on the example below.</p> <pre><code>export PROJECT_NUMBER=12345678901   # The GCP project id\nexport ZONE=europe-west1            # The bigquery table and Pub/Sub topic must be in the same zone\nexport BQ_DATASET=traces            # Choose a unique name for the bigquery dataset\nexport SINK_ID=traces-sink          # Choose a unique name for sink\n\n\n</code></pre> <p>STEP 1. Set up Google Cloud Platform.</p> <ul> <li> <p>Install gcloud.</p> </li> <li> <p>Log in to gcloud:</p> </li> </ul> <pre><code>gcloud auth application-default login\n\n</code></pre> <ul> <li>Enable the necessary APIs:</li> </ul> <pre><code>gcloud services enable dataflow compute_component logging storage_component storage_api bigquery pubsub datastore.googleapis.com cloudresourcemanager.googleapis.com\n\n</code></pre> <p>STEP 2. Create the destination dataset.</p> <pre><code>bq --location=$ZONE mk \\\\\n--dataset \\\\\n--description=\"Traces\" \\\\\n$PROJECT_ID:$BQ_DATASET\n\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>The vse command may be modified to conform to your dataset settings.</p> </li> <li> <p>GCP will write traces in a table named <code>cloud_trace</code> in the dataset created in this step.</p> </li> <li> <p>Save the name of your dataset as the Dataset ID, and the name of your table as the Table ID for use in this integration.</p> </li> </ul> <p>STEP 3. Create the sink.</p> <pre><code>gcloud alpha trace sinks create $SINK_ID bigquery.googleapis.com/projects/$PROJECT_NUMBER/datasets/$BQ_DATASET\n\n\n</code></pre> <p>A successful setup will produce an output similar to this:</p> <pre><code>You can give permission to the service account by running the following command.\ngcloud projects add-iam-policy-binding bigquery-project \\\\\n--member &lt;serviceAccount:export-0000001cbe991a08-3434@gcp-sa-cloud-trace.iam.gserviceaccount.com&gt; \\\\\n--role roles/bigquery.dataEditor\n\n\n</code></pre> <p>STEP 4. Copy the command printed in the terminal in the previous step and replace <code>bigquery-project</code> with your project id.</p> <p>STEP 5. Verify the sink was created successfully with the following command:</p> <pre><code>gcloud alpha trace sinks list\n\n</code></pre>"},{"location":"newoutput/gcp-traces/#create-an-integration","title":"Create an Integration","text":"<p>To start collecting traces for a GCP project, an integration must be created. The configuration requires the BigQuery dataset name (Dataset ID) and table name (Table ID) created in the previous section.</p> <p>STEP 1. From your Coralogix toolbar, navigate to Data Flow &gt; Integrations.</p> <p>STEP 2. From the Integrations section, select GCP Traces.</p> <p></p> <p>STEP 3. Click + ADD NEW.</p> <p></p> <p>STEP 4. Click SELECT FILE and select the key file that you created in the previous section.</p> <p>A confirmation appears that the file uploaded successfully.</p> <p>STEP 5. Click NEXT.</p> <p></p> <p>STEP 6. Create the BigQuery table according to the instructions in the integration.</p> <p>STEP 7. Click NEXT.</p> <p></p> <p>STEP 8. Select the application and subsystem settings.</p> <ul> <li> <p>Integration Name. The Project ID. This is auto-populated using the service account key.</p> </li> <li> <p>Application Name. The default application name. This is auto-populated using the service account key.</p> </li> <li> <p>Application / Subsystem Label Selection. Select labels that will be used to create the application name. The first label in <code>application_name_labels</code> matching a resource attribute name or a trace label will be used as application name. If no match is found and <code>application_name</code> is not empty, that value will be used. Otherwise, application name will be left empty. The same logic applies to Subsystem Label Selection.</p> </li> <li> <p>Subsystem Name. The default subsystem name.</p> </li> <li> <p>Dataset ID. The name of the destination dataset created during your BigQuery traces sink setup.</p> </li> <li> <p>Table ID. The name of the table created in the dataset during your BigQuery traces sink setup. The default is <code>cloud_trace</code>.</p> </li> </ul> <p>Step 9. Click NEXT.</p> <p></p>"},{"location":"newoutput/gcp-traces/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/generic-firehose-logs/","title":"CloudWatch Generic Logs using AWS Kinesis Data Firehose","text":"<p>Streamline the process of ingesting and analyzing logs from your AWS resources using our automated CloudWatch Generic Logs using AWS Kinesis Data Firehose integration package.</p>"},{"location":"newoutput/generic-firehose-logs/#overview","title":"Overview","text":"<p>AWS CloudWatch logs are records of events and data generated by various AWS resources, applications, and services. These logs play a crucial role in monitoring and troubleshooting your AWS environment. They provide insights into system behavior, errors, and operational performance, allowing you to detect issues, analyze trends, and ensure the reliability and security of your applications.</p>"},{"location":"newoutput/generic-firehose-logs/#benefits","title":"Benefits","text":"<ul> <li> <p>Gain deeper insights, conduct advanced analytics, set up proactive alerting, and monitor your AWS logs in real time, enhancing your ability to troubleshoot and maintain optimal system performance.</p> </li> <li> <p>Using AWS Kinesis Data Firehose streamlines the data delivery process by providing a reliable and scalable data transfer mechanism. It ensures that your log data reaches Coralogix with minimal latency and no loss of data.</p> </li> <li> <p>The generic nature of this integration allows you to flexibly select the source of your logs during the setup, accommodating various AWS resources and services.</p> </li> </ul>"},{"location":"newoutput/generic-firehose-logs/#configuration","title":"Configuration","text":"<p>STEP 1. From your Coralogix toolbar, navigate to Data Flow &gt; Integrations.</p> <p>STEP 2. In the Integrations section, select AWS Generic Logs via Firehose.</p> <p></p> <p>STEP 3. Click ADD NEW.</p> <p>STEP 4. Input your integration details.</p> <p></p> <ul> <li> <p>Integration Name. Enter a name for your integration. This will be used as a stack name in CloudFormation.</p> </li> <li> <p>API Key. Enter your Send-Your-Data API key or click CREATE A NEW KEY to create a new API key for the integration.</p> </li> <li> <p>Application Name. Enter an application name. The default name is AWS.</p> </li> <li> <p>Subsystem Name. Enter a subsystem name. The default name is Firehose.</p> </li> <li> <p>Enable Dynamic Metadata. When enabled, this sets the <code>applicationName</code> and <code>subsystemName</code> dynamically. These values are extracted based on the selected Input Source.</p> </li> <li> <p>Kinesis Stream ARN. [Optional] Enter the ARN of the Kinesis stream if using Amazon Kinesis Data Streams as source for logs.</p> </li> <li> <p>Input Source. The source of the data in AWS Kinesis Data Firehose determines the <code>integrationType</code> parameter value. Refer to the table below to understand how the <code>applicationName</code> and <code>subsystemName</code> will be dynamically extracted based on the selected Input Source.</p> <ul> <li> <p>For CloudWatch logs, use <code>CloudWatch_JSON</code>.</p> </li> <li> <p>For CloudTrail logs in CloudWatch, use <code>CloudWatch_CloudTrail</code>.</p> </li> <li> <p>For logs coming from EKS Fargate, use <code>EksFargate</code>.</p> </li> <li> <p>For data sources matching the Coralogix\u00a0log ingestion format, use <code>Default</code>.</p> </li> <li> <p>For other data sources, use <code>RawText</code>. This moves all the text to\u00a0<code>text</code>\u00a0field of log, adds severity of\u00a0<code>Info</code>, and generates a current timestamp. All further parsing of these logs should be done using parsing rules.</p> </li> </ul> </li> </ul> Type Dynamic applicationName Dynamic subsystemName Notes CloudWatch_JSON the cloudwatch log group none Supplied by AWS CloudWatch_CloudTrail the cloudwatch log group none Supplied by AWS Default \u2018applicationName\u2019 field \u2018subsystemName\u2019 field Needs to be supplied in the log to be used EksFargate \u2018kubernetes.namespace_name\u2019 field \u2018kubernetes.container_name\u2019 field Supplied by the default configuration WAF The web acl name none Supplied by AWS <ul> <li> <p>Kinesis Stream ARN. [Optional] Enter the ARN of the Kinesis stream if using Amazon Kinesis Data Streams.</p> </li> <li> <p>AWS Region. Select your AWS region from the dropdown menu.</p> </li> <li> <p>AWS PrivateLink (Advanced Settings). Enabling the use of AWS PrivateLink is recommended in order to ensure a secure and private connection between your VPCs and AWS services. Find out more here.</p> </li> </ul> <p>STEP 6. Click NEXT.</p> <p></p> <p>STEP 7. Review the instructions for your integration. Click\u00a0CREATE CLOUDFORMATION.</p> <p></p> <p>STEP 8. You will be rerouted to the AWS website. Verify that all of the auto pre-populated values are correct. Click Create Stack.</p> <p>STEP 9. Return to the Coralogix application where you will be presented with instructions on how to configure the log delivery from the selected input source to AWS Kinesis Data Firehose (if relevant).</p> <p></p> <p></p> <p>Notes:</p> <p>If you provide a Kinesis Stream ARN, Coralogix assumes that the data is in the stream and does not provide any additional instructions. It is the user\u2019s responsibility to deliver data to the stream. Instead of the instructions you will see a message that prompts the user to confirm the integration.</p> <p>STEP 10. Click\u00a0COMPLETE\u00a0to close the module.</p> <p>STEP 11. [Optional] Deploy the\u00a0extension package\u00a0of your choice to complement your integration needs. We offer the following extensions for data originating from CloudTrail and WAF:</p> <ul> <li> <p>AWS CloudTrail</p> </li> <li> <p>AWS WAF</p> </li> </ul> <p>STEP 12. View the logs by navigating to Explore &gt; Logs in your Coralogix toolbar. Find out more here.</p>"},{"location":"newoutput/generic-firehose-logs/#additional-resources","title":"Additional Resources","text":"DocumentationGetting Started with Coralogix"},{"location":"newoutput/generic-firehose-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/generic-incoming-webhooks/","title":"Generic Incoming Webhooks","text":"<p>Coralogix offers customers the option of automating the creation of their incoming webhooks to connect their applications to Coralogix.</p>"},{"location":"newoutput/generic-incoming-webhooks/#overview","title":"Overview","text":"<p>Use our incoming webhooks to simplify the process of generating a webhook URL to connect your applications to Coralogix. This feature enables you to automatically generate the webhook URL necessary to send us your telemetry and expands the selection of applications available for this purpose.</p>"},{"location":"newoutput/generic-incoming-webhooks/#intended-usage","title":"Intended Usage","text":"<p>Avoid utilizing incoming webhooks for high-traffic scenarios due to the latency overhead associated with each request. Standard log endpoints, such as singles, offer better performance in such cases.</p>"},{"location":"newoutput/generic-incoming-webhooks/#configuration","title":"Configuration","text":"<p>STEP 1. In your Coralogix navigation pane, click Data Flow &gt; Contextual Data.</p> <p>STEP 2. In the Contextual Data section, click Generic Webhook ADD.</p> <p></p> <p>STEP 3. Click ADD NEW.</p> <p></p> <p>STEP 4. Fill in the integration details:</p> <ul> <li> <p>Webhook Name: Name your webhook.</p> </li> <li> <p>Your API Key. Click CREATE NEW KEY to generate an API Key and name it.</p> </li> <li> <p>Enter an Application and Subsystem Name to use in the webhook.</p> </li> <li> <p>Content Type. Select either JSON or Text. If JSON is selected, the webhook expects requests with content-type as either <code>application/json</code> or <code>application/x-ndjson</code>.</p> </li> <li> <p>Timestamp. Select whether to generate the timestamp using the system time when the event was ingested or to extract it from the body of the event.</p> <ul> <li> <p>If extracting from the body of the event, select whether to extract using a Path or Regex. Generally when the content type is JSON, the extraction will use a path, and when the content is text, the extraction will use Regex.</p> </li> <li> <p>Enter the path or Regex to use to extract the timestamp.</p> </li> </ul> </li> </ul> <p></p> <p>STEP 5. Click GENERATE URL. The URL for the webhook will be automatically created. Copy and paste into the relevant application.</p> <p>For example, to create an incoming webhook with GitLab, you would paste the URL into the highlighted field, as in the following image.</p> <p></p> <p>NOTE: If you are working in Secure Mode, you will be able to download the URL once, when you first generate it, and the URL will appear masked in the system.</p>"},{"location":"newoutput/generic-incoming-webhooks/#additional-resources","title":"Additional Resources","text":"DocumentationExtension Packages"},{"location":"newoutput/generic-incoming-webhooks/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/getting-started-with-coralogix-alerts/","title":"Getting Started with Coralogix Alerts","text":"<p>Coralogix Alerts allow for timely detection of anomalies, proactive incident response, improved mean time to resolution (MTTR), reduced manual monitoring effort, customization and flexibility. Powered by machine learning, our alerts proactively notify teams of potential problems, correlate incidents, and provide root cause analysis. By reducing response fatigue and prioritizing incidents for immediate action, Coralogix Alerts is an indispensable tool for maintaining optimal performance.</p>"},{"location":"newoutput/getting-started-with-coralogix-alerts/#overview","title":"Overview","text":"<p>Alerting refers to the practice of setting up automated notifications or alarms that trigger when certain predefined conditions or thresholds are met. These conditions can be related to the performance, health, or behavior of the systems, applications, or infrastructure being monitored. The main purpose of alerting is to promptly notify you - DevOps teams and developers - when something unusual or problematic occurs, so you can take appropriate actions to resolve the issue before it escalates.</p> <p>Use our alerting capabilities for any of the following:</p> <ul> <li> <p>Timely Detection of Anomalies. Alerting allows organizations to detect anomalies and issues in real-time or near real-time. This helps to identify problems early on, reducing the impact on users and preventing more severe consequences.</p> </li> <li> <p>Proactive Incident Response. By providing immediate notifications, alerting enables teams to take proactive actions to address issues before they become critical incidents. This minimizes downtime and ensures better system reliability.</p> </li> <li> <p>Improved Mean Time to Resolution (MTTR). Prompt alerts allow you to respond quickly to incidents, leading to faster resolution times. This helps in reducing the mean time to resolution and, consequently, improving the overall system uptime and performance.</p> </li> <li> <p>Reduced Manual Monitoring Effort. Without alerting, teams would need to manually monitor systems constantly, which is impractical and error-prone. Automating alerting saves time and effort by flagging issues automatically.</p> </li> <li> <p>Customization and Flexibility. Define specific thresholds and conditions based on your knowledge of your system's behavior, ensuring that they receive relevant alerts tailored to your needs.</p> </li> </ul>"},{"location":"newoutput/getting-started-with-coralogix-alerts/#how-streama-enables-in-stream-machine-learning-alerting","title":"How Streama Enables In-Stream Machine Learning Alerting","text":"<p>Our alerts\u00a0are powered by our\u00a0Streama\u00a9 technology, which enables automatic triggering of your alerts as part of the streaming process,\u00a0without prior indexing. In other words, we process your data first and delay storage and indexing until all important decisions have been made. This gives you the value of alerting without paying for the expensive cost of Frequent Search Indexing. Using this technology, our alerts automatically learn your system and adapt to your evolving infrastructure, allowing for rapid setup that can track thousands of components and proactively alert on deviations from the norm.</p> <p></p>"},{"location":"newoutput/getting-started-with-coralogix-alerts/#how-alerting-works","title":"How Alerting Works","text":"<p>Check out this high-level workflow of how alerts are generated, triggered, and delivered to users.</p> <p>STEP 1. Setting Alert Rules</p> <p>Administrators, developers, or DevOps define alert rules within the observability platform. These rules specify the conditions under which an alert should be triggered. For instance, they might set a rule to alert when the CPU usage exceeds a certain threshold or when specific error messages appear in the logs.</p> <p>STEP 2. Data Collection and Analysis</p> <p>The Coralogix platform continuously collects data from the system, including logs, metrics, and traces. It processes and analyzes this data against the defined alert rules.</p> <p>STEP 3. Alert Triggering</p> <p>When the monitored data meets the conditions specified in the alert rules, an alert is triggered. This could be a sudden spike in error rates, high latency, low resource availability, or any other predefined anomaly.</p> <p>STEP 4. Alert Aggregation and Deduplication</p> <p>The alerting system may aggregate multiple similar alerts into a single notification to prevent overwhelming users with redundant notifications. Also, it may deduplicate alerts to avoid bombarding users with repetitive information.</p> <p>STEP 5. Notification and Escalation</p> <p>Once an alert is triggered and processed, the system sends notifications to the designated users or teams. Notifications can be delivered through various channels such as email, Slack, SMS, or integrated incident management platforms. If the situation remains unresolved, the alert may be escalated to higher-level teams or individuals.</p> <p>STEP 6. Alert Resolution and Acknowledgment</p> <p>The recipients of the alert acknowledge the alert and take appropriate actions to address the issue. Once the problem is resolved, they mark the alert as \u201cDone\u201d within the platform.</p> <p>STEP 7. Monitoring and Reporting</p> <p>Throughout the alerting process, the observability platform continuously monitors the system's status. It may keep track of acknowledgment status, resolution time, and other metrics to generate reports and help with post-incident analysis and improvement.</p>"},{"location":"newoutput/getting-started-with-coralogix-alerts/#what-we-offer","title":"What We Offer","text":"<p>Choose from any of the following alerts:</p> <ul> <li> <p>Standard Alert. Triggered by crossing a set quantity threshold of specific logs, this\u00a0feature allows you to monitor system performance, get notified when changes occur, and instantly pinpoint potential causes.</p> </li> <li> <p>Metric Alert. This monitoring alert triggers based on the values of metrics exceeding or falling below predefined thresholds. It is commonly used to detect performance issues, resource constraints, or abnormal behavior within a system, allowing administrators to take prompt actions to address potential problems.</p> </li> <li> <p>Machine-Learning Metric Alert. This utilizes advanced artificial intelligence algorithms to analyze incoming metric time series and predict their expected behavior for the next 24 hours. By establishing a baseline for normal behavior, this powerful capability enables you to proactively identify anomalies in real-time.</p> </li> <li> <p>Tracing Alert. Coralogix allows you not only to visualize your traces, but also set alerts on them. Our tracing alert allows you to be alerted automatically of specific tags and services on the basis of a specified latency.</p> </li> <li> <p>Flow Alert. This alert is designed to notify you when any combination of alert events occurs in a specific\u00a0sequence\u00a0within a defined\u00a0timeframe. For example, to be notified of an increase in HTTP error rate caused by high CPU utilization, a Flow Alert should be configured to trigger when a high CPU utilization alert is followed by a high HTTP error rate alert within a defined timeframe.</p> </li> <li> <p>Time Relative Alert. Automatically detect abnormal behavior in your system using this alert, which is triggered when a fixed ratio reaches a set threshold compared to a past time frame. Use this feature to receive automatic alerts regarding changes in your system\u2019s security, operations, and / or business behaviors over time. Then compare between each behavior across different time periods.</p> </li> <li> <p>Unique Count Alert. This triggers when the number of distinct or unique values in a specific metric or field exceeds a predefined threshold. It helps detect situations where a sudden increase or decrease in unique values may indicate potential anomalies or unexpected behavior within the system.</p> </li> <li> <p>Ratio Alert. Ratio alerts allow you to easily calculate a ratio between two log queries and trigger an alert when the ratio reaches a set threshold.</p> </li> <li> <p>New Value Alert. This alert is triggered by the first occurrence of a new value within a time interval. All values are tested against a list that is being dynamically created while the alert is active. The alert is set by a specific query to identify a subset of logs (if needed), is and defined with a key of choice to track for new values within the desired interval.</p> </li> </ul>"},{"location":"newoutput/getting-started-with-coralogix-alerts/#get-started","title":"Get Started","text":"<p>STEP 1. Sign up for a Coralogix account if you haven\u2019t already done so.</p> <p>STEP 2. Send your data to Coralogix using our two-step, out-of-the box\u00a0integration packages or select from our full list of\u00a0integrations. Many of our integration packages allow you to deploy our associated extension packages, unlocking a set of predefined dashboards, alerting and parsing rules, allowing you to jumpstart your Coralogix monitoring.</p> <p>Find out more on our Getting Started with Coralogix page.</p> <p>STEP 3. Set up your alerts. Use our extension packages for deploying predefined alerts or manual setup.</p>"},{"location":"newoutput/getting-started-with-coralogix-alerts/#coralogix-alerts-api","title":"Coralogix Alerts API","text":"<p>Optionally, you can set up our Alerts API. This enables you to create alerts that actively check system performance and notify you when there are changes to your data. Our Alerts API allows you to define, query, and manage your alerts programmatically, creating a powerful, dynamic alerting experience that can adapt.</p>"},{"location":"newoutput/getting-started-with-coralogix-alerts/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/github-data-ingestion/","title":"GitHub Data Ingestion","text":"<p>Collect your GitHub messages in the Coralogix platform using our automatic Contextual Data Integration Package. The package automatically generates a URL to be used when creating a GitHub webhook.</p>"},{"location":"newoutput/github-data-ingestion/#overview","title":"Overview","text":"<p>GitHub is a widely used web-based platform designed for version control, software development collaboration, and code hosting. It allows developers to efficiently manage and track changes to their codebase using the Git version control system. GitHub provides a range of features including repositories for code storage, issue tracking, pull requests for code review, continuous integration, and a collaborative environment for teams to work together on projects. It's a hub for open-source and private development, enabling individuals and teams to contribute, collaborate, and maintain high-quality software projects.</p> <p>Sending your GitHub logs to Coralogix streamlines log management, augments development monitoring, and enhances issue resolution. By routing GitHub logs into Coralogix, you gain a consolidated view of your code repository activities, enabling rapid anomaly detection, proactive debugging, and data-driven decision-making. This integration empowers development teams to optimize workflows, strengthen system reliability, and sustain operational effectiveness, utilizing Coralogix's analytics, alerts, and visualization tools to extract valuable insights from GitHub logs and ensure a seamless and resilient software development lifecycle.</p>"},{"location":"newoutput/github-data-ingestion/#get-started","title":"Get Started","text":"<p>STEP 1.\u00a0In your navigation pane, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 2.\u00a0In the Contextual Data section, select GitHub and click\u00a0+ ADD.</p> <p>STEP 3. Click ADD NEW.</p> <p>STEP 4.\u00a0Fill in the Integration Details:</p> <ul> <li> <p>Name.\u00a0Name your integration.</p> </li> <li> <p>Your API Key. Click\u00a0CREATE NEW KEY\u00a0to generate an API key and name it.</p> </li> <li> <p>Application Name and Subsystem Name. Enter an\u00a0application and subsystem name.</p> </li> </ul> <p></p> <p>STEP 5.\u00a0Click\u00a0GENERATE URL. The URL for the integration will be automatically created. Use this when creating a GitHub webhook.</p> <p></p> <p>STEP 6. Copy the Secret to be used as a shared secret in GitHub when creating the GitHub webhook.</p>"},{"location":"newoutput/github-data-ingestion/#create-a-github-webhook","title":"Create a GitHub Webhook","text":"<p>Create a GitHub webhook using your URL.</p> <p>STEP 1.\u00a0Log in to your GitHub account.</p> <p>STEP 2. If you do not have a project, create one and create a repository under this project. If you have already done that, move to the next step.</p> <p>STEP 3. In the top right corner, click on your name and select your repositories tab.</p> <p></p> <p>STEP 4.\u00a0Select the repository to which you want to add the webhook integration.</p> <p></p> <p>STEP 5. Click Settings &gt; Webhooks.</p> <p></p> <p>STEP 6. Complete the Add webhook form and select the events that should trigger the webhook.</p> <ul> <li> <p>Copy the URL generated by the Coralogix integration.</p> </li> <li> <p>Set Content type: application/json</p> </li> <li> <p>Copying the API key to the Secret field.</p> </li> </ul> <p>STEP 7. Click Add webhook.</p>"},{"location":"newoutput/github-data-ingestion/#example-log","title":"Example Log","text":"<pre><code>{\n\u00a0\u00a0\"github\": {\n\u00a0\u00a0\u00a0\u00a0\"ref\": \"refs/heads/main\",\n\u00a0\u00a0\u00a0\u00a0\"before\": \"02b0534caddfd9d221cc6ac61f6b6f7c226336a3\",\n\u00a0\u00a0\u00a0\u00a0\"after\": \"15cb0605d8097d645036c499ca84253e75d7367f\",\n\u00a0\u00a0\u00a0\u00a0\"repository\": {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"id\": 323160153,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"node_id\": \"MDEwOlJlcG9zaXRvcnkzMjMxNjAxNTM=\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"name\": \"my_first_repo\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"full_name\": \"test/my_first_repo\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"private\": true,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"owner\": {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"name\": \"test\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"email\": \"111111+test@users.noreply.github.com\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"login\": \"test\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"id\": 11111111,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"node_id\": \"MDQ6VXNlcjc2NDA4Mzcw\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"avatar_url\": \"https://avatars.githubusercontent.com/u/11111111?v=4\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"gravatar_id\": \"\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"url\": \"https://api.github.com/users/test\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"html_url\": \"https://github.com/test\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"followers_url\": \"https://api.github.com/users/test/followers\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"following_url\": \"https://api.github.com/users/test/following{/other_user}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"gists_url\": \"https://api.github.com/users/test/gists{/gist_id}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"starred_url\": \"https://api.github.com/users/test/starred{/owner}{/repo}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"subscriptions_url\": \"https://api.github.com/users/test/subscriptions\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"organizations_url\": \"https://api.github.com/users/test/orgs\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"repos_url\": \"https://api.github.com/users/test/repos\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"events_url\": \"https://api.github.com/users/test/events{/privacy}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"received_events_url\": \"https://api.github.com/users/test/received_events\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"type\": \"User\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"site_admin\": false\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"html_url\": \"https://github.com/test/my_first_repo\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"description\": \"It just for testing\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"fork\": false,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"url\": \"https://github.com/test/my_first_repo\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"forks_url\": \"https://api.github.com/repos/test/my_first_repo/forks\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"keys_url\": \"https://api.github.com/repos/test/my_first_repo/keys{/key_id}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"collaborators_url\": \"https://api.github.com/repos/test/my_first_repo/collaborators{/collaborator}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"teams_url\": \"https://api.github.com/repos/test/my_first_repo/teams\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"hooks_url\": \"https://api.github.com/repos/test/my_first_repo/hooks\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"issue_events_url\": \"https://api.github.com/repos/test/my_first_repo/issues/events{/number}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"events_url\": \"https://api.github.com/repos/test/my_first_repo/events\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"assignees_url\": \"https://api.github.com/repos/test/my_first_repo/assignees{/user}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"branches_url\": \"https://api.github.com/repos/test/my_first_repo/branches{/branch}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"tags_url\": \"https://api.github.com/repos/test/my_first_repo/tags\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"blobs_url\": \"https://api.github.com/repos/test/my_first_repo/git/blobs{/sha}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"git_tags_url\": \"https://api.github.com/repos/test/my_first_repo/git/tags{/sha}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"git_refs_url\": \"https://api.github.com/repos/test/my_first_repo/git/refs{/sha}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"trees_url\": \"https://api.github.com/repos/test/my_first_repo/git/trees{/sha}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"statuses_url\": \"https://api.github.com/repos/test/my_first_repo/statuses/{sha}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"languages_url\": \"https://api.github.com/repos/test/my_first_repo/languages\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"stargazers_url\": \"https://api.github.com/repos/test/my_first_repo/stargazers\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"contributors_url\": \"https://api.github.com/repos/test/my_first_repo/contributors\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"subscribers_url\": \"https://api.github.com/repos/test/my_first_repo/subscribers\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"subscription_url\": \"https://api.github.com/repos/test/my_first_repo/subscription\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"commits_url\": \"https://api.github.com/repos/test/my_first_repo/commits{/sha}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"git_commits_url\": \"https://api.github.com/repos/test/my_first_repo/git/commits{/sha}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"comments_url\": \"https://api.github.com/repos/test/my_first_repo/comments{/number}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"issue_comment_url\": \"https://api.github.com/repos/test/my_first_repo/issues/comments{/number}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"contents_url\": \"https://api.github.com/repos/test/my_first_repo/contents/{+path}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"compare_url\": \"https://api.github.com/repos/test/my_first_repo/compare/{base}...{head}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"merges_url\": \"https://api.github.com/repos/test/my_first_repo/merges\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"archive_url\": \"https://api.github.com/repos/test/my_first_repo/{archive_format}{/ref}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"downloads_url\": \"https://api.github.com/repos/test/my_first_repo/downloads\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"issues_url\": \"https://api.github.com/repos/test/my_first_repo/issues{/number}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"pulls_url\": \"https://api.github.com/repos/test/my_first_repo/pulls{/number}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"milestones_url\": \"https://api.github.com/repos/test/my_first_repo/milestones{/number}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"notifications_url\": \"https://api.github.com/repos/test/my_first_repo/notifications{?since,all,participating}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"labels_url\": \"https://api.github.com/repos/test/my_first_repo/labels{/name}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"releases_url\": \"https://api.github.com/repos/test/my_first_repo/releases{/id}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"deployments_url\": \"https://api.github.com/repos/test/my_first_repo/deployments\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"created_at\": 1608496588,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"updated_at\": \"2021-07-16T07:39:25Z\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"pushed_at\": 1626421494,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"git_url\": \"git://github.com/test/my_first_repo.git\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"ssh_url\": \"git@github.com:test/my_first_repo.git\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"clone_url\": \"https://github.com/test/my_first_repo.git\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"svn_url\": \"https://github.com/test/my_first_repo\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"homepage\": null,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"size\": 0,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"stargazers_count\": 0,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"watchers_count\": 0,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"language\": null,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"has_issues\": true,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"has_projects\": true,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"has_downloads\": true,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"has_wiki\": true,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"has_pages\": false,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"forks_count\": 0,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"mirror_url\": null,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"archived\": false,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"disabled\": false,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"open_issues_count\": 1,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"license\": null,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"forks\": 0,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"open_issues\": 1,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"watchers\": 0,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"default_branch\": \"main\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"stargazers\": 0,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"master_branch\": \"main\"\n\u00a0\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\u00a0\"pusher\": {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"name\": \"test\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"email\": \"111111+test@users.noreply.github.com\"\n\u00a0\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\u00a0\"sender\": {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"login\": \"test\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"id\": 1111111,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"node_id\": \"MDQ6VXNlcjc2NDA4Mzcw\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"avatar_url\": \"https://avatars.githubusercontent.com/u/11111111?v=4\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"gravatar_id\": \"\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"url\": \"https://api.github.com/users/test\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"html_url\": \"https://github.com/test\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"followers_url\": \"https://api.github.com/users/test/followers\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"following_url\": \"https://api.github.com/users/test/following{/other_user}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"gists_url\": \"https://api.github.com/users/test/gists{/gist_id}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"starred_url\": \"https://api.github.com/users/test/starred{/owner}{/repo}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"subscriptions_url\": \"https://api.github.com/users/test/subscriptions\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"organizations_url\": \"https://api.github.com/users/test/orgs\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"repos_url\": \"https://api.github.com/users/test/repos\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"events_url\": \"https://api.github.com/users/test/events{/privacy}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"received_events_url\": \"https://api.github.com/users/test/received_events\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"type\": \"User\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"site_admin\": false\n\u00a0\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\u00a0\"created\": false,\n\u00a0\u00a0\u00a0\u00a0\"deleted\": false,\n\u00a0\u00a0\u00a0\u00a0\"forced\": false,\n\u00a0\u00a0\u00a0\u00a0\"base_ref\": null,\n\u00a0\u00a0\u00a0\u00a0\"compare\": \"https://github.com/test/my_first_repo/compare/02b0534caddf...15cb0605d809\",\n\u00a0\u00a0\u00a0\u00a0\"commits\": [\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"id\": \"15cb0605d8097d645036c499ca84253e75d73611\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"tree_id\": \"a28d049bbe7fd047cf9171b13f969750662d1211\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"distinct\": true,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"message\": \"Update README.md\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"timestamp\": \"2021-07-16T09:44:53+02:00\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"url\": \"https://github.com/test/my_first_repo/commit/15cb0605d8097d645036c499ca84253e11111\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"author\": {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"name\": \"test\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"email\": \"11111111+test@users.noreply.github.com\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"username\": \"test\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"committer\": {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"name\": \"GitHub\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"email\": \"noreply@github.com\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"username\": \"web-flow\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"added\": [],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"removed\": [],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"modified\": [\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"README.md\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0],\n\u00a0\u00a0\u00a0\u00a0\"head_commit\": {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"id\": \"15cb0605d8097d645036c499ca84253e75d73611\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"tree_id\": \"a28d049bbe7fd047cf9171b13f969750662d1211\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"distinct\": true,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"message\": \"Update README.md\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"timestamp\": \"2021-07-16T09:44:53+02:00\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"url\": \"https://github.com/test/my_first_repo/commit/15cb0605d8097d645036c499ca84253e7511111\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"author\": {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"name\": \"test\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"email\": \"11111111+test@users.noreply.github.com\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"username\": \"test\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"committer\": {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"name\": \"GitHub\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"email\": \"noreply@github.com\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"username\": \"web-flow\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0},\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"added\": [],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"removed\": [],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"modified\": [\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"README.md\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0]\n\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0},\n\u00a0\u00a0\"source_system\": \"github\"\n}\n</code></pre>"},{"location":"newoutput/github-version-tags/","title":"GitHub Version Tags","text":"<p>GitHub Actions allows you to perform numerous tasks automatically, including using the cURL command to insert a new tag when a release is made or when a pull request is closed for example.</p> <p>This tutorial demonstrates how to build an automation that will create a new tag in Coralogix upon publishing a new release of your code.</p>"},{"location":"newoutput/github-version-tags/#coralogix-version-tags-api-endpoint","title":"Coralogix Version Tags API Endpoint","text":"<p>This document includes cluster-dependent URLs.</p> <p>Refer to the following table to select the correct Coralogix Version Tags API endpoint for the domain associated with your Coralogix account.</p> Domain Region Endpoint coralogix.us US1 https://webapi.coralogix.us/api/v1/external/tags/ coralogixstg.wpengine.com EU1 https://webapi.coralogixstg.wpengine.com/api/v1/external/tags/ eu2.coralogixstg.wpengine.com EU2 https://webapi.eu2.coralogixstg.wpengine.com/api/v1/external/tags/ app.coralogix.in AP1 (IN) https://webapi.app.coralogix.in/api/v1/external/tags/ coralogixsg.com AP2 (SG) https://webapi.coralogixsg.com/api/v1/external/tags/"},{"location":"newoutput/github-version-tags/#tutorial","title":"Tutorial","text":"<p>STEP 1. Create your Action as a .yml file inside our repository in the workflows directory .github/workflows/my-tag-automation.yml.</p> <p>Note:</p> <ul> <li>Remember to place the .github folder inside the root folder of your repository.</li> </ul> <p>STEP 2. Add the content of your action.</p> <ul> <li>Add the name of the automation and the events that activate it.</li> </ul> <pre><code>name: \"Create a tag\"\n\non:\n  release:\n    types: [published]\n</code></pre> <ul> <li>Add the 'job' of our automation -- the action that it will execute.</li> </ul> <pre><code>name: \"Create a tag\"\n\non:\n  release:\n    types: [published]\n\njobs:\n  run-updater:\n    runs-on: ubuntu-latest\n    steps:\n    - name: create a tag\n      run: |\n        curl --location --request POST '&lt;Cluster-endpoint&gt;' \\\n        --header 'Authorization: Bearer &lt;Tags-api-key&gt;' \\\n        --header 'Content-Type: application/json' \\\n        --data-raw '{ \n        \"name\": \"'\"${GITHUB_REF##*/}\"'\",\n        \"application\": [\"&lt;My-app&gt;\"],\n        \"subsystem\": [\"&lt;My-subsystem&gt;\"]\n        }'\n</code></pre> <p>${GITHUB_REF##*/} = a github action variable holding the reference of the action , in this context its the tag of the release.</p> <p> - Your Application name. You can input more than one name, use the comma delimiter \u2018,\u2019 between the names. <p> - Your Subsystem name. You can input more than one name, use the comma delimiter \u2018,\u2019 between the names. <p> - The endpoint depended on your Coralogix domain <p> - Alerts, Rules, and Tags API Key should be taken from Data Flow \u2013&gt; API Keys \u2013&gt; \u201cAlerts, Rules and Tags API Key\u201d <p></p> <p>STEP 3. After publishing a new release, the Action will run, and a new tag in Coralogix will be created.</p>"},{"location":"newoutput/github-version-tags/#additional-resources","title":"Additional Resources","text":"<ul> <li> <p>For more information about GitHub Actions visit: https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions.</p> </li> <li> <p>For more information about the usage of the cURL command with Coralogix tags visit: https://coralogixstg.wpengine.com/docs/version-tags-with-curl/.</p> </li> </ul>"},{"location":"newoutput/github-version-tags/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/gitlab-data-ingestion/","title":"GitLab Data Ingestion","text":"<p>Collect your GitLab logs in the Coralogix platform using our automatic Contextual Data Integration Package. The package automatically generates a URL to be used when creating a GitLab webhook.</p>"},{"location":"newoutput/gitlab-data-ingestion/#overview","title":"Overview","text":"<p>GitLab is a comprehensive web-based platform that offers end-to-end solutions for version control, software development lifecycle management, and collaboration. Built around the Git version control system, GitLab provides features such as repository management, issue tracking, continuous integration and delivery (CI/CD), code review, and a range of tools for project planning and monitoring. Notably, GitLab can be deployed either as a cloud-based service or as a self-hosted instance, giving organizations flexibility in managing their codebase and development processes. This all-in-one platform fosters streamlined teamwork, efficient development workflows, and enhanced transparency in software projects.</p> <p>Sending your GitLab logs to Coralogix streamlines log aggregation, strengthens monitoring capabilities, and enhances issue resolution for efficient software development. By directing GitLab logs into Coralogix, you gain a comprehensive view of your version control activities, enabling rapid anomaly detection, proactive debugging, and informed decision-making. This integration empowers development teams to optimize workflows, bolster system reliability, and maintain operational efficiency, leveraging Coralogix's analytics, alerts, and visualization tools to extract valuable insights from GitLab logs and ensure a collaborative and resilient software development environment.</p>"},{"location":"newoutput/gitlab-data-ingestion/#get-started","title":"Get Started","text":"<p>STEP 1.\u00a0In your navigation pane, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 2.\u00a0In the Contextual Data section, select GitLab and click\u00a0+ ADD.</p> <p></p> <p>STEP 3. Click ADD NEW.</p> <p>STEP 4.\u00a0Fill in the Integration Details:</p> <ul> <li> <p>Name.\u00a0Name your integration.</p> </li> <li> <p>Your API Key. Click\u00a0CREATE NEW KEY\u00a0to generate an API Key and name it.</p> </li> <li> <p>Application Name and Subsystem Name. Enter an\u00a0application and subsystem name.</p> </li> </ul> <p></p> <p>STEP 5.\u00a0Click\u00a0GENERATE URL. The URL for the integration will be automatically created. Use this when creating a GitLab webhook.</p> <p></p>"},{"location":"newoutput/gitlab-data-ingestion/#create-gitlab-webhook","title":"Create GitLab Webhook","text":"<p>Create a GitLab webhook using your URL.</p> <p>STEP 1.\u00a0Log in to your GitLab account.</p> <p>STEP 2. If you do not have a project, create one. If you have already done that, move to the next step.</p> <p>STEP 3. On the left top corner click on your projects and select your project.</p> <p></p> <p>STEP 4.\u00a0Select a project from your projects or click on New project to create a new project.</p> <p>STEP 5. In the new screen that opens, select Settings &gt; Webhooks.</p> <p></p> <p>STEP 6. In the URL field, enter the URL you generated in Coralogix.</p> <p></p> <p>STEP 7. In the Secret token field, enter the token section of the generated URL (the URL will look like this: https://integrations.cluster URL/v1/gitlab/v1/events/). <p>STEP 8. [Optional] Test your configuration and web-hook by clicking Test. If you get a response other than 200 OK you should check the configuration is correct.</p> <p>STEP 8. Once you finish the configuration, click on Add webhook.</p> <p></p>"},{"location":"newoutput/gitlab-data-ingestion/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/gitlab-version-tags/","title":"GitLab Version Tags","text":"<p>This tutorial demonstrates how to integrate Coralogix with your GitLab deployment pipelines.</p>"},{"location":"newoutput/gitlab-version-tags/#configuration","title":"Configuration","text":"<p>Use GitLab webhooks to inform Coralogix when a new build is issued.</p> <p>STEP 1. Log in to GitLab using your credentials and navigate to your project page.</p> <p>STEP 2. Click Settings &gt; Webhooks.</p> <p>STEP 3. Add the following URL:</p> <p>https://ng-api-http.DOMAIN/api/v1/external/gitlab?application=APPLICATION_NAME&amp;subsystem=SUBSYSTEM_NAME&amp;name=TAG_NAME</p> <p>Insert the following parameters:</p> <p>DOMAIN: Coralogix domain associated with your account</p> <p>APPLICATION_NAME: Application name as it appears in your Coralogix UI</p> <p>SUBSYSTEM_NAME: Subsystem name as it appears in your Coralogix UI. Add one or more subsystem names separated by a comma.</p> <p>TAG_NAME - Tag name. If this parameter is not inserted, values are taken from GitLab payload fields project.name and object_attributes.ref, as explained here.</p> <p>STEP 4. Add a Secret Token. This can be found in your Coralogix UI by navigating to Data Flow &gt; API\u00a0Keys\u00a0&gt;\u00a0Alerts,\u00a0Rules\u00a0and\u00a0Tags\u00a0API\u00a0Key.</p> <p></p> <p>Notes:</p> <ul> <li> <p>The Secret Token is obligatory.</p> </li> <li> <p>Input the Secret Token in the Secret Token field below the URL in GitLab.</p> </li> </ul> <p>STEP 5. Select Pipeline events as a trigger and click save changes.</p> <p></p>"},{"location":"newoutput/gitlab-version-tags/#additional-resources","title":"Additional Resources","text":"DocumentationGitLab Data Ingestion"},{"location":"newoutput/gitlab-version-tags/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/glossary-dataprime-operators/","title":"DataPrime Glossary: Operators & Expressions","text":"<p>This guide provides a full glossary of all available DataPrime operators and expressions.</p> <p>To hit the ground running using DataPrime and to view only our most frequently-used operators with examples, view our DataPrime Cheat Sheet.</p>"},{"location":"newoutput/glossary-dataprime-operators/#operators","title":"Operators","text":""},{"location":"newoutput/glossary-dataprime-operators/#block","title":"block","text":"<p>The\u00a0negation of\u00a0<code>filter</code>. Filters-out all events where the condition is\u00a0<code>true</code>. The same effect can be achieved by using\u00a0<code>filter</code>\u00a0with\u00a0<code>!(condition)</code>.</p> <pre><code>block $d.status_code &gt;= 200 &amp;&amp; $d.status_code &lt;= 299         # Leave all events which don't have a status code of 2xx\n</code></pre> <p>The\u00a0data\u00a0is\u00a0exposed using the following top-level fields:</p> <ul> <li> <p><code>$m</code>\u00a0-Event\u00a0metadata</p> <ul> <li> <p><code>timestamp</code></p> </li> <li> <p><code>severity</code>\u00a0- Possible values are\u00a0<code>V</code>ERBOSE,\u00a0<code>D</code>EBUG,\u00a0<code>I</code>NFO,\u00a0<code>W</code>ARNING,\u00a0<code>E</code>RROR,\u00a0<code>C</code>RITICAL</p> </li> <li> <p><code>priorityclass</code>\u00a0- Possible values are\u00a0<code>high</code>,\u00a0<code>medium</code>,\u00a0<code>low</code></p> </li> <li> <p><code>logid</code></p> </li> </ul> </li> <li> <p><code>$l</code>\u00a0-Event\u00a0labels</p> <ul> <li> <p><code>applicationname</code></p> </li> <li> <p><code>subsystemname</code></p> </li> <li> <p><code>category</code></p> </li> <li> <p><code>classname</code></p> </li> <li> <p><code>computername</code></p> </li> <li> <p><code>methodname</code></p> </li> <li> <p><code>threadid</code></p> </li> <li> <p><code>ipaddress</code></p> </li> </ul> </li> <li> <p><code>$d</code>\u00a0-The\u00a0user's\u00a0data</p> </li> </ul>"},{"location":"newoutput/glossary-dataprime-operators/#bottom","title":"bottom","text":"<p>No grouping variation</p> <p>Limits the rows returned to a specified number and order the result by a set of expressions.</p> <pre><code>order_direction := \"descending\"/\"ascending\" according to top/bottom\n\nbottom &lt;limit&gt; &lt;result_expression1&gt; [as &lt;alias&gt;] [, &lt;result_expression2&gt; [as &lt;alias2&gt;], ...] by &lt;orderby_expression&gt; [as alias&gt;]\n</code></pre> <p>For example, the following query:</p> <pre><code>bottom 5 $m.severity as $d.log_severity by $d.duration\n</code></pre> <p>Will result in logs of the following form:</p> <pre><code>[\n   { \"log_severity\": \"Debug\", \"duration\":  1000 }\n   { \"log_severity\": \"Warning\", \"duration\": 2000 },\n   ...\n]\n</code></pre> <p>Grouping variation</p> <p>Limits the rows returned to a specified number and group them by a set of aggregation expressions and order them by a set of expressions.</p> <pre><code>order_direction := \"descending\"/\"ascending\" according to top/bottom\n\nbottom &lt;limit&gt; &lt;(groupby_expression1|aggregate_function1)&gt; [as &lt;alias&gt;] [, &lt;(groupby_expression2|aggregate_function2)&gt; [as &lt;alias2&gt;], ...] by &lt;(groupby_expression1|aggregate_function1)&gt; [as &lt;alias&gt;]\n</code></pre> <p>For example, the following query:</p> <pre><code>bottom 10 $m.severity, count() as $d.number_of_severities by avg($d.duration) as $d.avg_duration\n</code></pre> <p>Will result in logs of the following form:</p> <pre><code>[\n   { \"severity\": \"Warning\", \"number_of_severities\": 50, avg_duration: 1000 },\n   { \"severity\": \"Debug\", \"number_of_severities\":  10, avg_duration: 2000 }\n   ...\n]\n</code></pre> <p>Supported aggregation functions are listed in \"Aggregation Functions\" section.</p>"},{"location":"newoutput/glossary-dataprime-operators/#choose","title":"choose","text":"<p>Leave only the keypaths provided, discarding all other keys. Fully supports nested keypaths in the output.</p> <pre><code>(choose|select) &lt;keypath1&gt; [as &lt;new_keypath&gt;],&lt;keypath2&gt; [as &lt;new_keypath&gt;],...\n</code></pre> <p>Examples:</p> <pre><code>choose $d.mysuperkey.myfield\nchoose $d.my_superkey.mykey as $d.important_value, 10 as $d.the_value_ten\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#convert","title":"convert","text":"<p>Convert the data types of keys.</p> <p>The <code>datatypes</code> keyword is optional and can be used for readability.</p> <pre><code>(conv|convert) [datatypes] &lt;keypath1&gt;:&lt;datatype1&gt;,&lt;keypath2&gt;:&lt;datatype2&gt;,...\n</code></pre> <p>Examples:</p> <pre><code>convert $d.level:number\nconv datatypes $d.long:number,$d.lat:number\nconvert $d.data.color:number,$d.item:string\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#count","title":"count","text":"<p>Returns a single row containing the number of rows produced by the preceding operators.</p> <pre><code>count [into &lt;keypath&gt;]\n</code></pre> <p>An alias can be provided to override the keypath the result will be written to.</p> <p>For example, the following part of a query</p> <pre><code>count into $d.num_rows\n</code></pre> <p>will result in a single row of the following form:</p> <pre><code>{ \"num_rows\": 7532 }\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#countby","title":"countby","text":"<p>Returns a row counting all the rows grouped by the expression.</p> <pre><code>countby &lt;expression&gt; [as &lt;alias&gt;] [into &lt;keypath&gt;]\n</code></pre> <p>An alias can be provided to override the keypath the result will be written into.</p> <p>For example, the following part of a query</p> <pre><code>countby $d.verb into $d.verb_count\n</code></pre> <p>will result in a row for each group.</p> <p>It is functionally identical to</p> <pre><code>groupby $data.verb calculate count() as $d.verb_count\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#create","title":"create","text":"<p>Create a new key and set its value to the result of the expression. Key creation is granular, meaning that parent keys in the path are not overwritten.</p> <pre><code>  (a|add|c|create) &lt;keypath&gt; from &lt;expression&gt; [on keypath exists (fail|skip|overwrite)] [on keypath missing (fail|create|skip)] [on datatype change (skip|fail|overwrite)\n</code></pre> <p>The creation can be controlled by adding the following clauses:</p> <ul> <li> <p>Adding <code>on keypath exists</code> allows to choose what to do when the keypath already exists.</p> </li> <li> <p><code>overwrite</code> - Overwrites the old value. This is the default value</p> </li> <li> <p><code>fail</code> - Fails the query</p> </li> <li> <p><code>skip</code> - Skips the creation of the key</p> </li> <li> <p>Adding <code>on keypath missing</code> allows to choose what to do when the new keypath does not exist.</p> </li> <li> <p><code>create</code> - Creates the key. This is the default value</p> </li> <li> <p><code>fail</code> - Fails the query</p> </li> <li> <p><code>skip</code> - Skips the creation of the new key</p> </li> <li> <p>Adding <code>on datatype changed</code> allows to choose what to do if the key already exists and the new data changes the datatype of the value</p> </li> <li> <p><code>overwrite</code> - Overwrites the value anyway. This is the default value</p> </li> <li> <p><code>fail</code> - Fails the query</p> </li> <li> <p><code>skip</code> - Leaves the key with the original value (and type)</p> </li> </ul> <p>Examples:</p> <pre><code>create $d.radius from 100+23\nc $d.log_data.truncated_message from $d.message.substring(1,50)\nc $data.trimmed_name from $data.username.trim()\n\ncreate $d.temperature from 100*23 on datatype changed skip\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#distinct","title":"distinct","text":"<p>Returns one row for each distinct combination of the provided expressions.</p> <pre><code>distinct &lt;expression&gt; [as &lt;alias&gt;] [, &lt;expression_2&gt; [as &lt;alias_2&gt;], ...]\n</code></pre> <p>This operator is functionally identical to <code>groupby</code> without any aggregate functions.</p>"},{"location":"newoutput/glossary-dataprime-operators/#enrich","title":"enrich","text":"<p>Enrich your logs using additional context from a lookup table.</p> <p>Upload your lookup table using the Data Flow &gt; Data Enrichment &gt; Custom Enrichment section. For more details, see Custom Enrichment documentation.</p> <pre><code>enrich &lt;value_to_lookup&gt; into &lt;enriched_key&gt; using &lt;lookup_table&gt;\n</code></pre> <ul> <li> <p><code>value_to_lookup</code> - A string expression that will be looked up in the lookup table.</p> </li> <li> <p><code>enriched_key</code> - Destination key to store the enrichment result in.</p> </li> <li> <p><code>lookup_table</code> - The name of the Custom Enrichment table to be used.</p> </li> </ul> <p>The table's columns will be added as sub-keys to the destination key. If <code>value_to_lookup</code> is not found, the destination key will be <code>null</code>. You can then filter the results using the DataPrime capabilities, such as filtering logs by specific value in the enriched field.</p> <p>Example:</p> <p>The original log:</p> <pre><code>{\n    \"userid\": \"111\",\n    ...\n}\n</code></pre> <p>The Custom Enrichment lookup table called <code>my_users</code>:</p> ID Name Department 111 John Finance 222 Emily IT <p>Running the following query:</p> <pre><code>enrich $d.userid into $d.user_enriched using my_users\n</code></pre> <p>Gives the following enriched log:</p> <pre><code>{\n    \"userid\": \"111\",\n    \"user_enriched\": {\n        \"ID\": \"111\",\n        \"Name\": \"John\",\n        \"Department\": \"Finance\"\n    },\n    ...\n}\n</code></pre> <p>Notes:</p> <ul> <li> <p>Run the DataPrime query <code>source &lt;lookup_table&gt;</code> to view the enrichment table.</p> </li> <li> <p>If the original log already contains the enriched key:</p> <ul> <li> <p>If <code>&lt;value_to_lookup&gt;</code> exists in the <code>&lt;lookup_table&gt;</code>, the sub-keys will be updated with the new value. If the <code>&lt;value_to_lookup&gt;</code> does not exist, their current value will remain.</p> </li> <li> <p>Any other sub-keys which are not columns in the <code>&lt;lookup_table&gt;</code> will remain with their existing values.</p> </li> </ul> </li> <li> <p>All values in the <code>&lt;lookup_table&gt;</code> are considered to be strings. This means that:</p> <ul> <li> <p>The <code>&lt;value_to_lookup&gt;</code> must be in a string format.</p> </li> <li> <p>All values are enriched in a string format. You may then convert them to your preferred format (e.g. JSON, timestamp) using the appropriate functions.</p> </li> </ul> </li> </ul> <p>For more information, see the enrich section in the DataPrime Glossary.</p>"},{"location":"newoutput/glossary-dataprime-operators/#extract","title":"extract","text":"<p>Extract data from some string value into a new object. Multiple extraction methods are supported.</p> <pre><code>(e|extract) &lt;expression&gt; into &lt;keypath&gt; using &lt;extraction-type&gt;(&lt;extraction-params&gt;) [datatypes keypath:datatype,keypath:datatype,...]\n</code></pre> <p>Here are the currently supported extraction methods, and their parameters:</p> <ul> <li> <p><code>regexp</code> - Create a new object based on regexp capture-groups</p> </li> <li> <p><code>e</code> - A regular expression with names capture-groups.</p> </li> </ul> <p>Example:</p> <pre><code>extract $d.my_text into $d.my_data using regexp(e=/user (?&lt;user&gt;.*) has logged in/)\n</code></pre> <ul> <li> <p><code>kv</code> - Extract a new object from a string that contains <code>key=value key=value...</code> pairs</p> </li> <li> <p><code>pair_delimiter</code> - The delimiter to expect between pairs. Default is (a space)</p> </li> <li> <p><code>key_delimiter</code> - The delimiter to expect separating between a key and a value. Default is <code>=</code>.</p> </li> </ul> <p>Examples:</p> <pre><code>extract $d.text into $d.my_kvs using kv()\ne $d.text into $d.my_kvs using kv(pair_delimiter=' ',key_delimiter='=')\n</code></pre> <ul> <li> <p><code>jsonobject</code> - Extract a new object from a string contains an encoded json object, potentially attempting to unescape the string before decoding it into a json</p> </li> <li> <p><code>max_unescape_count</code> - Max number of escaping levels to unescape before parsing the json. Default is 1. When set to 1 or more, the engine will detect whether the value contains an escaped JSON string and unescape it until its parsable or max unescape count ie exceeded.</p> </li> </ul> <p>Example:</p> <pre><code>e $d.json_message_as_str into $d.json_message using jsonobject(max_unescape_count=1)\n</code></pre> <p>Additional extraction methods will be supported in the future.</p> <p>It is possible to provide datatype information as part of the extraction, by using the <code>datatypes</code> clause. For example, adding <code>datatypes my_field:number</code> to an extraction would cause the extract <code>my_field</code> keypath to be a number instead of a string. For example:</p> <pre><code>extract $d.my_msg into $d.data using kv() datatypes my_field:number\n</code></pre> <p>Extracted data always goes into a new keypath as an object, allowing further processing of the new keys inside that new object. For example:</p> <pre><code># Assuming a dataset which look like that:\n{ \"msg\": \"query_type=fetch query_id=100 query_results_duration_ms=232\" }\n{ \"msg\": \"query_type=fetch query_id=200 query_results_duration_ms=1001\" }\n\n# And the following DataPrime query:\nsource logs\n  | extract $d.msg into $d.query_data using kv() datatypes query_results_duration_ms:number\n  | filter $d.query_data.query_results_duration_ms &gt; 500\n\n# The results will contain only the second message, in which the duration is larger than 500 ms\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#filter","title":"filter","text":"<p>Filter events, leaving only events for which the condition evaluates to true.</p> <pre><code>(f|filter|where) &lt;condition-expression&gt;\n</code></pre> <p>Examples:</p> <pre><code>f $d.radius &gt; 10\nfilter $m.severity.toUpperCase() == 'INFO'\nfilter $l.applicationname == 'recommender'\nfilter $l.applicationname == 'myapp' &amp;&amp; $d.msg.contains('failure')\n</code></pre> <p>Note: Comparison with null currently works only for scalar values and will always return null on json subtrees.</p>"},{"location":"newoutput/glossary-dataprime-operators/#groupby","title":"groupby","text":"<p>Groups the results of the preceding operators by the specified grouping expressions and calculates aggregate functions for every group created.</p> <pre><code>groupby &lt;grouping_expression&gt; [as &lt;alias&gt;] [, &lt;grouping_expression_2&gt; [as &lt;alias_2&gt;], ...] [calculate]\n  &lt;aggregate_function&gt; [as &lt;result_keypath&gt;]\n  [, &lt;aggregate_function_2&gt; [as &lt;result_keypath_2], ...]\n</code></pre> <p>For example, the following query:</p> <pre><code>groupby $m.severity calculate sum($d.duration)\n</code></pre> <p>Will result in logs of the following form:</p> <pre><code>{ \"severity\": \"Warning\", \"_sum\": 17045 }\n</code></pre> <p>The keypaths for the grouping expressions will always be under <code>$d</code>. Using the <code>as</code> keyword, we can rename the keypath for the grouping expressions and aggregation functions. The following query:</p> <pre><code>groupby $l.applicationname as $d.app calculate sum($d.duration) as $d.sum_duration\n</code></pre> <p>Will result in logs of the following form:</p> <pre><code>{ \"app\": \"web-api\", \"sum_duration\": 17045 }\n</code></pre> <p>Notes:</p> <ul> <li> <p>Supported aggregation functions are listed in \"Aggregation Functions\" section below.</p> </li> <li> <p>When querying with the groupby operator, you can now apply an aggregation function (such as<code>avg</code>, <code>max</code>, <code>sum</code>) to the bucket of results. This feature gives you the power to manipulate an aggregation expression inside the expression itself, allowing you to calculate and manipulate your data simultaneously. Examples of DataPrime expressions in aggregations can be found here.</p> </li> </ul>"},{"location":"newoutput/glossary-dataprime-operators/#limit","title":"limit","text":"<p>Limits the output to the first <code>&lt;event-count&gt;</code> events.</p> <pre><code>limit &lt;event-count&gt;\n</code></pre> <p>Examples</p> <pre><code>limit 100\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#move","title":"move","text":"<p>Move a key (including its child keys, if any) to a new location.</p> <pre><code>(m|move) &lt;source-keypath&gt; to &lt;target-keypath&gt;\n</code></pre> <p>Examples:</p> <pre><code>move $d.my_data.hostname to $d.my_new_data.host\nm $d.kubernetes.labels to $d.my_labels\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#orderby-sortby-order-by-sort-by","title":"orderby / sortby / order by / sort by","text":"<p>Sort the data by ascending/descending order of the expression value. Ordering by multiple expressions is supported.</p> <pre><code>(orderby|sortby|order by|sort by) &lt;expression&gt; [(asc|desc)] , ...\n</code></pre> <p>Examples:</p> <pre><code>orderby $d.myfield.myfield\norderby $d.myfield.myfield:number desc\nsortby $d.myfield desc\n</code></pre> <p>Note: Sorting numeric values can be done by casting expression to the type: e.g.<code>&lt;expression&gt;: number</code>. In some cases, this will be inferred automatically by the engine.</p>"},{"location":"newoutput/glossary-dataprime-operators/#redact","title":"redact","text":"<p>Replace all substrings matching a regexp pattern from some keypath value, effectively hiding the original content.</p> <p>The matching keyword is optional and can be used to increase readability.</p> <pre><code>redact &lt;keypath&gt; [matching] /&lt;regular-expression&gt;/ to '&lt;redacted_str&gt;'\nredact &lt;keypath&gt; [matching] &lt;string&gt; to '&lt;redacted_str&gt;'\n</code></pre> <p>Examples:</p> <pre><code>redact $d.mykey /[0-9]+/ to 'SOME_INTEGER'\nredact $d.mysuperkey.user_id 'root' to 'UNKNOWN_USER'\nredact $d.mysuperkey.user_id matchingn 'root' to 'UNKNOWN_USER'\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#remove","title":"remove","text":"<p>The negation of <code>choose</code>. Remove a keypath from the object.</p> <pre><code>r|remove &lt;keypath1&gt; [ \",\" &lt;keypath2&gt; ]...\n</code></pre> <p>Examples:</p> <pre><code>r $d.mydata.unneeded_key\nremove $d.mysuperkey.service_name, $d.mysuperkey.unneeded_key\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#replace","title":"replace","text":"<p>Replace the value of some key with a new value.</p> <p>If the replacement value changes the datatype of the keypath, the following will happen:</p> <ul> <li> <p><code>skip</code> - The replacement will be ignored</p> </li> <li> <p><code>fail</code> - The query will fail</p> </li> <li> <p><code>overwrite</code> - The new value will overwrite the previous one, changing the datatype of the keypath</p> </li> </ul> <pre><code>replace &lt;keypath&gt; with &lt;expression&gt; [on datatype changed skip/fail/overwrite]\n</code></pre> <p>Examples:</p> <pre><code>replace $d.message with null\nreplace $d.some_superkey.log_length_plus_10 with $d.original_log.length()+10 on datatype changed overwrite\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#roundtime","title":"roundtime","text":"<p>Rounds the time of the event into some time interval, possibly creating a new key for the result.</p> <p>If <code>source-timestamp</code> is not provided, then <code>$m.timestamp</code> is used as the source timestamp. If <code>source-timestamp</code> is provided, it should be of type (or cast to) <code>timestamp</code>.</p> <p>By default, the rounded result is written back to the source keypath <code>[source-timestamp]</code>. If <code>into &lt;target-keypath&gt;</code> is provided, then <code>[source-timestamp]</code> is not modified, and the result is written to a new <code>target-keypath</code>.</p> <p>Supported time intervals are:</p> <ul> <li> <p><code>Xns</code> - X nanoseconds (beware of the <code>source-timestamp</code>'s resolution)</p> </li> <li> <p><code>Xms</code> - X milliseconds</p> </li> <li> <p><code>Xs</code> - X seconds</p> </li> <li> <p><code>Xm</code> - X minutes</p> </li> <li> <p><code>Xh</code> - X hours</p> </li> <li> <p><code>Xd</code> - X days</p> </li> </ul> <p>And any combination of the above from bigger to smaller time unit, e.g. <code>1h30m15s</code>.</p> <pre><code>roundtime [source-timestamp] to &lt;time-interval&gt; [into &lt;target-keypath&gt;]\n</code></pre> <p>Examples:</p> <pre><code>roundtime to 1h into $d.tm\nroundtime $d.timestamp to 1h\nroundtime $d.my_timestamp: timestamp to 60m\nroundtime to 60s into $d.rounded_ts_to_the_minute\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#source","title":"source","text":"<p>Set the data source that your DataPrime query is based on.</p> <pre><code>(source|from) &lt;data_store&gt;\n</code></pre> <p>Where <code>&lt;data_store&gt;</code> can be either:</p> <ul> <li> <p><code>logs</code></p> </li> <li> <p><code>spans</code> (supported only in the API)</p> </li> <li> <p>The name of the custom enrichment. In this case, the command will display the custom enrichment table.</p> </li> </ul> <p>Examples:</p> <pre><code>source logs\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#top","title":"top","text":"<p>No grouping variation</p> <p>Limits the rows returned to a specified number and order the result by a set of expressions.</p> <pre><code>order_direction := \"descending\"/\"ascending\" according to top/bottom\n\ntop &lt;limit&gt; &lt;result_expression1&gt; [as &lt;alias&gt;] [, &lt;result_expression2&gt; [as &lt;alias2&gt;], ...] by &lt;orderby_expression&gt; [as alias&gt;]\n</code></pre> <p>For example, the following query:</p> <pre><code>top 5 $m.severity as $d.log_severity by $d.duration\n</code></pre> <p>Will result in logs of the following form:</p> <pre><code>[\n   { \"log_severity\": \"Warning\", \"duration\": 2000 },\n   { \"log_severity\": \"Debug\", \"duration\":  1000 }\n   ...\n]\n</code></pre> <p>Grouping variation</p> <p>Limits the rows returned to a specified number and group them by a set of aggregation expressions and order them by a set of expressions.</p> <pre><code>order_direction := \"descending\"/\"ascending\" according to top/bottom\n\ntop &lt;limit&gt; &lt;(groupby_expression1|aggregate_function1)&gt; [as &lt;alias&gt;] [, &lt;(groupby_expression2|aggregate_function2)&gt; [as &lt;alias2&gt;], ...] by &lt;(groupby_expression1|aggregate_function1)&gt; [as &lt;alias&gt;]\n</code></pre> <p>For example, the following query:</p> <pre><code>top 10 $m.severity, count() as $d.number_of_severities by avg($d.duration) as $d.avg_duration\n</code></pre> <p>Will result in logs of the following form:</p> <pre><code>[\n   { \"severity\": \"Debug\", \"number_of_severities\":  10, avg_duration: 2000 }\n   { \"severity\": \"Warning\", \"number_of_severities\": 50, avg_duration: 1000 },\n   ...\n]\n</code></pre> <p>Supported aggregation functions are listed in \"Aggregation Functions\" section.</p>"},{"location":"newoutput/glossary-dataprime-operators/#text-search-operators","title":"Text Search Operators","text":""},{"location":"newoutput/glossary-dataprime-operators/#find-text","title":"find / text","text":"<p>Search for the string in a certain keypath.</p> <pre><code>(find|text) &lt;free-text-string&gt; in &lt;keypath&gt;\n\n</code></pre> <p>Examples:</p> <pre><code>find 'host1000' in $d.kubernetes.hostname\ntext 'us-east-1' in $d.msg\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#lucene","title":"lucene","text":"<p>A generic lucene-compatible operator, allowing both free and wild text searches, and more complex search queries.</p> <p>Field names inside the lucene query are relative to\u00a0<code>$d</code>\u00a0(the root level of user-data).</p> <pre><code>lucene &lt;lucene-query-as-a-string&gt;\n\n</code></pre> <p>Examples:</p> <pre><code>lucene 'pod:recommender AND (is_error:true or status_code:404)'\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#wildfind-wildtext","title":"wildfind / wildtext","text":"<p>Search for the string in the entire user data. This can be used when the keypath in which the text resides is unknown.</p> <p>Note: The performance of this operator is worse than when using the\u00a0<code>find</code>/<code>text</code>\u00a0operator. Prefer using those operators when you know the keypath to search for.</p> <pre><code>(wildfind/wildtext) &lt;string&gt;\n\n</code></pre> <p>Examples:</p> <pre><code>wildfind 'my-region'\nwildfind ':9092'\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#expressions","title":"Expressions","text":"<p>DataPrime supports a limited set of javascript constructs that can be used in expressions.</p> <p>The data is exposed using the following top-level fields:</p> <ul> <li> <p><code>$m</code>\u00a0- Event metadata</p> <ul> <li> <p><code>timestamp</code></p> </li> <li> <p><code>severity</code>\u00a0- Possible values are\u00a0<code>V</code>ERBOSE,\u00a0<code>D</code>EBUG,\u00a0<code>I</code>NFO,\u00a0<code>W</code>ARNING,\u00a0<code>E</code>RROR,\u00a0<code>C</code>RITICAL</p> </li> <li> <p><code>priorityclass</code>\u00a0- Possible values are\u00a0<code>high</code>,\u00a0<code>medium</code>,\u00a0<code>low</code></p> </li> <li> <p><code>logid</code></p> </li> </ul> </li> <li> <p><code>$l</code>\u00a0- Event labels</p> <ul> <li> <p><code>applicationname</code></p> </li> <li> <p><code>subsystemname</code></p> </li> <li> <p><code>category</code></p> </li> <li> <p><code>classname</code></p> </li> <li> <p><code>computername</code></p> </li> <li> <p><code>methodname</code></p> </li> <li> <p><code>threadid</code></p> </li> <li> <p><code>ipaddress</code></p> </li> </ul> </li> <li> <p><code>$d</code>\u00a0- The user's data</p> </li> </ul>"},{"location":"newoutput/glossary-dataprime-operators/#field-access","title":"Field Access","text":"<p>Accessing nested data is done by using a keypath, similar to any programming language or json tool. Keys with special characters can be accessed using a map-like syntax, with the key string as the map index, e.g.\u00a0<code>$d.my_superkey['my_field_with_a_special/character']</code>.</p> <p>Examples:</p> <pre><code>$m.timestamp\n$d.my_superkey.myfield\n$d.my_superkey['my_field_with_a_special/character']\n$l.applicationname\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#language-constructs","title":"Language Constructs","text":"<p>All standard language constructs are supported:</p> <ul> <li> <p>Constants</p> </li> <li> <p>Nested field access, as mentioned above</p> </li> <li> <p>Basic math operations between numbers, including modulo (%)</p> </li> <li> <p>Boolean operations\u00a0<code>&amp;&amp;</code>,\u00a0<code>||</code>,\u00a0<code>!</code></p> </li> <li> <p>Comparisons</p> </li> <li> <p>String concatenations through\u00a0<code>concat</code>\u00a0(string interpolation will be supported soon)</p> </li> <li> <p>casting - A simple notation for casting data types: e.g.\u00a0<code>$d.temperature:number</code>. Type inference is automatically applied when possible. We'll support full type-inference soon, reducing the need for casting.</p> </li> </ul>"},{"location":"newoutput/glossary-dataprime-operators/#text-search","title":"Text Search","text":"<p>Boolean expressions for text search:</p> <ul> <li> <p><code>$d.field ~ 'text phrase'</code>\u00a0- case-insensitive search for a text phrase in a specific field.</p> </li> <li> <p><code>$d ~~ 'text phrase'</code>\u00a0- case-insensitive search for a text phrase in\u00a0<code>$d</code>.</p> </li> </ul>"},{"location":"newoutput/glossary-dataprime-operators/#scalar-functions","title":"Scalar Functions","text":"<p>Various functions can be used to transform values. All functions can be called as methods as well, e.g.\u00a0<code>$d.msg.contains('x')</code>\u00a0is equivalent to\u00a0<code>contains($d.msg,'x')</code>.</p>"},{"location":"newoutput/glossary-dataprime-operators/#string-functions","title":"String Functions","text":""},{"location":"newoutput/glossary-dataprime-operators/#chr","title":"chr","text":"<p><code>chr(number: number): string</code></p> <p>Returns the Unicode code point\u00a0<code>number</code>\u00a0as a single character string.</p>"},{"location":"newoutput/glossary-dataprime-operators/#codepoint","title":"codepoint","text":"<p><code>codepoint(string: string): number</code></p> <p>Returns the Unicode code point of the only character of\u00a0<code>string</code>.</p>"},{"location":"newoutput/glossary-dataprime-operators/#concat","title":"concat","text":"<p><code>concat(value: string, ...values: string): string</code></p> <p>Concatenates multiple strings into one.</p>"},{"location":"newoutput/glossary-dataprime-operators/#contains","title":"contains","text":"<p><code>contains(string: string, substring: string): bool</code></p> <p>Returns\u00a0<code>true</code>\u00a0if\u00a0<code>substring</code>\u00a0is contained in\u00a0<code>string</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#endswith","title":"endsWith","text":"<p><code>endsWith(string: string, suffix: string): bool</code></p> <p>Returns\u00a0<code>true</code>\u00a0if\u00a0<code>string</code>\u00a0ends with\u00a0<code>suffix</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#indexof","title":"indexOf","text":"<p><code>indexOf(string: string, substring: string): number</code></p> <p>Returns the position of\u00a0<code>substring</code>\u00a0in\u00a0<code>string</code>, or\u00a0<code>-1</code>\u00a0if not found.</p>"},{"location":"newoutput/glossary-dataprime-operators/#length","title":"length","text":"<p><code>length(value: string): number</code></p> <p>Returns the length of\u00a0<code>value</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#ltrim","title":"ltrim","text":"<p><code>ltrim(value: string): string</code></p> <p>Removes whitespace to the left of the string\u00a0<code>value</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#matches","title":"matches","text":"<p><code>matches(string: string, regexp: regexp): bool</code></p> <p>Evaluates the regular expression pattern and determines if it is contained within string.</p>"},{"location":"newoutput/glossary-dataprime-operators/#pad","title":"pad","text":"<p>Alias for\u00a0<code>padLeft</code></p> <p><code>pad(value: string, charCount: number, fillWith: string): string</code></p> <p>Left pads string to charCount. If\u00a0<code>size &lt; fillWith.length()</code>\u00a0of string, result is truncated. See\u00a0padLeft\u00a0for more details.</p>"},{"location":"newoutput/glossary-dataprime-operators/#padleft","title":"padLeft","text":"<p><code>padLeft(value: string, charCount: number, fillWith: string): string</code></p> <p>Left pads string to charCount. If\u00a0<code>size &lt; fillWith.length()</code>\u00a0of string, result is truncated.</p>"},{"location":"newoutput/glossary-dataprime-operators/#padright","title":"padRight","text":"<p><code>padRight(value: string, charCount: number, fillWith: string): string</code></p> <p>Right pads string to charCount. If\u00a0<code>size &lt; fillWith.length()</code>\u00a0of string, result is truncated.</p>"},{"location":"newoutput/glossary-dataprime-operators/#regexpsplitparts","title":"regexpSplitParts","text":"<p><code>regexpSplitParts(string: string, delimiter: regexp, index: number): string</code></p> <p>Splits string on regexp-delimiter, returns the field at index. Indexes start with 1.</p>"},{"location":"newoutput/glossary-dataprime-operators/#rtrim","title":"rtrim","text":"<p><code>rtrim(value: string): string</code></p> <p>Removes whitespace to the right of the string\u00a0<code>value</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#splitparts","title":"splitParts","text":"<p><code>splitParts(string: string, delimiter: string, index: number): string</code></p> <p>Splits string on delimiter, returns the field at index. Indexes start with 1.</p>"},{"location":"newoutput/glossary-dataprime-operators/#startswith","title":"startsWith","text":"<p><code>startsWith(string: string, prefix: string): bool</code></p> <p>Returns\u00a0<code>true</code>\u00a0if\u00a0<code>string</code>\u00a0starts with\u00a0<code>prefix</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#substr","title":"substr","text":"<p><code>substr(value: string, from: number, length: number?): string</code></p> <p>Returns the substring in\u00a0<code>value</code>, from position\u00a0<code>from</code>\u00a0and up to length\u00a0<code>length</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#tolowercase","title":"toLowerCase","text":"<p><code>toLowerCase(value: string): string</code></p> <p>Converts\u00a0<code>value</code>\u00a0to lowercase</p>"},{"location":"newoutput/glossary-dataprime-operators/#touppercase","title":"toUpperCase","text":"<p><code>toUpperCase(value: string): string</code></p> <p>Converts\u00a0<code>value</code>\u00a0to uppercase</p>"},{"location":"newoutput/glossary-dataprime-operators/#trim","title":"trim","text":"<p><code>trim(value: string): string</code></p> <p>Removes whitespace from the edges of a string\u00a0<code>value</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#ip-functions","title":"IP Functions","text":""},{"location":"newoutput/glossary-dataprime-operators/#ipinsubnet","title":"ipInSubnet","text":"<p><code>ipInSubnet(ip: string, ipPrefix: string): bool</code></p> <p>Returns true if ip is in the subnet of ipPrefix.</p>"},{"location":"newoutput/glossary-dataprime-operators/#ipprefix","title":"ipPrefix","text":"<p><code>ipPrefix(ip: string, subnetSize: number): string</code></p> <p>Returns the IP prefix of a given ip_address with subnetSize bits (e.g.:\u00a0<code>192.128.0.0/9</code>).</p>"},{"location":"newoutput/glossary-dataprime-operators/#string-iinterpolation","title":"String iInterpolation","text":"<ul> <li> <p><code>`this is an interpolated {$d.some_keypath} string`</code>\u00a0-\u00a0<code>{$d.some_keypath}</code>\u00a0will be replaced with the evaluated expression that is wrapped by the brackets</p> </li> <li> <p><code>`this is how you escape \\{ and \\} and \\``</code>\u00a0- Backward slash (<code>\\</code>) is used to escape characters like\u00a0<code>{</code>,\u00a0<code>}</code>\u00a0that are used for keypaths.</p> </li> </ul>"},{"location":"newoutput/glossary-dataprime-operators/#uuid-functions","title":"UUID Functions","text":""},{"location":"newoutput/glossary-dataprime-operators/#isuuid","title":"isUuid","text":"<p><code>isUuid(uuid: string): bool</code></p> <p>Returns true if uuid is valid.</p>"},{"location":"newoutput/glossary-dataprime-operators/#randomuuid","title":"randomUuid","text":"<p><code>randomUuid(): string</code></p> <p>Returns a random UUIDv4.</p>"},{"location":"newoutput/glossary-dataprime-operators/#uuid","title":"uuid","text":"<p>Deprecated:\u00a0use\u00a0<code>randomUuid</code>\u00a0instead</p> <p><code>uuid(): string</code></p> <p>Returns a random UUIDv4. See\u00a0randomUuid\u00a0for more details.</p>"},{"location":"newoutput/glossary-dataprime-operators/#general-functions","title":"General Functions","text":""},{"location":"newoutput/glossary-dataprime-operators/#firstnonnull","title":"firstNonNull","text":"<p><code>firstNonNull(value: any, ...values: any): any</code></p> <p>Returns the first non-null value from the parameters. Works only on scalars for now.</p>"},{"location":"newoutput/glossary-dataprime-operators/#if","title":"if","text":"<p><code>if(condition: bool, then: any, else: any?): any</code></p> <p>return either the\u00a0<code>then</code>\u00a0or\u00a0<code>else</code>\u00a0according to the result of\u00a0<code>condition</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#in","title":"in","text":"<p><code>in(comparand: any, value: any, ...values: any): bool</code></p> <p>Tests if the\u00a0<code>comparand</code>\u00a0is equal to any of the values in a set\u00a0<code>v1 ... vN</code>.</p>"},{"location":"newoutput/glossary-dataprime-operators/#recordlocation","title":"recordLocation","text":"<p><code>recordLocation(): string</code></p> <p>Returns the location of the record (e.g.: s3 URL)</p>"},{"location":"newoutput/glossary-dataprime-operators/#number-functions","title":"Number Functions","text":""},{"location":"newoutput/glossary-dataprime-operators/#abs","title":"abs","text":"<p><code>abs(number: number): number</code></p> <p>Returns the absolute value of\u00a0<code>number</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#ceil","title":"ceil","text":"<p><code>ceil(number: number): number</code></p> <p>Rounds the value up to the nearest integer</p>"},{"location":"newoutput/glossary-dataprime-operators/#e","title":"e","text":"<p><code>e(): number</code></p> <p>Returns the constant Euler\u2019s number.</p>"},{"location":"newoutput/glossary-dataprime-operators/#floor","title":"floor","text":"<p><code>floor(number: number): number</code></p> <p>Rounds the value down to the nearest integer</p>"},{"location":"newoutput/glossary-dataprime-operators/#frombase","title":"fromBase","text":"<p><code>fromBase(string: string, radix: number): number</code></p> <p>Returns the value of\u00a0<code>string</code>\u00a0interpreted as a base-radix number.</p>"},{"location":"newoutput/glossary-dataprime-operators/#ln","title":"ln","text":"<p><code>ln(number: number): number</code></p> <p>Returns the natural log of\u00a0<code>number</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#log","title":"log","text":"<p><code>log(base: number, number: number): number</code></p> <p>Returns the log of\u00a0<code>number</code>\u00a0in base\u00a0<code>base</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#log2","title":"log2","text":"<p><code>log2(number: number): number</code></p> <p>Returns the log of\u00a0<code>number</code>\u00a0in base 2. Equivalent to\u00a0<code>log(2, number)</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#max","title":"max","text":"<p><code>max(value: number, ...values: number): number</code></p> <p>Returns the largest number of all the numbers passed to the function</p>"},{"location":"newoutput/glossary-dataprime-operators/#min","title":"min","text":"<p><code>min(value: number, ...values: number): number</code></p> <p>Returns the smallest number of all the numbers passed to the function</p>"},{"location":"newoutput/glossary-dataprime-operators/#mod","title":"mod","text":"<p><code>mod(number: number, divisor: number): number</code></p> <p>Returns the modulus (remainder) of\u00a0<code>number</code>\u00a0divided by\u00a0<code>divisor</code>.</p>"},{"location":"newoutput/glossary-dataprime-operators/#pi","title":"pi","text":"<p><code>pi(): number</code></p> <p>Returns the constant Pi.</p>"},{"location":"newoutput/glossary-dataprime-operators/#power","title":"power","text":"<p><code>power(number: number, exponent: number): number</code></p> <p>Returns\u00a0<code>number^exponent</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#random","title":"random","text":"<p><code>random(): number</code></p> <p>Returns a pseudo-random value in the range\u00a0<code>0.0 &lt;= x &lt; 1.0</code>.</p>"},{"location":"newoutput/glossary-dataprime-operators/#randomint","title":"randomInt","text":"<p><code>randomInt(upperBound: number): number</code></p> <p>Returns a pseudo-random integer number between 0 and n (exclusive)</p>"},{"location":"newoutput/glossary-dataprime-operators/#round","title":"round","text":"<p><code>round(number: number, digits: number?): number</code></p> <p>Round\u00a0<code>number</code>\u00a0to\u00a0<code>digits</code>\u00a0decimal places</p>"},{"location":"newoutput/glossary-dataprime-operators/#sqrt","title":"sqrt","text":"<p><code>sqrt(number: number): number</code></p> <p>Returns square root of a number.</p>"},{"location":"newoutput/glossary-dataprime-operators/#tobase","title":"toBase","text":"<p><code>toBase(number: number, radix: number): string</code></p> <p>Returns the base-radix representation of\u00a0<code>number</code>.</p>"},{"location":"newoutput/glossary-dataprime-operators/#url-functions","title":"URL Functions","text":""},{"location":"newoutput/glossary-dataprime-operators/#urldecode","title":"urlDecode","text":"<p><code>urlDecode(string: string): string</code></p> <p>Unescapes the URL encoded in string.</p>"},{"location":"newoutput/glossary-dataprime-operators/#urlencode","title":"urlEncode","text":"<p><code>urlEncode(string: string): string</code></p> <p>Escapes string by encoding it so that it can be safely included in URL.</p>"},{"location":"newoutput/glossary-dataprime-operators/#date-time-functions","title":"Date / Time Functions","text":"<p>Functions for processing timestamps, intervals and other time-related constructs.</p>"},{"location":"newoutput/glossary-dataprime-operators/#time-units","title":"Time Units","text":"<p>Many date/time functions accept a time unit argument to tweak their behaviour. Dataprime supports time units from nanoseconds to days. They are represented as literal strings of the time unit name in either long or short notation:</p> <ul> <li> <p>long notation:\u00a0<code>'day'</code>,\u00a0<code>'hour'</code>,\u00a0<code>'minute'</code>,\u00a0<code>'second'</code>,\u00a0<code>'milli'</code>,\u00a0<code>'micro'</code>,\u00a0<code>'nano'</code></p> </li> <li> <p>short notation:\u00a0<code>'d'</code>,\u00a0<code>'h'</code>,\u00a0<code>'m'</code>,\u00a0<code>'s'</code>,\u00a0<code>'ms'</code>,\u00a0<code>'us'</code>,\u00a0<code>'ns'</code></p> </li> </ul>"},{"location":"newoutput/glossary-dataprime-operators/#time-zones","title":"Time Zones","text":"<p>Dataprime timestamps are always stored in the UTC time zone, but some date/time functions accept a time zone argument to tweak their behaviour. Time zone arguments are strings that specify a time zone offset, shorthand or identifier:</p> <ul> <li> <p>time zone offset in hours (e.g.\u00a0<code>'+01'</code>\u00a0or\u00a0<code>'-02'</code>)</p> </li> <li> <p>time zone offset in hours and minutes (e.g.\u00a0<code>'+0130'</code>\u00a0or\u00a0<code>'-0230'</code>)</p> </li> <li> <p>time zone offset in hours and minutes with separator (e.g.\u00a0<code>'+01:30'</code>\u00a0or\u00a0<code>'-02:30'</code>)</p> </li> <li> <p>time zone shorthand (e.g.\u00a0<code>'UTC'</code>,\u00a0<code>'GMT'</code>,\u00a0<code>'EST'</code>, etc.)</p> </li> <li> <p>time zone identifier (e.g.\u00a0<code>'Asia/Yerevan'</code>,\u00a0<code>'Europe/Zurich'</code>,\u00a0<code>'America/Winnipeg'</code>, etc.)</p> </li> </ul>"},{"location":"newoutput/glossary-dataprime-operators/#addinterval","title":"addInterval","text":"<p><code>addInterval(left: interval, right: interval): interval</code></p> <p>Adds two intervals together. Works also with negative intervals. Equivalent to\u00a0<code>left + right</code>.</p>"},{"location":"newoutput/glossary-dataprime-operators/#addtime","title":"addTime","text":"<p><code>addTime(t: timestamp, i: interval): timestamp</code></p> <p>Adds an interval to a timestamp. Works also with negative intervals. Equivalent to\u00a0<code>t + i</code>.</p>"},{"location":"newoutput/glossary-dataprime-operators/#difftime","title":"diffTime","text":"<p><code>diffTime(to: timestamp, from: timestamp): interval</code></p> <p>Calculates the duration between two timestamps. Positive if\u00a0<code>to &gt; from</code>, negative if\u00a0<code>to &lt; from</code>. Equivalent to\u00a0<code>to - from</code>.</p>"},{"location":"newoutput/glossary-dataprime-operators/#extracttime","title":"extractTime","text":"<p><code>extractTime(timestamp: timestamp, unit: dateunit | timeunit, tz: string?): number</code></p> <p>Extracts either a date or time unit from a\u00a0<code>timestamp</code>. Returns a floating point number for time units smaller than a\u00a0<code>'minute'</code>, otherwise an integer. Date units such as\u00a0<code>'month'</code>\u00a0or\u00a0<code>'week'</code>\u00a0start from 1 (not from 0).</p> <p>Function parameters:</p> <ul> <li> <p><code>timestamp</code>\u00a0(required) - the timestamp to extract from.</p> </li> <li> <p><code>unit</code>\u00a0(required) - the date or time unit to extract. Must be a string literal and one of:</p> <ul> <li> <p>any\u00a0time unit\u00a0in either long or short notation</p> </li> <li> <p>a date unit in long notation:\u00a0<code>'year'</code>,\u00a0<code>'month'</code>,\u00a0<code>'week'</code>,\u00a0<code>'day_of_year'</code>,\u00a0<code>'day_of_week'</code></p> </li> <li> <p>a date unit in short notation:\u00a0<code>'Y'</code>,\u00a0<code>'M'</code>,\u00a0<code>'W'</code>,\u00a0<code>'doy'</code>,\u00a0<code>'dow'</code></p> </li> </ul> </li> <li> <p><code>tz</code>\u00a0(optional) - a\u00a0time zone\u00a0to convert the timestamp before extracting.</p> </li> </ul> <p>Example 1: extract the hour in Tokyo</p> <pre><code>limit 1 | choose $m.timestamp.extractTime('h', 'Asia/Tokyo') as h # Result 1: 11pm { \"h\": 23 }\n</code></pre> <p>Example 2: extract the number of seconds</p> <pre><code>limit 1 | choose $m.timestamp.extractTime('second') as s # Result 2: 38.35 seconds { \"s\": 38.3510265 }\n</code></pre> <p>Example 3: extract the timestamp's month</p> <pre><code>limit 1 | choose $m.timestamp.extractTime('month') as m # Result 3: August { \"m\": 8 } \n</code></pre> <p>Example 4: extract the day of the week</p> <pre><code>limit 1 | choose $m.timestamp.extractTime('dow') as d # Result 4: Tuesday { \"d\": 2 }\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#formatinterval","title":"formatInterval","text":"<p><code>formatInterval(interval: interval, scale: timeunit?): string</code></p> <p>Formats\u00a0<code>interval</code>\u00a0to a string with an optional time unit\u00a0<code>scale</code>.</p> <p>Function parameters:</p> <ul> <li> <p><code>interval</code>\u00a0(required) - the interval to format.</p> </li> <li> <p><code>scale</code>\u00a0(optional) - the largest\u00a0time unit\u00a0of the interval to show. Defaults to\u00a0<code>nano</code>.</p> </li> </ul> <p>Example:</p> <pre><code>limit 3 | choose formatInterval(now() - $m.timestamp, 's') as i # Results: { \"i\": \"122s261ms466us27ns\" } { \"i\": \"122s359ms197us227ns\" } { \"i\": \"122s359ms197us227ns\" }\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#formattimestamp","title":"formatTimestamp","text":"<p><code>formatTimestamp(timestamp: timestamp, format: string?, tz: string?): string</code></p> <p>Formats a\u00a0<code>timestamp</code>\u00a0to a string with an optional format specification and destination time zone.</p> <p>Function parameters:</p> <ul> <li> <p><code>timestamp</code>\u00a0(required) - the timestamp to format.</p> </li> <li> <p><code>format</code>\u00a0(optional) - a date/time format specification for parsing timestamps. Defaults to\u00a0<code>'iso8601'</code>. The format can be any string with embedded date/time formatters, or one of several shorthands. Here are a few samples:</p> <ul> <li> <p><code>'%Y-%m-%d'</code>\u00a0- print the date only, e.g.\u00a0<code>'2023-04-05'</code></p> </li> <li> <p><code>'%H:%M:%S'</code>\u00a0- print the time only, e.g.\u00a0<code>'16:07:33'</code></p> </li> <li> <p><code>'%F %H:%M:%S'</code>\u00a0- print both date and time, e.g.\u00a0<code>'2023-04-05 16:07:33'</code></p> </li> <li> <p><code>'iso8601'</code>\u00a0- print a timestamp in ISO 8601 format, e.g.\u00a0<code>'2023-04-05T16:07:33.123Z'</code></p> </li> <li> <p><code>'timestamp_milli'</code>\u00a0- print a timestamp in milliseconds (13 digits), e.g.\u00a0<code>'1680710853123'</code></p> </li> </ul> </li> <li> <p><code>tz</code>\u00a0(optional) - the destination\u00a0time zone\u00a0to convert the timestamp before formatting.</p> </li> </ul> <p>Example 1: print a timestamp with default format and +5h offset</p> <pre><code>limit 1 | choose $m.timestamp.formatTimestamp(tz='+05') as ts # Result 1: { \"ts\": \"2023-08-29T19:08:37.405937400+0500\" }\n</code></pre> <p>Example 2: print only the year and month</p> <pre><code>limit 1 | choose $m.timestamp.formatTimestamp('%Y-%m') as ym # Result 2: { \"ym\": \"2023-08\" } \n</code></pre> <p>Example 3: print only the hours and minutes</p> <pre><code>limit 1 | choose $m.timestamp.formatTimestamp('%H:%M') as hm # Result 3: { \"hm\": \"14:11\" }\n</code></pre> <p>Example 4: print a timestamp in milliseconds (13 digits)</p> <pre><code>limit 1 | choose $m.timestamp.formatTimestamp('timestamp_milli') as ms # Result 4: { \"ms\": \"1693318678696\" }\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#fromunixtime","title":"fromUnixTime","text":"<p><code>fromUnixTime(unixTime: number, timeUnit: timeunit?): timestamp</code></p> <p>Converts a number of a specific time units since the UNIX epoch to a timestamp (in UTC). The UNIX epoch starts on January 1, 1970 - earlier timestamps are represented by negative numbers.</p> <p>Function parameters:</p> <ul> <li> <p><code>unixTime</code>\u00a0(required) - the amount of time units to convert. Can be either positive or negative and will be rounded down to an integer.</p> </li> <li> <p><code>timeUnit</code>\u00a0(optional) - the\u00a0time units\u00a0to convert. Defaults to\u00a0<code>'milli'</code>.</p> </li> </ul> <p>Example:</p> <pre><code>limit 1 | choose fromUnixTime(1658958157515, 'ms') as ts # Result: { \"ts\": 1658958157515000000 }\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#multiplyinterval","title":"multiplyInterval","text":"<p><code>multiplyInterval(i: interval, factor: number): interval</code></p> <p>Multiplies an interval by a numeric\u00a0<code>factor</code>. Works both with integer and fractional numbers. Equivalent to\u00a0<code>i * factor</code></p>"},{"location":"newoutput/glossary-dataprime-operators/#now","title":"now","text":"<p><code>now(): timestamp</code></p> <p>Returns the current time at query execution time. Stable across all rows and within the entire query, even when used multiple times. Nanosecond resolution if the runtime supports it, otherwise millisecond resolution.</p> <p>Example:</p> <pre><code>limit 3 | choose now() as now, now() - $m.timestamp as since # Results: { \"now\": 1693312549105874700, \"since\": \"14m954ms329us764ns\" } { \"now\": 1693312549105874700, \"since\": \"14m954ms329us764ns\" } { \"now\": 1693312549105874700, \"since\": \"14m960ms519us564ns\" }\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#parseinterval","title":"parseInterval","text":"<p><code>parseInterval(string: string): interval</code></p> <p>Parses an interval from a\u00a0<code>string</code>\u00a0with format\u00a0<code>NdNhNmNsNmsNusNns</code>\u00a0where\u00a0<code>N</code>\u00a0is the amount of each time unit. Returns\u00a0<code>null</code>\u00a0when the input does not match the expected format:</p> <ul> <li> <p>It consists of time unit components - a non-negative integer followed by the short time unit name. Supported time units are:\u00a0<code>'d'</code>,\u00a0<code>'h'</code>,\u00a0<code>'m'</code>,\u00a0<code>'s'</code>,\u00a0<code>'ms'</code>,\u00a0<code>'us'</code>,\u00a0<code>'ns'</code>.</p> </li> <li> <p>There must be at least one time unit component.</p> </li> <li> <p>The same time unit cannot appear more than once.</p> </li> <li> <p>Components must be decreasing in time unit order - from days to nanoseconds.</p> </li> <li> <p>It can start with\u00a0<code>-</code>\u00a0to represent negative intervals.</p> </li> </ul> <p>Example 1: parse a zero interval</p> <pre><code>limit 1 | choose '0s'.parseInterval() as i # Result 1: { \"i\": \"0ns\" }\n</code></pre> <p>Example 2: parse a positive interval</p> <pre><code>limit 1 | choose '1d48h0m'.parseInterval() as i # Result 2: { \"i\": \"3d\" } \n</code></pre> <p>Example 3: parse a negative interval</p> <pre><code>limit 1 | choose '-5m45s'.parseInterval() as i # Result 3: { \"i\": \"-5m45s\" }\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#parsetimestamp","title":"parseTimestamp","text":"<p><code>parseTimestamp(string: string, format: string?, tz: string?): timestamp</code></p> <p>Parses a timestamp from\u00a0<code>string</code>\u00a0with an optional format specification and time zone override. Returns\u00a0<code>null</code>\u00a0when the input does not match the expected format.</p> <p>Function parameters:</p> <ul> <li> <p><code>string</code>\u00a0(required) - the input from which the timestamp will be extracted.</p> </li> <li> <p><code>format</code>\u00a0(optional) - a date/time format specification for parsing timestamps. Defaults to\u00a0<code>'auto'</code>. The format can be any string with embedded date/time extractors, one of several shorthands, or a cascade of formats to be attempted in sequence. Here are a few samples:</p> <ul> <li> <p><code>'%Y-%m-%d'</code>\u00a0- parse date only, e.g.\u00a0<code>'2023-04-05'</code></p> </li> <li> <p><code>'%F %H:%M:%S'</code>\u00a0- parse date and time, e.g.\u00a0<code>'2023-04-05 16:07:33'</code></p> </li> <li> <p><code>'iso8601'</code>\u00a0- parse a timestamp in ISO 8601 format, e.g.\u00a0<code>'2023-04-05T16:07:33.123Z'</code></p> </li> <li> <p><code>'timestamp_milli'</code>\u00a0- parse a timestamp in milliseconds (13 digits), e.g.\u00a0<code>'1680710853123'</code></p> </li> <li> <p><code>'%m/%d/%Y|timestamp_second'</code>\u00a0- parse either a date or a timestamp in seconds, in that order</p> </li> </ul> </li> <li> <p><code>tz</code>\u00a0(optional) - a\u00a0time zone\u00a0override to convert the timestamp while parsing. This parameter will override any time zone present in the input. A time zone can be extracted from the string by using an appropriate format and omitting this parameter.</p> </li> </ul> <p>Example 1: parse a date with the default format</p> <pre><code>limit 1 | choose '2023-04-05'.parseTimestamp() as ts # Result 1: { \"ts\": 1680652800000000000 }\n</code></pre> <p>Example 2: parse a date in US format</p> <pre><code>limit 1 | choose '04/05/23'.parseTimestamp('%D') as ts # Result 2: { \"ts\": 1680652800000000000 } \n</code></pre> <p>Example 3: parse date and time with units</p> <pre><code>limit 1 | choose '2023-04-05 16h07m'.parseTimestamp('%F %Hh%Mm') as ts # Result 3: { \"ts\": 1680710820000000000 } \n</code></pre> <p>Example 4: parse a timestamp in seconds (10 digits)</p> <pre><code>limit 1 | choose '1680710853'.parseTimestamp('timestamp_second') as ts # Result 4: { \"ts\": 1680710853000000000 }\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#parsetotimestamp","title":"parseToTimestamp","text":"<p>Deprecated:\u00a0use\u00a0<code>parseTimestamp</code>\u00a0instead</p> <p><code>parseToTimestamp(string: string, format: string?, tz: string?): timestamp</code></p> <p>Parses a timestamp from\u00a0<code>string</code>\u00a0with an optional format specification and time zone override. See\u00a0parseTimestamp\u00a0for more details.</p>"},{"location":"newoutput/glossary-dataprime-operators/#roundinterval","title":"roundInterval","text":"<p><code>roundInterval(interval: interval, scale: timeunit): interval</code></p> <p>Rounds an interval to a time unit\u00a0<code>scale</code>. Smaller time units will be zeroed out.</p> <p>Function parameters:</p> <ul> <li> <p><code>interval</code>\u00a0(required) - the interval to round.</p> </li> <li> <p><code>scale</code>\u00a0(required) - the largest\u00a0time unit\u00a0of the interval to keep.</p> </li> </ul> <p>Example:</p> <pre><code>limit 1 | choose 2h5m45s.roundInterval('m') as i # Result: { \"i\": \"2h5m\" }\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#roundtime_1","title":"roundTime","text":"<p><code>roundTime(date: timestamp, interval: interval): timestamp</code></p> <p>Rounds a timestamp to the given interval. Useful for bucketing, e.g. rounding to\u00a0<code>1h</code>\u00a0for hourly buckets. Equivalent to\u00a0<code>date / interval</code>.</p> <p>Example:</p> <pre><code>groupby $m.timestamp.roundTime(1h) as bucket count() as n # Results: { \"bucket\": \"29/08/2023 15:00:00.000 pm\", \"n\": 40653715 } { \"bucket\": \"29/08/2023 14:00:00.000 pm\", \"n\": 1779386 }\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#subtractinterval","title":"subtractInterval","text":"<p><code>subtractInterval(left: interval, right: interval): interval</code></p> <p>Subtracts one interval from another. Equivalent to\u00a0<code>addInterval(left, -right)</code>\u00a0and\u00a0<code>left - right</code>.</p>"},{"location":"newoutput/glossary-dataprime-operators/#subtracttime","title":"subtractTime","text":"<p><code>subtractTime(t: timestamp, i: interval): timestamp</code></p> <p>Subtracts an interval from a timestamp. Equivalent to\u00a0<code>addTime(t, -i)</code>\u00a0and\u00a0<code>t - i</code>.</p>"},{"location":"newoutput/glossary-dataprime-operators/#timeround","title":"timeRound","text":"<p>Deprecated:\u00a0use\u00a0<code>roundTime</code>\u00a0instead</p> <p><code>timeRound(date: timestamp, interval: interval): timestamp</code></p> <p>Rounds a timestamp to the given interval. See\u00a0roundTime\u00a0for more details.</p>"},{"location":"newoutput/glossary-dataprime-operators/#tointerval","title":"toInterval","text":"<p><code>toInterval(number: number, timeUnit: timeunit?): interval</code></p> <p>Converts a\u00a0<code>number</code>\u00a0of specific time units to an interval. Works with both integer / floating point and positive / negative numbers.</p> <p>Function parameters:</p> <ul> <li> <p><code>number</code>\u00a0(required) - the amount of time units to convert.</p> </li> <li> <p><code>timeUnit</code>\u00a0(optional) - Time units\u00a0to convert. Defaults to\u00a0<code>nano</code>.</p> </li> </ul> <p>Example 1: convert a floating point number</p> <pre><code>limit 1 | choose 2.5.toInterval('h') as i # Result 1: { \"i\": \"2h30m\" } # Example 2: convert an integer number limit 1 | choose -9000.toInterval() as i # Result 2: { \"i\": \"-9us\" }\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#toiso8601datetime","title":"toIso8601DateTime","text":"<p>Deprecated</p> <p><code>toIso8601DateTime(timestamp: timestamp): string</code></p> <p>Alias to\u00a0<code>formatTimestamp(timestamp, 'iso8601')</code>.</p> <p>Formats\u00a0<code>timestamp</code>\u00a0to an ISO 8601 string with nanosecond output precision.</p> <p>Example:</p> <pre><code>limit 1 | choose $m.timestamp.toIso8601DateTime() as ts # Result: { \"ts\": \"2023-08-11T07:29:17.634Z\" }\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#tounixtime","title":"toUnixTime","text":"<p><code>toUnixTime(timestamp: timestamp, timeUnit: timeunit?): number</code></p> <p>Converts\u00a0<code>timestamp</code>\u00a0to a number of specific time units since the UNIX epoch (in UTC). The UNIX epoch starts on January 1, 1970 - earlier timestamps are represented by negative numbers.</p> <p>Function parameters:</p> <ul> <li> <p><code>timestamp</code>\u00a0(required) - the timestamp to convert.</p> </li> <li> <p><code>timeUnit</code>\u00a0(optional) - the\u00a0time units\u00a0to convert to. Defaults to\u00a0<code>'milli'</code>.</p> </li> </ul> <p>Example:</p> <pre><code>limit 1 | choose $m.timestamp.toUnixTime('hour') as hr # Result: { \"hr\": 470363 }\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#encoding-decoding-functions","title":"Encoding / Decoding Functions","text":""},{"location":"newoutput/glossary-dataprime-operators/#decodebase64","title":"decodeBase64","text":"<p><code>decodeBase64(value: string): string</code></p> <p>Decode a base-64 encoded string</p>"},{"location":"newoutput/glossary-dataprime-operators/#encodebase64","title":"encodeBase64","text":"<p><code>encodeBase64(value: string): string</code></p> <p>Encode a string into base-64</p>"},{"location":"newoutput/glossary-dataprime-operators/#case-expressions","title":"Case Expressions","text":"<p>Case expressions are special constructs in the language that allow choosing between multiple options in an easy manner and in a readable way. They can be wherever an expression is expected.</p>"},{"location":"newoutput/glossary-dataprime-operators/#case","title":"case","text":"<p>Choose between multiple values based on several generic conditions. Resort to a\u00a0<code>default-value</code>\u00a0if no condition is met.</p> <pre><code>case {\n  condition1 -&gt; value1,\n  condition2 -&gt; value2,\n  ...\n  conditionN -&gt; valueN,\n  _          -&gt; &lt;default-value&gt;\n}\n\n</code></pre> <p>Example:</p> <pre><code>case {\n  $d.status_code == 200 -&gt; 'success',\n  $d.status_code == 201 -&gt; 'created',\n  $d.status_code == 404 -&gt; 'not-found',\n  _ -&gt; 'other'\n}\n\n# Here's the same example inside the context of a query. A new field is created with the `case` result,\n# and then a filter will be applied, leaving only non-successful responses.\n\nsource logs | ... | create $d.http_response_outcome from case {\n  $d.status_code == 200 -&gt; 'success',\n  $d.status_code == 201 -&gt; 'created',\n  $d.status_code == 404 -&gt; 'not-found',\n  _                     -&gt; 'other'\n} | filter $d.http_response_outcome != 'success'\n\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#case_contains","title":"case_contains","text":"<p>A shorthand for\u00a0<code>case</code>\u00a0which allowing checking if a string\u00a0<code>s</code>\u00a0contains one of several substrings without repeating the expression leading to\u00a0<code>s</code>. The chosen value is the first which matches\u00a0<code>s.contains(substring)</code>.</p> <pre><code>case_contains {\n  s: string,\n  substring1 -&gt; result1,\n  substring2 -&gt; result2,\n  ...\n  substring3 -&gt; resultN\n}\n\n</code></pre> <p>Example:</p> <pre><code>case_contains {\n  $l.subsystemname,\n  '-prod-' -&gt; 'production',\n  '-dev-'  -&gt; 'development',\n  '-stg-'  -&gt; 'staging',\n  _        -&gt; 'test'\n}\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#case_equals","title":"case_equals","text":"<p>A shorthand for\u00a0<code>case</code>\u00a0which allowing comparing some expression\u00a0<code>e</code>\u00a0to several results without repeating the expression. The chosen value is the first which matches\u00a0<code>s == value</code></p> <pre><code>case_equals {\n  e: any,\n  value1 -&gt; result1,\n  value2 -&gt; result2,\n  ...\n  valueN -&gt; resultN\n}\n\n</code></pre> <p>Example:</p> <pre><code>case_equals {\n  $m.severity,\n  'info'   -&gt; true,\n  'warning -&gt; true,\n  _        -&gt; false\n}\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#case_greaterthan","title":"case_greaterthan","text":"<p>A shorthand for\u00a0<code>case</code>\u00a0which allows comparing\u00a0<code>n</code>\u00a0to multiple values without repeating the expression leading to\u00a0<code>n</code>. The chosen value is the first which matches\u00a0<code>expression &gt; value</code>.</p> <pre><code>case_greaterthan {\n  n: number,\n  value1: number -&gt; result1,\n  value2: number -&gt; result2,\n  ...\n  valueN: number -&gt; resultN,\n  _              -&gt; &lt;default-value&gt;\n}\n\n</code></pre> <p>Example:</p> <pre><code>case_greaterthan {\n  $d.status_code,\n  500 -&gt; 'server-error',\n  400 -&gt; 'client-error',\n  300 -&gt; 'redirection',\n  200 -&gt; 'success',\n  100 -&gt; 'information',\n  _   -&gt; 'other'\n}\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#case_lessthan","title":"case_lessthan","text":"<p>A shorthand for\u00a0<code>case</code>\u00a0which allows comparing a number\u00a0<code>n</code>\u00a0to multiple values without repeating the expression leading to\u00a0<code>n</code>. The chosen value is the first which matches\u00a0<code>expression &lt; value</code>.</p> <pre><code>case_lessthan {\n  n: number,\n  value1: number -&gt; result1,\n  value2: number -&gt; result2,\n  ...\n  valueN: number -&gt; resultN,\n  _              -&gt; &lt;default-value&gt;\n}\n\n</code></pre> <p>Example:</p> <pre><code>case_lessthan {\n  $d.temperature_celsius,\n  10 -&gt; 'freezing',\n  20 -&gt; 'cold',\n  30 -&gt; 'fun',\n  45 -&gt; 'hot',\n  _  -&gt; 'burning'\n}\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#_1","title":"DataPrime Glossary: Operators & Expressions","text":""},{"location":"newoutput/glossary-dataprime-operators/#aggregation-functions","title":"Aggregation Functions","text":""},{"location":"newoutput/glossary-dataprime-operators/#any_value","title":"any_value","text":"<p>Returns any non-null expression value in the group. If expression is not defined, it defaults to the\u00a0<code>$data</code>\u00a0object.</p> <pre><code>any_value(expression: any?)\n\n</code></pre> <p>Returns null if all expression values in the group are null.</p> <p>Example:</p> <pre><code>groupby $m.severity calculate any_value($d.url)\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#avg","title":"avg","text":"<p>Calculates the average value of a numerical expression in the group.</p> <pre><code>avg(expression: number)\n\n</code></pre> <p>Example:</p> <pre><code>groupby $m.severity calculate avg($d.duration) as average_duration\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#count_1","title":"count","text":"<p>Counts non-null expression values. If expression is not defined, all rows will be counted.</p> <pre><code>count(expression: any?) [into &lt;keypath&gt;]\n\n</code></pre> <p>An alias can be provided to override the keypath the result will be written to.</p> <p>For example, the following part of a query</p> <pre><code>count() into $d.num_rows\n\n</code></pre> <p>will result in a single row of the following form:</p> <pre><code>{ \"num_rows\": 7532 }\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#count_if","title":"count_if","text":"<p>Counts non-null expression values on rows which satisfy condition. If expression is not defined, all rows that satisfy condition will be counted.</p> <pre><code>count_if(condition: bool, expression: any?)\n\n</code></pre> <p>Example:</p> <pre><code>groupby $m.severity calculate count_if($d.duration &gt; 500) as $d.high_duration_logs\ngroupby $m.severity calculate count_if($d.duration &gt; 500, $d.company_id) as $d.high_duration_logs\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#distinct_count","title":"distinct_count","text":"<p>Counts non-null distinct expression values.</p> <pre><code>distinct_count(expression: any)\n\n</code></pre> <p>Example:</p> <pre><code>groupby $l.applicationname calculate distinct_count($d.username) as active_users\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#distinct_count_if","title":"distinct_count_if","text":"<p>Counts non-null distinct expression values on rows which satisfy condition.</p> <pre><code>distinct_count_if(condition: bool, expression: any)\n\n</code></pre> <p>Example:</p> <pre><code>groupby $l.applicationname calculate distinct_count_if($m.severity == 'Error', $d.username) as users_with_errors\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#max_1","title":"max","text":"<p>Calculates the maximum value of a numerical expression in the group.</p> <pre><code>max(expression: number | timestamp)\n\n</code></pre> <p>Example:</p> <pre><code>groupby $m.severity calculate max($d.duration)\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#min_1","title":"min","text":"<p>Calculates the minimum value of a numerical expression in the group.</p> <pre><code>min(expression: number | timestamp)\n\n</code></pre> <p>Example:</p> <pre><code>groupby $m.severity calculate min($d.duration)\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#percentile","title":"percentile","text":"<p>Calculates the approximate n-th percentile value of a numerical expression in the group.</p> <pre><code>percentile(percentile: number, expression: number, error_threshold: number?)\n\n</code></pre> <p>Since the percentile calculation is approximate, the accuracy may be controlled with the\u00a0<code>error_threshold</code>\u00a0parameter which ranges from\u00a0<code>0</code>\u00a0to\u00a0<code>1</code>\u00a0(defaults to\u00a0<code>0.01</code>). A lower value will result in better accuracy at the cost of longer query times.</p> <p>Example:</p> <pre><code>groupby $m.severity calculate percentile(0.99, $d.duration) as p99_latency\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#sample_stddev","title":"sample_stddev","text":"<p>Computes the sample standard deviation of a numerical expression in the group.</p> <pre><code>sample_stddev(expression: number)\n\n</code></pre> <p>Example:</p> <pre><code>groupby $m.severity calculate sample_stddev($d.duration)\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#sample_variance","title":"sample_variance","text":"<p>Computes the variance of a numerical expression in the group.</p> <pre><code>sample_variance(expression: number)\n\n</code></pre> <p>Example:</p> <pre><code>groupby $m.severity calculate sample_variance($d.duration)\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#stddev","title":"stddev","text":"<p>Computes the standard deviation of a numerical expression in the group.</p> <pre><code>stddev(expression: number)\n\n</code></pre> <p>Example:</p> <pre><code>groupby $m.severity calculate stddev($d.duration)\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#sum","title":"sum","text":"<p>Calculates the sum of a numerical expression in the group.</p> <pre><code>sum(expression: number)\n\n</code></pre> <p>Example:</p> <pre><code>groupby $m.severity calculate sum($d.duration) as total_duration\n\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#variance","title":"variance","text":"<p>Computes the variance of a numerical expression in the group.</p> <pre><code>variance(expression: number)\n\n</code></pre> <p>Example:</p> <pre><code>groupby $m.severity calculate variance($d.duration)\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#dp-expressions-in-aggregations","title":"DP Expressions in Aggregations","text":"<p>When querying with the groupby operator, you can now apply an aggregation function (such asavg, max, sum) to the bucket of results. This feature gives you the power to manipulate an aggregation expression inside the expression itself, allowing you to calculate and manipulate your data simultaneously.</p> <p>Example 1</p> <p>This examples takes logs which have some <code>connect_duration</code> and <code>batch_duration</code> fields, and calculates the ratio between the averages of those durations, per <code>region</code>.</p> <pre><code># Query\nsource logs \n  | groupby region aggregate avg(connect_duration) / avg(batch_duration)\n\n</code></pre> <p>Example 2</p> <p>This query calculates the percentage of logs which don\u2019t have a <code>kubernetes_pod_name</code> out of the total number of logs. The calculation is done per subsystem.</p> <pre><code># Query\nsource logs \n| groupby $l.subsystemname aggregate\n  sum(if(kubernetes.pod_name != null,1,0)) / count() as pct_without_pod_name\n\n</code></pre> <p>Example 3</p> <p>This query calculates the ratio between the maximum and minimum salary per department, and provides a <code>Based on N Employees</code> string as an additional column per row.</p> <pre><code># Query\nsource logs\n| groupby department_id aggregate\n    max(salary) / min(salary) as salary_ratio\n    `Based on {count()} Employees`)\n\n</code></pre> <p>Example 4</p> <p>This query calculates the ratio between error logs and info logs.</p> <pre><code>source logs\n| groupby $m.timestamp / 1h as hour aggregate \n    count_if($m.severity == '5') / count_if($m.severity == '3') as error_to_info_ratio\n</code></pre>"},{"location":"newoutput/glossary-dataprime-operators/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/golang-open-telemetry-instrumentation/","title":"Golang OpenTelemetry Instrumentation","text":"<p>This tutorial demonstrates how to instrument Golang applications to capture traces using OpenTelemetry and send them to Coralogix.</p> <p>The\u00a0OpenTelemetry-Go\u00a0instrumentation for traces is currently stable, while instrumentation for metrics in mixed beta. The OpenTelemetry Go SDK is documented\u00a0here.</p> <p>Only\u00a0manual instrumentation\u00a0is supported for Golang. The examples provided here do not represent a concrete standard for instrumenting Golang applications but serve as guides for basic implementation. You should review your code base and identify the best approach to manual instrumentation for your specific project.</p>"},{"location":"newoutput/golang-open-telemetry-instrumentation/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>A Coralogix account, set up on the Coralogix\u00a0domain</p> </li> <li> <p>Golang\u00a0installed</p> </li> </ul>"},{"location":"newoutput/golang-open-telemetry-instrumentation/#traces","title":"Traces","text":"<p>First, we will demonstrate instrumentation for a simple web application for traces.</p>"},{"location":"newoutput/golang-open-telemetry-instrumentation/#setup","title":"Setup","text":"<pre><code># create app directory\nmkdir goapp\ncd goapp\n\n# initialise golang app\ngo mod init github.com/my/app\n\n# create main.go\ntouch main.go\n\n# note that for each new module/package added to main.go\n# you will need run:\ngo mod tidy\n\n\n</code></pre>"},{"location":"newoutput/golang-open-telemetry-instrumentation/#initialise-trace-provider","title":"Initialise Trace Provider","text":"<pre><code>package main\n\nimport (\n  \"context\"\n  \"crypto/tls\"\n  \"fmt\"\n  \"log\"\n  \"net/http\" // Import the missing http package\n  \"os\"\n  \"time\"\n\n  \"github.com/coralogix/coralogix-opentelemetry-go/sampler\"\n  \"go.opentelemetry.io/otel\"\n  \"go.opentelemetry.io/otel/attribute\"\n  \"go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc\"\n  \"go.opentelemetry.io/otel/trace\"\n  \"go.opentelemetry.io/otel/sdk/resource\"\n  sdktrace \"go.opentelemetry.io/otel/sdk/trace\"\n  semconv \"go.opentelemetry.io/otel/semconv/v1.24.0\"\n  \"google.golang.org/grpc/credentials\"\n  \"math/rand\"\n)\n\nconst (\n    traceName = \"my_trace\"\n    coralogixEndpoint = \"ingress.eu2.coralogix.com:443\" //adjust your coralogix endpoint for your environment -&gt;https://coralogix.com/docs/coralogix-endpoints/\n    cxApplicationName = \"go-instrumentation\"\n    cxSubsystemName = \"manual-instro-traces\"\n)\n\nfunc init() {\n  ctx := context.Background()\n\n    // 1. define trace connection options\n  var headers = map[string]string{\n    \"Authorization\": \"Bearer &lt;your_private_key\" + os.Getenv(\"CX_TOKEN\"),\n  }\n\n  traceConnOpts := []otlptracegrpc.Option{\n    otlptracegrpc.WithTimeout(1 * time.Second),\n    otlptracegrpc.WithEndpoint(coralogixEndpoint),\n    otlptracegrpc.WithHeaders(headers),\n    otlptracegrpc.WithTLSCredentials(credentials.NewTLS(&amp;tls.Config{})),\n  }\n\n  // 2. set up a trace exporter\n  exporter, err := otlptracegrpc.New(ctx, traceConnOpts...)\n  if err != nil {\n    log.Fatalf(\"failed to create trace exporter: %v\", err)\n  }\n\n  // 3 define span resource attributes,\n    // these resource attributes will be added to all Spans\n    res, err := resource.Merge(\n        resource.Default(),\n        resource.NewWithAttributes(\n            semconv.SchemaURL,\n            semconv.ServiceNameKey.String(\"go-manual-instro-traces-example\"),\n            // cx.application.name and cx.subsystem.name are required for the\n            // spans being sent to the coralogix platform\n            attribute.String(\"cx.application.name\", cxApplicationName),\n            attribute.String(\"cx.subsystem.name\", cxSubsystemName),\n        ),\n    )\n    // 4. create batch span processor\n    //      Note: SpanProcessor is a processing pipeline for spans in the trace signal.\n    //      SpanProcessors registered with a TracerProvider and are called at the start and end of a\n    //      Span's lifecycle, and are called in the order they are registered.\n    //      https://pkg.go.dev/go.opentelemetry.io/otel/sdk/trace#SpanProcessor\n  sp := sdktrace.NewSimpleSpanProcessor(exporter)\n\n    // 5. add span processor and resource attributes to the trace provider\n  tp := sdktrace.NewTracerProvider(\n    sdktrace.WithSampler(sampler.NewCoralogixSampler(sdktrace.AlwaysSample())),\n    sdktrace.WithResource(res),\n    sdktrace.WithSpanProcessor(sp),\n  )\n\n    // 6. set the global trace provider\n  otel.SetTracerProvider(tp)\n}\n</code></pre> <p>The code above imports the required packages and defines an init function that instantiates and configures the global trace provider. The trace provider is configured to send traces to the Coralogix Domain\u00a0<code>ingress.coralogixstg.wpengine.com:443</code>\u00a0using the\u00a0<code>SimpleSpanProcessor</code>. It also fetches the Coralogix Private key from the env variable\u00a0<code>CX_TOKEN</code>.</p>"},{"location":"newoutput/golang-open-telemetry-instrumentation/#add-traces-and-spans","title":"Add Traces and Spans","text":"<p>Next we will create a simple webserver that returns a random number between 0 and 6 when called.</p> <pre><code>// http handle to roll dice\nfunc rollhanler(w http.ResponseWriter, r *http.Request) {\n  // 1. get tracer\n  tracer := otel.Tracer(\"cx.example.tracer\")\n\n  // 2. start a span\n  ctx, span := tracer.Start(r.Context(), \"rollhandle\", trace.WithSpanKind(trace.SpanKindServer))\n  defer span.End()\n\n  roll := rolldice(ctx)\n\n  // 3. add attributes to the span\n  // Note: these attribute could also be added using middleware\n  span.SetAttributes(\n    attribute.String(\"span.kind\", \"server\"),\n    attribute.String(\"resource.name\", r.Method+\" \"+r.URL.Path),\n    attribute.String(\"http.method\", r.Method),\n    attribute.String(\"http.url\", r.URL.Path),\n    attribute.String(\"http.route\", r.URL.Path),\n    attribute.String(\"http.target\", r.URL.String()),\n    attribute.String(\"http.useragent\", r.UserAgent()),\n    attribute.String(\"http.host\", r.Host),\n  )\n\n  fmt.Fprintf(w, \"rolled a %d\\n\", roll)\n}\n\n// rolldice returns a random number between 1 and 6\nfunc rolldice(ctx context.Context) int {\n  roll := rand.Intn(6) + 1\n\n  // 1. get tracer\n  tracer := otel.Tracer(traceName)\n\n  // 2. start a span from context\n  _, span := tracer.Start(ctx, \"roll-dice\")\n  defer span.End()\n\n  // 3. add attributes to the span\n  span.SetAttributes(attribute.Int(\"dice.roll\", roll))\n\n  return roll\n}\n\nfunc main() {\n  http.HandleFunc(\"/roll\", rollhanler)\n  log.Println(\"serving... :8080/roll\")\n  http.ListenAndServe(\":8080\", nil)\n}\n</code></pre> <p>Above we have defined to 2 functions,\u00a0<code>rollhandler</code>\u00a0which is the http handler function that is executed when the endpoint\u00a0<code>/roll</code>\u00a0is called, and\u00a0<code>rolldice</code>\u00a0which is a function that is used to generate the random number.</p> <p>The above demonstrates a key feature and requirement of Instrumentation, managing context. When starting a trace in the\u00a0<code>rollhandle</code>\u00a0function, a context is passed to the\u00a0<code>tracer.Start</code>\u00a0function, this function in turn returns a new context. We then use the returned context when calling the\u00a0<code>rolldice</code>\u00a0function. This chain of passing contexts between functions and creating traces from the context allows the tracer to establish and represent relationships between functions.</p> <p>We can run the application by doing the following:</p> <p></p> <p>When we execute the code above, this will create the following trace on our Coralogix console:</p> <p></p> <p>We can see that by passing the contexts correctly between functions, we get a proper representation of the relationship between them.</p>"},{"location":"newoutput/golang-open-telemetry-instrumentation/#troubleshooting-and-validation","title":"Troubleshooting and Validation","text":"<p>If traces are not visible in the Coralogix console, you can run the following checks.</p> <ul> <li> <p>Make sure your Coralogix PrivateKey is assigned to the environment variable\u00a0<code>CX_TOKEN</code>\u00a0and the\u00a0<code>coralogixEndpoint</code>\u00a0variable repris correct for your Coralogix Domain.</p> </li> <li> <p>Traces can be written to stdout to verify if they are being generated correctly, below we define an\u00a0<code>init()</code>\u00a0function that initialise the tracer with a\u00a0<code>stdouttrace.Exporter</code>\u00a0that prints to stdout:</p> </li> </ul> <pre><code>func initConsole() {\n\n    // Set up a trace exporter that writes to Stdout\n    // requires import: \"go.opentelemetry.io/otel/exporters/stdout/stdouttrace\"\n    exporter, err := stdouttrace.New(\n        stdouttrace.WithWriter(os.Stdout),\n        // Use human-readable output.\n        stdouttrace.WithPrettyPrint(),\n        // Do not print timestamps for the demo.\n        stdouttrace.WithoutTimestamps(),\n    )\n\n    // exporter, err = otlptrace.New()\n\n    if err != nil {\n        log.Fatalf(\"failed to create trace exporter: %v\", err)\n    }\n\n    // Set global trace provider\n    resource := resource.NewWithAttributes(semconv.SchemaURL, semconv.ServiceNameKey.String(\"manual-instro-traces\"))\n\n    // define a batch span processor to export spans\n    bsp := sdktrace.NewBatchSpanProcessor(exporter)\n\n    // create a trace provider\n    traceprovider := sdktrace.NewTracerProvider(\n        sdktrace.WithResource(resource),\n        sdktrace.WithSpanProcessor(bsp),\n    )\n\n    // Set the Tracer Provider globally\n    otel.SetTracerProvider(traceprovider)\n    otel.SetTextMapPropagator(propagation.TraceContext{})\n}\n\n</code></pre> <p>You can view more on Instrumentation in Golang using OpenTelemetry\u00a0here.</p>"},{"location":"newoutput/golang-open-telemetry-instrumentation/#metrics","title":"Metrics","text":"<p>Now we will demonstrate instrumentation for metrics using the same web application.</p>"},{"location":"newoutput/golang-open-telemetry-instrumentation/#initialise-metric-provider","title":"Initialise Metric Provider","text":"<pre><code>package main\n\nimport (\n    \"context\"\n    \"crypto/tls\"\n    \"fmt\"\n    \"log\"\n    \"math/rand\"\n    \"net/http\"\n    \"os\"\n    \"time\"\n\n    \"go.opentelemetry.io/otel\"\n    \"go.opentelemetry.io/otel/attribute\"\n    \"go.opentelemetry.io/otel/exporters/otlp/otlpmetric/otlpmetricgrpc\"\n    \"go.opentelemetry.io/otel/metric\"\n\n    sdkmetrics \"go.opentelemetry.io/otel/sdk/metric\"\n    \"go.opentelemetry.io/otel/sdk/resource\"\n    semconv \"go.opentelemetry.io/otel/semconv/v1.4.0\"\n    \"google.golang.org/grpc/credentials\"\n)\n\nconst (\n    coralogixEndpoint = \"ingress.coralogixstg.wpengine.com:443\"\n    cxApplicationName = \"go-instrumentation\"\n    cxSubsystemName   = \"manual-instro-traces\"\n    traceName         = \"cx.example.tracer\"\n)\n\nfunc init() {\n    ctx := context.Background()\n    // 1. define metrics connection options\n    var headers = map[string]string{\n        \"Authorization\": \"Bearer \" + os.Getenv(\"CX_TOKEN\"),\n    }\n\n    metricsConnOpts := []otlpmetricgrpc.Option{\n        otlpmetricgrpc.WithTimeout(1 * time.Second),\n        otlpmetricgrpc.WithEndpoint(coralogixEndpoint),\n        otlpmetricgrpc.WithHeaders(headers),\n        otlpmetricgrpc.WithTLSCredentials(credentials.NewTLS(&amp;tls.Config{})),\n    }\n\n    // 2. set up a metrics exporter\n    metricsExporter, err := otlpmetricgrpc.New(ctx, metricsConnOpts...)\n    if err != nil {\n        log.Fatalf(\"failed to create metrics exporter: %v\", err)\n    }\n\n    reader := sdkmetrics.NewPeriodicReader(metricsExporter)\n\n    // 3. create a controller\n\n    // 3. define resource attributes,\n    // these resource attributes will be added to all metrics\n    resource := resource.NewWithAttributes(\n        semconv.SchemaURL,\n        semconv.ServiceNameKey.String(\"go-manual-instro-traces-example\"),\n\n        // cx.application.name and cx.subsystem.name are required for the\n        // metrics being sent to the coralogix platform\n        attribute.String(\"cx.application.name\", cxApplicationName),\n        attribute.String(\"cx.subsystem.name\", cxSubsystemName),\n    )\n\n    // 4. create batch metrics processor\n    //      Note: MetricsProcessor is a processing pipeline for metrics in the metrics signal.\n    //      MetricsProcessors registered with a MeterProvider and are called at the start and end of a\n    //      Metric's lifecycle, and are called in the order they are registered.\n    //      https://pkg.go.dev/go.opentelemetry.io/otel/sdk/metric#MetricsProcessor\n    mp := sdkmetrics.NewMeterProvider(\n        sdkmetrics.WithResource(resource),\n        sdkmetrics.WithReader(reader),\n    )\n\n    // 5. set the global meter provider\n    otel.SetMeterProvider(mp)\n}\n\n</code></pre> <p>Above we define an <code>init</code> function that initializes the metrics instrumentation components for the application. It defines connection options for communicating with the Coralogix platform, including authorisation headers and endpoint details. An exporter is then set up to send metrics to Coralogix, and a periodic reader reads and exports these metrics at regular intervals. A metrics processor is then established to manage the collection, batching, and export of metrics data, and this processor is set as the default global meter provider. This ensures that the application is ready to capture and relay metrics to Coralogix seamlessly.</p> <p>As before, the Coralogix Private Key will be read in using an Environment variable, CX_TOKEN</p>"},{"location":"newoutput/golang-open-telemetry-instrumentation/#http-request-counter","title":"HTTP Request Counter","text":"<p>We will implement a simple HTTP Request Counter metric. To do this, we will define a web app with middleware that increments the counter for each request received.</p> <pre><code>// requestCounterMiddleware - middleware that counts each request\nfunc requestCounterMiddleware(reqCount metric.Int64Counter, next http.HandlerFunc) http.HandlerFunc {\n    return func(w http.ResponseWriter, r *http.Request) {\n        ctx := r.Context()\n        // increment counter\n        reqCount.Add(ctx, 1)\n        next(w, r)\n    }\n}\n</code></pre> <p>This middleware function increments a counter each time an HTTP request passes through it. Its parameters are the counter (<code>reqCount</code>) and the next HTTP handler (<code>next</code>) in the pipeline. Upon invocation:</p> <ul> <li> <p>It retrieves the context from the incoming request.</p> </li> <li> <p>Increments the request counter.</p> </li> <li> <p>Proceeds to the next HTTP handler in the chain.</p> </li> </ul> <pre><code>// http handle to roll dice\nfunc rollhanler(w http.ResponseWriter, r *http.Request) {\n    fmt.Fprintf(w, \"rolled a %d\\n\", rand.Intn(6)+1)\n}\n\n</code></pre> <p>We have defined an HTTP handler function which, when invoked, simulates rolling a dice and responds with the outcome to the client.</p> <pre><code>func main() {\n\n    // define a meter\n    meter := otel.Meter(\"goapp\")\n\n    // Create two synchronous instruments: counter and histogram\n    reqCounter, err := meter.Int64Counter(\n        \"http.request.counter\",\n        metric.WithDescription(\"HTTP Request counter\"),\n    )\n\n    if err != nil {\n        log.Fatalf(\"failed to create request count metric: %v\", err)\n    }\n\n    http.HandleFunc(\"/roll\", requestCounterMiddleware(reqCounter, rollhanler))\n    log.Println(\"serving... :8080/roll\")\n    http.ListenAndServe(\":8080\", nil)\n}\n</code></pre> <p>The main function does the following:</p> <ul> <li> <p>Initializes a meter named \"goapp\" to produce metrics instruments.</p> </li> <li> <p>Defines an Int64Counter named \"http.request.counter\", serving as a counter for incoming HTTP requests.</p> </li> <li> <p>Registers the \"/roll\" endpoint, wrapping the <code>rollhandler</code> with the <code>requestCounterMiddleware</code> to ensure each request is counted.</p> </li> </ul> <p>The application can be run and tested using the same method as described for the traces example.</p>"},{"location":"newoutput/golang-open-telemetry-instrumentation/#troubleshooting-and-validation_1","title":"Troubleshooting and Validation","text":"<p>Sent metrics can be view by visiting Grafana -&gt; Explore -&gt; Browse Metrics</p> <p>If you wish to validate that that the application is generating metrics correctly, similar to traces, it is possible to export metrics to stdout.</p> <p>First we define a stdout exporter and create a reader.</p> <pre><code>    // requires import: \"go.opentelemetry.io/otel/exporters/stdout/stdoutmetric\"\n        stdoutexporter, err := stdoutmetric.New()\n    if err != nil {\n        log.Fatalf(\"failed to create metrics exporter: %v\", err)\n    }\n\n    stdoutreader := sdkmetrics.NewPeriodicReader(stdoutexporter)\n</code></pre> <p>Then add the reader to the Metric Provider.</p> <pre><code>    mp := sdkmetrics.NewMeterProvider(\n        sdkmetrics.WithResource(resource),\n        sdkmetrics.WithReader(reader),\n        sdkmetrics.WithReader(stdoutreader), // &lt;----\n    )\n</code></pre> <p>Metrics will be printed to stdout as well as sent to Coralogix. Note that metrics are not sent instantly when there is a delay using the Periodic Reader.</p>"},{"location":"newoutput/golang-open-telemetry-instrumentation/#service-flows","title":"Service Flows","text":"<p>For customers with functional\u00a0Golang OpenTelemetry instrumentation, this section guides reconfiguring the existing setup to define, report, and monitor Coralogix\u00a0Service Flows.</p> <p>New customers or those who haven\u2019t configured the Golang OpenTelemetry instrumentation must follow the Setup instructions above. The steps in this section are included in those instructions.</p> <p>Add to the package:</p> <pre><code>  \"github.com/coralogix/coralogix-opentelemetry-go/sampler\"\n</code></pre> <p>Add to the code:</p> <pre><code>sdktrace.WithSampler(sampler.NewCoralogixSampler(trace.AlwaysSample())),\n</code></pre> <p>Run:</p> <pre><code>go mod tidy\n</code></pre>"},{"location":"newoutput/golang-open-telemetry-instrumentation/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/google-alert-center/","title":"Google Alert Center","text":""},{"location":"newoutput/google-alert-center/#overview","title":"Overview","text":"<p>Google Workspace Admin Alert Center offers real-time security alerts and insights that help you protect your organization from the latest threats, including phishing, malware, and other suspicious activity.</p> <p>The following tutorial will show you how to directly integrate Alert Center with Coralogix.</p>"},{"location":"newoutput/google-alert-center/#benefits","title":"Benefits","text":"<ul> <li> <p>Develop a centralized view of all security incidents in the organization.</p> </li> <li> <p>Contextualise and cross-reference the investigation with other activities.</p> </li> <li> <p>Set up custom dashboards to visualise security issues across products.</p> </li> </ul>"},{"location":"newoutput/google-alert-center/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Super admin permissions in Google Cloud.</p> </li> <li> <p>An existing project within Google Cloud.</p> </li> </ul>"},{"location":"newoutput/google-alert-center/#setup","title":"Setup","text":"<p>1. Configure a Service Account\u00a0and API Key to facilitate automated intermediation.</p> <p>2.\u00a0Set up\u00a0Domain Wide Delegation, to authorize your Service Account to read user data and send it to Coralogix. The OAuth Scope permission required is\u00a0<code>https://www.googleapis.com/auth/apps.alerts</code>.</p> <p>3.\u00a0From your Coralogix toolbar, navigate to\u00a0Data Flow\u00a0&gt;\u00a0Integrations.</p> <p>4.\u00a0From the\u00a0Integrations\u00a0section, select\u00a0Google Alerts Center.</p> <p>5.\u00a0Click\u00a0+ ADD NEW.</p> <p>6.\u00a0If you haven\u2019t already done so, click\u00a0GO TO GCP ACCOUNT\u00a0and create a key file. Then, click\u00a0NEXT.</p> <p>7.\u00a0Click\u00a0SELECT FILE\u00a0and upload the key file\u00a0you previously created.</p> <p>8.\u00a0A confirmation will appear when the file is uploaded successfully. Click\u00a0NEXT.</p> <p>9.\u00a0Fill in the settings:</p> <ul> <li> <p>Integration Name:\u00a0Enter a name for your integration. This field is automatically populated, but can be changed if you want.</p> </li> <li> <p>Organization Name:\u00a0Enter the name of the organization on Google Workspaces to be monitored. You can find this by navigating to\u00a0IAM &amp; Admin\u00a0&gt;\u00a0Settings\u00a0on Google Cloud.</p> </li> <li> <p>Organization ID:\u00a0Enter the ID of the organization to be monitored. You can find this by going to\u00a0IAM &amp; Admin\u00a0&gt;\u00a0Settings\u00a0on Google Cloud.</p> </li> <li> <p>Impersonated Email:\u00a0Enter a valid email address to be\u00a0impersonated\u00a0when connecting to Google Workspace using the service account created above.</p> </li> </ul> <p>10.\u00a0Click\u00a0COMPLETE\u00a0and finish the setup. Please wait a few minutes before the integration takes effect and your data is available on the platform.</p>"},{"location":"newoutput/google-alert-center/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email to\u00a0support@coralogix.com.</p>"},{"location":"newoutput/google-cloud-pub-sub/","title":"GCP Pub/Sub","text":"<p>Coralogix offers a number of different approaches for collecting logs from your Google Cloud environments including using GCP Log Explorer and Google Cloud Storage.</p> <p>The tutorial describes how to configure a Logs router to send logs to a Pub/Sub topic and deliver them to Coralogix using a push subscription on the topic.</p> <p>The main advantage of using a push subscription is that it avoids running any additional software (i.e. functions) in your GCP account which can contribute to operational overhead and costs.</p>"},{"location":"newoutput/google-cloud-pub-sub/#requirements","title":"Requirements","text":"<ul> <li> <p>Coralogix account</p> </li> <li> <p>GCP account with permissions to configure <code>Logging</code> and <code>Pub/Sub</code> services</p> </li> </ul>"},{"location":"newoutput/google-cloud-pub-sub/#query-parameters-endpoint-url","title":"Query Parameters: Endpoint URL","text":"<p>When you create your subscription, you will need to set the correct endpoint URL using the following parameters:</p> <ul> <li> <p><code>domain</code>: Your Coralogix domain</p> </li> <li> <p><code>key</code>: Your Coralogix Send-Your-Data API key</p> </li> <li> <p><code>application</code> (optional): Overrides the default application name</p> </li> <li> <p><code>subsystem</code> (optional): Overrides the default subsystem name</p> </li> <li> <p><code>computer</code> (optional): Sets the computer name (otherwise there is no computer name)</p> </li> <li> <p><code>application_name_source</code> (optional): Controls how application name is determined when the <code>application</code> parameter is not set</p> <ul> <li> <p><code>project_id_label_or_log_name</code>: Default behavior. The <code>project_id</code> label will be used first. If it is not available, the second segment of the log name will be used.</p> </li> <li> <p><code>log_name</code>: We recommend trying this if you see numeric project IDs in the application name, but would prefer human readable project names. This does not affect folder IDs, which are always numeric.</p> </li> </ul> </li> <li> <p><code>log_message</code> (optional): Controls how GCP audit logs are transformed</p> <ul> <li> <p>Defaults to <code>LogEntry</code> if not specified</p> </li> <li> <p><code>Payload</code>: parse only payload field content of the log entry as log text</p> </li> <li> <p><code>LogEntry</code>: parse whole log entry, for example:</p> </li> </ul> </li> </ul> <pre><code>{\n     \"spanId\": \"abcdefghijk\",\n     \"severity\": \"INFO\",\n     ...\n     \"jsonPayload\": {\n        ...\n     },\n     ...\n     \"receiveTimestamp\": \"2023-02-27T10:16:23.544899312Z\",\n     \"timestamp\": \"2023-02-27T10:16:20.035098Z\",\n }\n</code></pre>"},{"location":"newoutput/google-cloud-pub-sub/#configuration","title":"Configuration","text":"<p>To configure the ingestion of GCP log data to Coralogix, we will first create a new topic in Google Cloud Pub/Sub. Then we will configure the topic as a sink in the logs router and configure a subscription to push the data to Coralogix.</p> <p>STEP 1. Log in to GCP console</p> <p>STEP 2. Go to <code>Pub/Sub</code> / <code>Topics</code> and create a Topic</p> <ul> <li>Note - Uncheck the \u2018Add a default subscription\u2019 checkbox\u201d We will create a subscription in a later step.</li> </ul> <p>STEP 3. Go to <code>Logging</code> / <code>Logs Router</code> and create a Sink</p> <ul> <li> <p>Select sink service: Cloud Pub/Sub topic</p> </li> <li> <p>Select the Cloud Pub/Sub topic created in the previous step</p> </li> <li> <p>Optionally: Choose which logs should be included/excluded</p> </li> </ul> <p></p> <p>STEP 4. Go to <code>Pub/Sub</code> / <code>Subscriptions</code> and create a Subscription</p> <ul> <li> <p>Select the topic created in step 2, and make the following adjustments:</p> <ul> <li> <p>Set Delivery type to 'Push'.</p> </li> <li> <p>Set Endpoint URL to https://ingress./gcp/v1/logs?key=. If you like to pass additional attributes to Coralogix, set the endpoint to https://ingress./gcp/v1/logs?key=&amp;application=&amp;subsystem=. <li> <p>Set message retention to '1 day'.</p> </li> <li> <p>Set Retry policy to 'Retry after exponential backoff delay' (keep the default backoff values).</p> </li> <p></p> <p></p>"},{"location":"newoutput/google-cloud-pub-sub/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/google-workspace-alert-center/","title":"Google Workspace Alert Center","text":""},{"location":"newoutput/google-workspace-alert-center/#overview","title":"Overview","text":"<p>Google Workspace Alert Center offers real-time security alerts and insights that help you protect your organization from the latest threats, including phishing, malware, and other suspicious activity.</p> <p>The following tutorial will show you how to integrate Google Workspace Alert Center with Coralogix directly.</p>"},{"location":"newoutput/google-workspace-alert-center/#benefits","title":"Benefits","text":"<ul> <li> <p>Develop a centralized view of all security incidents in the organization.</p> </li> <li> <p>Contextualize and cross-reference the investigation with other activities.</p> </li> <li> <p>Set up custom dashboards to visualize security issues across products.</p> </li> </ul>"},{"location":"newoutput/google-workspace-alert-center/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Super admin permissions in Google Cloud</p> </li> <li> <p>An existing project within Google Cloud</p> </li> </ul>"},{"location":"newoutput/google-workspace-alert-center/#setup","title":"Setup","text":"<p>1. Configure a Service Account and API Key to facilitate automated intermediation.</p> <p>2. Set up Domain Wide Delegation, to authorize your Service Account to read user data and send it to Coralogix. The OAuth Scope permission required is <code>https://www.googleapis.com/auth/apps.alerts</code>.</p> <p>3. Navigate to API &amp; Services &gt; Library screen. Select Google Workspace Alert Center API and ensure it\u2019s enabled.</p> <p>4. From your Coralogix toolbar, navigate to Data Flow &gt; Integrations. Select Google Workspace Alert Center.</p> <p>5. Click + ADD NEW.</p> <p>6. If you haven\u2019t already done so, click GO TO GCP ACCOUNT and create a key file. Then, click NEXT.</p> <p>7. Click SELECT FILE and upload the key file you previously created.</p> <p>8. A confirmation will appear when the file is uploaded successfully. Click NEXT.</p> <p>9. Fill in the settings:</p> <ul> <li> <p>Integration Name: Enter a name for your integration. This field is automatically populated, but can be changed if you want.</p> </li> <li> <p>Impersonated Email: Enter a valid email address to be impersonated when connecting to Google Workspace using the service account created above.</p> </li> <li> <p>Application Name. Input your application name.</p> </li> <li> <p>Subsystem Name. The subsystem name, Google Workspace Alert Center, is read-only and cannot be modified.</p> </li> </ul> <p>10. Click COMPLETE and finish the setup.</p> <p>Please wait a few minutes before the integration takes effect, and your data is available on the platform.</p>"},{"location":"newoutput/google-workspace-alert-center/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by sending us an email to\u00a0support@coralogix.com.</p>"},{"location":"newoutput/google-workspace-users/","title":"Google Workspace Users","text":""},{"location":"newoutput/google-workspace-users/#overview","title":"Overview","text":"<p>This integration will let you collect all user details from your Google Workspace Admin Console along with their metadata. This provides your Coralogix features with additional user details to get better context. The integration will normalize user identifiers from relevant product logs into a\u00a0<code>cx_security.user</code>\u00a0key.</p>"},{"location":"newoutput/google-workspace-users/#benefits","title":"Benefits","text":"<ul> <li> <p>Monitor User Activities:\u00a0By ingesting user metadata, you can monitor actions such as user logins, network browsing activities, email communications, and more.</p> </li> <li> <p>Investigate Incidents:\u00a0Since each user entry is enriched with additional metadata, security administrators can easily get full user context within Coralogix.</p> </li> <li> <p>Maintain Compliance:\u00a0Additional user metadata will help you meet compliance requirements. You can generate a user activity report for auditing purposes, which is often used to maintain proof of compliance efforts.</p> </li> <li> <p>Detect Anomalies:\u00a0Establish a baseline for a role behavior. As you import additional context, you can set Alert Rules based on metadata, such as user department or role.</p> </li> <li> <p>Visualize Results:\u00a0Track your org\u2019s activity via Custom Dashboards. As you ingest user metadata, you can visualize statistics of total activities by department, role or any other user group.</p> </li> </ul>"},{"location":"newoutput/google-workspace-users/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Super admin permissions in Google Cloud.</p> </li> <li> <p>An existing project within your Google Cloud.</p> </li> </ul>"},{"location":"newoutput/google-workspace-users/#setup","title":"Setup","text":"<p>1. Configure a Service Account and API Key to facilitate automated intermediation.</p> <p>2. Set up Domain Wide Delegation to authorize your Service Account to read user data and send it to Coralogix. The OAuth Scope permission required is <code>https://www.googleapis.com/auth/admin.directory.user.readonly</code>.</p> <p>3. Navigate to API &amp; Services &gt; Library screen. Select Admin SDK API and ensure it\u2019s enabled.</p> <p>4.\u00a0From your Coralogix toolbar, navigate to Data Flow &gt; Integrations. Select Google Workspace Users.</p> <p>5.\u00a0Click\u00a0+ ADD NEW.</p> <p>6.\u00a0If you haven\u2019t already done so in step 1, click\u00a0GO TO GCP ACCOUNT\u00a0and create a key file. Then, click\u00a0NEXT.</p> <p>7.\u00a0Click\u00a0SELECT FILE\u00a0and upload the key file\u00a0you previously created.</p> <p>8.\u00a0A confirmation will appear when the file is uploaded successfully. Click\u00a0NEXT.</p> <p>9.\u00a0Fill in the settings:</p> <ul> <li> <p>Integration Name:\u00a0Enter a name for your integration. This field is automatically populated, but can be changed if you want.</p> </li> <li> <p>Organization Name:\u00a0Enter the organization name in Google Cloud where the service account was created. You can find this by navigating to\u00a0IAM &amp; Admin\u00a0&gt;\u00a0Settings\u00a0on Google Cloud.</p> </li> <li> <p>Organization ID:\u00a0Enter the organization ID of Google Cloud where the service account was created. You can find this by going to\u00a0IAM &amp; Admin\u00a0&gt;\u00a0Settings\u00a0on Google Cloud.</p> </li> <li> <p>Impersonated Email:\u00a0Enter a valid email address to be\u00a0impersonated\u00a0when connecting to Google Workspace using the service account created above.</p> </li> </ul> <p>10.\u00a0Click\u00a0COMPLETE\u00a0and finish the setup. Please wait a few minutes before the integration takes effect and user data is available.</p> <p>11.\u00a0Verify that the integration pipeline occurred. Go to\u00a0Data Flow\u00a0&gt;\u00a0Data Enrichment &gt; Custom Enrichment. The following two files should have been created:\u00a0<code>users_normalization</code>\u00a0and\u00a0<code>users_metadata</code></p> <p>12.\u00a0Check that user data was properly ingested. Go to the\u00a0Explore\u00a0screen, and run these DataPrime queries to verify that the enrichment files were uploaded with the user details:</p> <pre><code>source users_normalization\n\nsource users_metadata\n\n</code></pre> <p>If you are not getting a response, please wait a few minutes and try again.</p>"},{"location":"newoutput/google-workspace-users/#work-with-enriched-user-data","title":"Work with Enriched User Data","text":"<p>Once you have activated this integration, the service will automatically connect to your Google Workspace environment and read all relevant user details. User data will be stored under two files in\u00a0Custom Enrichment:\u00a0<code>users_normalization</code>and\u00a0<code>users_metadata</code>. This sync will run once a day, to keep an updated status of user details stored in Coralogix.</p>"},{"location":"newoutput/google-workspace-users/#automatic-normalization-of-a-user-key-across-product-logs","title":"Automatic normalization of a user key across product logs","text":"<p>All logs containing a user email in the\u00a0<code>cx_security.email</code>\u00a0key are automatically enriched into the\u00a0<code>cx_security.email_enriched</code>\u00a0key, which will show the user\u2019s display name and the email in brackets. This is done by the\u00a0<code>users_normalization</code>\u00a0CSV file created automatically in\u00a0Custom Enrichments.</p> <p>To enrich other keys containing the email address during log ingestion, select the key in\u00a0Data Flow\u00a0&gt;\u00a0Data Enrichment&gt;\u00a0Custom Enrichments\u00a0&gt;\u00a0<code>users_normalization</code>. To view the contents of this file, run the DataPrime query:</p> <pre><code>source users_normalization\n\n</code></pre>"},{"location":"newoutput/google-workspace-users/#view-additional-details-about-a-user","title":"View additional details about a user","text":"<p>The\u00a0<code>users_metadata</code>\u00a0CSV file in\u00a0Custom Enrichment\u00a0augments the user with additional metadata, such as department, title and manager\u2019s email. To view all user metadata and further filter results by the user, run the DataPrime query:</p> <pre><code>source users_metadata\n\n</code></pre> <p>You can also use DataPrime enrich commands to augment your logs with additional user context. Do this automatically during log ingestion by defining the\u00a0<code>cx_security.email_enriched</code>\u00a0key in\u00a0Data Flow\u00a0&gt;\u00a0Data Enrichment\u00a0&gt;\u00a0Custom Enrichments. This will allow you to define alert rules based on user context (e.g. department, role) and visualize these statistics in custom dashboards.</p>"},{"location":"newoutput/google-workspace-users/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogix.com.</p>"},{"location":"newoutput/grafana-api/","title":"Hosted Grafana API","text":"<p>Coralogix provides a secure hosted Grafana API for creating, editing, exporting, importing, querying, and other Grafana API operations. Through Grafana APIs, you can manage your hosted Grafana dashboards.</p>"},{"location":"newoutput/grafana-api/#prerequisites","title":"Prerequisites","text":"<p>To use the Grafana API to query your dashboard, first locate your:</p> <ul> <li> <p>Coralogix Alerts, Rules and Tags API key</p> </li> <li> <p>Coralogix domain</p> </li> </ul>"},{"location":"newoutput/grafana-api/#http-request-for-your-hosted-grafana","title":"HTTP Request for Your Hosted Grafana","text":"<p>The Alerts, Rules and Tags API key should be added as Coralogix token with each HTTP request. Your Coralogix Domain will be used to construct the Grafana API endpoint specific to your account.</p> <p>The API request should contain the following:</p> <ul> <li> <p>Headers:</p> <ul> <li> <p>\u2018token:\u2019 <li> <p>\u2018Content-type: application/json\u2018</p> </li> <li> <p>URL: https://ng-api-http./grafana/api/"},{"location":"newoutput/grafana-api/#examples","title":"Examples","text":"<ul> <li>GET Home Dashboard</li> </ul> <pre><code>curl --location --request GET 'https://ng-api-http.&lt;DOMAIN&gt;/grafana/api/dashboards/home' \\\n--header 'Content-type: application/json' \\\n--header 'Authorization: Bearer &lt;Alerts, Rules and Tags API Key&gt;' \\\n</code></pre> <ul> <li>Search all dashboards in your team:</li> </ul> <pre><code>curl --location --request GET 'https://ng-api-http.&lt;DOMAIN&gt;/grafana/api/search' \\\n--header 'Content-type: application/json' \\\n--header 'Authorization: Bearer &lt;Alerts, Rules and Tags API Key&gt;' \\\n</code></pre> <ul> <li>Get a dashboard and panels by uid:</li> </ul> <pre><code>curl --location --request GET 'https://ng-api-http.&lt;DOMAIN&gt;/grafana/api/dashboards/uid/&lt;UID&gt;' \\\n--header 'Content-type: application/json' \\\n--header 'Authorization: Bearer &lt;Alerts, Rules and Tags API Key&gt;' \\\n</code></pre> <ul> <li>Post Dashboard - create and update existing dashboards:</li> </ul> <pre><code>curl --location 'https://ng-api-http.&lt;domain&gt;/grafana/api/dashboards/db' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer &lt;Alerts, Rules and Tags API Key&gt;' \\\n--data '&lt;Dashboard JSON&gt;'\n</code></pre> <ul> <li>GET Annotations:</li> </ul> <pre><code>curl --location --request POST 'https://ng-api-http.coralogixstg.wpengine.com/grafana/api/dashboards/db' \\\n--header 'Content-type: application/json' \\\n--header 'Authorization: Bearer &lt;Alerts, Rules and Tags API Key&gt;' \\\n--data-binary \"@path/to/file\"\n</code></pre>"},{"location":"newoutput/grafana-api/#additional-resources","title":"Additional Resources","text":"<ul> <li> <p>Grafana\u2019s HTTP API Reference</p> </li> <li> <p>Grafana API supports the Host Grafana features.</p> </li> </ul>"},{"location":"newoutput/grafana-api/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/grafana-plugin/","title":"Grafana Plugin","text":"<p>You can now visualize your data using Coralogix\u2019s Hosted Grafana without having to integrate a personal Grafana instance with your Coralogix account. Use Grafana to visualize your logs and metrics with all the functions you are already familiar with. You will be able to access all Coralogix data sources automatically without the need for any plugins (recommended).</p> <p>For those wishing to use the Grafana plugin, this tutorial demonstrates how to connect and view your logs and metrics in your Coralogix dashboard using Grafana version 7+.</p>"},{"location":"newoutput/grafana-plugin/#connect-your-logs","title":"Connect Your Logs","text":"<p>STEP 1. Log in to Grafana.</p> <p>STEP 2. Navigate to Home &gt; Connections &gt; Data sources</p> <p></p> <p>STEP 3. Click Add data source.</p> <p></p> <p>STEP 4. Select Elasticsearch as your data source.</p> <p></p> <p>STEP 5. Define the Settings.</p> <ul> <li> <p>Name. Select a name for your data source.</p> </li> <li> <p>URL. Input the OpenSearch endpoint associated with your Coralogix domain.</p> </li> <li> <p>Skip TLS certification verification. Check to activate.</p> </li> <li> <p>Header. Input \"token\".</p> </li> <li> <p>Value. Input your Coralogix Logs Query Key. Access this by navigating to Data Flow &gt; API Keys in your Coralogix toolbar.</p> </li> <li> <p>Index name. Input \"Coralogix\".</p> </li> <li> <p>Pattern. Select No pattern.</p> </li> <li> <p>Time field name: \"coralogix.timestamp\"</p> </li> </ul> <p></p> <p></p> <p>STEP 6. Click Save &amp; test.</p> <p>A popup message will inform you if you have configured your data source successfully.</p> <p></p>"},{"location":"newoutput/grafana-plugin/#connect-your-metrics","title":"Connect Your Metrics","text":"<p>As part of Coralogix's metrics offering, we support the querying of metric data through the most prevalent time-series query language - PromQL. This allows you to use Coralogix as your metrics backend and add it as a Prometheus data source to your Grafana instance, as well as update existing dashboards, without any change to the queries or syntax.</p>"},{"location":"newoutput/grafana-plugin/#add-a-coralogix-promql-data-source","title":"Add a Coralogix PromQL Data Source","text":"<p>STEP 1. Log in to Grafana.</p> <p>STEP 2. Navigate to Home &gt; Connections &gt; Data sources</p> <p></p> <p>STEP 3. Click Add data source.</p> <p></p> <p>STEP 4. Select Prometheus as your data source.</p> <p></p> <p>STEP 5. Define the Settings.</p> <ul> <li> <p>Name. Select a name for your data source.</p> </li> <li> <p>URL. Input the PromQL endpoint associated with your Coralogix domain.</p> </li> </ul> <p>[table id=95 /]</p> <ul> <li> <p>Skip TLS certification verification. Check to activate.</p> </li> <li> <p>Header. Input \"token\".</p> </li> <li> <p>Value. Input your Coralogix Logs Query Key. Access this by navigating to Data Flow &gt; API Keys in your Coralogix toolbar.</p> </li> <li> <p>Query timeout. Set to 300s.</p> </li> <li> <p>HTTP Method. Set to POST.</p> </li> </ul> <p></p> <p></p> <p>STEP 6. Click Save &amp; test.</p> <p>A popup message will inform you if you have configured your data source successfully.</p>"},{"location":"newoutput/grafana-plugin/#useful-docker-commands","title":"Useful Docker Commands","text":"<p>If you have Grafana installed on a Docker container, use these useful commands to interact with Grafana:</p> <ul> <li>List all docker containers</li> </ul> <pre><code>docker ps -a\n</code></pre> <ul> <li>Log in to your Grafana container</li> </ul> <pre><code>sudo docker exec -it \"Grafana container ID\" /bin/bash\n</code></pre> <ul> <li>Restart Grafana</li> </ul> <pre><code>docker restart \"Grafana container ID\"\n</code></pre>"},{"location":"newoutput/grafana-plugin/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/groups/","title":"Assign User Roles via Groups","text":"<p>Streamline user management - role-based and data scope access - with Groups.</p> <p></p>"},{"location":"newoutput/groups/#overview","title":"Overview","text":""},{"location":"newoutput/groups/#role-based-access-control-rbac","title":"Role-Based Access Control (RBAC)","text":"<p>Coralogix users enjoy role-based access based on their membership in one or more Groups, each of which is assigned one or more roles.</p>"},{"location":"newoutput/groups/#data-scope","title":"Data Scope","text":"<p>In addition, by using Groups, organizations can control the scope of user data by limiting access to particular applications and subsystems.</p> <p>Only Organization and Platform administrators have the predefined permissions to manage Groups.</p>"},{"location":"newoutput/groups/#create-a-group","title":"Create a Group","text":"<p>STEP 1. Access your settings in the upper right-hand corner of the Coralogix toolbar.</p> <p>STEP 2. In the left-hand sidebar, click Groups. A list of existing groups will appear.</p> <p></p> <p>Step 3. Click Create New Group.</p> <p>STEP 4. Enter the new group details.</p> <p></p>"},{"location":"newoutput/groups/#group-name-description","title":"Group Name &amp; Description","text":"<p>Enter a name and description for the group.</p>"},{"location":"newoutput/groups/#select-role","title":"Select Role","text":"<p>Select the role for users in the group.</p>"},{"location":"newoutput/groups/#application-subsystem-scope","title":"Application &amp; Subsystem Scope","text":"<p>Add applications and subsystems to which the group users may have access. For example, choosing the application \u201cAWS\u201d will give group members access to AWS-related information only.</p> <p>Notes:</p> <ul> <li> <p>You can also choose a different\u00a0filter type to include several applications that start and end or include a search term.</p> </li> <li> <p>If no applications or subsystems are added, users in the group will have access to all applications and subsystems.</p> </li> </ul> <p>STEP 5. Click CREATE.</p>"},{"location":"newoutput/groups/#migrate-groups-from-legacy-to-custom-roles","title":"Migrate Groups from Legacy to Custom Roles","text":"<p>Legacy roles are predefined system roles from our sunsetted role management system. Existing customers must migrate the legacy roles assigned to their users to system and custom roles to prevent a loss of role-based access for your users.</p> <p>Select an existing Group from your list and modify the role in the Select Role drop-down menu to do so. Unselect the legacy role and choose a different role in its place.</p>"},{"location":"newoutput/groups/#system-roles","title":"System Roles","text":"<p>Coralogix offers seven predefined system roles.</p> System Role Description Org Admin Organization admins manage all teams and user access and settings. They can also manage and configure all Coralogix resources. They have full access to billing information and can revoke API keys. They can also promote users with fewer permissions to administrators. Platform Admin Platform admins have identical permissions to Org admins, except for management and read-only access to organization settings. Data Admin Data admins can view and modify all Coralogix monitoring features. They may not view and manage organization settings and are limited in their access to team settings. Observability Lead Observability leads can view all Coralogix monitoring features but are limited in their management and configuration permissions. They may not view and manage organization settings and are limited in their access to team settings. Standard User Read-only users do not have access to make changes within Coralogix. This comes in handy when you\u2019d like to share specific read-only views with a client or when a member of one team needs to share a dashboard or other resource with someone outside their team. Read-Only User Read-only users do not have access to make changes within Coralogix. This comes in handy when you\u2019d like to share specific read-only views with a client, or when a member of one team needs to share a dashboard or other resource with someone outside their team. No Access User Users have no permissions. Use this system role as a baseline to create custom roles with limited permissions. Security User Security users can perform daily security tasks and manage related configurations, such as incident investigation, alert configuration, security posture visibility, and extension deployment."},{"location":"newoutput/groups/#legacy-roles","title":"Legacy Roles","text":"<p>The list below presents the types of legacy roles\u00a0previously assigned to users.</p> Permissions Read Only Admin User Basic User Data Analyst Interface User Explore Screen - Create / Delete Saved Views \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Explore Screen - Create Widgets \u2714 \u2714 \u2714 \u2714 \u2714 Explore Screen - Delete Widgets \u2714 Insights \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Livetail \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Alerts - View Alerts \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Alerts - Create / Delete / Modify Alerts \u2714 \u2714 \u2714 Alerts - Delete alerts created by someone else \u2714 \u2714 \u2714 Data Flow - Webhooks \u2714 \u2714 \u2714 \u2714 Data Flow - API Key (Logs query key) \u2714 \u2714 \u2714 Data Flow - Archive Queries \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Data Flow - Setup Archive \u2714 \u2714 \u2714 \u2714 Settings - Preferences \u2714 \u2714 \u2714 \u2714 Settings - Notifications \u2714 \u2714 \u2714 \u2714 Settings - Can invite team members \u2714 Coralogix Dashboards - Read Coralogix Dashboards \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Coralogix Dashboards - Update Coralogix Dashboards \u2714 \u2714 \u2714 \u2714 \u2714 Coralogix Dashboards - Create / Delete visualizations and dashboards \u2714 \u2714 \u2714 \u2714 \u2714 Grafana - Read Grafana dashboards \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Grafana - Update Grafana dashboards \u2714 \u2714 \u2714 \u2714 \u2714 Grafana - Create / Delete visualizations and dashboards \u2714 \u2714 \u2714 \u2714 \u2714 View RUM \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Read incident data \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Acknowledge incidents \u2714 \u2714 \u2714"},{"location":"newoutput/groups/#additional-resources","title":"Additional Resources","text":"DocumentationRoles &amp; PermissionsTeams"},{"location":"newoutput/groups/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/guide-first-steps-coralogix/","title":"Getting Started with Coralogix","text":"<p>Coralogix is a cloud-based, SaaS analytics and monitoring platform that combines logs, metrics, and traces to gain full observability into your system using one tool. The platform ingests data from any digital source and transforms it using our core features, allowing you to fully understand your system, analyze that data efficiently, and respond to incidents before they become problems.</p> <p>To get started with Coralogix, sign up for a free account and import your system\u2019s telemetry. Once data is ingested by our platform, you can use our core features to obtain full observability in your Coralogix dashboard.</p> <p></p>"},{"location":"newoutput/guide-first-steps-coralogix/#sign-up-for-a-free-account","title":"Sign Up for a Free Account","text":"<p>Coralogix offers free account setup. If you don\u2019t already have an account, you can sign up\u00a0here. You will be prompted to create a new team.</p> <p></p> <p>If your organization already has an account, you may have different signup options depending on the permissions set by your organization administrator.</p>"},{"location":"newoutput/guide-first-steps-coralogix/#send-data-to-coralogix","title":"Send Data to Coralogix","text":"<p>Coralogix supports logs, metrics, and traces from many different sources using any of the following integrations. All integrations require:</p> <ul> <li> <p>Your Coralogix Send-Your-Data API key</p> </li> <li> <p>An endpoint associated with your Coralogix account domain</p> </li> <li> <p>Application and subsystem names to organize the data in your Coralogix account</p> </li> </ul>"},{"location":"newoutput/guide-first-steps-coralogix/#integration-packages","title":"Integration Packages","text":"<p>The easiest method for sending us your data is using our two-step, out-of-the box integration packages.</p> <p></p>"},{"location":"newoutput/guide-first-steps-coralogix/#integrations-interactive","title":"Integrations (Interactive)","text":"<p>For those integrations which are yet to be packaged, select a shipper for which to send us your data from our full list of integrations.</p> <ul> <li> <p>Cloud-Based Integrations. We offer a wide range of cloud-based shippers, including AWS, Azure, and GCP integrations.</p> </li> <li> <p>Telemetry Shippers. Choose from our many shippers, including integrations using OpenTelemetry, Prometheus, Fluentd and Fluent Bit.</p> </li> <li> <p>Push &amp; Pull Integrations. Choose from our list of push and pull integrations, including Cloudflare, Nagios, and Okta.</p> </li> <li> <p>Use-Cases. Select a use-case integration on the basis of the particular logs, metrics, or traces you\u2019d like to send us.</p> </li> </ul> <p></p>"},{"location":"newoutput/guide-first-steps-coralogix/#coralogix-apis","title":"Coralogix APIs","text":"<p>Optimize Coralogix's observability monitoring and unlock its most powerful features by using our wide range of APIs. Use them to send data to Coralogix, build visualizations, manage your data, and query it.</p> <ul> <li> <p>Data Ingestion APIs. Data is ingested seamlessly and reliably into the Coralogix platform using a wide range of APIs.</p> </li> <li> <p>Data Management APIs. Configure the Coralogix platform, customize your user interface, and optimize it for your observability requirements.</p> </li> <li> <p>Data Query APIs. Use these APIs to access and query your data.</p> </li> </ul> <p>Our APIs can be configured using our Helm charts, Terraform modules, or the Coralogix Operator.</p>"},{"location":"newoutput/guide-first-steps-coralogix/#get-familiar-with-coralogix-features","title":"Get Familiar with Coralogix Features","text":"<p>Once you have started sending us your data, Coralogix offers a rich bank of extensions to enrich your data with a set of predefined items \u2013 alerts, parsing rules, dashboards, saved views, actions, and more. Take our features tour to better understand these concepts and kick-start your observability monitoring process.</p> <p></p>"},{"location":"newoutput/guide-first-steps-coralogix/#coralogix-academy","title":"Coralogix Academy","text":"<p>Dive into Coralogix and discover the platform\u2019s vast capabilities tailored for both new and experienced users. Our newest Coralogix Academy Course will introduce you to Coralogix\u2019s main features and functionalities, so that you\u2019re ready to utilize the platform effectively. Through straightforward lesson plans, interactive sessions and practical exercises, you\u2019ll gain a complete understanding of how to turn Coralogix into an asset for all your observability operations.</p>"},{"location":"newoutput/guide-first-steps-coralogix/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up. Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/horizontal-bar-charts/","title":"Horizontal Bar Charts","text":"<p>The horizontal bar chart widget in Coralogix offers a new way to view your data. With horizontal bar charts, your data is sorted by value, in descending order by default. With vertical bar charts, data is usually sorted alphabetically by column name.</p>"},{"location":"newoutput/horizontal-bar-charts/#create-a-horizontal-bar-chart","title":"Create a Horizontal Bar Chart","text":"<p>Create a customized horizontal bar chart visualization.</p> <p>STEP 1. Drag and drop the Horizontal Bar Chart widget from the left-hand sidebar to get started.</p> <p></p> <p>STEP 2. Set the definitions for your Bar Chart in the right-hand sidebar.</p> <ul> <li> <p>Name &amp; Description. Create a name and description.</p> </li> <li> <p>Load data from. Select whether to load data from Frequent Search or Monitoring.</p> </li> </ul> <p></p> <ul> <li> <p>Source. Select a data source.</p> <ul> <li> <p>If the Source chosen is metrics, specify the metric or desired PromQL in the Query field. Use free text to search for a metric of your choice. As you do so, all relevant metrics will appear. Hover over any metric to view its system-generated metadata labels. Hover over a label to see its values.</p> </li> <li> <p>When creating a bar chart with metrics as the Source, the categories specified in the PromQL query appear automatically in the Group By field. Within the Group By field, reordering the categories by dragging and dropping is possible.</p> </li> <li> <p>Drag and drop categories from the Category field into the Stacking field, to stack by a particular category.</p> </li> </ul> </li> <li> <p>ADD FILTER. [Optional] Add a filter to your bar chart.</p> <ul> <li> <p>As opposed to the dashboard filter in the left-hand sidebar which affects the entire dashboard, this filter only affects the widget.</p> </li> <li> <p>The widget and dashboard filters operate in parallel to one another and intersect. If they negate one another, dashboard filters override widget filters.</p> </li> </ul> </li> <li> <p>Category. Select the fields by which you want to sort your bar chart from the dropdown menu.</p> </li> <li> <p>Aggregation: Aggregate by Count, Count Distinct, Sum, Min, Max, and/or Average.</p> <ul> <li> <p>In Bar charts, changing the aggregation type will change the type of data you see.</p> </li> <li> <p>For example, aggregating by Count, might show you the number of people in a country. On the other hand when aggregating by Average, for example, you need to provide additional parameters, such as height, which will give you a bar chart displaying the average height by country.</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Stack By. [Optional] Select a field by which to stack the chart. This shows you a second layer of data on the chart.</p> </li> <li> <p>Advanced. Select from the following advanced options.</p> <ul> <li> <p>Y AXIS View. Select whether the Y-axis of the chart should show the value or the category.</p> </li> <li> <p>Color Scheme. Select the color scheme for your chart.</p> </li> <li> <p></p> </li> <li> <p>Legend Colors By. Select whether you want your legend colors to be by aggregation or by category. By default the horizontal bar chart is by aggregation, which means a single color will be shown for all bars.</p> </li> <li> <p>Scale. Select whether you want the scale of the bar chart to be Logarithmic or Linear. The default setting is linear, however if you have large differences between the different values, it can be helpful to show the logarithmic scale instead. For example, if the majority of your values are under 1k and one value is 10k, using the logarithmic scale will show you an easier to read bar chart than the linear scale.</p> </li> <li> <p>Sort By. Select whether to sort the chart by column name or by value. By default the horizontal bar chart is sorted by value.</p> </li> <li> <p>Max Bars Per Graph. Select the maximum number of bars you want to show per graph.</p> </li> <li> <p>Group Name. [Optional] Customize the displayed group name.</p> </li> <li> <p>Unit. [Optional] Select the unit to display in the bar chart.</p> </li> </ul> </li> </ul> <p>STEP 3. [Optional] If you want to save your dashboard for future use, click SAVE in the upper right hand corner.</p>"},{"location":"newoutput/horizontal-bar-charts/#additional-resources","title":"Additional Resources","text":"DocumentationCustom DashboardsLine ChartsData TablesGaugesPie ChartsVertical Bar Charts"},{"location":"newoutput/horizontal-bar-charts/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/hosted-grafana-view/","title":"Hosted Grafana View","text":"<p>With Coralogix\u2019s cutting-edge visualization capabilities, you can instantly switch between views to see and better understand your data.</p> <p>You can now visualize your data using Coralogix's Hosted Grafana without having to integrate a personal Grafana instance with your Coralogix account.</p> <p>Use Grafana to visualize your logs and metrics with all the functions you are already familiar with. You will be able to access all Coralogix data sources automatically without the need for any plugins.</p> <p>On the right side of the top menu, click the open-source option and choose Grafana.</p> <p></p> <p>Grafana\u2019s main screen is displayed as part of Coralogix, where you can Search, Create and Edit your dashboards. Explore and navigate between Grafana pages.</p> <p></p> <p>You can query your logs and metrics in real-time to get all the insight you need.</p> <p>Be sure to select the correct data source when creating a panel: Logs or Metrics.</p> <p></p> <p>Logs are queried using Lucene (the same query syntax of the Coralogix Logs screen)</p> <p>Metrics should be queried with PromQL. For more information and \u2018How To\u2019 - visit this blog post with our top PromQL tips.</p> <p></p> <p>Note: Coralogix's Hosted Grafana instance(s) are currently running Grafana version 8.2.7</p> <p>Some Limitations:</p> <ul> <li> <p>Alerting is disabled</p> </li> <li> <p>Grafana Live Dashboard is disabled</p> </li> <li> <p>Dashboard minimum interval is 10 seconds</p> </li> <li> <p>Grafana Editor - Coralogix admin</p> </li> <li> <p>Grafana Viewer - All users</p> </li> </ul>"},{"location":"newoutput/hosted-opensearch-view/","title":"Hosted OpenSearch View","text":"<p>Visualize your data using Coralogix\u2019s Hosted OpenSearch without having to integrate your enterprise instance of OpenSearch with your Coralogix account.</p>"},{"location":"newoutput/hosted-opensearch-view/#overview","title":"Overview","text":"<p>With Coralogix\u2019s cutting-edge visualization capabilities, you can instantly switch between views to see and better understand your data.</p> <p>You can now visualize your data using Coralogix\u2019s Hosted OpenSearch without having to integrate your enterprise instance of OpenSearch with your Coralogix account.</p> <p>OpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, and observability applications licensed under Apache 2.0. Powered by\u00a0Apache Lucene\u00a0and driven by the\u00a0OpenSearch Project community, OpenSearch offers a vendor-agnostic toolset you can use to build secure, high-performance, cost-efficient applications.</p>"},{"location":"newoutput/hosted-opensearch-view/#get-started","title":"Get Started","text":"<p>STEP 1. Click on the bento menu in the upper-right hand corner of your Coralogix dashboard. Select OpenSearch in the dropdown menu.</p> <p></p> <p>STEP 2. The OpenSearch home screen is displayed. From this screen, navigate to Overview, Discover, Dashboard or Visualization pages by clicking on the three stacked horizontal lines on the top left corner of a screen.</p> <p></p> <p>STEP 3. Query application, system, security and cloud service logs in real-time using Lucene syntax language to get the insights you need using OpenSearch.</p> <p></p>"},{"location":"newoutput/hosted-opensearch-view/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/how-to-connect-a-wazuh-agent-to-the-sta/","title":"How to connect a Wazuh agent to the STA","text":"<p>Wazuh, a fork of the famous OSSEC project, is an agent-based solution for the detection of malicious activity at the host level. It can detect rootkits, malicious processes running on the host, and many other types of malicious network activities. The Coralogix STA can function as a Wazuh manager, allowing Wazuh agents to connect to it, pull policies from it, and forward their logs to it. These logs will be enriched and forwarded to Coralogix for further processing.</p> <p>Once you have successfully installed the STA in your environment, you can configure Wazuh agents to connect to it by completing the following steps:</p> <ol> <li> <p>Find out the relevant Wazuh NLB name (relevant only for cloud installations):</p> <ol> <li> <p>If you have installed the STA by using CloudFormation, open your AWS CloudFormation console and locate the stack you have deployed. In the list of the resources that were deployed (in the \"Resources\" tab) locate the ID of the \"WazuhNLB\" that was deployed (should be something like 'arn:aws:elasticloadbalancing:eu-west-1:746123456762:loadbalancer/net/STA-N-Wazuh-UJZ0XTYPZR41/1773e22e13f084de')</p> </li> <li> <p>If you have installed the STA by using Terraform, run the following command in the folder you applied the Terraform template from: <code>terraform state show 'module.sta_ng.module.sta_ng-spotfleet-small-wazuh1-eip1[0].aws_lb.WazuhNLB'</code>. Take note of the id of the NLB. (should be something like 'arn:aws:elasticloadbalancing:eu-west-1:746123456762:loadbalancer/net/STA-N-Wazuh-UJZ0XTYPZR41/1773e22e13f084de')</p> </li> </ol> </li> <li> <p>Find out the relevant Wazuh NLB DNS name:</p> <ol> <li> <p>If you have installed the STA using either CloudFormation or Terraform, open your AWS EC2 console and navigate to the Load Balancers section and then search for the value you took note of at the previous step. Copy the DNS name of that load balancer (Should be something like STA-N-Wazuh-UXYZXYZXYR41-1234e12e12f064ed.elb.eu-west-1.amazonaws.com)</p> </li> <li> <p>If you are using the STA in an on-prem environment just use the STA's host name</p> </li> </ol> </li> <li> <p>WAZUH_MANAGER environment variable is the value you took a note of at the previous step (the STA's Wazuh NLB DNS name). make sure to set <code>WAZUH_MANAGER</code> when encountered in next steps.</p> </li> <li> <p>Install the Wazuh agent on the relevant machines:</p> </li> <li> <p>In case you mirroring traffic using Virtual Tap and <code>Wazuh</code> wasn't disabled, skip this step</p> </li> <li> <p>In case you mirroring traffic using Virtual Tap and <code>Wazuh</code> was disabled, update docker command with the following code (for additional information regarding <code>CONFIGURATION_S3</code> and <code>TAP_INTERFACE</code> see Virtual Tap's URL):</p> </li> </ol> <pre><code>docker run -d \u2013name sta-wazuh_manager \\\n-e \"WAZUH_MANAGER=&lt;EXTRACTED_WAZUH_NLB_DNS_NAME&gt;\" \\\n-e \u201cSTA_SNIFFING_NLB=s3://&lt;CONFIGURATION_S3&gt;\u201d \\\n-e \u201cTAP_INTERFACE=&lt;TAP_INTERFACE&gt;\u201d \\\n-e \u2018STA_SNIFFING_FILTER=not dst port 4789\u2019 \\\n\u2013privileged \u2013net host coralogixrepo/sta-virtual-tap-docker\n</code></pre> <ul> <li>To install using Virtual Tap installation but only <code>Wazuh</code> without mirroring traffic:</li> </ul> <pre><code>docker run -d \u2013name sta-wazuh_manager \\\n-e \"WAZUH_MANAGER=&lt;ADD_EXTRACTED_WAZUH_NLB_DNS_NAME&gt;\" \\\n-e \"STA_DISABLE_TAP=TRUE\" \\\n\u2013privileged \u2013net host coralogixrepo/sta-virtual-tap-docker\n</code></pre> <ul> <li> <p>To install <code>Wazuh</code> using bare metal installation, run the following code:</p> <ul> <li> <p>in AWS/AZURE this should be set as the instances' user-data</p> </li> <li> <p>in GCP this should be set as the instance's startup script</p> </li> </ul> </li> </ul> <p>Ubuntu/Debian:</p> <pre><code>#!/bin/bash \n\nexport WAZUH_MANAGER=\"&lt;ADD_EXTRACTED_WAZUH_NLB_DNS_NAME&gt;\" \n\n\nwget https://packages.wazuh.com/4.x/apt/pool/main/w/wazuh-agent/wazuh-agent_4.3.10-1_amd64.deb\nsudo -E dpkg -i wazuh-agent_4.3.10-1_amd64.deb\necho 'wazuh_command.remote_commands=1' | sudo tee -a /var/ossec/etc/local_internal_options.conf\necho 'logcollector.remote_commands=1' | sudo tee -a /var/ossec/etc/local_internal_options.conf\necho 'sca.remote_commands=1' | sudo tee -a /var/ossec/etc/local_internal_options.conf\nsudo mkdir -p /wazuh-custom-commands    \necho 'IyEvYmluL2Jhc2gKCmRmIC1oIHwgZ3JlcCAtdiAnXi9kZXYvbG9vcFswLTldJyB8IHdoaWxlIElGUz0gcmVhZCAtciBsaW5lOwpkbwogIGVjaG8gImRpc2stdXNhZ2U6ICIkbGluZQpkb25lCg==' | base64 -d | sudo tee /wazuh-custom-commands/custom-df.sh    \necho 'IyEvYmluL2Jhc2gKCnBzIC1lZmwgfCB3aGlsZSBJRlM9IHJlYWQgLXIgbGluZTsKZG8KICBlY2hvICJwcm9jZXNzZXMtbGlzdDogIiRsaW5lCmRvbmUK' | base64 -d | sudo tee /wazuh-custom-commands/custom-ps.sh    \nsudo chmod +x /wazuh-custom-commands/custom-df.sh    \nsudo chmod +x /wazuh-custom-commands/custom-ps.sh\nsudo systemctl daemon-reload\nsudo systemctl enable wazuh-agent.service\nsudo service wazuh-agent start\nsleep 60\nsudo service wazuh-agent restart\n\n</code></pre> <p>RedHat:</p> <pre><code>#!/bin/bash \n\nexport WAZUH_MANAGER=\"&lt;ADD_EXTRACTED_WAZUH_NLB_DNS_NAME&gt;\" \n\n\nsudo rpm --import https://packages.wazuh.com/key/GPG-KEY-WAZUH    \nsudo echo [wazuh] &amp;gt; /etc/yum.repos.d/wazuh.repo     \nsudo echo gpgcheck=1 &amp;gt;&amp;gt; /etc/yum.repos.d/wazuh.repo  \nsudo echo gpgkey=https://packages.wazuh.com/key/GPG-KEY-WAZUH &amp;gt;&amp;gt; /etc/yum.repos.d/wazuh.repo  \nsudo echo enabled=1  &amp;gt;&amp;gt; /etc/yum.repos.d/wazuh.repo  \nsudo echo name=EL-Wazuh &amp;gt;&amp;gt; /etc/yum.repos.d/wazuh.repo  \nsudo echo baseurl=https://packages.wazuh.com/4.x/yum/  &amp;gt;&amp;gt; /etc/yum.repos.d/wazuh.repo  \nsudo echo protect=1  &amp;gt;&amp;gt; /etc/yum.repos.d/wazuh.repo  \nsudo -E yum install wazuh-agent audit -y \necho 'wazuh_command.remote_commands=1' | sudo tee -a /var/ossec/etc/local_internal_options.conf    \necho 'logcollector.remote_commands=1' | sudo tee -a /var/ossec/etc/local_internal_options.conf\necho 'sca.remote_commands=1' | sudo tee -a /var/ossec/etc/local_internal_options.conf    \nsudo mkdir -p /wazuh-custom-commands    \necho 'IyEvYmluL2Jhc2gKCmRmIC1oIHwgZ3JlcCAtdiAnXi9kZXYvbG9vcFswLTldJyB8IHdoaWxlIElGUz0gcmVhZCAtciBsaW5lOwpkbwogIGVjaG8gImRpc2stdXNhZ2U6ICIkbGluZQpkb25lCg==' | base64 -d | sudo tee /wazuh-custom-commands/custom-df.sh    \necho 'IyEvYmluL2Jhc2gKCnBzIC1lZmwgfCB3aGlsZSBJRlM9IHJlYWQgLXIgbGluZTsKZG8KICBlY2hvICJwcm9jZXNzZXMtbGlzdDogIiRsaW5lCmRvbmUK' | base64 -d | sudo tee /wazuh-custom-commands/custom-ps.sh    \nsudo chmod +x /wazuh-custom-commands/custom-df.sh    \nsudo chmod +x /wazuh-custom-commands/custom-ps.sh    \nsudo systemctl daemon-reload    \nsudo systemctl enable wazuh-agent    \nsudo systemctl start wazuh-agent\n\n</code></pre> <p>Windows (Powershell):</p> <pre><code>Invoke-WebRequest -Uri https://packages.wazuh.com/4.x/windows/wazuh-agent-4.3.0-1.msi -OutFile wazuh-agent.msi -UseBasicParsing `\nwazuh-agent.msi /quiet WAZUH_MANAGER=&lt;ADD_EXTRACTED_WAZUH_NLB_DNS_NAME&gt; `\nStart-Sleep -Seconds 30 `\nStart-Service -ServiceName WazuhSvc\n</code></pre> <ol> <li>Connect to the STA via SSH using the key pair you specified during the installation of the STA.</li> <li>Run the command <code>sta-wazuh-list-agents</code>. You should get something like this with the hostname of the monitored instance: <code>Available agents:   ID: 001, Name: ip-192-168-1-2, IP: any</code></li> <li>You should be able to see logs from the monitored instance that indicate interesting actions that took place on the monitored instance as recorded by Wazuh.</li> <li>You can review the data from Wazuh by using the Wazuh alerts and Wazuh file integrity monitor dashboards:</li> </ol>   ![](images/Screenshot-from-2021-12-12-14-43-06-1024x509.png)    Wazuh Alerts Overview     ![](images/Screenshot-from-2021-12-12-14-43-30-1024x507.png)    Processes on Wazuh agents     ![](images/image-2-1024x494.png)    File Integrity Monitor   <p>Good luck (:</p> <p>If you have any questions or need any additional help, please contact our support team via our 24/7 in-app chat!</p>"},{"location":"newoutput/how-to-install-coralogix-sta/","title":"How to install Coralogix STA","text":"<p>The Coralogix STA (Security Traffic Analyzer) is a tool by Coralogix for deep packet inspection, packet capturing, cloud configuration vulnerability scanning, and more.</p> <p>For additional information see the introduction doc.</p>"},{"location":"newoutput/how-to-install-coralogix-sta/#the-sta-can-be-installed-using-the-following-methods","title":"The STA can be installed using the following methods","text":"<ol> <li> <p>CloudFormation Template</p> <ul> <li>As this an AWS service, this installation only for AWS</li> </ul> </li> <li> <p>Terraform Template</p> <ul> <li>Available for AWS and Azure</li> </ul> </li> <li> <p>OVA image</p> </li> </ol> <p>In addition, STA can be installed in a limited internet access environment.</p>"},{"location":"newoutput/how-to-install-coralogix-sta/#pre-requisites","title":"Pre-requisites","text":"<p>Before you install the STA please make sure the following requests are met:</p>"},{"location":"newoutput/how-to-install-coralogix-sta/#aws","title":"AWS","text":"<ol> <li> <p>Configuration is saved using an AWS S3 bucket. It is recommended to use a dedicated bucket, if you won\u2019t define one, the STA will generate a bucket.</p> <ul> <li>Have an empty S3 bucket for holding the configuration.</li> </ul> </li> <li> <p>You have permissions to deploy EC2 instances, spot fleets, load-balancers and security groups in the AWS account you plan to deploy the STA in.</p> </li> <li> <p>Instances that you plan to mirror their traffic by using the VPC traffic mirroring feature belong to one of the following types:</p> <ul> <li><code>C4, D2, G3, G3s, H1, I3, M4, P2, P3, R4, X1, X1e, A1, C5, C5d, C5n, I3en, M5, M5a, M5ad, M5d, p3dn.24xlarge, R5, R5a, R5ad, R5d, T3, T3a, and z1d.</code></li> </ul> </li> <li> <p>If you are looking to monitor instances by using our Virtual Tap, make sure you can run privileged containers in that environment (for example in AWS FarGate you cannot do that) - to read more about the Virtual Tap, see this doc.</p> </li> </ol>"},{"location":"newoutput/how-to-install-coralogix-sta/#azure","title":"Azure","text":"<ol> <li> <p>Configuration is saved using an Azure\u2019s Storage Accounts service. It is recommended to use a dedicated container, so the configuration will be saved outside of the STA and can be modified/restored later</p> </li> <li> <p>Have an empty container for holding the configuration</p> </li> <li> <p>You have permissions to deploy VM instances, and create resources, in the Azure\u2019s account you plan to deploy the STA in</p> </li> <li> <p>mirroring instances\u2019 traffic can be performed only by using our Virtual Tap. To read more about the Virtual Tap, see this doc</p> </li> </ol>"},{"location":"newoutput/how-to-install-coralogix-sta/#deployment","title":"Deployment","text":""},{"location":"newoutput/how-to-install-coralogix-sta/#cloudformation-template","title":"CloudFormation Template","text":"<ol> <li> <p>Connect to your AWS account and on another tab, login to your Coralogix account</p> </li> <li> <p>From Coralogix UI, go to the Settings page and then to the Cloud Security tab</p> </li> <li> <p>Click \"Deploy Security Service\"</p> </li> <li> <p>From the top drop-down list named \"Deployment method\", choose the option \"CloudFormation\" (should already be selected)</p> </li> <li> <p>Fill in the various fields on the form and click \"Launch AWS CloudFormation\":</p> <ol> <li> <p>Set the CloudFormation's stack name (The default is \"CoralogixSecurity\")</p> </li> <li> <p>Optionally, fill in the name of an S3 bucket that will be used for storing the STA's configuration</p> </li> <li> <p>Optionally, configure the STA to use an encrypted disk</p> </li> <li> <p>Select the SSH key pair that will be used to connect to the STA</p> </li> <li> <p>Select the security group that will be assigned to the management network interface</p> </li> <li> <p>Optionally, fill in the name of an S3 bucket that will be used for storing the packets captured by the STA as compressed PCAP files</p> </li> <li> <p>If you chose to run the STA as a spot, you can set the maximum spot price here</p> </li> <li> <p>Select the subnet you'd like to run the STA in. Make sure that the security group you chose for the management interface belongs to this subnet. Otherwise the installation will fail</p> </li> <li> <p>Select the VPC you'd like to run the STA in. Make sure that the subnet you selected belongs to this VPC</p> </li> <li> <p>Tick the box below that says \"I acknowledge that AWS CloudFormation might create IAM resources.\" and click \"Create stack\"</p> </li> </ol> </li> </ol>"},{"location":"newoutput/how-to-install-coralogix-sta/#terraform-template","title":"Terraform Template","text":""},{"location":"newoutput/how-to-install-coralogix-sta/#aws-prerequisites","title":"AWS - Prerequisites","text":"<ol> <li> <p>Connect to your AWS account and on another tab, login to your Coralogix account</p> </li> <li> <p>From Coralogix UI, go to the Settings page and to the Cloud Security tab</p> </li> <li> <p>Click \"Deploy Security Service\"</p> </li> <li> <p>From the top drop-down list named \"Deployment method\", choose the option \"Terraform Template\"</p> </li> <li> <p>Click \"Launch tutorial\"</p> </li> </ol>"},{"location":"newoutput/how-to-install-coralogix-sta/#deployment-steps","title":"Deployment Steps","text":"<ol> <li> <p>Create an empty folder somewhere on your computer</p> </li> <li> <p>Download the terraform module files</p> <ul> <li> <p>AWS - download content from the following link</p> </li> <li> <p>Azure - download content from the following link</p> </li> <li> <p>For older versions, follow this link and download the desired version</p> </li> </ul> </li> <li> <p>Create file <code>values.auto.tfvars</code></p> <ul> <li> <p>Fill in the required variables in the values file</p> </li> <li> <p>for the explanation of variable types and expected content see comments from downloaded content</p> </li> </ul> </li> <li> <p>Run the command <code>terraform init</code> from the same folder</p> </li> <li> <p>Run the command <code>terraform plan</code> and examine the changes that are going to be applied to your environment</p> </li> <li> <p>Run the command <code>terraform apply</code> from the same folder and approve the changes</p> </li> </ol>"},{"location":"newoutput/how-to-install-coralogix-sta/#ova-file","title":"OVA File","text":"<ol> <li> <p>You can download the OVA file from the following links based on the environment you would like to use them at:</p> <ol> <li> <p>VirtualBox: https://coralogix-integrations.s3-eu-west-1.amazonaws.com/cloud-security/sta-ng.virtualbox.ova</p> </li> <li> <p>VMware ESXi: https://coralogix-integrations.s3-eu-west-1.amazonaws.com/cloud-security/sta-ng.vmware.ova</p> </li> </ol> </li> <li> <p>Once the file is downloaded, import the VM into the relevant environment and start it</p> </li> <li> <p>After the VM has finished loading, login to the VM with the user 'ubuntu' and the password 'Coralogix-STA!'</p> </li> <li> <p>Automatically, once the user is logged on, a series of questions will be presented. Please answer all of them with all the relevant information</p> </li> <li> <p>Run the command <code>passwd</code> and change the default password of the ubuntu user</p> </li> </ol>"},{"location":"newoutput/how-to-install-coralogix-sta/#sta-deployment-in-limited-internet-access-environments","title":"STA Deployment In Limited Internet Access Environments","text":"<p>STA requires access to S3 for its config files. In some environments Internet outbound access is required to be limited to specific IPs, which means no access to public S3 will be available. In order to allow connectivity using amazon private network - Set a designated\u00a0VPC gateway endpoint\u00a0that connects your VPC directly to Amazon S3.</p> <p>* Make sure your VPC's route table contains Coralogix\u2019s endpoints.</p> <p>* In addition, in such environments the following enrichment services will not work: <code>dns-rbls, unshorten-url, nist-cpe</code>, also updates to <code>Suricata</code> service will fail.</p>"},{"location":"newoutput/how-to-install-coralogix-sta/#next-steps","title":"Next Steps","text":"<p>After installing the STA, you can move forward in one of the following ways (or all of them) to get the most out of your newly installed STA:</p> <ol> <li> <p>Configure VPC traffic mirroring to allow the STA to analyze raw traffic. For this use the following tutorials: How to automate VPC Mirroring for Coralogix STA, Guide: Smarter AWS Traffic Mirroring for Stronger Cloud Security</p> </li> <li> <p>Deploy Wazuh agents in selected instances to get insights into the processes running inside them. For this use the following tutorial: How to connect a Wazuh agent to the STA</p> </li> <li> <p>Review alerts configured and modify them to be more accurate for your organization. You can find more about it in these tutorials: Security Traffic Analyzer (STA) Alerts, Alerts API</p> </li> <li> <p>Run the command <code>sta-get-installation-id</code> and copy the uuid that is displayed on the screen and save it in a safe place. This key is required to login to the STA with administrative privileges which might be needed as part of a troubleshooting session.</p> </li> <li> <p>Once the installation ID is safely stored and properly backed-up, run the command <code>sta-acknowledge-installation-id</code> and carefully follow the instructions on the screen to remove the installation ID from the STA</p> </li> </ol> <p>If you have any questions or need any additional help, please contact our support team via our 24/7 in-app chat!</p>"},{"location":"newoutput/how-to-protect-secrets-in-the-sta-config/","title":"How to protect secrets in the STA config?","text":"<p>Several STA customers have asked us to provide a mechanism for securely storing secrets in the STA config. Some of them said that they would like to prevent users that use the STA from seeing the Coralogix Send-Your-Data API key for example. Other customers were wondering whether it would be possible to share several configuration values between multiple STA installations without having to share all of the configuration or manually synchronize the configuration between the instances. The following steps will help you address both concerns.</p> <p>The STA supports the following two configuration storing methods:</p> <ol> <li> <p>Locally stored configuration - The configuration is stored and managed locally on the STA instance. This is the default method which is recommended for small POCs or test environments.</p> </li> <li> <p>Storing configuration on an S3 bucket - The configuration is stored on an S3 bucket and periodically synchronized to the STA. This is the recommended method for production deployments.</p> </li> </ol> <p>In both methods the STA now supports the following mechanisms for mechanisms for storing configuration values securely:</p> <ol> <li> <p>As a reference to an AWS Secrets Manager secret - That allows you to store the secret value as an AWS secret, add a permission to the STA's role to access the new secret.</p> </li> <li> <p>As a value encrypted by the STA - That allows you to encrypt the secret value by using STA internal mechanisms which use AES256 and a unique key to encrypt the value.</p> </li> </ol> <p>You can now mix and match these as needed. For example, you can store the STA configuration on an S3 bucket and have some of the values stored as secrets in AWS Secrets Manager and encrypt (some or all of them) by using the STA's encryption. Here is how to do it:</p>"},{"location":"newoutput/how-to-protect-secrets-in-the-sta-config/#storing-the-configuration-on-s3","title":"Storing the configuration on S3","text":"<p>It is possible to provide an S3 bucket name at the CloudFormation/Terraform template, that way, the STA will start by copying the configuration to that bucket (if it is empty) or downloading the configuration from it and applying it automatically.</p> <p>If you want to configure an existing STA that uses local configuration (the default) to use an S3 bucket to hold its configuration, here are the steps you need to take:</p> <ol> <li> <p>Login to the STA by using SSH</p> </li> <li> <p>Run the command <code>sta-edit-config</code></p> </li> <li> <p>If this is the first time you are doing this, you'll probably be asked to choose a configuration editor. Choose one from the list presented by typing its corresponding number</p> </li> <li> <p>Find the field \"sync_config_from\" and set its value to the bucket name with <code>s3://</code> as prefix. For example: <code>\"sync_config_from\": \"s3://my-sta-config-bucket\"</code>,</p> </li> <li> <p>Exit the editor while saving your changes</p> </li> <li> <p>You'll be presented with the following question. Just answer \"y\": <code>NOTE: The new config file includes a configuration to use an S3 bucket. If you'll choose to continue, any additional configuration from now on will have to be done using the configured bucket. Are you sure you want to continue?</code></p> </li> <li> <p>If all goes well, the following line should be prompted to indicate that the new configuration has been applied: <code>Configuration updated successfully.</code></p> </li> </ol>"},{"location":"newoutput/how-to-protect-secrets-in-the-sta-config/#storing-configuration-value-as-an-aws-secrets","title":"Storing configuration value as an AWS secrets","text":"<p>To strengthen the security of the STA, it is now possible to configure the STA to store any of the values for the various configuration settings as secrets stored in AWS Secrets Manager. Here are the steps you need to take to get it done:</p> <ol> <li> <p>Install an STA as you normally would and verify it is working correctly</p> </li> <li> <p>Login to the STA by using SSH</p> </li> <li> <p>Run the command <code>sta-get-status-short</code> and verify that all services mentioned in the output are in any of the following statuses only: \"OK\", \"NOT_RUNNING_NOW\" or \"RUNNING_NOW\"</p> </li> <li> <p>Login to your AWS console and go to the AWS Secrets Manager</p> </li> <li> <p>Create a secret for storing the configuration value or values that you want to store as a secret. The secret can either be a plaintext value that contains a single configuration value you want to store as a secret or a JSON key/value dictionary that contains multiple configuration values you want to store as a secret.</p> </li> <li> <p>Modify the IAM role attached to the STA to allow access to this secret's ARN by either attaching an existing policy or creating an inline policy</p> </li> <li> <p>Login to the STA by using SSH</p> </li> <li> <p>Run the command <code>sta-edit-config</code></p> </li> <li> <p>Set the value for the configuration key you want to a value that matches the following pattern: <code>${{&lt;aws_secret_region_name&gt;;&lt;aws_secret_arn&gt;[;&lt;secret_keyvalue_keyname&gt;]}}</code>     For example, here is a value for a key-value type of secret that has a key named \"private_key\" that we would like to use: <code>${{eu-west-1;arn:aws:secretsmanager:eu-west-1:746543792062:secret:test-sta-od-encrypt-secrets-RfAkDs;private_key}}</code>     Another example, is a value for a plain-text type of secret that is expected to contain the entire configuration value:     ${{eu-west-1;arn:aws:secretsmanager:eu-west-1:746543792062:secret:test-sta-od-encrypt-secrets-RfAkDs}}</p> </li> </ol>"},{"location":"newoutput/how-to-protect-secrets-in-the-sta-config/#storing-configuration-values-as-encrypted-values","title":"Storing configuration values as encrypted values","text":"<p>To strengthen the security of the STA even more, it is now possible to configure the STA to store any of the values for the various configuration settings as encrypted values that can only be decrypted by the root user on the STA. It is possible to store these encrypted values anywhere one can store STA configuration: local, S3 and AWS Secrets Manager. Currently encrypted values are only supported on \"on-demand\" and on-prem STA installations. This is something that we plan to fix on future STA versions. Here are the steps you need to take to get it done:</p> <ol> <li> <p>Install an STA as you normally would and verify it is working correctly</p> </li> <li> <p>Login to the STA by using SSH</p> </li> <li> <p>Run the command <code>sta-get-status-short</code> and verify that all services mentioned in the output are in any of the following statuses only: \"OK\", \"NOT_RUNNING_NOW\" or \"RUNNING_NOW\"</p> </li> <li> <p>Run the command <code>sta-encrypt-config-value</code> and enter the value you want to encrypt and then hit Ctrl+D (If you need to abort just click Ctrl+C)</p> </li> <li> <p>The result should be something similar to this: <code>${{b64:enc:ASSDDSFsdfdsfSDFDSDSFDSFsdfdsf12322DFFD==}}</code></p> </li> <li> <p>Copy this result and paste it as the value of an AWS secret in AWS Secrets Manager or as a value of the relevant configuration setting in the sta.conf that is stored on the S3 bucket or as a value of the relevant local configuration setting directly by using the command <code>sta-edit-config</code></p> </li> <li> <p>If you changed the value of a secret or the config that is stored on the S3 bucket it may take up to 3-4 minutes for it to be automatically applied.</p> </li> </ol> <p>This feature, in addition to being a security related feature can also be used to simplify the configuration of multiple STA instances. You can create an AWS secret for several configuration values and then use the same secret reference in multiple STA instances and set the permissions to the STA instances correctly. That way you are essentially sharing configuration values between multiple STA instances while storing them only once.</p> <p>We hope you found this guide helpful. If you have any further questions, don't hesitate to contact us via the chat.</p>"},{"location":"newoutput/how-to-protect-secrets-in-the-sta-config/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/ignore-errors/","title":"Ignore Errors","text":"<p>As part of Error Tracking, we understand that not all errors are created equal. Some errors may be inconsequential, recurring, or known issues that do not warrant immediate attention. To enhance your Real User Monitoring experience, we provide a feature that allows you to selectively ignore errors. This functionality empowers you to focus on the critical issues that truly impact user experience and streamline your error tracking process.</p>"},{"location":"newoutput/ignore-errors/#ignoring-errors-benefits","title":"Ignoring Errors: Benefits","text":"<p>Enjoy the following feature benefits:</p> <ul> <li> <p>Noise Reduction. In a dynamic web environment, it's common for non-critical errors to occur. Ignoring these errors helps reduce noise in your error tracking system, making it easier to pinpoint and address the most impactful issues.</p> </li> <li> <p>Customized Monitoring. With the ability to ignore specific error types, you can tailor your RUM setup to align with your priorities and goals. This ensures that you receive alerts and insights on errors that matter most to your business.</p> </li> <li> <p>Efficient Resource Utilization. By filtering out low-priority errors, you can optimize your resources and reduce the time and effort spent on investigating and resolving issues that have minimal impact on user satisfaction and business operations.</p> </li> </ul>"},{"location":"newoutput/ignore-errors/#getting-started","title":"Getting Started","text":"<p>Refer to our Browser SDK Installation Guide for Ignore Error setup instructions or get started by following these steps:</p> <ul> <li>Using Regex, you can set up the SDK to reject all request with a full or partial URL, as in the following example.</li> </ul> <pre><code>ignoreUrls: [\n    '&lt;https://my-server.com/user/123&gt;', // will ignore all requests match this url\n    /.*\\\\.svg/, // will ignore all requests match this regex - all svg files\n    /.*material-override.*/, // will ignore all requests match this regex - all urls with the string material-override\n  ],\n\n</code></pre> <ul> <li>You also have the option setup the SDK to reject all errors with a particular message or string, as in the following example.</li> </ul> <pre><code>ignoreErrors: [\n    'This is a custom error', // will ignore all errors with this message\n    /.*my-error.*/, // will ignore all errors match this regex - all errors with the string my-error\n  ],\n\n</code></pre>"},{"location":"newoutput/ignore-errors/#next-steps","title":"Next Steps","text":"<p>Get started with Error Tracking. Use our dedicated User Manual for support.</p>"},{"location":"newoutput/ignore-errors/#additional-resources","title":"Additional Resources","text":"DocumentationRUM SDK Installation Guide"},{"location":"newoutput/ignore-errors/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/import-export-custom-dashboards/","title":"Import & Export Custom Dashboards","text":"<p>Effortlessly share your custom dashboards within your organization by importing and exporting them, eliminating the need to recreate them and minimizing overhead.</p>"},{"location":"newoutput/import-export-custom-dashboards/#import-a-custom-dashboard","title":"Import a Custom Dashboard","text":"<p>STEP 1. Hover over + New Dashboard above the dashboard list.</p> <p>STEP 2. Click IMPORT.</p> <p></p> <p>STEP 3. In the popup that appears, select a JSON file to upload or paste your copied JSON file.</p> <p>STEP 4. Click IMPORT.</p>"},{"location":"newoutput/import-export-custom-dashboards/#export-a-custom-dashboard","title":"Export a Custom Dashboard","text":"<p>STEP 1. Click the Export button on the top right-hand side of the dashboard, or click on the ellipsis next to the dashboard name in the right-hand column and select EXPORT. The latter option allows you to export a dashboard not currently in use.</p> <p></p> <p>STEP 2. In the popup that appears, click EXPORT. Click on the copy icon to copy the dashboard JSON file.</p> <p></p>"},{"location":"newoutput/import-export-custom-dashboards/#additional-resources","title":"Additional Resources","text":"DocumentationCustom Dashboards"},{"location":"newoutput/import-export-custom-dashboards/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/incidents/","title":"Incidents","text":"<p>Our Incidents Screen displays all of your triggered alert events within the Coralogix platform. View all those events which are currently triggered or those triggered within a specific time frame. With easy-to-use functionalities and the ability to drill-down into events of interest, the feature ensures top-notch monitoring and analysis.</p>"},{"location":"newoutput/incidents/#overview","title":"Overview","text":"<p>The Coralogix Incidents Screen simplifies your alert response journey from start to finish. Perfect for DevOps teams and SREs looking to eliminate context switching, users can easily identify triggered alert events of interest and drill-down into the underlying logs, metrics, and traces all from within the Coralogix platform. Coralogix uniquely enables users to analyze any relevant archived data alongside events that triggered the current alert. This unified approach brings events and observability together for faster triage, analysis and remediation.</p>"},{"location":"newoutput/incidents/#benefits","title":"Benefits","text":"<p>Use the Incidents Screen to:</p> <ul> <li> <p>View all those alerts events which are currently triggered or those triggered within a specific time frame.</p> </li> <li> <p>Organize incidents by alert definition.</p> </li> <li> <p>Search alerts by name.</p> </li> <li> <p>Filter alerts by type, severity, or other chosen parameters.</p> </li> <li> <p>Select and modify incident status.</p> </li> <li> <p>Instantly drill-down into any triggered event to view its contextual information and underlying data.</p> </li> </ul>"},{"location":"newoutput/incidents/#how-it-works","title":"How It Works","text":"<p>Triggered alert events, bundled as incidents, are presented in your Incidents Screen according to the Group By Tags and Notification Settings set in your alert definition.</p>"},{"location":"newoutput/incidents/#group-by-tags","title":"Group By Tags","text":"<p>The Incidents Screen presents all of the individual permutations for all key-value tags that are selected in the Group By Conditions defined in your alert.</p> <p></p> <p>The Group By feature allows you to group alerts by one or more key-value tags that are aggregated into a histogram. An alert is triggered whenever the condition threshold is met for a specific aggregated key within a specified timeframe.</p> <p>If using 2 tags for\u00a0Group By, matching logs, metrics or traces will first be aggregated by the parent tag (ie.\u00a0<code>applicationName</code>), then by the child tag (ie.\u00a0<code>subsystemName</code>). An alert will fire when the threshold meets the unique combination of both parent and child. Only data that includes the selected\u00a0Group By\u00a0tags will be included in the count.</p> <p>For every alert that is triggered, one or more events form an incident. If the user has defined them in his / her alert setup, specific events within an incident are organized by key-value Group By tags.</p>"},{"location":"newoutput/incidents/#notification-settings","title":"Notification Settings","text":"<p>Incident events are organized by the Notification Settings defined in your alert.</p> <ul> <li> <p>If you choose to trigger a single alert when at least one key-value tag combination meets your Group By Conditions, all events for that alert will be consolidated within one incident in your Incidents Screen.</p> </li> <li> <p>If you choose to trigger a separate alert for each key-value combination that meets your Group By Conditions, you will see separate incidents for each key-value tag combination.</p> </li> </ul> <p></p>"},{"location":"newoutput/incidents/#prerequisites","title":"Prerequisites","text":"<ul> <li>S3 archive bucket defined for querying alert logs in your Explore Screen</li> </ul>"},{"location":"newoutput/incidents/#incidents-screen","title":"Incidents Screen","text":"<p>To view all of your triggered alerts, navigate to Alerts &gt; Incidents in your Coralogix toolbar.</p>"},{"location":"newoutput/incidents/#triggered-incidents","title":"Triggered Incidents","text":"<p>The Triggered tab presents all those alert events which are currently triggered and have yet to be resolved. Events are sorted by \u2018Last Triggered\u2019 timestamp.</p> <p></p>"},{"location":"newoutput/incidents/#all-incidents","title":"All Incidents","text":"<p>The All Incidents tab presents all those events triggered within a specific time frame of your choosing, regardless of their current status and duration. Events are sorted by \u2018Last Triggered\u2019 timestamp.</p> <p></p> <p>Beyond our default time frame selection (QUICK), you have the option of querying incidents for a timeframe relative to the present time (RELATIVE) or within a custom timeframe (CUSTOM). In addition, our Version Benchmarks feature allows you to use tags to compare between timelines (TAG).</p>"},{"location":"newoutput/incidents/#group-by-alert-definition","title":"Group by Alert Definition","text":"<p>For both the Triggered and All Incidents tabs, you may group events by alert definition in the upper right-hand corner of your screen. Doing so will aggregate all alert permutations, consolidating individual incidents under one alert name.</p> <p></p> <p>Expand an alert group to view the individual permutations that triggered the alert, with additional information about each permutation. This includes alert status, alert type, permutation details, etc. An alert group can contain up to 1,000 permutations.</p>"},{"location":"newoutput/incidents/#filter-incidents","title":"Filter Incidents","text":"<p>Filter incidents using the filters in the left-hand sidebar.</p> <p></p> <p>Filter incidents by alert type, alert severity, and state.</p>"},{"location":"newoutput/incidents/#incident-status","title":"Incident Status","text":"<p>Incidents may have one of three statuses: TRIGGERED, ACKNOWLEDGED, or RESOLVED. Statuses change on an automatic or manual basis.</p>"},{"location":"newoutput/incidents/#automatic-change-to-status","title":"Automatic Change to Status","text":"<p>Once a triggered alert is resolved, the status of the original incident automatically changes to RESOLVED. If you have activated the Notify When Resolved settings in your alert, a new resolve event is sent.</p> <p>Once resolved, an incident is closed. If the alert is then triggered, a new incident appears.</p>"},{"location":"newoutput/incidents/#manual-change-to-status","title":"Manual Change to Status","text":"<p>Clicking on a TRIGGERED status will present a drop-down menu in which you can choose to ACKNOWLEDGE or RESOLVE an incident. Doing so automatically defines you as the assignee. You may unassign yourself or replace the assignee.</p> <p></p> <p>Modify incident status from your Incidents Screen or Incident Details Screen.</p> <p></p>"},{"location":"newoutput/incidents/#incident-details-screen","title":"Incident Details Screen","text":"<p>Clicking on left-hand ellipses (\u2026) of any incident permutation opens the Incident Details Screen, which presents the details of the incident from start to finish:</p> <ul> <li> <p>Alert name and status (triggered, acknowledged or resolved)</p> </li> <li> <p>Event history and timestamp</p> </li> <li> <p>Severity, time window, application and subsystem</p> </li> <li> <p>Alert query</p> </li> <li> <p>Assignee</p> </li> </ul> <p></p> <p>Open an alert definition and edit by clicking on the pen icon.</p>"},{"location":"newoutput/incidents/#group-by-state","title":"Group By State","text":"<p>For single alerts when at least one key-value tag pair combination meets your Group By Conditions, the Group By State grid displays all of the permutations for the key-value tags established.</p> <p></p>"},{"location":"newoutput/incidents/#watch-data","title":"Watch Data","text":"<p>Click WATCH DATA for any event to see its triggered logs, metrics or traces.</p> <p></p> <ul> <li> <p>Clicking WATCH DATA in the upper-right screen will present you with the raw data for the last event in the incident.</p> </li> <li> <p>Clicking WATCH DATA next to a specific event will present you with the raw data for that event.</p> </li> </ul> <p></p>"},{"location":"newoutput/incidents/#additional-resources","title":"Additional Resources","text":"DocumentationConnect S3 ArchiveGet Started with Coralogix Alerts"},{"location":"newoutput/incidents/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/incidents-api/","title":"Incident Management API","text":""},{"location":"newoutput/incidents-api/#overview","title":"Overview","text":"<p>This document outlines the Incident Management API. It includes various methods for managing incidents, such as retrieving incident details, listing incidents, aggregating incidents, assigning and unassigning incidents and acknowledging or resolving incidents. The\u00a0<code>IncidentsService</code>\u00a0is designed to handle all operations related to incident management within Coralogix.</p>"},{"location":"newoutput/incidents-api/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, please make sure you have the following:</p> <ul> <li> <p>API Key for Alerts, Rules &amp; Tags to successfully authenticate.</p> </li> <li> <p>Management API Endpoint that corresponds with your Coralogix subscription.</p> </li> <li> <p>Administrator permissions to manage your services.</p> </li> </ul>"},{"location":"newoutput/incidents-api/#authentication","title":"Authentication","text":"<p>Coralogix API uses API keys to authenticate requests. You can view and\u00a0manage your API keys\u00a0from the Data Flow tab in Coralogix. You need to use this API key in the Authorization request header to successfully connect.</p>"},{"location":"newoutput/incidents-api/#example","title":"Example:","text":"<p><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\"</code></p> <p>Then, use one of our designated\u00a0Management Endpoints\u00a0to structure your header.</p> <p><code>d @ ng-api-grpc.coralogixstg.wpengine.com:443</code></p> <p>For the Incidents Service API, the service name will be\u00a0<code>IncidentsService</code>.</p> <p><code>com.coralogixapis.incidents.v1.IncidentsService/</code></p> <p>The complete request header should look like this:</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.incidents.v1.IncidentsService/\n\n</code></pre>"},{"location":"newoutput/incidents-api/#sample-request","title":"Sample Request","text":"<p>Lists all available incidents based on specified filters and order. In this case, incidents are shown per\u00a0<code>assignee</code>. The list is ordered in an unspecified direction and sorted by time created.</p> <pre><code>grpcurl -H \"Authorization: Bearer APY_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.incidents.v1.IncidentsService/ListIncidents &lt;&lt;EOF \n{\n    \"filter\": {\n        \"assignee\": [\n            {\n                \"value\": \"assignee@coralogixstg.wpengine.com\"\n            }\n        ]},\n    \"order_bys\": [\n        {\n            \"direction\": \"ORDER_BY_DIRECTION_UNSPECIFIED\",\n            \"incident_field\": \"INCIDENTS_FIELDS_CREATED_TIME\"\n        }\n    ]\n}\nEOF\n\n</code></pre>"},{"location":"newoutput/incidents-api/#sample-response","title":"Sample Response","text":"<pre><code>{\n    \"incidents\": [\n        {\n            \"assignments\": [\n                {\n                    \"assigned_to\": {\n                        \"user_id\": {\n                            \"value\": \"assignee@coralogixstg.wpengine.com\"\n                        }\n                    },\n                    \"assigned_by\": {\n                        \"user_id\": {\n                            \"value\": \"assignee@coralogixstg.wpengine.com\"\n                        }\n                    }\n                }\n            ],\n            \"events\": [],\n            \"contextual_labels\": [\n                {\n                    \"key\": \"alert_id\",\n                    \"value\": \"e2e1e00f-552f-4dfc-9d24-ab9d21d4979c\"\n                },\n                {\n                    \"key\": \"alert_name\",\n                    \"value\": \"inalert\"\n                },\n                {\n                    \"key\": \"alert_type\",\n                    \"value\": \"Standard\"\n                },\n                {\n                    \"key\": \"alert_severity\",\n                    \"value\": \"Info\"\n                },\n                {\n                    \"key\": \"alert_group_by_fields\",\n                    \"value\": \"coralogix.metadata.applicationName , coralogix.metadata.subsystemName\"\n                },\n                {\n                    \"key\": \"alert_notification_group_id\",\n                    \"value\": \"ab8dee0e-063b-43c2-89a3-bdbb068ff851\"\n                },\n                {\n                    \"key\": \"alert_notification_group_grouping_fields\",\n                    \"value\": \"coralogix.metadata.applicationName , coralogix.metadata.subsystemName\"\n                },\n                {\n                    \"key\": \"alert_notification_group_integrations_ids\",\n                    \"value\": \"\"\n                }\n            ],\n            \"display_labels\": [\n                {\n                    \"key\": \"coralogix.metadata.subsystemName\",\n                    \"value\": \"coralogix-operator\"\n                },\n                {\n                    \"key\": \"coralogix.metadata.applicationName\",\n                    \"value\": \"staging\"\n                }\n            ],\n            \"id\": {\n                \"value\": \"cdfaf78b-27ee-401f-8d13-ebd2daa08232\"\n            },\n            \"name\": null,\n            \"state\": \"INCIDENT_STATE_TRIGGERED\",\n            \"status\": \"INCIDENT_STATUS_TRIGGERED\",\n            \"description\": null,\n            \"severity\": \"INCIDENT_SEVERITY_INFO\",\n            \"created_at\": {\n                \"seconds\": \"1703677320\",\n                \"nanos\": 0\n            },\n            \"closed_at\": null,\n            \"last_state_update_time\": {\n                \"seconds\": \"1706088981\",\n                \"nanos\": 286000000\n            },\n            \"last_state_update_key\": {\n                \"value\": \"8cde7807-dedc-418b-b542-62d78fead629\"\n            },\n            \"is_muted\": {\n                \"value\": false\n            }\n        }\n    ]\n}\n\n</code></pre>"},{"location":"newoutput/incidents-api/#api-endpoints","title":"API Endpoints","text":"<p>\u26a0\ufe0f This is only a list of endpoints. For detailed schema, please consult the whole\u00a0specification file in GitHub.</p>"},{"location":"newoutput/incidents-api/#incidentsservice","title":"IncidentsService","text":"<p>The IncidentsService is designed to provide a set of functionalities and operations to facilitate the effective management, monitoring, and resolution of incidents. Here are some key methods within the IncidentsService:</p> Method Name Description ListIncidents Lists incidents based on filters. This method is used to retrieve a collection of incidents that match certain criteria. ListIncidentAggregations Lists incident aggregations. This method is used to retrieve aggregated information about incidents, grouped by specific parameters. GetIncident Retrieves detailed information about a specific incident. This method is used to get comprehensive details regarding a single incident. GetIncidentEvents Retrieves events associated with a particular incident. This method is used to obtain a chronological sequence of events related to an incident. BatchGetIncident Retrieves details for multiple incidents. This method is designed to efficiently retrieve information for a batch of incidents in a single request. AssignIncidents Assigns incidents to specific users or teams. This method is used for managing the assignment of incidents to responsible parties. UnassignIncidents Unassigns incidents from users or teams. This method is used to remove assignment associations for incidents. AcknowledgeIncidents Acknowledges incidents. PaginationRequest Retrieves pagination information for incidents. CloseIncidents Closes incidents. DeleteIncidents Deletes incidents. ResolveIncidents Resolves incidents."},{"location":"newoutput/incidents-api/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email to\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/insights-detection/","title":"Insights Detection","text":"<p>STA is a tool for analyzing network traffic and host based activities. Using open-source services such as <code>zeek</code> and <code>suricata</code>, it enriches these events with other internal services.</p> <p>To reduce the total cost of ownership of the STA, we have introduced a new insights service within the STA that will automatically detect possible threats and security related anomalies in your traffic.\u00a0</p> <p>Using the Coralogix Platform, configure alerts based on those insights and receive instant notifications if anomalies occur.</p>"},{"location":"newoutput/insights-detection/#insights-overview","title":"Insights Overview","text":"<p>The following table contains a detailed list of the possible insights that can be detected by the <code>sta-insights</code> events.</p> <p>Note:</p> <p>To reduce the chances for false-positives, during the first three days the STA will only learn the patterns seen in the traffic. After that it will start sending events about anomalies to Coralogix.</p>"},{"location":"newoutput/insights-detection/#types-of-insights","title":"Types of Insights","text":"NameDescriptionPossible AttacksMessageSub MessageFile Similarity Insightchanges to file paths that are very similar to others which encountered recentlyFile encryption based Ransomware<code>\"Detected changes to file paths very similar to others seen recently &lt;hamming_distance&gt;\"</code><code>AnomalousSimhash::Document_Similarity_insight</code>Connection To Suspiciously Looking Domain Nameconnections to suspicious domain using frequency score algorithmsDGA activities<code>Detected connection to suspiciously looking domain name &lt;value&gt;</code><code>AnomalousDomain-stats::Connection_to_suspiciously_looking_domain_name_insight</code>Connection to baby domainsconnections to domains that created less than 90 daysPhishing, C2C attacks<code>Detected baby domain &lt;value&gt; connection</code><code>AnomalousDomain-stats::Baby-domain-connection_insight</code>Connection to possible malicious IPs/Domainsconnection to IPs/Domains which flagged as malicious by at least one DNSRBLPhishing, C2C attacks<code>Detected connection to IPs/Domains flagged as malicious by at least one DNSRBL</code><code>AnomalousDNSRBL::Malicious_domain_ip_insight</code>New top level domainencountered with new top level domainPhishing, C2C attacks<code>Detected new top_level_domain &lt;value&gt;</code><code>AnomalousTLD::New_TLD_Insight</code>Connection with redirection to another domainconnection to URL which redirects to another domainEvasion techniques<code>Detected connection to URL redirecting to another domain &lt;value&gt;redirected to &lt;redirected_value&gt;</code><code>AnomalousUnsortenURL::url_redirecting_to_another_domain_insight</code>DNS over TCPdetects DNS queries over TCPdownload/upload payloads via DNS<code>Detected dns over tcp</code><code>AnomalousDns::Dns_over_tcp_insight</code>Public IP echo requestsdetects requests for public IP using echo commands such as <code>ifconfig.me</code>Geographical identification<code>Detected Request for public IP echo &lt;command&gt;</code><code>AnomalousRdp::New_Rdp_cookie_insight</code>SSH/RDP new country connectiondetects connection using SSH/RDP from a new countryC2C attacks<code>Detected ssh/rdp connection to new country &lt;name&gt; for ip &lt;value&gt;</code><code>AnomalousNewCountryConnection::SSH_RDP_New_Country_Conn_Insight</code>Number of lateral connections in given timedetects more than 10 wide internal connections from one source in 10 minutesNetwork scan/propagation<code>Detected more than &lt;number&gt; lateral connections in &lt;time_in_minutes&gt; &lt;machine_tag_name&gt;</code><code>AnomalousConn::10_lateral_connections_in_10min_insight</code>number of NXDOMAIN responses in given timedetects more than 100 NXDOMAIN responses in 10 minutesDGA activities<code>Detected more than 100 NXDOMAIN responses in 10min</code><code>AnomalousNXDOMAIN::100_NXDOMAIN_responses_in_10min_insight</code>Connection to/from new countrydetects connection to/from new encountered countryC2C attacks, DGA activity<code>Detected connection to/from new country &lt;name&gt; for ip &lt;value&gt;</code><code>AnomalousGeo::New_country_insight</code>New FTP commanddetects new encountered FTP commandsFile transfer anomalies<code>Detected new ftp command &lt;command&gt;</code><code>AnomalousFtp::New_Ftp_command_insight</code>new HTTP methoddetects new encountered HTTP methodNetwork anomalies, Log4Shell for example<code>Detected new http method &lt;name&gt;</code><code>AnomalousHttp::New_HTTP_method_insight</code>SSH/RDP with new destination connectiondetects connection using SSH/RDP to a new destinationC2C attacks, DGA activity<code>Detected ssh/rdp connection to new destination &lt;dest_host,tag_name&gt; for ip &lt;value&gt;</code><code>AnomalousNewCountryConnection::SSH_RDP_New_Destination_Conn_Insight</code>New MySQL instancedetects new MySQL instance creation queryRogue server<code>Detected New MySQL query &lt;source_ip_tag_name&gt;</code><code>AnomalousMySQL::New_MySQL_query_insight</code>New AWS outbound connectionDetects new AWS outbound connectionC2C attacks<code>Detected new outbound connection &lt;orig_host,tag_name&gt;</code><code>AnomalousConn::New_AWS_outbound_connection_insight</code>new RDP cookiedetects new RDP cookiebrute force attempt, lateral movement, network propagation/scanning, etc.<code>Detected new rdp cookie &lt;cookie&gt;</code><code>AnomalousRdp::New_Rdp_cookie_insight</code>New software typedetects new software typeMalicious executable software, C2C attacks<code>Detected new software type &lt;type&gt;</code><code>AnomalousSoftware::New_Software_type_insight</code>New software with reported CVEsdetects new software with reported CVEsExecution of known Exploited Vulnerabilities<code>Detected connection to new software &lt;name&gt; &lt;version&gt; with CVEs &lt;values&gt;</code><code>AnomalousNIST::New_software_with_CVEs_insight</code>New MySQL commanddetects new MySQL commandC2C attacks, SQL injection<code>Detected new MySql command &lt;command&gt;</code><code>AnomalousMySql::New_MySql_command_insight</code>New tunnel typedetects new tunnel type for trafficC2C attack, man in the middle<code>Detected new tunnel type &lt;type&gt;</code><code>AnomalousTunnel::New_Tunnel_type_insight</code>Outbound connection from DB serverdetects outbound connection from data base serverC2C attack, data exfiltration<code>Detected outbound connection from DB &lt;name&gt; server on port &lt;port&gt;</code><code>AnomalousSatatsInfo::outbound_connection_from_DB_server_insight</code>Outbound connection using servicesdetects connections/attemptsvia SMB, SSH, FTP, Kerbros, MySQL, LDAPData exfiltration, outbound scanning, etc.<code>Detected outbound connection to service &lt;name&gt;</code><code>AnomalousConn::outbound_&lt;service&gt;_connection_insight</code>invalid certification via TLS connectiondetects TLS connection with invalid certificationMan in the middle<code>Detected TSL connection with invalid certification</code><code>AnomalousConn::TSL_connection_with_invalid_certification_insight</code>"},{"location":"newoutput/insights-detection/#handle-insights","title":"Handle Insights","text":"<p>After insight events are sent to Coralogix, you can find them under the <code>Explore</code> section with subsystem name <code>sta_insight</code>.</p>   ![](images/image-43-1024x426.png)    Example - New Country Connection Insight   <p>To enable alerts from within Coralogix, navigate to the <code>Alerts</code> section and set them accordingly. Find out more regarding alerts here.</p>"},{"location":"newoutput/insights-detection/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/install-opentelemetry-ec2/","title":"Install OpenTelemetry on an EC2 Instance","text":"<p>This tutorial demonstrates how to set up an EC2 instance with OpenTelemetry Collector.</p>"},{"location":"newoutput/install-opentelemetry-ec2/#setup","title":"Setup","text":""},{"location":"newoutput/install-opentelemetry-ec2/#installation","title":"Installation","text":"<p>STEP 1. Create an EC2 instance.</p> <p>STEP 2. Download and install OpenTelemetry. On this page, select the package for your OS. Before downloading it, edit the name of the package to include <code>otelcol-contrib</code> instead of <code>otelcol</code>.</p> <p>For example, replace</p> <pre><code> wget &lt;https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v0.86.0/otelcol_0.86.0_linux_amd64.deb&gt;\n</code></pre> <p>with</p> <pre><code>wget &lt;https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v0.86.0/otelcol-contrib_0.86.0_linux_amd64.deb&gt;\n</code></pre>"},{"location":"newoutput/install-opentelemetry-ec2/#configuration","title":"Configuration","text":"<p>Edit\u00a0<code>/etc/otelcol-contrib/config.yaml</code> using this example configuration:</p> <pre><code>receivers:\n  filelog:\n    start_at: beginning\n    include:\n      - /example.log\n    include_file_path: true\n    # Specify multiline pattern only if there are multiline logs like stack errors.\n    # Usually log entries are in single line and this setting can be omit.\n    # multiline: {line_start_pattern: \"\\\\\\\\n\"}\n  hostmetrics:\n      collection_interval: 30s\n      scrapers:\n        cpu:\n        memory:\nprocessors:\n  batch:\n    send_batch_size: 1024\n    send_batch_max_size: 2048\n    timeout: \"1s\"\n  resourcedetection:\n    detectors: [env, ec2, system]\nexporters:\n  coralogix:\n    domain: \"Domain\"\n    private_key: \"Private key\"\n    application_name: \"Application Name\"\n    subsystem_name: \"Subsystem Name\"\n    timeout: 30s\n\nservice:\n  pipelines:\n    logs:\n      receivers: [ filelog ]\n      processors: [resourcedetection, batch]\n      exporters: [ coralogix ]\n    metrics:\n      receivers: [ hostmetrics ]\n      processors: [resourcedetection, batch]\n      exporters: [ coralogix ]\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>Coralogix recommends the default otel-integration chart settings for\u00a0batch processors\u00a0in all collectors. Learn more here.</p> </li> <li> <p>Learn more about receivers and exporters, the <code>filelog</code> receiver, and the <code>hostmetrics</code> receiver.</p> </li> </ul>"},{"location":"newoutput/install-opentelemetry-ec2/#collect-ec2-tags-optional","title":"Collect EC2 Tags [Optional]","text":"<p>To add EC2 tags, add a role to your instance.</p> <p></p>"},{"location":"newoutput/install-opentelemetry-ec2/#create-a-new-policy","title":"Create a New Policy","text":"<p>STEP 1. Navigate to IAM &gt; Policies. Click Create policy.</p> <p>STEP 2. Select the following options.</p> <p></p> <p>STEP 3. Enter a policy name. Click Save.</p>"},{"location":"newoutput/install-opentelemetry-ec2/#create-a-new-role","title":"Create a New Role","text":"<p>STEP 1. Navigate to IAM &gt; Roles. Click Create role.</p> <p>STEP 2. Select the following options.</p> <p></p> <p>STEP 3. Click Next.</p> <p>STEP 4. Click on the checkbox next to the rule you just created.</p> <p>STEP 5. Enter a rule name. Click Create rule.</p>"},{"location":"newoutput/install-opentelemetry-ec2/#add-iam-role-to-your-instance","title":"Add IAM Role to Your Instance","text":"<p>STEP 1. Open your instance from the EC2 instance screen.</p> <p>STEP 2. Add the IAM role to the empty IAM.</p> <p></p> <p>STEP 3. Select the IAM role. Click Update IAM role.</p> <p></p> <p>STEP 4. Allow tags in the metadata instance.</p> <p></p> <p>STEP 5. Update the <code>resourcedetection</code> processor at the processors.</p> <pre><code>resourcedetection:\n    # ecs &amp; docker detectors not required when using ecslogresourcedetection for logs\n    detectors: [env, ec2, system]\n    ec2:\n      tags:\n        - ^TagName$\n    timeout: 2s\n    override: false\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>It is mandatory to specify the tags parameter, as it is not added by default.</p> </li> <li> <p>Set any Regex to limit the number of tags or use <code>^.*$</code> to get all tags.</p> </li> </ul>"},{"location":"newoutput/install-opentelemetry-ec2/#validation","title":"Validation","text":"<p>Start Otel to validate your setup, using this example from Ubuntu Linux.</p> <pre><code>/usr/bin/otelcol-contrib --config=file:/etc/otelcol-contrib/config.yaml\n\n</code></pre>"},{"location":"newoutput/install-opentelemetry-ec2/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/installing-coralogix-sta-gcp/","title":"Installing Coralogix STA - GCP","text":"<p>The Coralogix Security Traffic Analyzer (STA) is a Coralogix tool designed for tasks such as deep packet inspection, packet capturing, cloud configuration vulnerability scanning, and other security-related functions.</p> <p>In Google Cloud Platform (GCP), the implementation of STA is exclusively carried out through Terraform.</p>"},{"location":"newoutput/installing-coralogix-sta-gcp/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Configuration saved using Cloud Storage</p> </li> <li> <p>The following permissions deployed: VM Compute Instance, Cloud Storage, Networks and Subnetworks, IAM roles</p> </li> </ul>"},{"location":"newoutput/installing-coralogix-sta-gcp/#deployment","title":"Deployment","text":"<p>STEP 1. Connect to GCP using <code>gcloud</code> or any other method of authentication.</p> <p>STEP 2. Download the Terraform template here.</p> <p>STEP 3. Once the files are extracted, from the folder run <code>terraform init</code>.</p> <p>STEP 4. Fill in the information in the <code>values.auto.tfvars</code> file:</p> <ul> <li> <p>The STA must connect to the internet. When selecting <code>STA-Public-Access</code> to <code>false</code>, make sure the STA is located on a subnetwork that has an NAT gateway attached.</p> </li> <li> <p><code>STA-Config-Cloud-Storage-Bucket</code> - Optional to manage independently, but mandatory to use. If not provided by the user, Terraform will create one for you.</p> </li> <li> <p><code>STA-Subnetwork-Mgmt</code> and <code>STA-Subnetwork-VxLanSniffing</code> are optional. Terraform will create them when not provided.</p> </li> <li> <p><code>STA-IP-CIDR-for-Mgmt-Nic</code> and <code>STA-IP-CIDR-for-VxSniffing-Nic</code> - If <code>STA-Subnetwork-Mgmt</code> and <code>STA-Subnetwork-VxLanSniffing</code> are not provided. Select which CIDR range will be used in the newly created subnetwork. Note that 'STA-Subnetwork-Mgmt' - defaults to '172.30.0.0/24\u2019 and 'STA-Subnetwork-Mgmt' - defaults to '172.30.1.0/24\u2019</p> </li> <li> <p><code>STA-Subnetwork-RawSniffing</code> is mandatory. Select the subnetwork that holds the instances you wish to mirror to the STA.</p> </li> <li> <p><code>STA-Ingress-SSH-Address</code> - The IP address that will be allowed to manage (SSH) the STA</p> </li> <li> <p>SSH key management in the STA:</p> <ul> <li> <p><code>GCP-block-project-ssh-keys</code> - Set to false to block SSH keys that are defined on the GCP project level.</p> </li> <li> <p><code>GCP-SSH-Key-Required</code> -Set to true\\false if SSH key is required or not.</p> <ul> <li> <p>When true:</p> <ul> <li> <p><code>GPC-Existing-SSH-Public-Key-full-path</code> - when used, the key content will be read and used to manage the STA.</p> </li> <li> <p>If the <code>GPC-Existing-SSH-Public-Key-full-path</code> variable is left empty, <code>GPC-New-SSH-Public-Key-name</code> will create a new SSH key to manage the STA on the stack directory.</p> </li> <li> <p>When both variables are left empty - a new key will be created with the name <code>STA_GCP_key</code></p> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>STEP 5. Run <code>terraform plan</code> and review the deployment.</p> <p>STEP 6. Run <code>terraform apply</code> and type yes.</p>"},{"location":"newoutput/installing-coralogix-sta-gcp/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Security Traffic Analyzer"},{"location":"newoutput/installing-coralogix-sta-gcp/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/integration-packages/","title":"Integration Packages","text":"<p>Access our Integration Packages to access a series of integrations in a one-step process. Use this to extend your platform capabilities with packages and sources, without expending unnecessary time and resources.</p>"},{"location":"newoutput/integration-packages/#prerequisites","title":"Prerequisites","text":"<p>Verify you are signed into your cloud provider with your credentials.</p>"},{"location":"newoutput/integration-packages/#access-an-integration-package","title":"Access an Integration Package","text":"<p>STEP 1. In your navigation pane, click Data Flow &gt; Extensions. View the list of available integrations.</p> <p>STEP 2. Select the integration of choice.</p> <p>STEP 3. View application details in the App Overview. For more information, click Integration Details. You will also see a list of extension packages available for the integration of choice.</p> <p>STEP 4. Click + NEW INTEGRATION.</p> <p></p> <p>STEP 5. Input the Integration Details.</p> <p></p> <ul> <li> <p>Input an existing Coralogix Send-Your-Data API key or click CREATE NEW KEY.</p> </li> <li> <p>Input Application and Subsystem names.</p> </li> <li> <p>Input S3 Bucket Name, if applicable.</p> </li> <li> <p>Select your region from the dropdown list.</p> </li> <li> <p>Fill any other relevant fields.</p> </li> </ul> <p>STEP 6. Click NEXT.</p> <p>STEP 7. View the instructions for your integration. Click CREATE.</p> <p></p> <p>STEP 8. You will be rerouted to the website for the integration. Verify that all of the auto pre-populated values are correct and click Create Stack.</p> <p>STEP 9. Go back to the Coralogix application and click COMPLETE to close the module and go back to the integration page.</p> <p></p> <p>STEP 10. View your integration information.</p> <p></p> <p>STEP 11. Deploy the extension package of your choice to complement your integration needs.</p> <p>STEP 12. Once the verification process is complete and you have deployed your extension package, view your logs in your Coralogix dashboard.</p>"},{"location":"newoutput/integration-packages/#additional-resources","title":"Additional Resources","text":"DocumentationExtension Packages"},{"location":"newoutput/integration-packages/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/intercom-logs/","title":"Intercom Data Ingestion","text":"<p>Collect your Intercom events in the Coralogix platform using our automatic Contextual Data Integration Package. The package automatically generates a URL to be used when creating an Intercom private application.</p>"},{"location":"newoutput/intercom-logs/#overview","title":"Overview","text":"<p>Intercom is a customer messaging platform that facilitates personalized and interactive communication between businesses and their customers. Offering a suite of tools including live chat, targeted email campaigns, in-app messages, and customer support features, Intercom enables companies to engage with users at various stages of their journey, from acquisition and onboarding to retention and support. Its user-friendly interface and automation capabilities streamline customer interactions, helping businesses build stronger relationships and provide timely assistance to drive customer satisfaction and loyalty.</p> <p>Companies that use Intercom for customer relations can ingest Intercom events in their logs, in order to correlate them with other events in their systems, and then seamlessly send them to Coralogix and take advantage of Coralogix log analytic capabilities, alerts, and top-notch visualization features.</p>"},{"location":"newoutput/intercom-logs/#get-started","title":"Get Started","text":"<p>Before adding the Intercom integration in Coralogix, you first need to create a private Intercom application which will provide you with the Client Secret key, which needs to be entered when creating the integration in Coralogix. Next, by using the contextual data integration flow in Coralogix, a URL will be generated for you to enter in the webhooks page in Intercom.</p> <p>STEP 1. Sign in to Intercom and navigate to the Developer Hub.</p> <p></p> <p>STEP 2. Click on the New app button, and fill out the form as shown in the following image:</p> <p>STEP 3. Select Basic information on the left-hand side menu, and copy the Client secret key that we will need in the next step. </p> <p>STEP 4.\u00a0In a different tab, go to the Coralogix app, and in your Coralogix toolbar, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 5.\u00a0In the Contextual Data section, select Intercom and click\u00a0+ ADD.</p> <p>STEP 6. Click ADD NEW.</p> <p></p> <p>STEP 7.\u00a0Fill in the Integration Details:</p> <ul> <li> <p>Integration Name.\u00a0Name your integration.</p> </li> <li> <p>Your API Key. Enter the Client secret key that you copied from Intercom.</p> </li> <li> <p>Application Name and Subsystem Name. Enter an\u00a0application and subsystem name.</p> </li> </ul> <p></p> <p>STEP 8.\u00a0Click\u00a0GENERATE URL. The URL for the integration will be automatically created. Use this for the next step when creating the Intercom webhook.</p> <p></p> <p>STEP 9. Copy the generated URL.</p> <p>STEP 10. Go back to the Intercom tab and click on Webhooks in the left-hand side menu. </p> <p>STEP 11. In the Your request endpoint URL field, enter the Coralogix generated URL that you previously copied.</p> <p>STEP 12. Select the Webhook topics from the dropdown menu that you are interested in, e.g. <code>conversation.user.created</code>.</p> <p> STEP 13. Click Save.</p>"},{"location":"newoutput/intercom-logs/#example-log","title":"Example Log","text":"<pre><code>{\n   \"source_system\"  :  \"intercom\" ,\n   \"intercom\"  : {\n     \"type\"  :  \"notification_event\" ,\n     \"app_id\"  :  \"pc1eeznt\" ,\n     \"data\"  : {\n       \"type\"  :  \"notification_event_data\" ,\n       \"item\"  : {\n         \"type\"  :  \"conversation\" ,\n         \"id\"  :  \"4\" ,\n         \"created_at\"  :  1619423555 ,\n         \"updated_at\"  :  1619424688 ,\n         \"user\"  : {\n           \"type\"  :  \"lead\" ,\n           \"id\"  :  \"60787e73099bb148057c1111\" ,\n           \"user_id\"  :  \"12345678-0000-aaaa-1111-abcdefg12345\" ,\n           \"name\"  :  \"Indigo Umbrella from Munich\" ,\n           \"email\"  :  \"\" ,\n           \"do_not_track\"  :  null \n        },\n         \"assignee\"  : {\n           \"type\"  :  \"nobody_admin\" ,\n           \"id\"  :  null \n        },\n         \"admin_assignee_id\"  :  null ,\n         \"team_assignee_id\"  :  null ,\n         \"conversation_message\"  : {\n           \"type\"  :  \"conversation_message\" ,\n           \"id\"  :  \"822898059\" ,\n           \"url\"  :  \"http://localhost:63342/aux-data-ingress/intercom-demo.html?_ijt=615bri6jf9tqgpevlc7cc102sj\" ,\n           \"subject\"  :  \"\" ,\n           \"body\"  :  \"&amp;lt;p&amp;gt;test&amp;lt;/p&amp;gt;\" ,\n           \"author\"  : {\n             \"type\"  :  \"user\" ,\n             \"id\"  :  \"60787e73099bb148057c1111\" \n          },\n           \"attachments\"  : []\n        },\n         \"conversation_parts\"  : {\n           \"type\"  :  \"conversation_part.list\" ,\n           \"conversation_parts\"  : [\n            {\n               \"type\"  :  \"conversation_part\" ,\n               \"id\"  :  \"8863805399\" ,\n               \"part_type\"  :  \"comment\" ,\n               \"body\"  :  \"&amp;lt;p&amp;gt;Hi, can you explain how this works?&amp;lt;/p&amp;gt;\" ,\n               \"created_at\"  :  1619424687 ,\n               \"updated_at\"  :  1619424687 ,\n               \"notified_at\"  :  1619424687 ,\n               \"assigned_to\"  :  null ,\n               \"author\"  : {\n                 \"type\"  :  \"user\" ,\n                 \"id\"  :  \"60787e73099bb148057c1111\" ,\n                 \"name\"  :  null ,\n                 \"email\"  :  \"\" \n              },\n               \"attachments\"  : [],\n               \"external_id\"  :  null \n            }\n          ],\n           \"total_count\"  :  1 \n        },\n         \"conversation_rating\"  : {},\n         \"open\"  :  true ,\n         \"state\"  :  \"open\" ,\n         \"snoozed_until\"  :  null ,\n         \"read\"  :  true ,\n         \"metadata\"  : {},\n         \"tags\"  : {\n           \"type\"  :  \"tag.list\" ,\n           \"tags\"  : []\n        },\n         \"tags_added\"  : {\n           \"type\"  :  \"tag.list\" ,\n           \"tags\"  : []\n        },\n         \"custom_attributes\"  : {},\n         \"links\"  : {\n           \"conversation_web\"  :  \"https://app.intercom.com/a/apps/pc1eeznt/conversations/4\" \n        }\n      }\n    },\n     \"links\"  : {},\n     \"id\"  :  \"notif_4ef0ca4e-0974-4aef-94e8-5f60c39eb5f8\" ,\n     \"topic\"  :  \"conversation.user.replied\" ,\n     \"delivery_status\"  :  \"pending\" ,\n     \"delivery_attempts\"  :  1 ,\n     \"delivered_at\"  :  0 ,\n     \"first_sent_at\"  :  1619424689 ,\n     \"created_at\"  :  1619424689 ,\n     \"self\"  :  null \n  }\n}\n\n</code></pre>"},{"location":"newoutput/intercom-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/introduction-to-kubernetes-observability-using-opentelemetry/","title":"Introduction to Kubernetes Observability using OpenTelemetry","text":"<p>Coralogix offers Kubernetes Observability using OpenTelemetry for comprehensive Kubernetes and application observability. Using our OpenTelemetry Chart, the integration enables you to simplify the collection of logs, metrics, and traces from the running application in your pods to the cluster-level components of your Kubernetes cluster, while enabling our Kubernetes Dashboard.</p>"},{"location":"newoutput/introduction-to-kubernetes-observability-using-opentelemetry/#observability-explained","title":"Observability Explained","text":""},{"location":"newoutput/introduction-to-kubernetes-observability-using-opentelemetry/#kubernetes-observability","title":"Kubernetes Observability","text":"<p>Kubernetes observability is essential for monitoring a Kubernetes cluster's health, performance, resource utilization, and workloads. It involves collecting and analyzing metrics, logs and traces from the cluster and underlying machines to ensure the stability and optimal operation of the cluster.</p> <p>When managing and monitoring Kubernetes components, consider these critical areas:</p> <ul> <li> <p>Cluster Health. Monitoring the overall health of the Kubernetes cluster is crucial. This includes checking the status and availability of the master and worker nodes and the control plane components such as the API server, kube-proxy, and scheduler.</p> </li> <li> <p>Resource Utilisation. Observing the resource utilization of cluster nodes and individual pods is essential for identifying bottlenecks, optimizing resource allocation, and ensuring efficient utilization of cluster resources. Extracting metrics and metadata from the underlying components provides the CPU, memory consumption, system load, and file system activity.</p> </li> <li> <p>Networking. Monitoring Kubernetes networking is crucial for smooth pod and service communication. This involves observing network traffic, latency, and error rates to detect and troubleshoot connectivity issues, identify performance bottlenecks, and improve network configurations.</p> </li> <li> <p>Application Performance. Observing the performance of applications running on Kubernetes is essential for delivering a reliable and responsive user experience.</p> </li> <li> <p>Logging and Tracing. Logging and tracing play a vital role in understanding the behaviour and troubleshooting of Kubernetes components and applications. By collecting and analysing logs and traces, you can gain insights into system events, diagnose issues, and perform root cause analysis. Implementing effective logging and tracing strategies is important to capture relevant information for observability purposes.</p> </li> </ul>"},{"location":"newoutput/introduction-to-kubernetes-observability-using-opentelemetry/#application-observability","title":"Application Observability","text":"<p>Application observability focuses on monitoring and understanding the behavior of applications running on the Kubernetes cluster. It includes collecting and analyzing metrics, logs, and traces specific to the applications to gain insights into their performance and identify any issues or bottlenecks. This includes monitoring response times, throughput, error rates, and other application-specific metrics.</p>"},{"location":"newoutput/introduction-to-kubernetes-observability-using-opentelemetry/#integration-overview","title":"Integration Overview","text":"<p>Integrating OpenTelemetry with Kubernetes enables comprehensive Kubernetes and application observability. The OpenTelemetry Integration Chart is a solution that combines two dependent charts into a single Helm installation for Kubernetes clusters: the OpenTelemetry Agent and the OpenTelemetry Cluster Collector. Both are built on the OpenTelemetry Collector Helm Chart, but are configured for optimal performance while collecting different data sources from Kubernetes. Together, they simplify the collection of logs, metrics, and traces from the running application in pods to the cluster-level components of your Kubernetes cluster.</p> <p></p> <p>Additionally, the OpenTelemetry Integration chart enables the collection of telemetry data needed for the Kubernetes Dashboard setup. This dashboard is a powerful web-based interface for monitoring and managing Kubernetes clusters. It provides real-time CPU, memory, network, and disk usage metrics for nodes and pods. Users can track resource trends, optimize workload placement, and troubleshoot issues effectively. The dashboard also displays Kubernetes events for quick problem identification and resolution. Streamlining cluster management ensures efficient performance and smooth operation of applications.</p>"},{"location":"newoutput/introduction-to-kubernetes-observability-using-opentelemetry/#opentelemetry-agent","title":"OpenTelemetry Agent","text":"<p>The OpenTelemetry Agent simplifies the collection of logs, metrics, and traces from applications running in your Kubernetes cluster. It is configured to deploy as a <code>daemonset</code> and runs on every node in the cluster. The agent maps metadata - such as Kubernetes attributes, Kubelet metrics, and host data - to the collected telemetry. This is particularly beneficial for high-traffic clusters or when utilizing our APM capabilities.</p> <p>The agent comes with several pre-configured processors and receivers:</p> <ul> <li> <p>Kubernetes Attributes Processor. This processor enriches data with Kubernetes metadata, such as pod and deployment information.</p> </li> <li> <p>Kubernetes Log Collection. Enables native Kubernetes log collection with OpenTelemetry Collector, eliminating the need for multiple agents like Fluentd, Fluent Bit, or Filebeat.</p> </li> <li> <p>Host Metrics. Collects native Linux and Windows node resource data.</p> </li> <li> <p>Kubelet Metrics. Fetches running container metrics from the local Kubelet.</p> </li> <li> <p>Traces. Collects data in various formats such as Jaeger, OpenTelemetry Protocol, or Zipkin.</p> </li> <li> <p>Metrics. Collects application metrics via the OpenTelemetry Protocol.</p> </li> <li> <p>Span Metrics. Converts traces into requests, duration, and error metrics using the spanmetrics processor.</p> </li> </ul>"},{"location":"newoutput/introduction-to-kubernetes-observability-using-opentelemetry/#opentelemetry-cluster-collector","title":"OpenTelemetry Cluster Collector","text":"<p>The OpenTelemetry Cluster Collector retrieves data from the cluster level, including Kubernetes events, cluster metrics, and additional Kubernetes-specific metrics. It enables you to gain insights into the health and performance of various objects within the cluster, such as deployments, nodes, and pods.</p> <ul> <li> <p>Cluster Metrics Receiver. The Kubernetes Cluster receiver collects cluster-level metrics from the Kubernetes API server.</p> </li> <li> <p>Kubernetes Events Receiver. This receiver collects Kubernetes events and sends them to the kube-events subsystem. It allows you to take advantage of other features, such as the Kubernetes Dashboard and alerting, using Kubernetes events as the primary source of information.</p> </li> <li> <p>Kubernetes Extra Metrics. This preset enables the collection of extra Kubernetes-related metrics, such as node information, pod status, or container I/O metrics. These metrics are collected in particular for the\u00a0Kubernetes Dashboard.</p> </li> <li> <p>Integration Presets. This chart provides support to integrate with various applications (e.g. mysql) running on your cluster to monitor them out of the box.</p> </li> </ul>"},{"location":"newoutput/introduction-to-kubernetes-observability-using-opentelemetry/#kubernetes-dashboard","title":"Kubernetes Dashboard","text":"<p>The OpenTelemetry Integration chart enables the collection of essential metrics needed for the Kubernetes Dashboard setup. The Kubernetes Cluster Receiver is an essential part that provides cluster-level metrics and entity events from the Kubernetes API server. It can report metrics of allocatable resource types such as <code>cpu</code> and <code>memory</code> and give an update on node conditions (e.g. <code>Ready</code>, <code>MemoryPressure</code>). As a whole, the metrics gathered are useful for the Kubernetes Dashboard to report on the health of your cluster.</p>"},{"location":"newoutput/introduction-to-kubernetes-observability-using-opentelemetry/#next-steps","title":"Next Steps","text":"<p>View our basic configuration instructions here.</p> <p>Advanced configuration instructions can be found here.</p>"},{"location":"newoutput/introduction-to-kubernetes-observability-using-opentelemetry/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by emailing support@coralogixstg.wpengine.com</p>"},{"location":"newoutput/introduction-to-microsoft-azure/","title":"Introduction to Microsoft Azure","text":"<p>The Coralogix Azure integrations enable the collection of logs and metrics from your Azure environment. Gain insights into role, user, group and directory management, successful and failed sign-in events, and application management data that helps you understand your users' experience and immediately troubleshoot any errors.</p> <p>Connect to Microsoft Azure to:</p> <ul> <li> <p>Visualize the performance of Event Hub, Blob and Queue Storage and correlate their performance with your applications</p> </li> <li> <p>Collect standard Azure Monitor metrics for all Azure services</p> </li> <li> <p>Tag your Azure metrics with Azure-specific information about the associated resource, such as region, resource group, and custom Azure tags</p> </li> <li> <p>Correlate data from your Azure applications across logs, metrics, APM tracing, user activity, and more within your Coralogix account</p> </li> </ul>"},{"location":"newoutput/introduction-to-microsoft-azure/#overview","title":"Overview","text":"<p>Coralogix Azure integrations allow you to monitor activity in the Microsoft Entra ID, a cloud-based identity and access management service. Here\u2019s how it works:</p> <ol> <li> <p>Microsoft Entra ID sends logs and metrics to Azure Monitor.</p> </li> <li> <p>Azure Monitor streams the data to Azure Event Hub.</p> </li> <li> <p>Once Azure Monitor receives the data, an event hub triggers its Azure function to send the data onto an HTTP source on a Coralogix-hosted collector. The HTTP source receives and ingests the monitoring data from the Azure function. It can be configured for logs or metrics.</p> </li> <li> <p>After being triggered by its event hub, an Azure function sends the logs or metrics to a configured HTTP source on a hosted collector.</p> </li> </ol> <p></p>"},{"location":"newoutput/introduction-to-microsoft-azure/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>An Azure subscription associated to the Microsoft Entra ID</p> </li> <li> <p>Microsoft Entra ID requirements met in order to export logs to reports</p> </li> <li> <p>Azure resource group</p> </li> </ul>"},{"location":"newoutput/introduction-to-microsoft-azure/#configuration","title":"Configuration","text":""},{"location":"newoutput/introduction-to-microsoft-azure/#basics","title":"Basics","text":"<p>Coralogix offers integrations for Azure Event Hub, Blob Storage, Queue Storage, and Diagnostic Metrics. Each integration provides instructions for setting up the ingestion pipeline from Microsoft Entra ID to Coralogix. This includes the creation of a storage account and storage, as well as configuration of Terraform and Azure Resource Manager (ARM) templates with the resource created.</p>"},{"location":"newoutput/introduction-to-microsoft-azure/#azure-resource-manager-arm-deployments","title":"Azure Resource Manager (ARM) Deployments","text":"<p>Coralogix provides Azure Resource Manager (ARM) custom template deployments to build your log and metric pipelines. Each template creates an Event Hub to which Azure Monitor streams logs or metrics, an Azure function for sending monitoring data to Coralogix, and storage accounts to which the function writes its own log messages about successful and failed transmissions.</p> <p>Select an automatic integration using ARM custom template deployments:</p> <ul> <li> <p>Event Hub</p> </li> <li> <p>Blob Storage</p> </li> <li> <p>Queue Storage</p> </li> <li> <p>Diagnostic Data</p> </li> </ul>"},{"location":"newoutput/introduction-to-microsoft-azure/#optional-configurations","title":"Optional Configurations","text":"<p>Coralogix offers optional configurations for particular use-cases utilizing our Azure deployments. If you require resource monitoring in Azure storage accounts or Event Hubs that cannot be made public, deploy our function apps with virtual network (VNet) support.</p>"},{"location":"newoutput/introduction-to-microsoft-azure/#terraform-modules","title":"Terraform Modules","text":"<p>Create all of your resources in one Coralogix-provided file. Using our Terraform modules, you can easily install and manage Coralogix integrations with Azure services as modules in your infrastructure code:</p> <ul> <li> <p>Event Hub</p> </li> <li> <p>Blob Storage via Event Grid</p> </li> <li> <p>Queue Storage</p> </li> <li> <p>Diagnostic Data</p> </li> </ul>"},{"location":"newoutput/introduction-to-microsoft-azure/#azure-platform-monitoring","title":"Azure Platform Monitoring","text":"<p>Each integration provides you with a pre-made package of extensions, allowing you easily and swiftly to monitor the data in your Coralogix dashboard. See our Azure Platform Monitoring Guide.</p>"},{"location":"newoutput/introduction-to-microsoft-azure/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/ip-access-control/","title":"IP Access Control","text":"<p>At Coralogix we take the safety and security of your information very seriously. As a result, access to your account is already protected with SHA-256 with RSA Encryption.  </p> <p>Several of our largest customers asked for more, and we listened: They requested the ability to restrict access to their Teams from predefined IP addresses, or network ranges. That\u2019s why we introduced the IP Access feature. You can think of this as an Access Control List (ACL) for Coralogix.</p>"},{"location":"newoutput/ip-access-control/#configure-ip-access","title":"Configure IP Access","text":"<p>STEP 1. Log in to your Coralogix Team. STEP 2. Navigate to settings in your Coralogix dashboard by clicking on your initials in the upper right-hand corner.</p> <p></p> <p>STEP 3. Click IP Access in the left-hand sidebar.</p> <p></p> <p>STEP 4. Click ENABLE IP ACCESS.</p> <p>STEP 5. Using CIDR notation, enter the IP address of the host or networks to which you would like to restrict access. Single host addresses do not require a network mask, but you could use /32 if you choose to.  </p> <p>For the first address defined in the ACL, configure your local network address (for example x.y.z.0/24), or at least the public IP address of the local host (a.b.c.d) currently used to access your Coralogix account (we detect this IP address automatically for you). When defining a CIDR block, we automatically display on the right side of the UI the corresponding authorized network range.</p> <p>STEP 6. Once you have defined the desired IP address and the Name (optional), click Send to save the address to the ACL.</p> <p></p>"},{"location":"newoutput/ip-access-control/#faqs","title":"FAQs","text":"<p>What if I would like to open access to my Coralogix Team to a list of addresses (network ranges or hosts)?</p> <p>You can easily do that by creating a .csv file with any of the following structures:</p> <ul> <li> <p>CIDR block, name</p> </li> <li> <p>hostIP, name</p> </li> <li> <p>Or you could omit the name altogether, although it is recommended to use a name, which would allow you to easily visualize in the UI which address or range of addresses belong where, for example: Engineering, Support, etc.</p> </li> </ul> <p>The following example shows what your .csv file could look like:  </p> <p>104.18.10.234, TestPC 104.18.11.0/24, Support Etc.  </p> <p>Please note that the space between the comma (,) and the name is optional. If you omit the name, then \u201cN/A\u201d will be used to designate the host address or network range. Once you have created your .csv file, click CHOOSE A CSV FILE in the UI followed by Open to add.\u00a0</p> <p>Can I easily edit/ delete addresses / hosts previously authorized? </p> <p>Yes of course, you can do that from the UI. You can also enable/disable a network range or host without deleting it.</p> <p>Does the IP Access feature supports IPv6? </p> <p>Currently the IP Access feature only supports IPv4.</p> <p>What happens when I remove the last authorized network range or host? </p> <p>When you do this, you will be effectively removing any ACL restriction to your system.</p>"},{"location":"newoutput/ip-access-control/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/jaeger-client/","title":"Jaeger Client","text":"<p>With the help of Coralogix, you can now monitor trace information. Keep an eye on requests which allow you to monitor latency, and know how much time a request would take to finish.</p> <p>You will also be able to better troubleshoot, pinpoint bottlenecks, and identify which service or services are contributing to the application latency.</p> <p>First, you'll need to connect Jaeger to your Coralogix account to start collecting your tracing data.</p>"},{"location":"newoutput/jaeger-client/#in-the-navigation","title":"In the Navigation","text":"<p>Once you start sending tracing data, you can view it by clicking on the dotted menu in the top right corner, and selecting Jaeger.</p> <p></p> <p></p> <p>Once in the Jaeger UI, you can select any service from the services you are collecting tracing for and hit search to see the results.</p> <p></p> <p></p> <p>You can search by:</p> <ul> <li>Service.</li> <li>Tags.</li> <li>Max and Min Duration.</li> <li>Operation.</li> </ul>"},{"location":"newoutput/jaeger-client/#direct-from-the-logs-screen","title":"Direct from the Logs Screen","text":"<p>You can also view the trace for a specific log via the Logs Screen by creating a custom Coralogix Action.</p> <p>To add an Action that links your logs to traces, go to Logs &gt; Settings &gt; Manage actions and add a new action \u201cView Trace\u201d with the following URL structure:</p> <p>https://YOUR_TEAM_NAME.com/tracing/trace/{{$d.YOUR_TRACE_ID_KEY}}?uiFind={{$d.YOUR_SPAN_ID_KEY}}</p> <p>You can then use this custom Action by clicking on the 3 dots on the log line you want to view the trace for. Then select Actions and click on your new \"View Trace\" action to open the Jaeger UI.</p> <p>Here, you can see the flow, the time it took for the request to finish, and the different services that were executed.</p> <p></p> <p>Our support team is always here 24/7 to assist you with anything you need. Send us an email to support@coralogixstg.wpengine.com, or simply log in to your account and send us a message via our in-app chat.</p> <p>In addition to providing trace visualizations in the Jaeger UI, Coralogix offers hosted visualizations for logs in Kibana and for metrics in Grafana.</p>"},{"location":"newoutput/java-opentelemetry-instrumentation/","title":"Java OpenTelemetry Instrumentation","text":"<p>This tutorial demonstrates how to instrument your Java applications to capture OpenTelemetry traces and send them to Coralogix.</p> <p>OpenTelemetry-Java automatic instrumentation is the most efficient method for adding instrumentation to Java applications. Requiring minimal modifications to the code, it uses a Java agent that can be attached to any Java 8+ application and dynamically injects bytecode to capture telemetry from several popular libraries and frameworks.</p> <p>Note! While this tutorial demonstrates how to send data directly to Coralogix, we do not recommend doing so. Instead, we recommend using the OpenTelemetry Collector as a centralized place for sending your data to us.</p>"},{"location":"newoutput/java-opentelemetry-instrumentation/#installation","title":"Installation","text":"<p>The instructions below conform to the latest OpenTelemetry\u00a0Java Auto Instrumentation, currently Java agent version <code>1.22.1</code>.</p> <p>STEP 1. Download and distribute the agent JAR.</p> <ul> <li> <p>Download the latest OpenTelemetry Java agent.</p> </li> <li> <p>Apply the agent to each service host or container that requires access to it.</p> </li> <li> <p>The JVM will require access to the agent to function properly.</p> </li> </ul> <p>STEP 2. Update the JVM configuration.</p> <p>Either of the following options may be used as the template, with the following changes:</p> <ul> <li> <p><code>JAVA_TOOL_OPTIONS</code>: Replace the path to the java agent JAR and the <code>coralogix-opentemetry</code> JAR files with the file's location downloaded and distributed in\u00a0STEP 1 above.</p> </li> <li> <p><code>OTEL_EXPORTER_OTLP_ENDPOINT</code>: Select the OpenTelemetry endpoint associated with your Coralogix domain.</p> </li> <li> <p><code>OTEL_RESOURCE_ATTRIBUTES</code>: Specify your Service.Name.</p> </li> <li> <p><code>OTEL_RESOURCE_ATTRIBUTES</code>: Specify Your Coralogix Send-Your-Data API key, application, and subsystem name.</p> </li> <li> <p>Specify 4 RESOURCE_ATTRIBUTES for application and subsystem:</p> <ul> <li> <p><code>application.name</code>: Replace with the name used for the identification of your Coralogix application name</p> </li> <li> <p><code>api.name</code>: Replace with the name used for the identification of your Coralogix subsystem name</p> </li> <li> <p><code>cx.application.name</code>: Replace with the name used for the identification of your Coralogix application name</p> </li> <li> <p><code>cx.subsystem.name</code>: Replace with the name used for the identification of your Coralogix subsystem name</p> </li> </ul> </li> </ul> <p>Option 1 (recommended): Leveraging environment variables</p> <ul> <li>Pass the Configuration parameters as system properties environment variables in the JVM:</li> </ul> <pre><code>export JAVA_TOOL_OPTIONS=\"-javaagent:path/to/opentelemetry-javaagent.jar -Dotel.javaagent.extensions=build/libs/opentelemetry-java-instrumentation-extension-demo-1.0-all.jar\"\n\nexport OTEL_TRACES_EXPORTER=\"otlp\"\nexport OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=\"grpc\"\nexport OTEL_EXPORTER_OTLP_ENDPOINT=\"&lt;coralogix_otel_endpoint&gt;\"\nexport OTEL_EXPORTER_OTLP_TRACES_HEADERS=\"Authorization=Bearer &lt;CXPrivateKey&gt;\"\nexport OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;ServiceName&gt;,application.name=&lt;CXApplicationName&gt;,api.name=&lt;CXSubsystemName&gt;,cx.application.name=&lt;CXApplicationName&gt;,cx.subsystem.name=&lt;CXSubsystemName&gt;\n\njava -javaagent:&lt;/path/to/&gt;opentelemetry-javaagent.jar -jar myapp.jar\n\n</code></pre> <ul> <li>View the full range of configuration options here.</li> </ul> <p>Option 2: Changing the Java command line</p> <ul> <li>Enable the instrumentation agent using the <code>-javaagent</code> flag to the JVM and pass the Configuration parameters as Java system properties (<code>-D</code> flags):</li> </ul> <pre><code>java -javaagent:path/to/opentelemetry-javaagent.jar \\\\\n    -Dotel.javaagent.extensions=build/libs/opentelemetry-java-instrumentation-extension-demo-1.0-all.jar\"\n    -Dotel.traces.exporter=otlp \\\\\n    -Dotel.exporter.otlp.traces.protocol=grpc \\\\\n    -Dotel.exporter.otlp.traces.endpoint=&lt;coralogix_otel_endpoint&gt; \\\\\n    -Dotel.exporter.otlp.traces.headers=Authorization=Bearer \"&lt;CXPrivateKey&gt;\" \\\\\n    -Dotel.resource.attributes=service.name=&lt;ServiceName&gt;,application.name=&lt;CXApplicationName&gt;,api.name=&lt;CXSubsystemName&gt;,cx.application.name=&lt;CXApplicationName&gt;,cx.subsystem.name=&lt;CXSubsystemName&gt; \\\\\n    -jar myapp.jar\n</code></pre>"},{"location":"newoutput/java-opentelemetry-instrumentation/#troubleshooting","title":"Troubleshooting","text":"<p>Confirm that the instrumentation was installed by looking for the following log lines in your console:</p> <pre><code>[otel.javaagent 2023-02-14 10:40:00:811 +0000] [main] INFO io.opentelemetry.javaagent.tooling.VersionLogger - opentelemetry-javaagent - version: 1.21.1\n\n\n</code></pre>"},{"location":"newoutput/java-opentelemetry-instrumentation/#example","title":"Example","text":"<p>JPetStore 6 is a full web application built on top of MyBatis 3, Spring 5, and Stripes.</p> <p>STEP 1. Install a version of the Java Development Kit. It will not deploy when using only the Java Runtime Environment.</p> <p>STEP 2. Download the JDK for your platform.</p> <p>STEP 3. Clone the jpetstore git repo:</p> <pre><code>git clone &lt;https://github.com/kazuki43zoo/mybatis-spring-boot-jpetstore.git&gt;\n\n</code></pre> <p>STEP 4. Run the following:</p> <pre><code>./mvnw clean package -DskipTests=true\n</code></pre> <p>STEP 5. Download the OpenTelemetry agent.</p> <p>STEP 6. Apply the environment variables:</p> <pre><code>export OTEL_TRACES_EXPORTER=\"otlp\"\nexport OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=\"grpc\"\nexport OTEL_EXPORTER_OTLP_ENDPOINT=\"&lt;coralogix_otel_endpoint&gt;\"\nexport OTEL_EXPORTER_OTLP_TRACES_HEADERS=\"Authorization=Bearer &lt;CXPrivateKey&gt;\"\nexport OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;ServiceName&gt;,application.name=&lt;CXApplicationName&gt;,api.name=&lt;CXSubsystemName&gt;,cx.application.name=&lt;CXApplicationName&gt;,cx.subsystem.name=&lt;CXSubsystemName&gt;\n\n</code></pre> <p>STEP 7. Run the project with the agent:</p> <pre><code>java -javaagent:&lt;/path/to/&gt;opentelemetry-javaagent.jar -jar mybatis-spring-boot-jpetstore-2.0.0-SNAPSHOT.jar\n\n</code></pre> <p>STEP 8. Navigate to http://localhost:8080/.</p> <p>STEP 9. View the traces in your Coralogix dashboard.</p> <p></p>"},{"location":"newoutput/java-opentelemetry-instrumentation/#additional-instrumentation-using-annotations","title":"Additional Instrumentation using Annotations","text":"<p>STEP 1. Open the project with your favorite IDE/text editor.</p> <p>STEP 2. Edit the <code>pom.xml</code> file and add the following maven dependency:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.opentelemetry.instrumentation&lt;/groupId&gt;\n    &lt;artifactId&gt;opentelemetry-instrumentation-annotations&lt;/artifactId&gt;\n    &lt;version&gt;1.22.1&lt;/version&gt;\n&lt;/dependency&gt;\n\n</code></pre> <p></p> <p>STEP 3. Edit file src/main/java/com/kazuki43zoo/jpetstore/ui/Cart.java.</p> <p>STEP 4. Add this import to the Java file where the methods you want to trace are present.</p> <pre><code>import io.opentelemetry.instrumentation.annotations.WithSpan;\n\n</code></pre> <p>STEP 5. Before the function you wish to trace, add the annotation \u201cWithSpan\u201d.</p> <p></p> <p>Example: <code>cart.java</code></p> <p>STEP 6. Rebuild your springboot application applying the changes (using either your IDE or command line).</p> <p>STEP 7. Run your application again with the Otel agent:</p> <pre><code>java -javaagent:&lt;/path/to/&gt;opentelemetry-javaagent.jar -jar mybatis-spring-boot-jpetstore-2.0.0-SNAPSHOT\n\n</code></pre>"},{"location":"newoutput/java-opentelemetry-instrumentation/#validation","title":"Validation","text":"<p>In your Coralogix dashboard, navigate to Explore &gt; Tracing to view the traces generated by your application.</p> <p>Your traces should now contain additional spans on annotated methods.</p> <p></p> <p></p>"},{"location":"newoutput/java-opentelemetry-instrumentation/#service-flows","title":"Service Flows","text":"<p>For customers with functional\u00a0Java OpenTelemetry instrumentation, this section guides reconfiguring the existing setup to define, report, and monitor Coralogix\u00a0Service Flows.</p> <p>For new customers or those who haven\u2019t configured the Java OpenTelemetry instrumentation, you must follow the Installation instructions above. The steps in this section are included in those instructions.</p> <ul> <li> <p>Download v1.1 of the <code>coralogix-opentelemetry</code> extension jar from our\u00a0jfrog artifatory.</p> </li> <li> <p>Update the JVM configuration to include\u00a0<code>JAVA_TOOL_OPTIONS</code>. Replace the path to the Java <code>agent JAR and the coralogix-opentemetry JAR files with the file's location</code> downloaded and distributed in\u00a0STEP 1.</p> </li> <li> <p>If leveraging environment variables as part of the JVM configuration (Option 1), add the following:</p> </li> </ul> <pre><code>export JAVA_TOOL_OPTIONS=\"-javaagent:path/to/opentelemetry-javaagent.jar -Dotel.javaagent.extensions=path/to/opentelemetry-java-extensions-1.1.jar\"\n</code></pre> <ul> <li>If changing the Java command line as part of the JVM configuration (Option 2), add the following:</li> </ul> <pre><code>java -javaagent:path/to/opentelemetry-javaagent.jar -Dotel.javaagent.extensions=path/to/opentelemetry-java-extensions-1.0.jar -jar your-jar\n</code></pre>"},{"location":"newoutput/java-opentelemetry-instrumentation/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/jenkins-telemetry/","title":"Jenkins Telemetry","text":"<p>Monitoring Jenkins telemetry is critical to understanding what is causing delays and failures in your CI/CD pipelines. Coralogix leverages the OpenTelemetry plugin for Jenkins to monitor metrics and traces. Logs and tagging are supported using our Coralogix plugin for Jenkins.</p>"},{"location":"newoutput/jenkins-telemetry/#supported-os","title":"Supported OS","text":"<p>Linux, Windows, macOS</p>"},{"location":"newoutput/jenkins-telemetry/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Jenkins server</p> </li> <li> <p>Jenkins account with permissions to add plugins and \u201csystem\u201d configuration adjusted</p> </li> </ul>"},{"location":"newoutput/jenkins-telemetry/#installation","title":"Installation","text":"<p>STEP 1. To install the OpenTelemetry plugin for Jenkins, navigate to Manage Jenkins &gt; Plugins &gt; Available Plugins and search for OpenTelemetry Plugin.</p> <p>STEP 2. Select the checkbox to the left of the plugin and click Install.</p> <p>STEP 3. Restart Jenkins.</p>"},{"location":"newoutput/jenkins-telemetry/#configuration","title":"Configuration","text":"<p>Configure the OpenTelemetry plugin by accessing the system configuration under Manage Jenkins &gt; System. Scroll down to locate the OpenTelemetry section.</p> <p>STEP 1. Configure the OpenTelemetry endpoint adjusted for your Coralogix domain.</p> <p>STEP 2. Under Authentication, create a Bearer Token Authentication token by clicking +Add. You\u2019ll want to create a Secret text similar to the example below.</p> <p></p> <p>STEP 3. Once you\u2019ve created the credential, select it in the Token dropdown.</p> <p>At this point, your configuration should look similar to this:</p> <p></p>"},{"location":"newoutput/jenkins-telemetry/#advanced-configuration","title":"Advanced Configuration","text":"<p>STEP 1. In the Advanced Configuration section, set a Service name for all the payloads submitted by the OpenTelemetry plugin. Choose something that is appropriate for your deployment. You can adjust the Disabled resource providers and Steps to be ignored if needed or leave the default options.</p> <p>STEP 2. In Configuration properties, you will set the OpenTelemetry Java SDK configuration. At a minimum, add the following two lines:</p> <pre><code>otel.exporter.otlp.protocol=http/protobuf\notel.resource.attributes=cx.application.name=Jenkins-otel,cx.subsystem.name=Jenkins-otel\n\n</code></pre> <p>The otel.exporter.otlp.protocol sets the OTLP exporter to use the http/protobuf protocol instead of grpc, as this is our preferred protocol. The otel.resource.attributes is needed to set your application name and subsystem. You can add additional resource attributes as needed by appending additional key-value pairs using comma separations.</p> <p>STEP 3. Set your Service namespace to something appropriate for your use case.</p>"},{"location":"newoutput/jenkins-telemetry/#optional-configuration","title":"Optional Configuration","text":"<p>This section will detail some of the optional configurations that can be made.</p>"},{"location":"newoutput/jenkins-telemetry/#export-opentelemetry-configuration-as-environment-variables","title":"Export OpenTelemetry Configuration as Environment Variables","text":"<p>The \u201cExport OpenTelemetry configuration as environment variables\u201d checkbox allows you to capture all of the configured OpenTelemetry SDK values as environment variables. This feature is necessary if you intend to install any of the optional Extensions that can instrument non-Jenkins executables like Maven, Ansible, and others. You can read about the extension in the OpenTelemetry documentation.</p> <p>Notes:</p> <ul> <li> <p>This setting will expose your Coralogix private key in the OTEL_EXPORTER_OTLP_HEADERS variable.</p> </li> <li> <p>You may also have other services running on your Jenkins hosts that also use/set these environment variables. Be aware of that before enabling as it could affect other services.</p> </li> </ul>"},{"location":"newoutput/jenkins-telemetry/#disabled-resource-providers","title":"Disabled Resource Providers","text":"<p>The \u201cDisabled resource providers\u201d section allows you to disable any resource provider processors that would otherwise generate metadata to enrich your Traces and Metrics. Sometimes, you can get conflicting metadata by having too many resource providers enabled by default. You can disable specific providers using this setting. You can read about the available resource providers for the Java SDK here.</p>"},{"location":"newoutput/jenkins-telemetry/#logs-exporter","title":"Logs Exporter","text":"<p>While we recommend our Coralogix plugin for Jenkins for logs and tags, you can use the OpenTelemetry plugin to collect logs instead. Beware that this option has limitations regarding the scope of logging from the OpenTelemetry Collector. To enable logs exporting, you\u2019ll need to add two additional lines to the Configuration Properties under Advanced:</p> <pre><code>otel.logs.exporter=otlp\notel.logs.mirror_to_disk=true\n\n</code></pre> <p>This will enable the OpenTelemetry plugin to submit system, audit, and pipeline logs to Coralogix. However, the scope of pipeline logs is limited to anything that is executed directly by the Jenkins agents. If you install the appropriate Extensions in your environment, you can capture external applications, such as Maven or Ansible.</p>"},{"location":"newoutput/jenkins-telemetry/#disable-metric-or-trace-pipeline","title":"Disable Metric or Trace Pipeline","text":"<p>If you only want metrics or traces, you can disable the other pipeline by setting the appropriate exporter configuration to <code>none</code> in the \u201cConfiguration Properties\u201d under Advanced like below:</p> <pre><code>otel.traces.exporter=none\nOR\notel.metrics.exporter=none\n</code></pre>"},{"location":"newoutput/jenkins-telemetry/#validation","title":"Validation","text":"<p>After configuring the Opentelemetry plugin, restart your Jenkins instance. Upon logging back in, you should start seeing a variety of metrics and traces being submitted to the Coralogix platform aligned with your configured application and subsystem names.</p>"},{"location":"newoutput/jenkins-telemetry/#troubleshooting","title":"Troubleshooting","text":"<p>If you don\u2019t see any metrics or traces arriving in your Coralogix account, make sure to double-check your configurations of the OpenTelemetry plugin. You can use the Noteworthy active configuration properties and Active resource attributes section of the Advanced configuration to quickly see what settings the Java OpenTelemetry SDK is using.</p> <p>If all Jenkins configuration is as expected, double-check your Jenkins host\u2019s environment variables as other OpenTelemetry-based applications could be setting variables conflicting with the Jenkins OpenTelemetry plugin.</p> <p>If all your settings are correct, ensure your Jenkins nodes can reach the ingress endpoint. Be aware of any Proxies or security appliances/applications in the egress route to the internet.</p> <p>If the endpoint is reachable but you still don\u2019t see any payloads, try submitting to a local OpenTelemetry Collector instead of directly to Coralogix. Review our OpenTelemetry Collector documentation for steps to deploy locally. Once deployed locally, adjust the OTLP Endpoint in the configuration to point to your locally hosted OpenTelemetry Collector.</p>"},{"location":"newoutput/jenkins-telemetry/#additional-resources","title":"Additional Resources","text":"DocumentationOpenTelemetry Plugin for JenkinsOpenTelemetry Documentation for Java SDK and API"},{"location":"newoutput/jenkins-telemetry/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogix.com.</p>"},{"location":"newoutput/json-stringify/","title":"Json Stringify","text":"<p>Json Stringify rule allows you to change an array or an object from a json formatted to text.</p> <p>This rule helps you reduce the number of fields but still keep the data in\u00a0\u00a0Coralogix to be searchable using free text search.</p> <p>Steps to\u00a0\u00a0configure:</p> <ol> <li>Click on Data flow.</li> <li>Click on parsing rules.</li> <li>Select Stringify\u00a0\u00a0Json field rule from the rules list.</li> </ol> <p></p> <p>4-Configure your group name, rule matchers and all you see needed for your rule.</p> <p></p> <ul> <li>Source Field: the field you want to convert to text.</li> <li>Keep or delete source field option.</li> </ul> <p>Select keep if you want to keep the original field with its content.</p> <p>Select Delete if you want to delete the original field and its content.</p> <ul> <li>Destination field is the new field where you want the content of the source to be put under.</li> </ul> <p>In the example below, I have a field called sessionIssuer which is an abject and I am going to change it to text and give it a new name called application.</p>"},{"location":"newoutput/json-stringify/#original-message","title":"Original Message:","text":"<p>New message with delete source field selected</p> <p></p> <p>New message with keep source field.</p> <p></p> <p>In case you have more questions please contact support through chat or send us an email to support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/jumpcloud-coralogix-integration/","title":"JumpCloud","text":"<p>JumpCloud\u2019s open directory platform allows the user to unify technology stack across identity, access, and device management, in a manner that doesn\u2019t sacrifice security or functionality.</p> <p>JumpCloud can be integrated to Coralogix using a designated script</p>"},{"location":"newoutput/jumpcloud-coralogix-integration/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>A virtual machine used as an intermediate - It is recommended to use at least 2 vCPU and 2GB of RAM</p> </li> <li> <p>Access to JumpCloud platform as Administrator for the API Key</p> </li> </ol> <p>It is recommended to create a read-only administrator account for the API key used in the integration</p> <p>The API key can be found by clicking your initials on the top right of the JumpCloud platform and then clicking \"My API Key\"</p> <p></p>"},{"location":"newoutput/jumpcloud-coralogix-integration/#step-1-creating-the-relevant-files-in-the-virtual-machine","title":"STEP 1. Creating the relevant files in the virtual machine","text":"<ul> <li> <p>Login to the Instance that will be used for shipping the JumpCloud logs</p> </li> <li> <p>Optional - Create a directory called \"JumpCloud\" to host all the integration files</p> </li> <li> <p>Copy the script content and paste it in a file called <code>script.ps1</code> on the instance in a location of your choosing</p> </li> <li> <p>Copy the configuration file content and paste it in a file called <code>configurations.json</code> on the instance in a location of your choosing</p> </li> <li> <p>Edit the configuration file in the following manner</p> <ul> <li> <p>jumpcloud.api_key - Your JumpCloud admin private key.</p> </li> <li> <p>siem.headers.private_key - Your Coralogix private key. can be found in your Coralogix account under Settings &gt; Send Your Data. Learn more about your\u00a0Send-Your-Data API key.</p> </li> <li> <p>siem.url - https://ingress./logs/datastream <ul> <li>Coralogix domain - the region-specific endpoint associated with your Coralogix Account.\u00a0You can find your Domain here.</li> </ul> <li> <p>custom_log_fields.reqHost - Application name</p> </li> <li> <p>custom_log_fields.customField - Subsystem name</p> </li>"},{"location":"newoutput/jumpcloud-coralogix-integration/#step-2-installing-powershell-non-windows","title":"STEP 2. Installing PowerShell (Non Windows)","text":"<p>In non-Windows operation systems, install PowerShell on the instance by copying and running the following script</p> <pre><code># Update the list of packages\nsudo apt-get update\n# Install pre-requisite packages.\nsudo apt-get install -y wget apt-transport-https software-properties-common\n# Download the Microsoft repository GPG keys\nwget -q \"https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/packages-microsoft-prod.deb\"\n# Register the Microsoft repository GPG keys\nsudo dpkg -i packages-microsoft-prod.deb\n# Update the list of packages after we added packages.microsoft.com\nsudo apt-get update\n# Install PowerShell\nsudo apt-get install -y powershell\n# Start PowerShell\npwsh\n</code></pre> <p>Note: After the PowerShell installation was successful, create a file called powershell.config.json in the correct path (the example uses PowerShell 7)</p> <ul> <li> <p>For MacOS - /usr/local/microsoft/powershell/7</p> </li> <li> <p>For Linux - /opt/microsoft/powershell/7</p> </li> </ul> <p>with the following content</p> <pre><code>{\n    \"LogLevel\": \"Critical\"\n}\n</code></pre> <p>For more information, visit Microsoft documentation</p>"},{"location":"newoutput/jumpcloud-coralogix-integration/#step-3-install-the-jumpcloud-module","title":"STEP 3. Install the JumpCloud module","text":"<pre><code>Install-Module -Name JumpCloud\n</code></pre>"},{"location":"newoutput/jumpcloud-coralogix-integration/#step-4-creating-the-crontab","title":"STEP 4. Creating the Crontab","text":"<ul> <li>In the virtual machine instance, create a cronjob with the following command (edit according to the files paths you saved on step 1)</li> </ul> <pre><code>crontab -e\n</code></pre> <ul> <li>Inside the crontab document, paste the following line</li> </ul> <pre><code>* * * * * /usr/bin/pwsh /home/ubuntu/script.ps1 -config_file:/home/ubuntu/configurations.json 2&gt;&amp;1\n</code></pre> <ul> <li>Save &amp; quit the crontab</li> </ul> <p>After successfully completing the provided steps, logs will start to ingest to the provided Coralogix account</p> <p>Log example</p> <pre><code>{\n  \"jumpcloud\": {\n    \"initiated_by\": {\n      \"id\": \"637212c33f396457d287dad6\",\n      \"type\": \"admin\",\n      \"email\": \"admin@coralogixstg.wpengine.com\"\n    },\n    \"geoip\": {\n      \"country_code\": \"IL\",\n      \"timezone\": \"Asia/Jerusalem\",\n      \"latitude\": 32.0803,\n      \"continent_code\": \"AS\",\n      \"region_name\": \"Tel Aviv\",\n      \"longitude\": 34.7805,\n      \"region_code\": \"TA\"\n    },\n    \"useragent\": {\n      \"minor\": \"0\",\n      \"os\": \"Mac OS X\",\n      \"os_minor\": \"15\",\n      \"os_major\": \"10\",\n      \"os_version\": \"10.15.7\",\n      \"version\": \"108.0.0.0\",\n      \"os_patch\": \"7\",\n      \"patch\": \"0\",\n      \"os_full\": \"Mac OS X 10.15.7\",\n      \"major\": \"108\",\n      \"name\": \"Chrome\",\n      \"os_name\": \"Mac OS X\",\n      \"device\": \"Mac\"\n    },\n    \"mfa\": false,\n    \"event_type\": \"admin_login_attempt\",\n    \"success\": true,\n    \"service\": \"directory\",\n    \"organization\": \"637212c33f396\",\n    \"@version\": \"1\",\n    \"client_ip\": \"10.20.30.40\",\n    \"id\": \"639f9ea9ac5d37\",\n    \"jc_timestamp\": \"2022-12-18T14:57:03.801Z\",\n    \"reqHost\": \"jumpcloud\",\n    \"customField\": \"jumpcloud\",\n    \"severity\": \"info\"\n  }\n}\n</code></pre>"},{"location":"newoutput/jumpcloud-scim-identity-management-integration/","title":"JumpCloud SCIM Identity Management","text":"<p>Send your logs to Coralogix using the JumpCloud SCIM Identity Management Integration.</p> <p>The Custom SCIM Identity Management integration allows you to provision, update, and deprovision users and groups from JumpCloud in applications that support SCIM. Leverage this integration to centralize user lifecycle management, sync user data, manage groups, and control access and authorization from the JumpCloud Admin Portal.</p>"},{"location":"newoutput/jumpcloud-scim-identity-management-integration/#configuration","title":"Configuration","text":"<p>STEP 1. Single Sign-On</p> <p>Login to JumpCloud portal. Select SSO &gt; Coralogix SSO application</p> <p></p> <p>STEP 2. Service Provider Configuration</p> <ul> <li>Navigate to the Identity Management tab.</li> </ul> <p></p> <ul> <li>Input the Base URL as one of the following values for SCIM Connector Base URL. The URL should correspond to the Coralogix domain where your data is stored.</li> </ul> RegionTenant URLUS1https://ng-api-http.coralogix.us/scimEU1https://ng-api-http.coralogixstg.wpengine.com/scimEU2https://ng-api-http.eu2.coralogixstg.wpengine.com/scimAP1 (IN)https://ng-api-http.app.coralogix.in/scimAP2 (SG)https://ng-api-http.coralogixsg.com/scim <ul> <li>Add the Token Key. This can be found in your Coralogix team settings &gt; Configure SAML &gt; Provisioning Token</li> </ul> <p></p> <p></p> <ul> <li>Input the email address of a user that belongs to one of the relevant groups in your SSO under Test User Email.</li> </ul> <p></p> <p>STEP 3. Test Connection</p> <ul> <li>Click Test Connection.\u00a0 You should see the following popup confirming the connection was successful:</li> </ul> <p></p> <ul> <li>Ensure Group management is enabled.</li> </ul> <p>STEP 4. Activation</p> <ul> <li>Click Activate.</li> </ul> <p></p> <ul> <li>Click Save.</li> </ul> <p>Step 5. Assign your groups to the Coralogix SSO application</p> <ul> <li> <p>Go to JumpCloud \"User Groups\".</p> </li> <li> <p>For each group that should be managed by SCIM, attach it to the Coralogix SSO application as shown below.</p> </li> </ul> <p></p>"},{"location":"newoutput/jumpcloud-scim-identity-management-integration/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/k8s-otel-advancedconfig/","title":"Kubernetes Complete Observability: Advanced Configuration","text":"<p>Coralogix offers Kubernetes Observability using OpenTelemetry for comprehensive Kubernetes and application observability. This tutorial will guide you through advanced configuration options for Kubernetes clusters.</p> <p>For basic configuration, view our tutorial here.</p>"},{"location":"newoutput/k8s-otel-advancedconfig/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Kubernetes\u00a0v1.24+ installed, with the command-line tool\u00a0kubectl</p> </li> <li> <p>Helm\u00a0v3.9+ installed and configured</p> </li> </ul>"},{"location":"newoutput/k8s-otel-advancedconfig/#overview","title":"Overview","text":"<p>The OpenTelemetry Integration Chart utilises the values.yaml file as its default configuration. This configuration is based on the OpenTelemetry Collector Configuration for both the OpenTelemetry Agent Collector and OpenTelemetry Cluster Collector.</p>"},{"location":"newoutput/k8s-otel-advancedconfig/#default-configuration-in-3-easy-steps","title":"Default Configuration in 3 Easy Steps","text":"<p>STEP 1. Create a new YAML-formatted override file that defines certain values for the OpenTelemetry Integration Chart.</p> <p>The following global values are the minimum required configurations to getting the chart working:</p> <pre><code># values.yaml\nglobal:\n  domain: \"&lt;coralogix-endpoint&gt;\"\n  clusterName: \"&lt;k8s-cluster-name&gt;\"\n\n</code></pre> <ul> <li> <p>Input the following values:</p> <ul> <li> <p><code>domain</code>: Choose the\u00a0OpenTelemetry endpoint\u00a0for the\u00a0domain\u00a0associated with your Coralogix account.</p> </li> <li> <p><code>clusterName</code>: You are also required to specify as a cluster identifier.</p> </li> </ul> </li> <li> <p>You also may copy all or any other configurations from the repository <code>values.yaml</code> file.</p> </li> <li> <p>If you'd like to provide your own overrides for an array values such as\u00a0<code>extraEnvs</code>,\u00a0<code>extraVolumes</code>\u00a0or\u00a0<code>extraVolumeMounts</code>, be aware that Helm does not support merging arrays. Instead, arrays are nulled out.</p> </li> <li> <p>In case you'd like to provide your own values for these arrays, first copy over any existing array values\u00a0from the provided\u00a0<code>values.yaml</code>\u00a0file.</p> </li> </ul> <p>STEP 2. Save this file as <code>values.yaml</code></p> <p>STEP 3. Install with the <code>helm upgrade --install</code> command</p> <pre><code>helm upgrade --install otel-integration coralogix-charts-virtual/otel-integration -f values.yaml -n $NAMESPACE\n\n</code></pre>"},{"location":"newoutput/k8s-otel-advancedconfig/#optional-configurations","title":"Optional Configurations","text":""},{"location":"newoutput/k8s-otel-advancedconfig/#enabling-dependent-charts","title":"Enabling Dependent Charts","text":"<p>The OpenTelemetry Agent is primarily used for collecting application telemetry, while the OpenTelemetry Cluster Collector is primarily used to collect cluster-level data. Depending on your requirements, you can either use the default configuration that enables both components, or you can choose to disable either of them by modifying the <code>enabled</code> flag in the <code>values.yaml</code> file under the <code>opentelemetry-agent</code> or <code>opentelemetry-cluster-collector</code> section as shown below:</p> <pre><code>...\nopentelemetry-agent:\n  enabled: true\n  mode: daemonset\n...\nopentelemetry-cluster-collector:\n  enabled: true\n  mode: deployment\n\n</code></pre>"},{"location":"newoutput/k8s-otel-advancedconfig/#installing-the-chart-on-clusters-with-mixed-operating-systems-linux-and-windows","title":"Installing the Chart on Clusters with Mixed Operating Systems (Linux and Windows)","text":"<p>Installing <code>otel-integration</code> is also possible on clusters that support running Windows workloads on Windows node alongside Linux nodes (such as EKS, AKS or GKE). The <code>kube-state-metrics</code> and collector will be installed on Linux nodes, as these components are supported only on Linux operating systems. Conversely, the agent will be installed on both Linux and Windows nodes as a daemonset, in order to collect metrics for both operating systems. In order to do so, the chart needs to be installed with few adjustments.</p> <p>Adjust the Helm command in STEP 10 of the basic configuration to use the <code>values-windows.yaml</code> file as follows:</p> <pre><code>helm upgrade --install otel-coralogix-integration coralogix/otel-integration -n $NAMESPACE -f values-windows.yaml --set global.domain=\"coralogixstg.wpengine.com\" --set global.clusterName=\"&lt;cluster name&gt;\"\n\n</code></pre>"},{"location":"newoutput/k8s-otel-advancedconfig/#service-pipelines","title":"Service Pipelines","text":"<p>The OpenTelemetry Collector Configuration guides you to initialise components and then add them to the pipelines in the <code>service</code> section. It is important to ensure that the telemetry type is supported. For example, the <code>[prometheus](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver#prometheus-receiver)</code> receiver documentation in the README states that it only supports <code>metrics</code>. Therefore, the following <code>prometheus</code> receiver can only be defined under <code>receivers</code> and added to the <code>metrics</code> pipelines in the <code>service</code> block to enable it.</p> <pre><code>opentelemetry-agent:\n...\n    config:\n        receivers:\n            prometheus:\n        config:\n          scrape_configs:\n            - job_name: opentelemetry-infrastructure-collector\n              scrape_interval: 30s\n              static_configs:\n                - targets:\n                    - ${MY_POD_IP}:8888\n\n        ...\n      service:\n        pipelines:\n                logs:\n                    ...\n                metrics:\n                    receivers:\n                    - prometheus\n          traces:\n                        ...\n\n</code></pre>"},{"location":"newoutput/k8s-otel-advancedconfig/#coralogix-exporter","title":"Coralogix Exporter","text":"<p>In both charts, you have the option to configure the sending of logs, metrics, and / or traces to Coralogix. This can be done by configuring the Coralogix Exporter for different pipelines. The default <code>values.yaml</code> file includes all three options, but you can customize it by removing the <code>coralogix</code> exporter from the <code>pipelines</code> configuration for either <code>logs</code>, <code>metrics</code>, or <code>traces</code>.</p> <p>The following <code>opentelemetry-agent</code> exporter configuration also applies to the <code>opentelemetry-cluster-collector</code>:</p> <pre><code>global:\n  domain: \"&lt;coralogix-domain&gt;\"\n  clusterName: \"&lt;cluster-name&gt;\"\n  defaultApplicationName: \"otel\"\n  defaultSubsystemName: \"integration\"\n...\nopentelemetry-agent:\n...\n  config:\n    ...\n    exporters:\n      coralogix:\n        timeout: \"30s\"\n        private_key: \"${CORALOGIX_PRIVATE_KEY}\"\n                ## Values set in \"global\" section\n        domain: \"{{ .Values.global.domain }}\"\n                application_name: \"{{ .Values.global.defaultApplicationName }}\"\n        subsystem_name: \"{{ .Values.global.defaultSubsystemName }}\"\n    service:\n      pipelines:\n        metrics:\n          exporters:\n            - coralogix\n                            ...\n        traces:\n          exporters:\n            - coralogix\n                            ...\n        logs:\n          exporters:\n            - coralogix\n\n</code></pre>"},{"location":"newoutput/k8s-otel-advancedconfig/#opentelemetry-agent","title":"OpenTelemetry Agent","text":"<p>The OpenTelemetry Agent is enabled and deployed as a <code>daemonset</code> by default. This creates an Agent pod per node. Allowing the collection of logs, metrics, and traces from application pods to be sent to OpenTelemetry pods hosted on the same node and spreads the ingestion load across the cluster. Be aware that the OpenTelemetry Agent pods consumes resources (e.g., CPU &amp; memory) from each node on which it runs.</p> <pre><code>opentelemetry-agent:\n  enabled: true\n  mode: daemonset\n\n</code></pre> <p>Notes:</p> <ul> <li>If there are nodes without a running OpenTelemetry Agent pod, the hosted pods of applications may be missing metadata attributes (e.g. node info and host name) in the telemetry sent.</li> </ul>"},{"location":"newoutput/k8s-otel-advancedconfig/#agent-presets","title":"Agent Presets","text":"<p>The multi-instanced OpenTelemetry Agent can be deployed across multiple nodes as a <code>daemonset</code>. It provides presets for collecting host metrics, Kubernetes attributes, and Kubelet metrics. When logs, metrics, and traces are generated from a pod, the collector enriches them with the metadata associated with the hosting machine. This metadata is very useful for linking infrastructure issues with performance degradation in services.</p> <p>For more information on presets, refer to the Configuration of OpenTelemetry Collector.</p> <pre><code># example\nopentelemetry-agent:\n... \n  presets:\n    logsCollection:\n      enabled: true\n    kubernetesAttributes:\n      enabled: true\n    hostMetrics:\n      enabled: true\n    kubeletMetrics:\n      enabled: true\n\n</code></pre> <p>For example, enabling the <code>kubeletMetrics</code> preset to <code>true</code> will add configuration <code>kubeletstats</code> receiver that will pull node, pod, container, and volume metrics from the API server of the host\u2019s kubelet. It will send it down metric pipeline.</p> <pre><code># example\nreceivers:\n    kubeletstats:\n        auth_type: serviceAccount\n        collection_interval: 20s\n        endpoint: ${K8S_NODE_NAME}:10250\n\n</code></pre>"},{"location":"newoutput/k8s-otel-advancedconfig/#receivers","title":"Receivers","text":"<p>Once configured, you will be able to send logs, metrics, and traces to be collected in the OpenTelemetry Agent pods before exporting them to Coralogix.</p> <p>To achieve this, you need to first instrument your application with OpenTelemetry SDKs and expose the Collector to a corresponding receiver. It is recommended to use the OTLP receiver (OpenTelemetry protocol) for transmission over gRPC or HTTP endpoints.</p> <p>The <code>daemonset</code> deployment of the OpenTelemetry Agent also uses <code>hostPort</code> for the <code>otlp</code> port, allowing agent pod IPs to be reachable via node IPs, as follows:</p> <pre><code># K8s daemonset otlp port config\nports:\n- containerPort: 4317\n  hostPort: 4317\n  name: otlp\n  protocol: TCP\n\n</code></pre> <p>The following examples demonstrate how to configure an Auto-Instrumented JavaScript application to send traces to the agent pod\u2019s gRPC receiver.</p> <p>STEP 1. Set the Kubernetes environment variables of the JavaScript application\u2019s deployment/pod as in the example below. Define the <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> as the configured <code>NODE_IP</code> and <code>OTLP_PORT</code>. Configure <code>OTEL_TRACES_EXPORTER</code> to send in the <code>otlp</code> format. Choose <code>OTEL_EXPORTER_OTLP_PRO</code> as <code>grpc</code>.</p> <pre><code># kubernetes deployment manifest's env section\nspec:\n  containers:\n        ... \n    env:\n  - name: NODE_IP\n    valueFrom:\n      fieldRef:\n        fieldPath: status.hostIP\n  - name: OTLP_PORT\n    value: \"4317\"\n  - name: OTEL_EXPORTER_OTLP_ENDPOINT\n    value: \"http://$(NODE_IP):$(OTLP_PORT)\"\n  - name: OTEL_TRACES_EXPORTER\n    value: \"otlp\"\n    - name: OTEL_EXPORTER_OTLP_PROTOCOL\n    value: \"grpc\"\n\n</code></pre> <p>STEP 2. By default the agent has the otlp receiver configured as follows:</p> <pre><code># collector config\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: ${MY_POD_IP}:4317\n      http:\n        endpoint: ${MY_POD_IP}:4318\n\n</code></pre> <p>Notes:</p> <ul> <li> <p><code>${MY_POD_IP}</code> is a container environment variable that is mapped to the pod's IP address.</p> </li> <li> <p>The agent is also preconfigured to collect data from <code>jaeger</code>.</p> </li> </ul>"},{"location":"newoutput/k8s-otel-advancedconfig/#processors","title":"Processors","text":"<p>Processors are generally used to process logs, metrics, and traces before the data is exported. This may include, for example, modifying or altering attributes or sampling traces.</p> <p>In the example below, a <code>k8sattributes</code> processor is used to automatically discovers k8s resources (pods), extract metadata from them and add the extracted metadata to the relevant logs, metrics and spans as resource attributes.</p> <pre><code># default in values.yaml\nprocessors:\n    k8sattributes:\n    filter:\n      node_from_env_var: KUBE_NODE_NAME\n    extract:\n      metadata:\n        - \"k8s.namespace.name\"\n        - \"k8s.deployment.name\"\n        - \"k8s.statefulset.name\"\n        - \"k8s.daemonset.name\"\n        - \"k8s.cronjob.name\"\n        - \"k8s.job.name\"\n        - \"k8s.pod.name\"\n        - \"k8s.node.name\"\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>The <code>k8sattributes</code> processor is enabled by default at the <code>preset</code> level as <code>kubernetesAttributes</code> and further extended in the default <code>values.yaml</code>.</p> </li> <li> <p>More information can be found in the Kubernetes Attributes Processor README.</p> </li> </ul>"},{"location":"newoutput/k8s-otel-advancedconfig/#opentelemetry-cluster-collector","title":"OpenTelemetry Cluster Collector","text":"<p>Enable the <code>opentelemetry-cluster-collector</code> by setting <code>enabled</code> to <code>true</code>.</p> <pre><code>opentelemetry-cluster-collector:\n  enabled: true\n  mode: deployment\n\n</code></pre> <p>Notes:</p> <ul> <li>The cluster collector operates as a <code>deployment</code> workload with a minimal replica of 1 to avoid duplication of telemetry data.</li> </ul>"},{"location":"newoutput/k8s-otel-advancedconfig/#cluster-collector-presets","title":"Cluster Collector Presets","text":"<p>The cluster collector is best suited to enable presets such as Kubernetes Events and Cluster Metrics. A smaller instance count of the <code>deployment</code> is sufficient to query the Kubernetes API.</p> <pre><code>    presets:\n    clusterMetrics:\n      enabled: true\n    kubernetesEvents:\n      enabled: true\n    kubernetesExtraMetrics:\n      enabled: true\n\n</code></pre> <p>For example, if you enable the <code>kubernetesEvents</code> preset, the Kubernetes objects receiver configuration will be added dynamically during the Helm installation. This configuration enables the collection of <code>events.k8s.io</code> objects from the Kubernetes API server.</p>"},{"location":"newoutput/k8s-otel-advancedconfig/#kubernetes-events-reducing-the-amount-of-collected-data","title":"Kubernetes Events: Reducing the Amount of Collected Data","text":"<p>When collecting Kubernetes events using the cluster collector, it is common for the number of events to reach millions, especially in large clusters with numerous nodes and constantly scaling applications. To collect only the relevant data, you can use the following settings.</p>"},{"location":"newoutput/k8s-otel-advancedconfig/#cleaning-data","title":"Cleaning Data","text":"<p>By default, a transform processor named\u00a0<code>transform/kube-events</code> is configured to remove some unneeded fields from Kubernetes events collected. You may override this or alter the fields as desired.</p> <pre><code>processors:\n    transform/kube-events:\n      log_statements:\n        - context: log\n          statements:\n            - keep_keys(body[\"object\"], [\"type\", \"eventTime\", \"reason\", \"regarding\", \"note\", \"metadata\", \"deprecatedFirstTimestamp\", \"deprecatedLastTimestamp\"])\n            - keep_keys(body[\"object\"][\"metadata\"], [\"creationTimestamp\"])\n            - keep_keys(body[\"object\"][\"regarding\"], [\"kind\", \"name\", \"namespace\"])\n\n</code></pre>"},{"location":"newoutput/k8s-otel-advancedconfig/#filtering-kubernetes-events","title":"Filtering Kubernetes Events","text":"<p>In large-scale environments, where there are numerous events occurring per hour, it may not be necessary to process all of them. In such cases, you can use an additional OpenTelemetry processor to filter out the events that do not need to be sent to Coralogix.</p> <p>Below is a sample configuration for reference. This configuration filters out any event that has the field\u00a0<code>reason</code>\u00a0with one of those values\u00a0<code>BackoffLimitExceeded|FailedScheduling|Unhealthy</code>.</p> <pre><code>processors:\n  filter/kube-events:\n    logs:\n      log_record:\n        - 'IsMatch(body[\"reason\"], \"(BackoffLimitExceeded|FailedScheduling|Unhealthy)\") == true'\n\n</code></pre>"},{"location":"newoutput/k8s-otel-advancedconfig/#collecting-only-warning-events","title":"Collecting Only Warning Events","text":"<p>Currently, Kubernetes has two different types of events:\u00a0<code>Normal</code>\u00a0and\u00a0<code>Warning</code>. As we have the ability to filter events according to their type, you may choose to collect only\u00a0<code>Warning</code>\u00a0events, as these events are key to troubleshooting. One example could be the use of a filter processor to drop all unwanted <code>Normal</code> typed events.</p> <pre><code>processors:\n  filter/kube-events:\n    logs:\n      log_record:\n        - 'IsMatch(body[\"object\"][\"type\"], \"Normal\")'\n\n</code></pre>"},{"location":"newoutput/k8s-otel-advancedconfig/#kubernetes-infrastructure-monitoring","title":"Kubernetes Infrastructure Monitoring","text":"<p>If you already have an existing log shipper (e.g. Fluentd, Filebeat) in place and your goal is to monitor all Kubernetes elements of your cluster, follow these steps to enable only the necessary collection of metrics and Kubernetes events to be sent to Coralogix.</p> <p>STEP 1. Copy the following into a YAML-formatted override file and save as <code>values.yaml</code>.</p> <pre><code>global:\n  domain: \"&lt;coralogix-endpoint&gt;\"\n  clusterName: \"&lt;k8s-cluster-name&gt;\"\n\nopentelemetry-agent:\n  presets:\n    logsCollection:\n      enabled: false\n  config:\n    exporters:\n      logging: {}\n    receivers:\n      zipkin: null\n      jaeger: null\n\n    service:\n      pipelines:\n        traces: \n          exporters:\n            - logging\n          receivers:\n            - otlp\n        logs:\n          exporters: \n            - logging\n          receivers:\n            - otlp\n\n</code></pre> <p>STEP 2. Install with the <code>helm upgrade --install</code> command.</p> <pre><code>helm upgrade --install otel-integration coralogix-charts-virtual/otel-integration -f values.yaml -n $NAMESPACE\n\n</code></pre>"},{"location":"newoutput/k8s-otel-advancedconfig/#next-steps","title":"Next Steps","text":"<p>Validation instructions can be found here.</p>"},{"location":"newoutput/k8s-otel-advancedconfig/#additional-resources","title":"Additional Resources","text":"DocumentationGitHub Repository"},{"location":"newoutput/k8s-otel-advancedconfig/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com</p>"},{"location":"newoutput/kafka-and-amazon-msk/","title":"AWS MSK & Kafka","text":"<p>Use Coralogix Kafka lambdas to seamlessly send Kafka Topics data to Coralogix.</p>"},{"location":"newoutput/kafka-and-amazon-msk/#overview","title":"Overview","text":"<p>Many organizations collect important operational and business data in Kafka topics and would like to aggregate, analyze, and correlate it with data collected from other sources, in order to enable a better view of the technology and business stack and improve its management.</p> <p>Use Coralogix Kafka lambdas to seamlessly send Kafka Topics data to Coralogix with our AWS serverless application repository to gain these desired insights into your data.</p>"},{"location":"newoutput/kafka-and-amazon-msk/#requirements","title":"Requirements","text":"<ul> <li> <p>AWS account with permissions to create lambdas and IAM roles</p> </li> <li> <p>A ready-made MSK cluster</p> </li> <li> <p>A ready-made Kafka cluster</p> </li> </ul>"},{"location":"newoutput/kafka-and-amazon-msk/#installation","title":"Installation","text":"<p>STEP 1. Navigate to this application page.</p> <p>STEP 2. Fill in the required parameters</p> <p>STEP 3. Check this checkbox: \"I acknowledge that this app creates custom IAM roles.\"</p> <p>STEP 4. Click Deploy.</p>"},{"location":"newoutput/kafka-and-amazon-msk/#parameters-and-descriptions","title":"Parameters and Descriptions","text":"Variable Description Application Name Stack name of this application created via AWS CloudFormation ApplicationName Application name as it appears in your Coralogix UI If your log is JSON format, use its dynamic value, for example: <code>$.level1.level2.value</code> CoralogixRegion Region [Europe, Europe2, India, Singapore, or US] associated with your Coralogix account\u00a0domain In case that you want to use Custom domain, leave this as default and write the custom domain in the <code>CustomDomain</code> filed. CustomDomain Coralogix custom domain. Leave empty if you do not use a custom domain. FunctionArchitecture Function supports x86_64 or arm64 FunctionMemorySize Max memory for the function itself FunctionTimeout Maximum time in seconds the function may be allowed to run MSKClusterArn ARN of the Amazon MSK Kafka cluster NotificationEmail Failure notification email address PrivateKey Coralogix Send-Your-Data API Key SsmEnabled True, if you want to store your coralogix private_key as a secret False, if you do not SubsystemName Subsystem name as it appears in your Coralogix UI If your log is in JSON format, use its dynamic value, for example: <code>$.level1.level2.value</code>. Topic Name of the Kafka topic used to store records in your Kafka cluster LayerARN Coralogix SSM Layer ARN <p>Notes:</p> <ul> <li> <p>You can dynamically set the application and subsystem names by setting the corresponding parameter above with a filter string with the following syntax: <code>$.first_key.additional_key</code>.</p> </li> <li> <p>The example <code>$.computedValues.functionName</code> uses the functionName of a computedValues array as your dynamic value.</p> </li> </ul>"},{"location":"newoutput/kafka-and-amazon-msk/#automation","title":"Automation","text":"<p>You can include SAM (Serverless Application Model) in your automation frameworks. If you need access to the latest and greatest Lambda code go to coralogix-aws-serverless/src at master \u00b7 coralogix/coralogix-aws-serverless.</p>"},{"location":"newoutput/kafka-and-amazon-msk/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/kubernetes-collector/","title":"Kubernetes Infrastructure Monitoring","text":"<p>Our Kubernetes Dashboard is a powerful web-based interface for monitoring and managing Kubernetes clusters. It provides real-time metrics on CPU, memory, network, and disk usage for nodes and pods. Users can track resource trends, optimize workload placement, and troubleshoot issues efficiently. The dashboard also displays Kubernetes events for quick problem identification and resolution. It streamlines cluster management, ensuring efficient performance and smooth application operation.</p> <p>For simple and easy automated installation of the Kubernetes Dashboard, Coralogix has created Kubernetes Infrastructure Monitoring. With this new feature, the exact data (metrics and K8s events) needed for the installation is located and installed via OpenTelemetry, without the need to manually install each different component.</p>"},{"location":"newoutput/kubernetes-collector/#how-it-works","title":"How It Works","text":"<p>Kubernetes Infrastructure Monitoring is a preset of the OpenTelemetry Collector. It was designed to collect that information which is required to allow the Kubernetes Dashboard and its associated out-of-the-box features to function. The integration includes receivers and processors that are configured to enhance your telemetry data and push it to Coralogix.</p>"},{"location":"newoutput/kubernetes-collector/#before-you-begin","title":"Before You Begin","text":"<ul> <li> <p>In order to successfully set up Kubernetes Infrastructure Monitoring, you may not have an OpenTelemetry Helm chart installed.</p> </li> <li> <p>If you have previously installed OpenTelemetry using the Coralogix Exporter, manually upgrade this Helm chart to its latest version to enable the Kubernetes Dashboard feature.</p> </li> <li> <p>Before installing the Kubernetes Dashboard, ensure you have the following prerequisites installed:</p> <ul> <li> <p>Kubernetes version 1.24+</p> </li> <li> <p>Helm version 3.9+</p> </li> </ul> </li> </ul>"},{"location":"newoutput/kubernetes-collector/#configuration","title":"Configuration","text":"<p>Install the Kubernetes Dashboard using Kubernetes Infrastructure Monitoring.</p> <p>STEP 1. In your Coralogix toolbar, navigate to Data Flow &gt; Integrations.</p> <p>STEP 2. From the Integrations section, select *Kubernetes Infrastructure Monitoring*.</p> <p></p> <p>STEP 3. Click + SETUP COLLECTOR.</p> <p>STEP 4. Enter a name for your integration.</p> <p>STEP 5. Enter a Send-Your-Data API key or click CREATE A NEW KEY to generate a new dedicated API key.</p> <p></p> <p>STEP 6. Click NEXT.</p> <p>STEP 7. Check the Helm version by using the <code>helm version</code> command. *Kubernetes Infrastructure Monitoring* requires Helm v3.9 or above.</p> <p>STEP 8. Create the Coralogix Helm repository by copying the <code>helm repo add</code> command and running it. Click NEXT.</p> <p></p> <p>STEP 9. Before setting up the integration, make sure your Kubernetes secret named <code>coralogix-opentelemetry-key</code> , containing your <code>PRIVATE_KEY</code>, is present in your cluster. If it is not, create it by copying and running the command shown in the installer.</p> <p></p> <p>STEP 10. Copy and run the <code>helm upgrade</code> command shown in the installer. Make sure you replace the <code>&lt;cluster name&gt;</code> with your Kubernetes cluster name.</p> <p></p> <p>STEP 11. Mark the checkbox to confirm you have run the Helm command. Click COMPLETE.</p> <p></p>"},{"location":"newoutput/kubernetes-collector/#kick-start-your-monitoring","title":"Kick-Start Your Monitoring","text":"<p>Coralogix offers a variety of out-of-the-box data extensions. Each tailored extension unlocks a set of predefined items \u2013 alerts, parsing rules, dashboards, saved views, actions, and more \u2013 allowing you to jumpstart Coralogix monitoring of your external-facing resources.</p> <p>When you configure Kubernetes Infrastructure Monitoring, the associated Kubernetes OpenTelemetry extension package is automatically deployed.</p> <p></p> <p>This package provides you with a series predefined features, allowing you to hit the ground running and instantly monitor your data. It includes:</p> <ul> <li> <p>Predefined alerts</p> </li> <li> <p>Predefined parsing rules</p> </li> <li> <p>Predefined Grafana dashboards</p> </li> </ul>"},{"location":"newoutput/kubernetes-collector/#additional-resources","title":"Additional Resources","text":"DocumentationKubernetes DashboardKubernetes Dashboard Cluster View"},{"location":"newoutput/kubernetes-collector/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/kubernetes-dashboard/","title":"Kubernetes Dashboard","text":"<p>With the increasing use of Kubernetes to automate the deployment, management, and scaling of containerized applications or micro-services, organizations have gained agility in deployments and reduced costs. However, this transition has not been without challenges. As teams oversee a dynamic combination of virtual machines, containers, and applications, it can be difficult to anticipate and diagnose performance issues across large distributed systems.</p> <p>This is where Coralogix comes in. We provide comprehensive visibility into the health and performance of Kubernetes environments using the Kubernetes Dashboard. This powerful web-based interface monitors and manages Kubernetes clusters, providing real-time metrics on CPU, memory, network, and disk usage for nodes and pods. Users can track resource trends, optimize workload placement, and efficiently troubleshoot issues. The dashboard also displays Kubernetes events for quick problem identification and resolution. It streamlines cluster management, ensuring efficient performance and smooth application operation.</p> <p>Moreover, our Streama\u00a9 architecture allows you to monitor your data on this dashboard at a third of the cost, without prior indexing.</p> <p>By using this feature, managers, developers, and operations teams can effortlessly visualize and monitor their complex Kubernetes environment, and gain actionable insights using analysis tools driven by machine learning, including forecasting and anomaly detection.</p>"},{"location":"newoutput/kubernetes-dashboard/#overview","title":"Overview","text":"<p>Our Kubernetes Dashboard grants you a full and complete picture of the services that power your applications.</p> <ul> <li> <p>System Overview. Access comprehensive information for all of the clusters, nodes, and pods operating in your system.</p> </li> <li> <p>Resource Graphs. Monitor the status of CPU utilization, network bytes, and disk bytes within and across all services.</p> </li> <li> <p>K8 Events. View all Kubernetes events - automatically generated objects created in response to state changes in your node and pods - that occurred in a particular context.</p> </li> </ul>"},{"location":"newoutput/kubernetes-dashboard/#how-can-i-use-the-kubernetes-dashboard","title":"How Can I Use the Kubernetes Dashboard?","text":"<p>Look at these use cases to get a feel for the many ways the\u00a0Kubernetes Dashboard\u00a0can serve you.</p> <p>Troubleshoot System Issues</p> <p>Company A experiences timeouts for applications that call services hosted on a Kubernetes Cluster. The operator navigates to the Kubernetes Dashboard, inspects nodes and pods, and identifies a node that has repeatedly maxed out its CPU. The operator determines which service is causing problems by examining traces and initiating remediation.</p> <p>Optimize Resource Use</p> <p>Company B seeks to optimize its cluster costs as part of its continuous improvement. The operator opens the Kubernetes Dashboard in Coralogix, inspects the company\u2019s nodes, and observes a memory pattern where nodes are maxed out but CPU usage is only around 25%.</p> <p></p> <p>This indicates that the Company is not selecting the correct type of nodes for their workloads; their workloads are very memory intensive, but not CPU intensive. They begin work to switch their node type from a compute-optimized instance to a memory-optimized instance.</p>"},{"location":"newoutput/kubernetes-dashboard/#setup","title":"Setup","text":"<p>Two Kubernetes Dashboard setup options are available.</p>"},{"location":"newoutput/kubernetes-dashboard/#coralogix-opentelemetry-helm-deployment","title":"Coralogix OpenTelemetry Helm Deployment","text":"<ul> <li> <p>[Recommended] If you have previously installed OpenTelemetry using the Coralogix Exporter to send us your data or wish to do so now, manually upgrade this Helm chart to its latest version to enable the Kubernetes Dashboard feature.</p> </li> <li> <p>For a lightweight and semi-automated installation of the Kubernetes dashboard, use the Coralogix Kubernetes Collector. This option is available only to those who do not have an OpenTelemetry Helm chart installed. It was designed to collect only the required information to allow the Kubernetes Dashboard and its associated out-of-the-box features to function. The Collector includes receivers and processors that are configured to enhance your telemetry data and push it to Coralogix.</p> </li> </ul>"},{"location":"newoutput/kubernetes-dashboard/#using-your-own-opentelemetry-or-prometheus","title":"Using Your Own OpenTelemetry or Prometheus","text":"<ul> <li>Customers using their own OpenTelemetry or Prometheus to collect their Kubernetes events can employ these existing integrations to set up the Kubernetes Dashboard by sending us your missing metrics or labels.</li> </ul> <p>STEP 1. In your Coralogix toolbar, navigate to Dashboard &gt; K8s Dashboard.</p> <p>STEP 2. Under I\u2019ve Installed, select OpenTelemetry or Prometheus and click GO \u2192.</p> <p></p> <p>STEP 4. If all the metrics and labels are present, the Kubernetes Dashboard is opened. If there are missing metrics or labels, a screen appears detailing the missing metrics and/or labels.</p> <p></p> <p>STEP 5. Provide any missing metrics or labels. Click DONE, RELOAD MY DATA \u2192 to continue to the Kubernetes Dashboard.</p>"},{"location":"newoutput/kubernetes-dashboard/#engage-with-your-kubernetes-dashboard","title":"Engage with Your Kubernetes Dashboard","text":"<p>Access your Kubernetes dashboard in your Coralogix toolbar by navigating to Dashboard &gt; k8s Dashboard. Click on either the Nodes or Pods tab. For each, you will be presented with a System Overview, Resource Graphs, K8 Events.</p> <p></p>"},{"location":"newoutput/kubernetes-dashboard/#system-overview","title":"System Overview","text":"<p>View comprehensive information for all of the clusters, nodes, and pods operating in your system.</p> <ul> <li> <p>Nodes. Filter by Cluster, Namespace, and &amp; Node.</p> </li> <li> <p>Pods. Filter by Cluster, Node, Namespace, Service, App, &amp; Pod.</p> </li> <li> <p>Monitor the status of CPU utilization, network bytes, and disk bytes within and across all services.</p> </li> <li> <p>All the data is presented in a simple table with sortable columns to help you focus on the requested information for your observability needs.</p> </li> </ul>"},{"location":"newoutput/kubernetes-dashboard/#resource-graphs","title":"Resource Graphs","text":"<p>Resource graphs provide you with additional granularity regarding CPU utilization, network bytes, and disk bytes within and across all services.</p> <p>When hovering over a specific service in the legend of the CPU Utilization and Memory Used Bytes resource graphs, a widget appears displaying:</p> <ul> <li> <p>usage (how much is currently in use),</p> </li> <li> <p>requests (how much use has been requested), and</p> </li> <li> <p>limits (when it is necessary to kill or restart the service).</p> </li> </ul> <p>These are compared with the overall capacity of that node.</p> <p></p>"},{"location":"newoutput/kubernetes-dashboard/#k8s-events","title":"K8s Events","text":"<p>OpenTelemetry users wishing to view Kubernetes events must deploy our Kubernetes OpenTelemetry extension package. Access it by navigating to Data Flow &gt; Extensions in your Coralogix toolbar.</p> <p></p> <p>An event in Kubernetes is an object in the framework that is automatically generated in response to a change of status in your resources\u2014 nodes or pods. State changes lie at the center of this. For example, phases across a pod\u2019s lifecycle\u2014like a transition from\u00a0pending\u00a0to\u00a0running, or statuses like\u00a0successful\u00a0or\u00a0failed\u00a0may trigger a K8s event. As such, they are are an invaluable resource when troubleshooting issues in your Kubernetes cluster.</p> <ul> <li> <p>Clicking on <code>&gt;</code> for an event will expand it, displaying the event in its entirety.</p> </li> <li> <p>Filter events according to severity to meet your specific query needs.</p> </li> <li> <p>Hover over a particular event to view its correlated annotation across all resource graphs in your Kubernetes dashboard.</p> </li> </ul>"},{"location":"newoutput/kubernetes-dashboard/#additional-resources","title":"Additional Resources","text":"Kubernetes DashboardKubernetes CollectorKubernetes Dashboard Cluster ViewRelated DocumentationApplication Performance MonitoringLabel Mapping"},{"location":"newoutput/kubernetes-dashboard/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/kubernetes-dashboard-cluster-view/","title":"Kubernetes Dashboard Cluster View","text":"<p>With the rise in use of Kubernetes to automate the deployment, management and scaling of\u00a0containerized\u00a0applications or micro-services, organizations have brought agility to deployments and cut costs. But this transition has not been without challenges. As teams are tasked with overseeing a dynamic combination of virtual machines, containers, and applications, it may become difficult to anticipate and diagnose performance issues across large distributed systems.</p> <p>This is where Coralogix comes in, providing comprehensive visibility into the health and performance of your Kubernetes environments. With our Kubernetes Dashboard, managers, developers, and operations teams can effortlessly visualize and monitor their complex Kubernetes environment and gain actionable insights.</p> <p>For those customers using the Coralogix Kubernetes Collector, the Kubernetes Dashboard Cluster View is an invaluable addition to the Coralogix Kubernetes Dashboard. It provides admins and DevOps personnel with a high-level overview of their environment, allowing them to understand trends and leads before drilling down into specific problems. Use this feature to gain insights into past issues or to proactively prevent future ones.</p>"},{"location":"newoutput/kubernetes-dashboard-cluster-view/#how-can-i-use-the-cluster-view","title":"How Can I Use the Cluster View?","text":"<p>Cluster View presents the current status of your Kubernetes clusters with a variety of different widgets designed to help you view and manage your Kubernetes clusters, pods and nodes. Take a look at these use-cases to get a feel for the many ways that the\u00a0Cluster View\u00a0can serve you.</p>"},{"location":"newoutput/kubernetes-dashboard-cluster-view/#use-case-1-monitoring-and-troubleshooting-cluster-health","title":"Use Case 1: Monitoring and Troubleshooting Cluster Health","text":"<p>A DevOps engineer would like to see the status of all pods in a cluster including the Quality of Service (QOS), pod status, and reasons for unhealthy pods.</p> <p>The Cluster View provides a comprehensive solution for DevOps engineers to monitor and troubleshoot the health of pods within a Kubernetes cluster. With its intuitive interface and powerful features, the dashboard allows engineers to quickly assess the status of pods based on Quality of Service (QoS), identify any unhealthy pods, and understand the reasons behind their unhealthy state.</p>"},{"location":"newoutput/kubernetes-dashboard-cluster-view/#use-case-2-resource-optimization","title":"Use Case 2: Resource Optimization","text":"<p>As a DevOps engineer responsible for managing a Kubernetes cluster, you need a way to quickly identify the nodes and applications that are consuming the highest amount of resources. Additionally, you want the ability to drill down into the specific causes of high resource usage to optimize the cluster's performance and cost efficiency.</p> <p>Within the Cluster View, you can also identify applications or micro-services that are placing the highest demands on the cluster's resources. The dashboard provides visualizations that highlight the resource usage of different applications, allowing you to identify the ones causing the most strain. Once you identify the nodes or applications with high resource usage, you can drill down into specific details using the Kubernetes Dashboard. By clicking on a particular node or application, you can access detailed logs, metrics, and performance data related to the selected entity.</p>"},{"location":"newoutput/kubernetes-dashboard-cluster-view/#what-is-included","title":"What is Included?","text":"<p>The Cluster View contains a set of widgets that allow you to assess your Kubernetes clusters, helping you make business decisions and enabling you to solve existing problems and prevent potential problems before they occur.</p>"},{"location":"newoutput/kubernetes-dashboard-cluster-view/#cluster-cpu","title":"Cluster CPU","text":"<p>The Cluster CPU widget displays how much of the cluster\u2019s total CPU is currently in use. It shows the CPU usage (how many nodes are in use), requests (how many nodes have requested usage) and limit (when to kill or restart the service), as well as showing you the total CPU percentage currently in use.</p> <p>The widget also shows you what the current utilization status is and warns if the cluster is being under or over-utilized. This allows you to avoid paying for unused resources.</p> <p></p>"},{"location":"newoutput/kubernetes-dashboard-cluster-view/#cluster-memory","title":"Cluster Memory","text":"<p>The Cluster Memory widget displays how much of the cluster\u2019s total memory is currently in use. It shows the memory usage, requests and limit. The widget also shows you what the current utilization status is and warns if the cluster is being under or over-utilized. This allows you to avoid the risk of applications being thrown out or apps being killed if the node they are on reaches capacity.</p> <p></p>"},{"location":"newoutput/kubernetes-dashboard-cluster-view/#nodes-by-region","title":"Nodes by Region","text":"<p>The Nodes by Region widget displays the spread of nodes throughout the globe based on your AWS region location.</p> <p></p>"},{"location":"newoutput/kubernetes-dashboard-cluster-view/#node-versions","title":"Node Versions","text":"<p>The Node Versions widget shows the current Kubelet versions in use in the cluster\u2019s nodes, allowing you to avoid the use of outdated versions that can cause problems going forward. The widget shows the current version, how many nodes are using the current version, and the previous two versions in use.</p> <p></p>"},{"location":"newoutput/kubernetes-dashboard-cluster-view/#pods","title":"Pods","text":"<p>The Pods widget consists of three statistics about pods:</p> <ul> <li> <p>Pod Quality of Service (QoS) is measured by three levels: Guaranteed (pods which have requests and limits defined), Burstable (pods which have either requests or limits defined, but not both), and Best-Effort (pods which have neither requests nor limits defined). When there is a lack of resources, pods which have guaranteed QOS will be prioritized over pods which have burstable QOS, and those will be prioritized over pods with the best-effort QOS.</p> </li> <li> <p>Status shows how many pods are running, how many are pending, how many succeeded and how many failed.</p> </li> <li> <p>*Failure Reasons* breaks down the failed pods and specifies what the different reasons are for the pods failing. Note that not all failed pods are reported with the reason, so the number of failed pods vs. failure reasons might not match 1 to 1.</p> </li> </ul> <p></p>"},{"location":"newoutput/kubernetes-dashboard-cluster-view/#pending-pods-volume","title":"Pending Pods Volume","text":"<p>The Pending Pods Volume widget shows the number of pods that were in Pending status for more than 5 minutes within the timeframe selected at the top right hand side of the dashboard. This lets you see if there are trends the number of pods in pending state for any given timeframe.</p> <p></p>"},{"location":"newoutput/kubernetes-dashboard-cluster-view/#apps-and-nodes-tables","title":"Apps and Nodes Tables","text":"<p>Apps are applications that are deployed and managed using Kubernetes features and resources. These include various components such as Deployments, DaemonSets, and StatefulSets, etc. The apps and nodes tables show you the top 10 services for each of the following tables.</p> <ul> <li> <p>Top CPU intensive apps - Most CPU intensive apps by average over the selected time range.</p> </li> <li> <p>Top memory intensive apps - Most memory intensive apps by average over the selected time range.</p> </li> <li> <p>Top CPU intensive nodes - Most CPU intensive nodes by average over the selected time range.</p> </li> <li> <p>Top memory intensive nodes - Most memory intensive nodes by average over the selected time range.</p> </li> <li> <p>Top disk intensive nodes (for both reading and writing) - Most disk intensive nodes by average over the selected time range.</p> </li> </ul> <p></p>"},{"location":"newoutput/kubernetes-dashboard-cluster-view/#additional-resources","title":"Additional Resources","text":"DocumentationKubernetes DashboardKubernetes Collector"},{"location":"newoutput/kubernetes-dashboard-cluster-view/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/kubernetes-observability-opentelemetry/","title":"Kubernetes Observability using OpenTelemetry","text":"<p>Coralogix offers Kubernetes Observability using OpenTelemetry for comprehensive Kubernetes and application observability. Using our OpenTelemetry Chart, the integration enables you to simplify the collection of logs, metrics, and traces from the running application in your pods to the cluster-level components of your Kubernetes cluster, while enabling our Kubernetes Dashboard.</p>"},{"location":"newoutput/kubernetes-observability-opentelemetry/#observability-explained","title":"Observability Explained","text":""},{"location":"newoutput/kubernetes-observability-opentelemetry/#kubernetes-observability","title":"Kubernetes Observability","text":"<p>Kubernetes observability is essential for monitoring a Kubernetes cluster's health, performance, resource utilization, and workloads. It involves collecting and analyzing metrics, logs and traces from the cluster and underlying machines to ensure the stability and optimal operation of the cluster.</p> <p>When managing and monitoring Kubernetes components, consider these critical areas:</p> <ul> <li> <p>Cluster Health. Monitoring the overall health of the Kubernetes cluster is crucial. This includes checking the status and availability of the master and worker nodes and the control plane components such as the API server, kube-proxy, and scheduler.</p> </li> <li> <p>Resource Utilisation. Observing the resource utilization of cluster nodes and individual pods is essential for identifying bottlenecks, optimizing resource allocation, and ensuring efficient utilization of cluster resources. Extracting metrics and metadata from the underlying components provides the CPU, memory consumption, system load, and file system activity.</p> </li> <li> <p>Networking. Monitoring Kubernetes networking is crucial for smooth pod and service communication. This involves observing network traffic, latency, and error rates to detect and troubleshoot connectivity issues, identify performance bottlenecks, and improve network configurations.</p> </li> <li> <p>Application Performance. Observing the performance of applications running on Kubernetes is essential for delivering a reliable and responsive user experience.</p> </li> <li> <p>Logging and Tracing. Logging and tracing play a vital role in understanding the behaviour and troubleshooting of Kubernetes components and applications. By collecting and analysing logs and traces, you can gain insights into system events, diagnose issues, and perform root cause analysis. Implementing effective logging and tracing strategies is important to capture relevant information for observability purposes.</p> </li> </ul>"},{"location":"newoutput/kubernetes-observability-opentelemetry/#application-observability","title":"Application Observability","text":"<p>Application observability focuses on monitoring and understanding the behavior of applications running on the Kubernetes cluster. It includes collecting and analyzing metrics, logs, and traces specific to the applications to gain insights into their performance and identify any issues or bottlenecks. This includes monitoring response times, throughput, error rates, and other application-specific metrics.</p>"},{"location":"newoutput/kubernetes-observability-opentelemetry/#integration-overview","title":"Integration Overview","text":"<p>Integrating OpenTelemetry with Kubernetes enables comprehensive Kubernetes and application observability. The OpenTelemetry Integration Chart is a solution that combines two dependent charts into a single Helm installation for Kubernetes clusters: the OpenTelemetry Agent and the OpenTelemetry Cluster Collector. Both are built on the OpenTelemetry Collector Helm Chart, but are configured for optimal performance while collecting different data sources from Kubernetes. Together, they simplify the collection of logs, metrics, and traces from the running application in pods to the cluster-level components of your Kubernetes cluster.</p> <p></p> <p>Additionally, the OpenTelemetry Integration chart enables the collection of telemetry data needed for the Kubernetes Dashboard setup. This dashboard is a powerful web-based interface for monitoring and managing Kubernetes clusters. It provides real-time CPU, memory, network, and disk usage metrics for nodes and pods. Users can track resource trends, optimize workload placement, and troubleshoot issues effectively. The dashboard also displays Kubernetes events for quick problem identification and resolution. Streamlining cluster management ensures efficient performance and smooth operation of applications.</p>"},{"location":"newoutput/kubernetes-observability-opentelemetry/#opentelemetry-agent","title":"OpenTelemetry Agent","text":"<p>The OpenTelemetry Agent simplifies the collection of logs, metrics, and traces from applications running in your Kubernetes cluster. It is configured to deploy as a <code>daemonset</code> and runs on every node in the cluster. The agent maps metadata - such as Kubernetes attributes, Kubelet metrics, and host data - to the collected telemetry. This is particularly beneficial for high-traffic clusters or when utilizing our APM capabilities.</p> <p>The agent comes with several pre-configured processors and receivers:</p> <ul> <li> <p>Kubernetes Attributes Processor. This processor enriches data with Kubernetes metadata, such as pod and deployment information.</p> </li> <li> <p>Kubernetes Log Collection. Enables native Kubernetes log collection with OpenTelemetry Collector, eliminating the need for multiple agents like Fluentd, Fluent Bit, or Filebeat.</p> </li> <li> <p>Host Metrics. Collects native Linux and Windows node resource data.</p> </li> <li> <p>Kubelet Metrics. Fetches running container metrics from the local Kubelet.</p> </li> <li> <p>Traces. Collects data in various formats such as Jaeger, OpenTelemetry Protocol, or Zipkin.</p> </li> <li> <p>Metrics. Collects application metrics via the OpenTelemetry Protocol.</p> </li> <li> <p>Span Metrics. Converts traces into requests, duration, and error metrics using the spanmetrics processor.</p> </li> </ul>"},{"location":"newoutput/kubernetes-observability-opentelemetry/#opentelemetry-cluster-collector","title":"OpenTelemetry Cluster Collector","text":"<p>The OpenTelemetry Cluster Collector retrieves data from the cluster level, including Kubernetes events, cluster metrics, and additional Kubernetes-specific metrics. It enables you to gain insights into the health and performance of various objects within the cluster, such as deployments, nodes, and pods.</p> <ul> <li> <p>Cluster Metrics Receiver. The Kubernetes Cluster receiver collects cluster-level metrics from the Kubernetes API server.</p> </li> <li> <p>Kubernetes Events Receiver. This receiver collects Kubernetes events and sends them to the kube-events subsystem. It allows you to take advantage of other features, such as the Kubernetes Dashboard and alerting, using Kubernetes events as the primary source of information.</p> </li> <li> <p>Kubernetes Extra Metrics. This preset enables the collection of extra Kubernetes-related metrics, such as node information, pod status, or container I/O metrics. These metrics are collected in particular for the\u00a0Kubernetes Dashboard.</p> </li> <li> <p>Integration Presets. This chart provides support to integrate with various applications (e.g. mysql) running on your cluster to monitor them out of the box.</p> </li> </ul>"},{"location":"newoutput/kubernetes-observability-opentelemetry/#kubernetes-dashboard","title":"Kubernetes Dashboard","text":"<p>The OpenTelemetry Integration chart enables the collection of essential metrics needed for the Kubernetes Dashboard setup. The Kubernetes Cluster Receiver is an essential part that provides cluster-level metrics and entity events from the Kubernetes API server. It can report metrics of allocatable resource types such as <code>cpu</code> and <code>memory</code> and give an update on node conditions (e.g. <code>Ready</code>, <code>MemoryPressure</code>). As a whole, the metrics gathered are useful for the Kubernetes Dashboard to report on the health of your cluster.</p>"},{"location":"newoutput/kubernetes-observability-opentelemetry/#next-steps","title":"Next Steps","text":"<p>View our basic configuration instructions here.</p> <p>Advanced configuration instructions can be found here.</p>"},{"location":"newoutput/kubernetes-observability-opentelemetry/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by emailing support@coralogixstg.wpengine.com</p>"},{"location":"newoutput/label-mapping/","title":"Label Mapping","text":"<p>With our new Label Mapping feature, as you view your data sources in your Kubernetes Dashboard, you will be instantly notified when metric labels are missing from your data. We will guide you to easily locate these metric labels in your data and correlate them to the metrics in your Kubernetes Dashboard.</p>"},{"location":"newoutput/label-mapping/#feature","title":"Feature","text":"<p>Use the Label Mapping feature to:</p> <ul> <li> <p>Highlight missing keys in your Coralogix data sources</p> </li> <li> <p>Provide a list of metric labels in your data that are relevant to the data displayed in your Kubernetes Dashboard</p> </li> <li> <p>Enjoy full customization of the labels identifying your metrics</p> </li> </ul> <p></p>"},{"location":"newoutput/label-mapping/#pair-data-sources","title":"Pair Data Sources","text":"<p>STEP 1. You will receive an alert when parts of your data are not yet correlated to Coralogix data sources: \u201cSome label sources are missing. PAIR DATA SOURCES.\u201d This alert may appear in an empty state in your dashboard, during your discovery process, or when you click on a Nodes / Pods tool tip.</p> <p></p> <p>STEP 2. Click on the message and you will be redirected to a Missing Source Correlations pop-up page. The pop-up will display a list of keys missing a source used for the current page. Select the value in your data to pair with the relevant Coralogix data source.</p> <p></p> <p>STEP 3. (Optional) View OTHER LABELS in the expanded drop down menu and customize according to your needs.</p> <p>STEP 4. Click UPDATE CORRELATIONS.</p> <p>STEP 5. Once updated, your Coralogix dashboard will reload. The previously missing dependencies will now appear as a new field on the page.</p>"},{"location":"newoutput/label-mapping/#additional-resources","title":"Additional Resources","text":"<p>Kubernetes Dashboard</p>"},{"location":"newoutput/label-mapping/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/log-parsing-rules/","title":"Log Parsing Rules","text":"<p>Rules help you to process, parse, and restructure log data to prepare for monitoring and analysis. Doing so can extract information of importance, structure unstructured logs, discard unnecessary parts of the logs, mask fields for compliance reasons, fix misformatted logs, block log data from being ingested based on log content, and much more.\u00a0\u00a0</p>"},{"location":"newoutput/log-parsing-rules/#overview","title":"Overview","text":"<p>At Coralogix, rules are organized inside Rule Groups. Each group has a name and a set of rules with a logical AND/OR relationship between them. Logs are processed according to the order of Rule Group (top to bottom) and then by the order of rules within the Rule Group and according to the logical operators between them (AND/OR).</p> <p>Rules are applied only to new logs - that is, those logs ingested by Coralogix after the creation of a rule.</p> <p>Find out more by viewing our Rules API tutorial.</p>"},{"location":"newoutput/log-parsing-rules/#rules-groups","title":"Rules Groups","text":"<p>In order to create a rule group in the Coralogix UI, go to Data Flow &gt;Parsing Rules and click on the \u2018NEW RULE GROUP\u2019 button or choose one of the quick rule creation options from the boxes below. The options for rules include:</p> <ul> <li> <p>Parse</p> </li> <li> <p>Extract</p> </li> <li> <p>Extract JSON</p> </li> <li> <p>Replace</p> </li> <li> <p>Block</p> </li> <li> <p>Timestamp Extract</p> </li> <li> <p>Remove Fields</p> </li> <li> <p>[New] Stringify JSON Field</p> </li> <li> <p>[New] Parse JSON Field</p> </li> </ul> <p>A rule group definition form is divided into two sections: Description and Rule Matcher.</p>"},{"location":"newoutput/log-parsing-rules/#description","title":"Description","text":"<p>Each group has a name and can have an optional description.</p>"},{"location":"newoutput/log-parsing-rules/#rule-matcher","title":"Rule Matcher","text":"<p>The Rule Matcher section defines a query. Only logs that match the Rule Matcher query will be processed by the group. This is important in making sure that only intended logs go through the group rules, as well as for performance reasons.\u00a0</p> <p>The rule matcher query is defined by selecting a set of applications, subsystems, and severities. Only logs that fit all components of the query will be processed by the group. All entries in this section are optional. Not selecting a field or defining a RegEx means that the Rule Group will run on all of your logs.\u00a0</p> <p></p>"},{"location":"newoutput/log-parsing-rules/#rules","title":"Rules","text":"<p>Here are the different rule types that can be created in Coralogix:</p>"},{"location":"newoutput/log-parsing-rules/#parse","title":"Parse","text":"<p>Parse Rule uses RegEx named capture groups. The groups become the parsed log fields and the value associated with each group becomes the field\u2019s value. The RegEx doesn\u2019t have to match the entire log, only the RegEx named capture groups (the values within the \u2018&lt; &gt;\u2019) and the value they capture will be part of the reconstructed log.</p> <p>The following example takes a Heroku L/H type error log that is sent from Heroku as an unstructured log and converts it to JSON log.</p> <p></p> <p>The original log:</p> <pre><code>sock=client at=warning code=H27 desc=\"Client Request Interrupted\" method=POST path=\"/submit/\" host=myapp.herokuapp.com fwd=17.17.17.17 dyno=web.1 connect=1ms service=0ms status=499 bytes=0\n</code></pre> <p>The RegEx:</p> <pre><code>^(sock=)?(?P&lt;sock&gt;(\\S*))\\s*at=(?P&lt;severity&gt;\\S*)\\s*code=(?P&lt;error_code&gt;\\S*)\\s*desc=\"(?P&lt;desc&gt;[^\"]*)\"\\s*method=(?P&lt;method&gt;\\S*)\\s*path=\"(?P&lt;path&gt;[^\"]*)\" host=(?P&lt;host&gt;\\S*)\\s* (request_id=)?(?P&lt;request_id&gt;\\S*)\\s*fwd=\"?(?P&lt;fwd&gt;[^\"\\s]*)\"?\\s*dyno=(?P&lt;dyno&gt;\\S*)\\s*connect=(?P&lt;connect&gt;\\d*)(ms)?\\s*service=(?P&lt;service&gt;\\d*)(ms)?\\s*status=(?P&lt;status&gt;\\d*)\\s* bytes=(?P&lt;bytes&gt;\\S*)\\s*(protocol=)?(?P&lt;protocol&gt;[^\"\\s]*)$\n</code></pre> <p>The resulting log:</p> <pre><code>{ \u00a0\u00a0\n\u201csock\u201d: \u201cclient\u201d, \u00a0\u00a0\n\u201cseverity\u201d:\u201dwarning\u201d, \u00a0\u00a0\n\u201cerror_code\u201d:\u201dH27\u201d, \u00a0\u00a0\n\u201cdesc\u201d:\u201dClient Request Interrupted\", \u00a0\u00a0\n\u201cmethod\u201d:\u201dPOST\u201d, \u00a0\u00a0\n\u201cpath\u201d:\u201d/submit/\u201d, \u00a0\u00a0\n\u201chost\u201d:\u201dmyapp.herokuapp.com\u201d, \u00a0\u00a0\n\u201crequest_id\u201d:\u201d\u201d, \u00a0\u00a0\n\u201cfwd\u201d:\u201d17.17.17.17\u201d, \u00a0\u00a0\n\u201cdyno\u201d:\u201dweb.1\u201d, \u00a0\u00a0\n\u201cconnect\u201d:\u201d1\u201d, \u00a0\u00a0\n\u201cservice\u201d:\u201d0\u201d, \u00a0\u00a0\n\u201cstatus\u201d:\u201d499\u201d, \u00a0\u00a0\n\u201cbytes\u201d:\u201d0\u201d, \u00a0\u00a0\n\u201cprotocol\u201d:\u201d\u201d \n}\n</code></pre>"},{"location":"newoutput/log-parsing-rules/#extract","title":"Extract","text":"<p>Unlike a Parse Rule, an Extract Rule will leave the original log intact and will just add fields to it at the root. The following example extracts information from the field message and creates two new fields, \u2018bytes\u2019, and \u2018status\u2019 that can be queried and visualized in Coralogix.</p> <p></p> <p>The original log:</p> <pre><code>{\"level\":\"INFO\", \"message\": \"200 bytes sent status is OK\"}\n</code></pre> <p>The RegEx:</p> <pre><code>message\"\\s*:\\s*\"(?P&lt;bytes&gt;\\d+)\\s*.*?status\\sis\\s(?P&lt;status&gt;[^\"]+)\n</code></pre> <p>The resulting log:</p> <pre><code>{\n\n\"level\":\"INFO\",\u00a0\n\n\"message\": \"200 bytes sent status is OK\",\n\n\u201cBytes\u201d:\u201d200\u201d,\n\n\u201cstatus\u201d:\u201dOK\u201d\n\n}\n</code></pre> <p>If the original log is unstructured it will add fields based on the named capture groups and will store the original log inside a field called text. Using the previous example, it will switch the original log to be unstructured.</p> <pre><code>\"level\":\"INFO\", \"message\": \"200 bytes sent status is OK\",\n</code></pre> <p>The resulting log:</p> <pre><code>{\n\n\"bytes\" : \"200\" ,\n\n\"text\" : \"\\\"level\\\":\\\"INFO\\\", \\\"message\\\": \\\"200 bytes sent status is OK\\\"\" ,\n\n\"status\" : \"OK\"\n}\n</code></pre>"},{"location":"newoutput/log-parsing-rules/#json-extract","title":"JSON Extract","text":"<p>JSON Extract rules take the value of a key and use it to overwrite one of Coralogix metadata fields. In this example, we extract the value from a field called \u2018worker\u2019 and use it to populate the Coralogix metadata field called Category.</p> <p>The source field is always text and the JSON Key field should contain the field name that you want to extract the value from. The destination field is the metadata field that will be overwritten with this value. This rule is frequently used to extract and set severity and to set metadata fields like \u2018Category\u2019 that influence the classification algorithms.\u00a0</p> <p></p> <p>The original log:</p> <pre><code>{\n\n\u201ctransaction_ID\u201d:12543,\n\n\u201cworker\u201d:\u201dA23\u201d,\n\n\u201cMessage\u201d:\u201dsuccess\u201d\n\n}\n</code></pre> <p>The log will not change in the Logs interface but the Coralogix metadata field \u2018category\u2019 will be populated with \u201cA23\u201d in the above example.</p>"},{"location":"newoutput/log-parsing-rules/#replace","title":"Replace","text":"<p>A common use case for a Replace rule is to repair misformatted JSON logs. In the following example, the JSON logs are sent with a date prefix which breaks the JSON format and turns them into unstructured logs. The following RegEx identifies the substring to replace in the log.</p> <p>Original log:</p> <pre><code>2020-08-07 {\u201cstatus\u201d:\u201dOK\u201d, \u201cuser\u201d:John Smith\u201d, \u201cops\u201d:\u201dJ1\u201d}\n</code></pre> <p>RegEx:</p> <pre><code>.*{\n</code></pre> <p>The resulting log:</p> <pre><code>{\u201cstatus\u201d:\u201dOK\u201d, \u201cuser\u201d:John Smith\u201d, \u201cops\u201d:\u201dJ1\u201d}\n</code></pre> <p></p>"},{"location":"newoutput/log-parsing-rules/#nested-fields","title":"Nested Fields","text":"<p>The following example shows how to use a Replace rule to rebuild a log with nested fields. Nested fields are a constraint in the Extract and Parse rules.\u00a0</p> <p></p> <p>The original log is:</p> <pre><code>{\"ops\":\"G1\",\"user\":\"John Smith-2125 Sierra Ventura Dr.-Sunnyvale-CA-94054\",\"status\":\"305\"}\n</code></pre> <p>The Regex:</p> <pre><code>(.*user\"):\"([^-]*)-([^-]*)-([^-]*)-([^-]*)-([^-]*)\",([^$]*)\n</code></pre> <p>Each of the parentheses represents a capture group that can be addressed by $n n=1..7 in this case.</p> <p>The replacement string:</p> <pre><code>$1:{\"name\":\"$2\",\"address\":\"$3\",\"city\":\"$4\",\"state\":\"$5\",\"zip\":\"$6\"},$7\n</code></pre> <p>This is the resulting log:</p> <pre><code>{\n   \"ops\":\"G1\",\n   \"User\":{\n      \"name\":\"John Smith\",\n      \"address\":\"2125 Sierra Ventura Dr.\",\n      \"city\":\"Sunnyvale\",\n      \"state\":\"CA\",\n      \"Zip\":\"94054\"\n   },\n   \"status\":\"305\"\n}\n\n\n</code></pre>"},{"location":"newoutput/log-parsing-rules/#inner_json","title":"inner_json","text":"<p>inner_json is a special replace rule that know how to take a field value in a json log that includes a stringified (escaped) legal json, and transform it into an object.</p> <p>In the following example, the original log is:</p> <pre><code>{ \n \"server\":\"opa\",\n \"IBC\":\"45ML\",\n \"thread\":\"1201\",\n \"message\":\"{\\\"first_name\\\":\\\"John\\\", \\\"last_name\\\":\\\"Smith\\\", \\\"userID\\\":\\\"AB12345\\\", \\\"duration\\\":45}\"  \n}\n</code></pre> <p>The field message has a string value that is an escaped legal json.</p> <p>Our special replace rule will change the name of the field from the original name \"message\" to \"inner_json\":</p> <p>The regex:</p> <p>\"message\"\\s*:\\s*\"{\\s*\\\\\"</p> <p>Identifies a field name message with the beginning of the escaped json.</p> <p>The replacement string is:</p> <p>\"inner_json\":\"{\\\"</p> <p>Replaces only the field name and nothing else.</p> <p>The resulting log:</p> <pre><code>{\n   \"server\":\"opa\",\n   \"inner_json\":{\n     \"first_name\":\"John\" ,\n     \"last_name\":\"Smith\" ,\n     \"userID\":\"AB12345\" ,\n     \"duration\":45 \n  }\n}\n</code></pre>"},{"location":"newoutput/log-parsing-rules/#block","title":"Block","text":"<p>Block rules allow you to filter out your incoming logs. Like with other rules the heart of the rule will be a RegEx, identifying the logs to be blocked (or allowed). In this example, all the logs that have the substring 'sql_error_code 28000' will be blocked.</p> <p>RegEx:</p> <pre><code>sql_error_code=28000\n</code></pre> <p></p>"},{"location":"newoutput/log-parsing-rules/#block-rules-have-two-additional-options","title":"Block rules have two additional options:","text":"<ul> <li> <p>Block all matching logs: Will block any log that matches the Rule Matcher and Block Rule</p> </li> <li> <p>Block all non-matching logs: Will block any log that does not match the Rule Matcher and Block Rule</p> </li> </ul> <p></p> <p>Checking the \u201cView blocked logs in Livetail\u201d option will block the logs but archive them to S3 (if achieving is enabled under TCO &gt; Archive) and the logs will be visible in LiveTail. This is a more refined option to give logs a low priority, as described here. Only 15% of low priority logs volume is counted against quota.</p> <p>The block logic indicates if the rule will block all logs that do match the RegEx or the inverse; all logs that do not match the regex. In our example above, checking the \u201cblock all non-matching logs\u201d option would have blocked all logs except those that include the string <code>sql_error_code\\s*=\\s*28000</code> </p>"},{"location":"newoutput/log-parsing-rules/#timestamp-extract","title":"Timestamp Extract","text":"<p>The Coralogix timestamp for each log entry is assigned by our receiving endpoint when logs arrive. The Timestamp Extract rule allows you to override the Coralogix timestamp for the log entry with the value of your custom timestamp field from your log. When you are configuring this rule make sure you are choosing the right source field, then choose a field format standard you are most familiar with. Finally, you will need to edit the Time format text box with the format that matches your logs time field. After setting all three, you should use the Sample log tester and expect a match that is indicating the rule was able to parse your time field format to the Coralogix time format, which is Unix timestamps in nanoseconds divided by 100 (17 digits).</p> <p>Let's check a few examples:</p> <p>giving this log entry:</p> <pre><code>{ \n  \"transaction_ID\":12543,\n  \"worker\":\"A23\",\n  \"Message\":\"success\",\n  \"time\":\"2021-01-11T15:04:05.000000+0100\"\n}\n</code></pre> <p>I was choosing the strftime standard and set the time format to match my time field string format, which in this case is also matching to the suggested default format.</p> <p></p> <p>We can see it resulted in a match so we can go ahead and save this rule.</p> <p>giving this log entry:</p> <pre><code>{ \n  \"transaction_ID\":12543,\n  \"worker\":\"A23\",\n  \"Message\":\"success\",\n  \"time\":\"2021-01-11T00:12:34+01:00\"\n}\n</code></pre> <p>Now, I was choosing the Go time layouts standard and set the time format to match my time field string format.</p> <p></p> <p>We can see it resulted in a match.</p> <p>giving this log entry:</p> <pre><code>{ \n  \"transaction_ID\":12543,\n  \"worker\":\"A23\",\n  \"Message\":\"success\",\n  \"time\":\"03/Mar/2021:08:34:12 +0000\"\n}\n</code></pre> <p>Now, I was choosing the strftime standard and set the time format to match my time field string format. This use-case is a bit more challenging since my time field format has few differences than the default suggestion, %Y-%m-%dT%H:%M:%S.%f%z, e.g. 2021-01-11T15:04:05.000000+0100. One of them is the month format, which in our case is of type abbreviated month name, hence, I needed to look for strftime reference and found that %b represents the abbreviated month name format. Using %b and adding the other needed changes I was able to create the matching format.</p> <p></p> <p>We can see it resulted in a match.</p> <p>giving this log entry:</p> <pre><code>{ \n  \"transaction_ID\":12543,\n  \"worker\":\"A23\",\n  \"Message\":\"success\",\n  \"time\":\"2021-01-11T15:04:05.000000+0100\"\n}\n</code></pre> <p>I chose again strftime standard but set the wrong time format ignoring the %z (time zone part), which resulted in no match to my time field string format. We should amend the time format before saving the rule, otherwise, it will not work.</p> <p></p>"},{"location":"newoutput/log-parsing-rules/#remove-fields","title":"Remove Fields","text":"<p>The Remove Fields rule allows you to easily drop specific fields from any JSON log entry that contains them on the Coralogix side, which is not always an option at Runtime. Thus, creating a cleaner structure for your logs to give better visibility into your data. 10% of the data volume that is being removed is counted against the quota, similar to blocked data (by a block rule).\u00a0</p> <p>Let's check an example, first name your rule and give it a meaningful description.</p> <p></p> <p>open the Excluded fields dialog box, it will open a list of all available fields previously mapped to your indices, and choose the fields to drop.</p> <p></p> <p>Verify your rule is working using the Sample log section.</p> <p></p>"},{"location":"newoutput/log-parsing-rules/#sample-log","title":"Sample log","text":"<p>When creating a rule you can use the 'sample log' area of the screen to verify your rule. Simply create or paste a log into this area and it will show you the results of the rule processing the log. In this example, a Parse rule processes the log in the \"Sample log\" area and the result is displayed in the \"Results\" area:</p> <p></p>"},{"location":"newoutput/log-parsing-rules/#stringify-json-fields","title":"Stringify JSON Fields","text":"<p>This rule allows you to change an array or an object from JSON format to text in order to reduce the number of fields but still keep the data in\u00a0\u00a0Coralogix to be searchable using free text search.</p> <p>Steps to\u00a0\u00a0configure:</p> <ol> <li> <p>Click on Data flow.</p> </li> <li> <p>Click on parsing rules.</p> </li> <li> <p>Select Stringify\u00a0\u00a0Json field rule from the rules list.</p> </li> <li> <p>Configure your group name, rule matchers, and all you see needed for your rule.</p> </li> </ol> <p></p> <ul> <li> <p>Source Field: the field you want to convert to text.</p> </li> <li> <p>Keep or delete source field option.</p> <ul> <li> <p>Select Keep if you want to keep the original field with its content.</p> </li> <li> <p>Select Delete if you want to delete the original field and its content.</p> </li> </ul> </li> <li> <p>Destination field is the new field where you want the content of the source to be put under.</p> </li> </ul> <p>In the example below, I have a field called sessionIssuer which is an abject and I am going to change it to text and give it a new name called application.</p> <p>Original Message:</p> <p></p> <p></p> <p>New message with delete source field selected:</p> <p></p> <p>New message with keep source field:</p> <p></p>"},{"location":"newoutput/log-parsing-rules/#parse-json-field","title":"Parse JSON Field","text":"<p>With the Parse Json field rule, you can transform escaped or stringified logs to JSON with no effort.</p> <p>Select the field that is escaped, select a destination field to write and you are all done.</p> <ul> <li> <p>Select the Source field is the field you are trying to unescape.</p> </li> <li> <p>Select Merge into and Overwrite only if you are trying to overwrite or merge the data to an already existing field. If you are trying to create a new field these 2 options are not relevant.</p> </li> <li> <p>Keep source field option keeps the source field and its content.</p> </li> <li> <p>Delete the source field it will remove the source field and its content.</p> </li> </ul> <p>In the example below we are unescaping the log and making it in JSON format.</p> <p></p> <p>Original Message:</p> <p></p> <p>Message after the rule:</p> <p></p> <p>Note: If you chose a destination field that will cause a mapping exception, a message will pop up to let you know that you will be creating an exception if you apply this rule.</p> <p></p>"},{"location":"newoutput/log-parsing-rules/#rule-group-logic","title":"Rule Group Logic","text":"<p>To add rules to a group following the creation of the first log you have to take two actions:</p> <ul> <li>Select the Rule type from the \u201cADD RULE\u2019 dropdown list</li> </ul> <p></p> <ul> <li>Select the logical relationship between the last rule and the new one (And/Or)</li> </ul> <p></p> <p>Example: Rule-1 AND Rule-2 will mean that a log will always be processed by both rules. Rule-1 OR Rule-2 means that the log will be processed by either Rule-1 or Rule-2, whichever matches first, or neither if none match the log. In other words, if Rule-1 is a match to the log, then Rule-2 will not be applied to it at all.</p>"},{"location":"newoutput/log-parsing-rules/#rule-groups-order-of-execution","title":"Rule Groups: Order of Execution","text":"<p>Logs are processed by the different Rule Groups according to the order of the groups. When \u201centering\u201d a group the log is matched against the \u201cRule matcher\u201d query first. If it matches, it will continue into the Group Rules. Within the group, the log will be matched against the Rules according to their order and logic. It will continue to be matched down the list of rules until it is matched by a rule that is followed by an OR logical operator. The OR and AND operators in the group do not follow the mathematical order of operations. Rules are applied instantly so that the output of one rule becomes the input of the next one.</p> <p>Note: It is clear that the order of rules and groups is important and can affect a log\u2019s processing outcome.\u00a0</p> <p>As an example look at these two rules that parse Heroku Postgres logs:</p> <p>Postgres follower, https://regex101.com/r/IyjCIj/4</p> <p>Postgres leader, https://regex101.com/r/aQJsp5/2</p> <p>The follower log has an extra entry at the end, follower_lag_commits. This means that the Leader rule will capture both logs because it is less restrictive and all other fields match. Follower will match only the follower logs (the first test string is not captured in the follower example because it doesn\u2019t have the extra entry). This means that the follower rule would be executed first.\u00a0</p> <p>Another consideration is performance. The best practice is to put Block rules first and to use the \u2018Rule Matcher\u2019 when possible. It will prevent unnecessary processing of logs and speed up your data processing.</p> <p>You can change the order of execution of rules within a group by dragging the rule into a new position relative to other rules.</p> <p></p> <p>The same goes for Rule Groups. You can change their order by dragging them up and down the list.</p> <p></p>"},{"location":"newoutput/log-parsing-rules/#searching-for-rule-groups","title":"Searching for Rule Groups","text":"<p>In order to quickly find a Rule Group of interest, you can use the search function to search for rules. You can use rules or group names in the free text search field.</p> <p></p>"},{"location":"newoutput/log-parsing-rules/#editing-rules-and-groups","title":"Editing rules and groups","text":"<p>In order to edit a Rule Group or a Rule within a group, click on the group, make your changes, and click on 'SAVE CHANGES'.</p> <p></p>"},{"location":"newoutput/log-query-simply-retrieve-data/","title":"Log Query - Simply Retrieve Your Data","text":"<p>Coralogix is a powerful tool for querying your logs. By mastering how to query you will be able to find specific events out of millions of logs generated by your applications.</p> <p>With this skill, you will comfortably be able to investigate issues, create alerts, and visualize your data.</p> <p>In the following section you will learn about:</p> <ul> <li> <p>Field types</p> </li> <li> <p>Different types of queries</p> </li> </ul> <p>Find out about our Direct Archive Query HTTP API here.</p>"},{"location":"newoutput/log-query-simply-retrieve-data/#concepts","title":"Concepts","text":""},{"location":"newoutput/log-query-simply-retrieve-data/#indexing-and-field-mapping","title":"Indexing and field mapping","text":"<p>In order to master how to query logs, it is important to first understand how Coralogix indexes your data after it has been analyzed. Indexing logs is important because it allows you to quickly retrieve (usually within a few seconds) matching logs using:</p> <ol> <li> <p>Free-text searches</p> </li> <li> <p>Regular expressions</p> </li> <li> <p>Field searches</p> </li> </ol> <p>Tip: We recommend serializing your logs as JSON to get maximum value from Coralogix analytics features. Read more about parsing unstructured logs to JSON using parsing rules here.</p>"},{"location":"newoutput/log-query-simply-retrieve-data/#data-types","title":"Data types","text":"<p>Coralogix supports the following data types:</p> <p>Text: This type represents unstructured, human-readable content that is analyzed into terms before indexing.</p> <p>Keyword: This type represents text that does not pass through the analyzer before indexing. This makes it suitable for regular expressions, aggregation, and sorting. The syntax to use the keyword data type in your query is: .keyword <p>Note: Coralogix does not create the keyword type when a field is longer than 256 characters.</p> <p>Numeric: This type is suitable for range queries and arithmetic aggregations (avg, max, min, sum). The syntax to use the numeric data type in your query is: .numeric <p>Date: This type enables you to filter by timestamp or plot time-series graphs. Values should be formatted as epoch milliseconds.</p> <p>Geopoint: This type allows you to plot longitude and latitude pairs on a Grafana map.</p> <p>Object: This type represents a hierarchy. This means that it may contain fields of any other type (including objects).</p>"},{"location":"newoutput/log-query-simply-retrieve-data/#notes","title":"Notes","text":"<ol> <li> <p>Coralogix has a default limit of 1000 mapped fields. You can view your teams mapped field statistics under Account settings \u2192 Mapping stats</p> </li> <li> <p>Explicit mapping is supported for timestamps and geopoints. Appending _timestamp or _geopoint to your field name will map it as a date or geopoint respectively. For example, a field named duration_timestamp is mapped as a date.</p> </li> <li> <p>Dynamic mapping is used for all other fields. This means that at the time of indexing, a new field\u2019s value determines the mapped data type.</p> </li> <li> <p>Arrays are valid JSON, however, there is no dedicated array data type in Coralogix. This means that:</p> <ol> <li> <p>A field may contain multiple values, and they should all be of the same data type, otherwise a Mapping Exception will occur.</p> </li> <li> <p>The first value in an array determines the field mapping.</p> </li> <li> <p>For an array of objects, it is not possible to query each object independently.</p> </li> </ol> </li> <li> <p>Each field in the log is mapped as 3 data types:</p> <ol> <li> <p>Text or Object or Date or Geopoint</p> </li> <li> <p>Keyword</p> </li> <li> <p>Numeric</p> </li> </ol> </li> </ol>"},{"location":"newoutput/log-query-simply-retrieve-data/#querying-your-logs","title":"Querying your logs","text":"<p>To get started, navigate to the Explore screen. This screen allows you to query logs from the index or from your Amazon S3 archive.</p> <p></p> <p>The supported query languages are Lucene or DataPrime (for archive queries). This tutorial will cover how to use Lucene queries on Coralogix.</p>"},{"location":"newoutput/log-query-simply-retrieve-data/#lucene-query-syntax-reference","title":"Lucene query syntax reference","text":"<p>A Lucene query is composed of Terms and Operators. Terms are extracted from your log by the analyzer. There are 2 types of terms:</p> <ol> <li> <p>Single term: This is a word in your Text field.</p> </li> <li> <p>Phrase: This is a group of words surrounded by double quotation marks (\u201c)</p> </li> </ol> <p>This tool helps you better understand how the Coralogix analyzer extracts terms from your Text fields.</p>"},{"location":"newoutput/log-query-simply-retrieve-data/#free-text-search","title":"Free text search","text":"<p>Use this type of search to match terms in ANY field of your log.</p> Query Results a very interesting log message Matches logs containing these terms. They may appear in any field and in any order \"a very interesting log message\" Matches this exact phrase in any field <p>Note: Text fields will pass through an analyzer before indexing. The analyzed text is separated into the \u201cterms\u201d used to index your logs.</p>"},{"location":"newoutput/log-query-simply-retrieve-data/#field-search","title":"Field search","text":"<p>Use this type of search to restrict which field MUST match your search term.</p> <p>Examples:</p> Query Results msg:interesting Matches logs containing this term in the msg field msg:\u201ca very interesting log message!\u201d Matches this exact phrase in the msg field. msg.keyword:\u201da very interesting message!\u201d Matches logs that contain the phrase (including the !)"},{"location":"newoutput/log-query-simply-retrieve-data/#range-search","title":"Range search","text":"<p>Use this to query a range of matching numeric values</p> <p>Examples:</p> Query Results status_code.numeric:[200 TO 299] Matches status codes between 200 and 299 (including 200 and 299) status_code.numeric:{199 TO 300} Matches status codes between 200 and 299 (excluding 199 and 300) status_code.numeric:[200 TO 300} Matches status codes between 200 and 299 (including 200 but excluding 300) status_code.numeric:{199 TO 299] Matches status codes between 200 and 299 (excluding 199 but including 299)"},{"location":"newoutput/log-query-simply-retrieve-data/#regex-search","title":"Regex search","text":"<p>Regular expressions are available to match patterns in your log. Coralogix supports Lucene regex engine standard operators</p> <p>The regex pattern to be matched should be enclosed in forward slashes \u201c/\u201d.</p> <p>Note: Whenever possible, we recommend using regex searches against keywords, because this data type is not passed through the analyzer.</p> <p>Examples:</p> Query Results msg.keyword:/.*what an interesting message!.*/ Matches logs that contain the pattern \u201cwhat an interesting message!\u201d (including the !) version.keyword:/.*v.[1-5].[0-9]{2}.*/ Matches logs that contain the patterns like \u201cv.1.24\u201d or \u201cv.5.69\u201d in the version field"},{"location":"newoutput/log-query-simply-retrieve-data/#boolean-operators-and-grouping","title":"Boolean Operators and Grouping","text":"<p>The operators AND, OR, and NOT can be used to combine multiple filters and create more precise queries.</p> <p>Parentheses \u201c()\u201d should be used to determine operator precedence whenever you have multiple operators in a query.</p> <p>Examples:</p> Query Results msg:\u201dfailed transaction\u201d AND level: \u201cERROR\u201d NOT env:\u201dstaging\u201d Matches ERROR level logs that contain the phrase \"failed transaction\" (msg:\"failed transaction\" AND (cluster:\"eu\" OR cluster:\"us\")) NOT env:\"staging\" Matches logs from the \"eu\" or \"us\" clusters that contain the phrase \"failed transaction\" but not from the \"staging environment\""},{"location":"newoutput/log-query-simply-retrieve-data/#additional-resources","title":"Additional Resources","text":"APIDirect Archive Query HTTP API"},{"location":"newoutput/log-query-simply-retrieve-data/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/logs-screen/","title":"Logs Screen","text":"<p>This tutorial will help you navigate through the main capabilities in the Coralogix logs screen.</p> <p>The tutorial is organized by functionality:</p> <ul> <li> <p>Queries and filters</p> </li> <li> <p>Rapid visualizations</p> </li> <li> <p>Custom views</p> </li> </ul>"},{"location":"newoutput/logs-screen/#queries-and-filters","title":"Queries and Filters","text":"<p>Queries are a combination of time selection, search query, and filter selections.</p>"},{"location":"newoutput/logs-screen/#time-selection","title":"Time selection","text":"<p>To make a time selection click on the time selection box on the upper right corner of the browser window</p> <p></p> <p>This will open few options for time selections:</p> <p></p> <ul> <li> <p>Quick - Select a predefined query interval with one click</p> </li> <li> <p>Relative - Choose a time window between (current time - t1) and (current time - t2)</p> </li> <li> <p>Custom - Choose between two absolute dates and times</p> </li> <li> <p>Tags - Choose an application tag.</p> </li> </ul> <p>Dragging the mouse across a section of the logs flow graph will zoom into the previous query time range and set a new time interval.</p> <p></p> <p>During root cause and impact analysis, it is often the case that users would like to define a query window around a specific event represented by a log. In order to do so, move the mouse to the left of the log's timestamp. Click the 3 dots that appear on the screen and choose the time window you want the query to run, around this specific log.</p> <p></p>"},{"location":"newoutput/logs-screen/#search-queries","title":"Search queries","text":"<p>Search queries are still conducted mainly from the \"Search logs\" entry field. You can read about queries here. The new logs screen includes facilities that help you build queries based on the log fields. When clicking on a json field value a drop-down menu opens.</p> <p></p> <p>Clicking on the option \"INCLUDE IN QUERY\" will add the expression '_exists_: \"IP_geoip.ip\"' to the query. Choosing the exclude option will add the expression 'NOT _exists_: \"IP_geoip\"' to the query. You can also copy the complete JSON path to the clipboard, which can help in building query expressions.</p>"},{"location":"newoutput/logs-screen/#filters","title":"Filters","text":"<p>The filters section occupies the left side of the Logs tab. It enables the selection of specific values to be queries per filter. By default, the screen opens with application, subsystem, and severity filters, but the filters are completely customizable and metadata or log field are available.</p> <p></p> <p>Filters can be minimized and deleted using the ^ and x icons to the right of the filter name. The graph icon allows for rapid visualization of the filter values (see next section).</p> <p>Filters can be added by clicking the \"ADD FILTER\" button at the top of the filter section. They can also be added by choosing the \"ADD TO FILTER LIST\" option in the dropdown menu that opens when clicking on a JSON field.</p> <p></p>"},{"location":"newoutput/logs-screen/#choose-the-row-format","title":"Choose the row format","text":"<p>We know that during the analysis you can find that each log takes too much space on the screen. This is why you can use the Row Format button to change the way how very long logs are displayed. There are following options:</p> <ul> <li> <p>1-Line - logs are condensed into one line</p> </li> <li> <p>2-Line - logs are condensed into two lines</p> </li> <li> <p>Condensed - the whole log is visible but without breaking into lines</p> </li> <li> <p>JSON - the default view where JSON objects are parsed</p> </li> </ul>   ![](images/Logs_-Row_Format-1024x224.jpg)    _Row Format - JSON_   <p>If you would like to see the whole log you can click on the + sign:</p>   ![](images/Logs_-_1line_Row_Format-1-1024x243.jpg)    _Row Format - 1-Line_"},{"location":"newoutput/logs-screen/#clearing-a-query","title":"Clearing a query","text":"<p>Clicking on the x to the right of the query string will delete the query string but will not affect the rest of the query parameters (filters and time window).</p> <p></p> <p>Clicking on the \"RESET\" button will clear all query parameters (including filters).</p>"},{"location":"newoutput/logs-screen/#rapid-visualizations","title":"Rapid visualizations","text":"<p>When performing root cause and impact analysis it is very common to look at values distribution, the distribution of IP addresses, user names, subsystems, etc. Within a set of logs, this can give us clues about offending components, where is the problem is, and what is its impact. Coralogix users can generate very sophisticated visualizations and dashboards using Grafana, OpenSearch and other tools. The logs screen provides ways to rapidly generate count-based visualizations as part of the analyst log flow. These visualizations can be pinned to your main dashboard and to the tags tab.</p> <p></p> <p>Choose the logarithmic option if there is an order of magnitude variation in the distribution.</p> <p></p> <p>The filters section on the left shows the count-per-filter value. It will also provide a way to generate a count distribution graph for the filtered key. Hover with the mouse above the specific filter area. Click on the graph icon to the right of the filter name, and a graph similar to the above will be generated.</p> <p></p> <p>A third option of creating a rapid visualization can be found in the settings menu. The visualization will track the count of logs (same as the log flow graph at the top of the screen) per time window selected and the query parameters that are currently effective in the logs screen.</p> <p></p>"},{"location":"newoutput/logs-screen/#custom-views","title":"Custom views","text":"<p>Custom views help users look at the specific log information that is important for them, as well as create views that will help other users be more efficient, even if they are less technical or don't know the logs as well.</p> <p>A view is defined by a query (see the \"Queries and filters\" section) that creates the initial logs set that the view starts with, and columns (each column corresponds with a log field) that define the log data the user will see. 'Text\" is a special column (or field) that holds the entire log (excluding metadata). Every field can be removed or added to a view.</p> <p>Adding a column can be done by clicking on a JSON field and choosing the \"ADD AS COLUMN\" option from the dropdown menu.</p> <p></p> <p>It can also be added from the 'COLUMNS\" button.</p> <p></p> <p>Clicking on the \"COLUMNS\" button will open the \"Manage Columns\" window. You can move fields between the two lists by dragging them. Press the \"APPLY\" button to make sure the action takes effect. The \"Manage Columns\" window can also be accessed from the\u00a0 \"SETTINGS\" menu.</p> <p>A view can be saved by using the \"SAVE VIEW\" option.</p> <p></p> <p>Clinking on it will open a window where you enter the name for the custom view, and indicate if it includes the query selection, if it is the new default view, and if it is private or available to all the account users.</p> <p></p> <p>To open a saved view, click on save view or the arrow near the view name and scroll down to the list of saved views (\"MY VIEWS\" and \"SHARED VIEWS\").</p>"},{"location":"newoutput/logs-screen/#static-query-links","title":"Static query links","text":"<p>You can retrieve your data by performing queries and opening public saved views within the URL address.</p> <p>Examples:</p> <pre><code>https://YOUR_TEAM_NAME.coralogixstg.wpengine.com/#/query-new/logs?query=YOUR_QUERY\n</code></pre> <pre><code>https://YOUR_TEAM_NAME.coralogixstg.wpengine.com/#/query-new/logs?query=Field_Name_1:Value_1%20AND%20Field_Name_2:Value_2&amp;startTime=1591740610000&amp;endTime=1591741210000\n</code></pre> <pre><code>https://YOUR_TEAM_NAME.coralogixstg.wpengine.com/#/query-new/logs?viewName=YOUR_VIEW\n</code></pre> <p>Notes:</p> <ul> <li> <p>The time parameter should be in epoch 13 format.</p> </li> <li> <p>We support static query links only to public views.</p> </li> <li> <p>When a view is saved with query parameters the time frame will be the time range of the saved query.</p> </li> </ul> <p>Our customers are one of our main sources of innovation, we'd love to hear from you! Please send us your feedback to support@coralogixstg.wpengine.com or via the chat box in the lower right corner of the screen.</p>"},{"location":"newoutput/logs-screen-highlighting/","title":"Highlight & Share","text":"<p>During complex investigations, it is common to come across some logs that key to the root-cause-analysis process. As part of your workflow, you might need to quickly jump to these specific logs, share or draw attention to them.\u00a0</p> <p>Logs screen highlighting helps you work more efficiently by allowing you to:</p> <ul> <li>Highlight multiple logs</li> <li>Quickly navigate to the highlighted logs</li> <li>Share the highlighted logs.</li> </ul> <p>Limitations:</p> <ol> <li>The maximum number of logs that can be highlighted is 20.</li> <li>Logs can only be highlighted on the logs screen. (You cannot highlight logs on the alerts screen for example)</li> </ol> <p>This tutorial will guide you on how to highlight logs on the logs screen.</p>"},{"location":"newoutput/logs-screen-highlighting/#getting-started","title":"Getting started","text":""},{"location":"newoutput/logs-screen-highlighting/#search-for-logs","title":"Search for logs","text":"<p>Open the logs screen and perform any search.</p> <p></p>"},{"location":"newoutput/logs-screen-highlighting/#highlight-a-log","title":"Highlight a log","text":"<p>There are 2 ways to mark your logs.</p> <p>Option 1: Using the menu on each log</p> <p></p> <p>Option 2: Using keyboard shortcuts</p> <p>(Mac) Press CMD + Click on a log</p> <p>(PC) CTRL + Click on a log</p> <p></p>"},{"location":"newoutput/logs-screen-highlighting/#navigate-to-highlighted-logs","title":"Navigate to highlighted logs","text":"<p>After logs are highlighted, click on a log number to bring it into focus</p> <p></p>"},{"location":"newoutput/logs-screen-highlighting/#share-highlighted-logs","title":"Share highlighted logs","text":"<p>After logs are highlighted, copy the URL in your browser and share it with your team members.\u00a0</p> <p></p>"},{"location":"newoutput/logs-screen-highlighting/#clear-highlighted-logs","title":"Clear highlighted logs","text":"<p>To un-mark highlighted logs use any of the following steps:</p> <ol> <li>CMD + click on a highlighted log (Mac users) or CTRL + click (for PC users)</li> <li>Open the menu on a highlighted log. Click on Highlight Log</li> <li>Click on the Clear All at the bottom of the browser.</li> </ol> <p></p>"},{"location":"newoutput/lookup-tables/","title":"Lookup Tables","text":""},{"location":"newoutput/lookup-tables/#overview","title":"Overview","text":"<p>Coralogix enables you to enrich and filter your logs using additional context from a lookup table. For example, enrich user activity logs with the user\u2019s department and then retrieve logs of all users in the Finance department.</p> <p>A lookup table, also known as a reference table, is a specific type of data structure used to simplify data lookup operations. It's a table that contains a set of values or information that can be used to quickly find corresponding values in another dataset. Lookup tables are especially useful when you want to map one value to another, such as converting a code to a meaningful description.</p> <p>When it comes to log analysis, lookup tables help to add the relevant context to your logs. This enhances the efficiency, accuracy, and consistency of log analysis by simplifying data enrichment, normalization, and interpretation. This is particularly valuable when dealing with obscure or unclear log data that requires contextual information for users to understand any given situation and to take appropriate action.</p>"},{"location":"newoutput/lookup-tables/#how-can-i-use-lookup-tables","title":"How Can I Use Lookup Tables?","text":"<p>Take a look at the following use cases to get a feel for the many ways in which lookup tables can help you.</p>"},{"location":"newoutput/lookup-tables/#use-case-1-detecting-unauthorized-access-to-cloud-resources","title":"Use Case 1: Detecting Unauthorized Access to Cloud Resources","text":"<p>In cloud environments, multiple users, teams, and applications interact with a diverse range of resources. These resources can include databases, VMs, storage buckets, and more. Ensuring that only authorized users access specific resources is crucial for maintaining data integrity and security. Detecting unauthorized access quickly is paramount, as it can prevent potential data breaches, financial losses, and reputational damage.</p> <p>By incorporating information from lookup tables directly into log entries, you can provide analysts with more context making it easier for them to identify and respond to unauthorized access attempts. For instance, if an analyst is reviewing log entries related to user interactions with cloud resources, the lookup table can bring context on the user role (e.g. based on identity store) and the sensitivity level of the cloud resource (e.g. based on AWS resource tags). This makes it far easier for you to to identify unauthorized access to sensitive data, improve your organization\u2019s security posture and adhere to compliance requirements.</p> <p>See example below. For more details and syntax, see our DataPrime Cheatsheet.</p> <p></p>"},{"location":"newoutput/lookup-tables/#use-case-2-user-behavior-profiling-for-better-product-development-and-targeted-marketing","title":"Use Case 2: User Behavior Profiling for Better Product Development and Targeted Marketing","text":"<p>Analyzing user behavior through logs can reveal patterns and preferences that guide product development and marketing efforts. Lookup tables can match user IDs with customer profiles, enabling deeper analysis and personalization - without wasting any time searching through extensive databases for each log entry. By enhancing user behavior understanding, you can develop targeted and cost-effective campaigns while improving customer satisfaction.</p>"},{"location":"newoutput/lookup-tables/#use-case-3-product-sku-mapping-for-better-inventory-management-and-increased-sales","title":"Use Case 3: Product SKU Mapping for Better Inventory Management and Increased Sales","text":"<p>E-commerce businesses can analyze log data to track product popularity, availability and customer buying patterns. Lookup tables that map SKU codes to product names enable efficient product performance analysis.</p> <p>With instant access to product names based on SKUs, you can eliminate the need to query product databases repeatedly. This can help you optimize inventory management, pricing strategies, and marketing campaigns, leading to increased revenue and reduced inventory costs.</p>"},{"location":"newoutput/lookup-tables/#configuration","title":"Configuration","text":"<p>First, upload the lookup table:</p> <p>STEP 1. From your Coralogix toolbar, go to\u00a0Data Flow\u00a0&gt;\u00a0Data Enrichment.</p> <p>STEP 2. Scroll down to the Custom Enrichment\u00a0section, and click Add Custom Enrichment.</p> <p></p> <p>STEP 3. Name your new enrichment and provide an (optional) description.</p> <p>STEP 4. Select and upload the CSV file that contains your lookup table.</p> <p>STEP 5. Click\u00a0CREATE.</p>"},{"location":"newoutput/lookup-tables/#enriching-your-logs","title":"Enriching Your Logs","text":"<p>There are two possible ways to enrich your logs:</p> <ul> <li> <p>Select a log key to look up for a key value and enrich the logs automatically during ingestion. The logs are saved with the enriched fields. The advantages of this mode:</p> <ul> <li> <p>Logs are automatically enriched.</p> </li> <li> <p>The logs themselves include the enrichment data, which makes it easier to consume everywhere (by any query, and also by third-party products that read the logs from the S3 bucket).</p> </li> </ul> </li> <li> <p>Use the DataPrime\u00a0<code>enrich</code>\u00a0operator to look up a value in this table and enrich the log dynamically for the purpose of the query. The advantages of this mode:</p> <ul> <li> <p>It allows you to enrich old logs already ingested into Coralogix.</p> </li> <li> <p>The enrichment does not increase the size of the stored logs, as the enrichment is done dynamically, only for the query results.</p> </li> </ul> </li> </ul>"},{"location":"newoutput/lookup-tables/#syntax-for-data-enrichment-using-the-dataprime-enrich-operator","title":"Syntax for Data Enrichment Using the DataPrime Enrich Operator","text":"<pre><code>enrich &lt;value_to_lookup&gt; into &lt;enriched_key&gt; using &lt;lookup_table&gt;\n\n</code></pre> <p>The\u00a0<code>&lt;value_to_lookup&gt;</code>\u00a0(name of a log key or the actual value) will be looked up in the Custom Enrichment\u00a0<code>&lt;lookup_table&gt;</code>\u00a0and a key called\u00a0<code>&lt;enriched_key&gt;</code>\u00a0will be added to the log, containing all table columns as sub-keys. If the\u00a0<code>&lt;value_to_lookup&gt;</code>\u00a0is not found in the\u00a0<code>&lt;lookup_table&gt;</code>, the\u00a0<code>&lt;enriched_key&gt;</code>\u00a0will still be added but with \u201cnull\u201d values, in order to preserve the same structure for all result logs. You can then filter the results using the DataPrime capabilities, such as filtering logs by specific value in the enriched field.</p>"},{"location":"newoutput/lookup-tables/#example","title":"Example","text":"<p>For the original log:</p> <pre><code>{\n\"userid\": \"111\",\n...\n}\n\n</code></pre> <p>And the Custom Enrichment lookup table called \u201cmy_users\u201d:</p> ID Name Department 111 John Finance 222 Emily IT <p>Running the following query:</p> <pre><code>enrich $d.userid into $d.user_enriched using my_users\n\n</code></pre> <p>Gives the following enriched log:</p> <pre><code>{\n\"userid\": \"111\",\n\"user_enriched\": {\n    \"ID: \"111\",\n    \"Name\": \"John\",\n    \"Department\": \"Finance\"\n    },\n...\n}\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>Run the DataPrime query\u00a0<code>source &lt;lookup_table&gt;</code>\u00a0to view the enrichment table.</p> </li> <li> <p>If the original log already contains the enriched key:</p> <ul> <li> <p>If\u00a0<code>&lt;value_to_lookup&gt;</code>\u00a0exists in the\u00a0<code>&lt;lookup_table&gt;</code>, the sub-keys will be updated with the new value. If the\u00a0<code>&lt;value_to_lookup&gt;</code>\u00a0does not exist, their current value will remain.</p> </li> <li> <p>Any other sub-keys which are not columns in the\u00a0<code>&lt;lookup_table&gt;</code>\u00a0will remain with their existing values.</p> </li> </ul> </li> <li> <p>All values in the\u00a0<code>&lt;lookup_table&gt;</code>\u00a0are considered to be strings. This means that:</p> <ul> <li> <p>The\u00a0<code>&lt;value_to_lookup&gt;</code>\u00a0must be in a string format.</p> </li> <li> <p>All values are enriched in a string format. You may then convert them to your preferred format (e.g. JSON, timestamp) using the appropriate functions.</p> </li> </ul> </li> </ul>"},{"location":"newoutput/lookup-tables/#examples","title":"Examples","text":""},{"location":"newoutput/lookup-tables/#example-1","title":"Example 1","text":"<p>This example uses a MySQL query log that contains queries and their return-code. The return-code is then enriched using a lookup table <code>mysql_return_codes</code> that contains the message-templates for each return code. These message are then aggregated, showing the top 10 most-occurring messages.</p> <pre><code># logs data:\n...\n{ \"event_type\": \"mysql_query_log\", \"query\": ..., \"return_code\": 1020 }\n...\n{ \"event_type\": \"mysql_query_log\", \"query\": ..., \"return_code\": 1027 }\n...\n\n# mysql_return_codes enrichment data (CSV):\nError,Message\n...\n1020,Record has changed since last read in table '%s'.\n...\n1026,Error writing file '%s' (errno: %d - %s).\n1027,'%s' is locked against change.\n...\n\n# Query\nsource logs\n| filter event_type == 'mysql_query_log'\n**| enrich return_code into return_code_details using mysql_return_codes**\n| top 10 return_code_details.Message by count()\n\n# Output\n{ \n  \"return_code_details\": { \"Message\": \"Can't write. duplicate key in table '%s'.\" },\n  \"_count\": 2873\n}\n{ \n  \"return_code_details\": { \"Message\": \"'%s' is locked against change.\" },\n  \"_count\": 804\n}\n...\n\n</code></pre>"},{"location":"newoutput/lookup-tables/#example-2","title":"Example 2","text":"<p>In this example, we have logs that contain a \u201cproduct_type\u201d key. We\u2019d like to count the number of products that belong to the <code>Furniture</code> category. Each product type belongs to a category, and that mapping resides in the <code>product_type_details</code> lookup table. However, the product-types in the lookup table are written in upper-case (e.g. <code>CHAIRS</code>), and in the logs, the product-type is not necessarily in upper-case (e.g. <code>Chairs</code>). The query uses <code>enrich toUpperCase(product_type) ...</code> in order to enrich the data based on the upper-case version of the product-type in the logs.</p> <pre><code># logs data:\n{ \n \"product_type\": \"Chairs\",\n \"product_name\": \"Kitchen Chair - Blue\"\n ...\n}\n{\n  \"product_type\": \"Electric Guitars\",\n  \"product_name\": \"Les Paul 70s Deluxe\"\n  ...\n}\n\n# product_type_details data (CSV)\ntype,category\n...\n\"CHAIRS\",\"Furniture\"\n\"ELECTRIC GUITARS\",\"Musical Instruments\"\n...\n\n# Query\nsource logs\n| enrich **toUpperCase(product_type)** into details using product_type_details\n| filter details.category == 'Furniture'\n| count\n\n# Results\n...\n{ \"product_type\": \"Chairs\", ... , \"details\": { \"category\": \"Furniture\" } }\n{ \"product_type\": \"Tables\", ... , \"details\": { \"category\": \"Furniture\" } }\n...\n\n\n</code></pre>"},{"location":"newoutput/lookup-tables/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email to\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/machine-learning-metric-alert/","title":"Machine-Learning Metric Alert","text":"<p>Static alerts often result in alert fatigue, causing important notifications to be lost in a flood of irrelevant ones. Misconfigurations can cause missed events or frequent false alarms, requiring constant manual adjustments. What\u2019s more, static alerts fail to adapt to system changes.</p> <p>The Coralogix Machine-Learning Metric Alert solves these issues by automatically learning your system and adapting to your evolving infrastructure. This allows you to quickly set up alerts that track thousands of components and proactively alert on deviations from the norm. These alerts are powered by our Streama\u00a9 technology, which allows them to run on the Coralogix monitoring pipeline at a third of the cost, without prior indexing.</p>"},{"location":"newoutput/machine-learning-metric-alert/#overview","title":"Overview","text":"<p>The Machine-Learning Metric Alert utilizes advanced artificial intelligence algorithms to analyze incoming metric time series and predict their expected behavior for the next 24 hours. This powerful capability enables businesses to proactively identify anomalies in real-time, providing valuable insights and timely alerts. By establishing a baseline for normal behavior, teams can quickly respond to issues, minimizing downtime, enhancing operational efficiency, and ensuring a seamless user experience. With customizable thresholds and notification settings, the More Than Usual and Less Than Usual conditions adapt to diverse software environments, making them an indispensable tool for maintaining optimal performance.</p>"},{"location":"newoutput/machine-learning-metric-alert/#how-streama-enables-in-stream-machine-learning-alerting","title":"How Streama Enables In-Stream Machine Learning Alerting","text":"<p>The Machine-Learning Metric Alert is powered by our Streama\u00a9 technology, which enables automatic triggering of your alerts as part of the streaming process, without prior indexing. In other words, we process your data first and delay storage and indexing until all important decisions have been made. Using this technology, our alerts automatically learn your system and adapt to your evolving infrastructure, allowing for rapid setup that can track thousands of components and proactively alert on deviations from the norm.</p> <p></p>"},{"location":"newoutput/machine-learning-metric-alert/#user-manual","title":"User Manual","text":"<p>Create metric alerts with the More Than Usual or Less Than Usual conditions using our advanced machine learning model. When the value of your metric exceeds or falls short of a predefined baseline that is considered normal, an alert is triggered to notify you of the deviation.</p> <p>Our machine learning model establishes the baseline standard for your metric. It is applied daily for the next 24 hours, using data from the past 7 days, and is based on a maximum of 500 permutations per metric.</p> <p>When a metric surpasses or remains under this predefined threshold, it suggests that something out of the ordinary has occurred, potentially indicating a problem or an opportunity for corrective action. An alert is triggered to notify you of a deviation matching your More Than Usual or Less Than Usual conditions.</p>"},{"location":"newoutput/machine-learning-metric-alert/#create-a-metric-alert-with-the-more-than-usual-condition","title":"Create a Metric Alert With the More Than Usual Condition","text":"<p>STEP 1. Follow the instructions for creating a metric alert and select More Than Usual in the Conditions section.</p> <p>STEP 2. Enter a minimum threshold below which the alert will not be triggered.</p> <p>STEP 3. Enter a percentage (for over x %) and timeframe (of the last x minutes) to determine the length of time for which the metric must be more than usual within the time window.</p> <p>STEP 4. Select the percentage (at least x %) of the timeframe which needs to have values for the alert to trigger.</p> <p>STEP 5. Finish setting up the alert as specified in the instructions for creating a metric alert.</p> <p></p>"},{"location":"newoutput/machine-learning-metric-alert/#create-a-metric-alert-with-the-less-than-usual-condition","title":"Create a Metric Alert With the Less Than Usual Condition","text":"<p>STEP 1. Follow the instructions for creating a metric alert and select Less Than Usual in the Conditions section.</p> <p>STEP 2. Enter a triggering threshold above which the alert will not be triggered.</p> <p>STEP 3. Enter a percentage (for over x %) and timeframe (of the last x minutes) to determine the length of time for which the metric must be less than usual within the time window.</p> <p>STEP 4. Select the percentage (at least x %) of the timeframe which needs to have values for the alert to trigger.</p> <p>STEP 5. Finish setting up the alert as specified in the instructions for creating a metric alert.</p> <p></p>"},{"location":"newoutput/machine-learning-metric-alert/#how-can-i-use-the-machine-learning-metric-alert","title":"How Can I Use the Machine Learning Metric Alert?","text":"<p>Take a look at these use-cases to get a feel for the many ways that the Machine Learning Metric Alert can serve you.</p>"},{"location":"newoutput/machine-learning-metric-alert/#use-case-1-metrics","title":"Use Case 1: Metrics","text":"<p>A developer needs to closely monitor the average latency of route responses within her organization's services. The services are situated in a complex cloud environment, and consist of numerous routes, each with a wide range of response times.</p> <p>To accomplish this, she defines a specialized Machine Learning Metric Alert using the More Than Usual conditions. This alert is designed to detect instances when response times deviate from the norm, intelligently grouping routes by their respective labels.</p> <p>The alert system incorporates prediction modeling techniques to account for the variability in response times across different routes. It considers that certain routes may have response times an order of magnitude higher than others, ensuring accurate and reliable insights into the system's performance.</p> <p>By monitoring the services in this way, the developer can gain a holistic understanding of their behavior and proactively identify and address potential performance bottlenecks.</p>"},{"location":"newoutput/machine-learning-metric-alert/#use-case-2-events2metrics","title":"Use Case 2: Events2Metrics","text":"<p>A DevOps engineer takes advantage of the Machine-Learning Metric Alert solution to monitor and receive notifications when a response time exceeds its usual duration.</p> <p>Leveraging the power of our innovative Events2Metrics functionality, he effortlessly converts transaction logs into meaningful metrics that precisely track response times. By extracting the relevant data from the transaction logs, this Coralogix tool creates a dedicated metric specifically designed to monitor a response time. This numeric key becomes the foundation for establishing alerts that keep you informed whenever the response time deviates from its expected range.</p> <p>The DevOps engineer receives instant notifications when a transaction's response time surpasses its usual duration, allowing him to quickly identify and address any performance bottlenecks or potential issues within his system.</p> <p></p>"},{"location":"newoutput/machine-learning-metric-alert/#use-case-3-security","title":"Use Case 3: Security","text":"<p>As a security analyst responsible for monitoring the outgoing traffic of hosts within your organization's cloud environment, your primary objective is to ensure the security and integrity of sensitive data, detecting any potential breaches or leaks that may occur.</p> <p>One of the challenges you face is the significant variation in the amount of traffic generated by different hosts within the same platform. To address this, you utilize the More Than Usual condition for the Machine-Learning Metric Alert. By tracking the outgoing traffic and grouping it using the <code>host_name</code> label, you can effectively identify any anomalies that may indicate a data breach.</p> <p>Coralogix goes a step further by learning the unique traffic-out patterns of each individual host in the environment. It creates a dedicated model for every host name, enabling you to gain a comprehensive understanding of their typical traffic-out behaviors. This personalized approach ensures that your monitoring efforts are tailored to the specific characteristics of each host, minimizing false alarms and maximizing the detection of genuine threats.</p> <p>When the outgoing traffic of a host exceeds its usual levels, the metric alert triggers an immediate response from your security analyst team. This timely notification allows you to swiftly investigate and take appropriate action to mitigate any potential risks to your organization's data assets.</p>"},{"location":"newoutput/machine-learning-metric-alert/#use-case-4-resource-utilization-metrics-cpu-memory-disk-io","title":"Use Case 4: Resource Utilization Metrics (CPU, Memory, Disk I/O)","text":"<p>In a cloud or data center environment, monitoring the utilization of resources like CPU, memory, and disk I/O is essential for ensuring smooth operations.</p> <p>An alert for lower-than-usual resource utilization can indicate underutilization or idleness of critical systems, which might mean that an application is down, or there's a misconfiguration causing underperformance. This type of alert helps in optimizing resource allocation and can also serve as an early warning for system failures.</p>"},{"location":"newoutput/machine-learning-metric-alert/#additional-resources","title":"Additional Resources","text":"DocumentationMetric AlertsFlow Alerts"},{"location":"newoutput/machine-learning-metric-alert/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/management-api-endpoints/","title":"Management API Endpoints","text":"<p>Coralogix offers a regional endpoint necessary for our Management APIs.</p> <p>Select the relevant endpoint based on the region associated with your Coralogix region and domain.</p> Coralogix Region Endpoint US1 https://ng-api-grpc.coralogix.us:443/ US2 https://ng-api-grpc.cx498.coralogixstg.wpengine.com:443/ EU1 https://ng-api-grpc.coralogixstg.wpengine.com:443/ EU2 https://ng-api-grpc.eu2.coralogixstg.wpengine.com:443/ AP1 (IN) https://ng-api-grpc.app.coralogix.in:443/ AP2 (SG) https://ng-api-grpc.coralogixsg.com:443/"},{"location":"newoutput/management-api-endpoints/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/managing-the-sta/","title":"Managing the STA","text":"<p>The Snowbit STA comes, prebuilt with tools and services that automatically handle the majority of the management issues of the STA so you won't have to do anything.</p> <p>Just in case you'll need it, the STA comes with a collection of commands that will help you control it. In this article, we'll explain each command and when to use it.</p>"},{"location":"newoutput/managing-the-sta/#sta-commands","title":"STA Commands","text":""},{"location":"newoutput/managing-the-sta/#sta-acknowledge-installation-id","title":"<code>sta-acknowledge-installation-id</code>","text":"<p>When you have finished installing the STA, it is recommended that you will run the command <code>sta-get-installation-id</code> and store its output in some secure place and then run this command. The installation ID is required to get privileged access to the STA, thing which might be required by our support team. Once you run the acknowledgement command, it will erase the installation ID from the instance.</p>"},{"location":"newoutput/managing-the-sta/#sta-get-status-short","title":"<code>sta-get-status-short</code>","text":"<p>This command will print something similar to the following output which will indicate the current status of every service in the STA. If the STA was just installed it is normal that it takes for some services some time (up to 10 minutes) to fully stabilize:</p> <p></p>"},{"location":"newoutput/managing-the-sta/#sta-test-coralogix-connection","title":"<code>sta-test-coralogix-connection</code>","text":"<p>This command will test the connection to Coralogix by sending a dummy event to your Coralogix account. If you see it in Coralogix within a second or two it means that the connection to Coralogix is working properly.</p>"},{"location":"newoutput/managing-the-sta/#sta-get-version","title":"<code>sta-get-version</code>","text":"<p>Provides the current version of the STA.</p>"},{"location":"newoutput/managing-the-sta/#sta-update-version","title":"<code>sta-update-version</code>","text":"<p>By default the STA is updating itself seamlessly. This command requires 1 additional argument which can be set to a <code>&lt;specific_version&gt;/latest</code> update. in case you provided a specific version for the STA using this command, please note that the STA won\u2019t update itself until this command will be executed again with <code>\"latest\"</code> as an argument.</p>"},{"location":"newoutput/managing-the-sta/#sta-get-defacto-config","title":"<code>sta-get-defacto-config</code>","text":"<p>This command returns the STA\u2019s current full configuration as JSON output.</p>"},{"location":"newoutput/managing-the-sta/#sta-edit-config","title":"<code>sta-edit-config</code>","text":"<p>Editing STA\u2019s configuration can be performed only when not using a remote storage such as S3 (highly recommended).</p> <p>this holds the full configuration for the STA and all its relevant components and services.</p>"},{"location":"newoutput/managing-the-sta/#sta-select-config-editor","title":"<code>sta-select-config-editor</code>","text":"<p>By using this command you can select your preferred editor for example <code>VI</code> or <code>nano</code>. please see command above for the editing constraints.</p>"},{"location":"newoutput/managing-the-sta/#sta-diag-validate-config","title":"<code>sta-diag-validate-config</code>","text":"<p>In case you are using a remote storage, you can validate the current <code>sta.conf</code> file\u2019s schema. Note that when editing the configuration the STA is validating the schema on the fly - so this command is not mandatory when configuration changes are executed.</p>"},{"location":"newoutput/managing-the-sta/#sta-encrypt-config-value","title":"<code>sta-encrypt-config-value</code>","text":"<p>As the STA\u2019s configuration can hold delicate information that you might want to prevent from other users to read it, you can encrypt desired values by following the steps after running the command.</p>"},{"location":"newoutput/managing-the-sta/#sta-get-service-logs","title":"<code>sta-get-service-logs</code>","text":"<p>Using this command, you can investigate different services that are running within the STA. additional argument is required which is representing the desired service. The service name can be found when running the command <code>sta-get-status-short</code> that presented above.</p>"},{"location":"newoutput/managing-the-sta/#sta-lookup-rule","title":"<code>sta-lookup-rule</code>","text":"<p>This command allows you to find a Suricata rule used by the STA by using its SID as mentioned here: How to Modify an STA Suricata Rule</p>"},{"location":"newoutput/managing-the-sta/#sta-wazuh-list-agents","title":"<code>sta-wazuh-list-agents</code>","text":"<p>If you have installed Wazuh agents and have connected them to the STA, this command will display a list of all the connected agents like this:</p> <pre><code>Available agents: \n   ID: 001, Name: ip-172-31-29-200, IP: any\n   ID: 002, Name: ip-172-31-29-10, IP: any\n   ID: 003, Name: ip-172-31-30-89, IP: any\n   ID: 004, Name: ip-172-31-27-174, IP: any\n   ID: 005, Name: ip-172-31-31-73, IP: any\n   ID: 006, Name: ip-172-31-30-60, IP: any\n   ID: 007, Name: ip-172-31-21-241, IP: any\n   ID: 008, Name: ip-172-31-17-78, IP: any\n   ID: 009, Name: ip-172-31-29-40, IP: any\n   ID: 010, Name: ip-172-31-23-114, IP: any\n</code></pre> <p><code>sta-wazuh-add-agent</code></p> <p>You can add Wazuh agents from different machines manually using this command. This command requires 2 additional arguments: <code>agent name</code> and <code>agent ip</code>. To see the constraints for those arguments please run this command without no arguments. Please note that if you are installing Wazuh agents using our Documentation, those agents are added automatically and no actions are required.</p> <p><code>sta-wazuh-remove-agent</code></p> <p>You can remove Wazuh agents from different machines manually using this command. This command requires 1 additional argument: <code>agent id</code>. To get all agents IDs use <code>sta-wazuh-list-agents</code> command. Please note that the STA is monitoring each agent, and in case no communication is received from the agent for some time, the STA removes it automatically.</p> <p><code>sta-wazuh-restart-agents</code></p> <p>By using this command, the STA restarting all Wazuh agents that are currently connected.</p> <p><code>sta-wazuh-get-key</code></p> <p>Use this command to get Wazuh agent\u2019s key. requires 1 additional argument: <code>agent id</code>. To get all agents IDs use <code>sta-wazuh-list-agents</code> command.</p>"},{"location":"newoutput/managing-the-sta/#sta-force-rules-updater","title":"<code>sta-force-rules-updater</code>","text":"<p>Normally, the STA will update its set of rules for Suricata, Zeek and Wazuh every day at 07:07AM at the STA's local time. If you want it to update these rules now run this command.</p>"},{"location":"newoutput/managing-the-sta/#sta-force-sync-configs","title":"<code>sta-force-sync-configs</code>","text":"<p>Normally, the STA will attempt to synchronize its config files from the S3 bucket that has been configured during the installation phase every three minutes. If you have made a change to your configuration and would like to apply these changes now you can run this command.</p>"},{"location":"newoutput/managing-the-sta/#sta-diag-general","title":"<code>sta-diag-general</code>","text":"<p>Provides a general overview of the STA's performance including both network, processor, disk and memory metrics.</p>"},{"location":"newoutput/managing-the-sta/#sta-diag-network","title":"<code>sta-diag-network</code>","text":"<p>Provides detailed real-time information about the network usage in the STA.</p>"},{"location":"newoutput/managing-the-sta/#sta-diag-top","title":"<code>sta-diag-top</code>","text":"<p>Launches a \"top\" like tool that provides information about processes, threads, memory and tasks metrics.</p>"},{"location":"newoutput/managing-the-sta/#sta-diag-delete-traffic-mirror-sessions","title":"<code>sta-diag-delete-traffic-mirror-sessions</code>","text":"<p>Currently supported for STA deployed on AWS. This command will delete a Traffic Mirror Session depended on a required argument representing available <code>&lt;traffic_mirror_target_id&gt; (tmt-*)</code> or <code>&lt;traffic_mirror_filter_id&gt; (tmf-*)</code></p>"},{"location":"newoutput/managing-the-sta/#sta-diag-disk-usage","title":"<code>sta-diag-disk-usage</code>","text":"<p>This command will provide current disk usage separated per root level folders. To see the disk usage iterated recursively over whole paths use the following command: <code>**sta-diag-disk-usage-detailed**</code>.</p>"},{"location":"newoutput/managing-the-sta/#sta-diag-dump-cache","title":"<code>sta-diag-dump-cache</code>","text":"<p>using this command, you can dump all into console all available cache enrichment files. the output is separated per enrichment represented as files. this is the representation per line:</p> <ul> <li> <p>Enrichment file\u2019s full path</p> </li> <li> <p>File\u2019s name written in encoded base64</p> </li> <li> <p>decoded content from the name written above</p> </li> <li> <p>relevant socket that provided that enrichment</p> </li> </ul>"},{"location":"newoutput/managing-the-sta/#sta-test-enrich","title":"<code>sta-test-enrich</code>","text":"<p>Tests the enrichment of data by the various services in the STA.</p>"},{"location":"newoutput/managing-the-sta/#sta-set-mgmt-ip","title":"<code>sta-set-mgmt-ip</code>","text":"<p>Useful for on-prem installations where each server has a manual management IP address. In such cases use this command to specify the address you would like the STA to have.</p>"},{"location":"newoutput/managing-the-sta/#sta-set-mgmt-ip-dhcp","title":"<code>sta-set-mgmt-ip-dhcp</code>","text":"<p>Useful for on-prem installations. In case you used the <code>sta-set-mgmt-ip</code> command to force the STA to use a manually set IP address for its management network interface and now interested in reverting that to the default (using an IP address from a DHCP).</p>"},{"location":"newoutput/managing-the-sta/#sta-reload-mgmt-nic","title":"<code>sta-reload-mgmt-nic</code>","text":"<p>This command tears down current <code>management NIC</code> and bringing it up again. use this when you want to restart the management NIC that handles all STA\u2019s traffic communication.</p>"},{"location":"newoutput/managing-the-sta/#sta-get-disk-usage","title":"<code>sta-get-disk-usage</code>","text":"<p>Run this command to see how the storage is used per root\u2019s folders. If you wish to see every folder separately in a recursive manner use <code>**sta-get-disk-usage-detailed**</code>.</p>"},{"location":"newoutput/managing-the-sta/#sta-diag-get-debug-package","title":"<code>sta-diag-get-debug-package</code>","text":"<p>by using this command, the STA collects all possible logs from all services, system processes, etc. and compresses them into one archive. This command is available so you\u2019ll be able to provide our support a full state of your STA.</p> <p>Note - This command is a very heavy process. Use this command as last resort for investigating issues in your STAs as this archive hold everything and it similar to look for a needle in a haystack.</p>"},{"location":"newoutput/managing-the-sta/#sta-get-metric-value","title":"<code>sta-get-metric-value</code>","text":"<p>by using this command, you are able to inspect specific metric files and perform aggregation functions in specific timeframe. the initial path for all metric files is: <code>/coralogix/sta/metrics/</code> - from there, locate the desired <code>Whisper</code> file which representing your metric. The supported aggregation functions are: <code>max|min|last|avg|len</code>.</p>"},{"location":"newoutput/managing-the-sta/#notes","title":"Notes","text":"<ul> <li> <p>Other commands that exist on the STA are meant to be used by Coralogix customer success team in very special cases.</p> </li> <li> <p>If you think that you need root access to the STA, there is a way to get that. Please contact us via the chat in Coralogix for additional details.</p> </li> </ul>"},{"location":"newoutput/managing-your-organization/","title":"Create an Organization","text":"<p>Coralogix offers the option of collectively managing multiple teams per organization via an Organization Administrator.</p>"},{"location":"newoutput/managing-your-organization/#overview-concepts","title":"Overview &amp; Concepts","text":""},{"location":"newoutput/managing-your-organization/#organization","title":"Organization","text":"<p>An organization in Coralogix is a group of teams associated with a single Coralogix customer that are managed together by an Organization Administrator. Organizations enable you to treat your teams as part of an entity, rather than as individual Coralogix customers. Multiple domains may be treated as part of one organization.</p> <p>The benefits of managing your teams as part of an organization include:</p> <ul> <li> <p>Flexibility. Move data usage units between different teams in your organization.</p> </li> <li> <p>Cost Optimization. More efficient data usage allows for pricing benefits.</p> </li> </ul>"},{"location":"newoutput/managing-your-organization/#teams-in-coralogix","title":"Teams in Coralogix","text":"<p>A team in Coralogix is a platform environment with its own unique URL and settings. An organization may have multiple teams, with each team consisting of multiple users. Teams are best used within an organization to truly compartmentalize data and enable true separation between users. This allows you to ship specific data to one team while shipping a different set of data to a different set of users.</p>"},{"location":"newoutput/managing-your-organization/#organization-administrator","title":"Organization Administrator","text":"<p>The Organization Administrator (Org Admin) is the first user created in an organization and is responsible for its overall management. The Org Admin is the only user with permission to access the Organization Settings.</p> <p>If an Org Admin was not automatically created for your organization, or if you are an existing customer wishing to enjoy Organization settings, contact Coralogix Support at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/managing-your-organization/#organization-settings","title":"Organization Settings","text":"<p>Org Admins have a dedicated section in their settings from which they can access the following:</p> <ul> <li> <p>My Teams. Add, connect, and disconnect teams within your organization.</p> </li> <li> <p>Manage Admins. Grant org admin status to other users so that they can can view and/or create teams.</p> </li> <li> <p>Quota Manager. View how each team consumes their Coralogix units and and move units between teams.</p> </li> <li> <p>Organization Settings. Establish which domains may join the organization and what what organizational attributes will be granted to different users during the signup process.</p> </li> </ul> <p>These settings give the Org Admin complete control over their organization and enable them to maintain the structure of their organization as they deem appropriate.</p>"},{"location":"newoutput/managing-your-organization/#access-organization-settings","title":"Access Organization Settings","text":"<p>STEP 1. Navigate to your personal settings on the right-hand side of your Coralogix toolbar. Click on Settings in the lower left-hand corner.</p> <p>STEP 2. In the left-hand side bar, select one of the following tabs in your organization menu:</p> <ul> <li> <p>My Teams</p> </li> <li> <p>Manage Admins</p> </li> <li> <p>Quota Manager</p> </li> <li> <p>Organization Settings</p> </li> </ul> <p></p>"},{"location":"newoutput/managing-your-organization/#create-an-organization","title":"Create an Organization","text":"<p>In order to begin working with Coralogix, the first step is to create your organization. As part of creating your organization, you will also create your first user (the Org Admin) and your first team.</p> <p>Note: If users from your organization (who share the same work email domain) are already registered with Coralogix, the organization may have already been created. In that case, you may be able to join an existing team or create a new team. If these options are not possible, you will be instructed to contact your Org Admin to request to be added to an existing team or create a new team.</p> <p>The following procedure assumes no other users who share your work email domain are registered with Coralogix.</p> <p>STEP 1. Sign up for a Coralogix account by providing your work email.</p> <p></p> <p>STEP 2. Check your email to verify your account. Click ACTIVATE, and you will be rerouted to the Coralogix page.</p> <p></p> <p>STEP 3. Complete your user account setup. If you are the first from your organization to set up a Coralogix account, you will be prompted to input your organization name and will automatically become the Org Admin. Click SAVE AND CONTINUE.</p> <p></p> <p>STEP 4. You will be prompted to create a new team. Enter a name for the team, then click CREATE NEW TEAM.</p> <p></p>"},{"location":"newoutput/managing-your-organization/#subsequent-users","title":"Subsequent Users","text":"<p>Once an organization has been created, subsequent users who open an account with the same organization domain or affiliated domain defined by the Org Admin will have different signup options depending on the permissions set by the Org Admin.</p> <p>When joining an existing team with SSO configured, users will be required to sign in using their SSO credentials. When not using SSO, the user will need to create an account and only then will they be able to login.</p> <p>If an org admin allows subsequent users to join an existing team:</p> <ul> <li>Option 1: In this case the Organization administrator chose to enable joining existing organization teams, but chose to disable the option to allow the user to create new teams to be automatically connected to the organization. The user can either choose one of the team or chose the option to contact their administrator (as seen in the image below). If they choose to contact their administrator, an email will be sent on their behalf and the org admin can chose what additional steps will be taken (e.g. create a new team for the user or invite them to one of the existing teams).</li> </ul> <p></p> <ul> <li>Option 2: In this case, the Org Admin chose to enable joining existing teams and also enable the option to allow the user to create new teams to be automatically connected to the organization. The user may either join an existing team, or create a new team.</li> </ul> <p></p> <p>If they choose to create a new team, the Complete Your Account Setup page appears. They need to fill in their details, then click SAVE AND CONTINUE.</p> <p></p> <p>The Create New Team page will appear. The user should enter a name for their new team and click CREATE NEW TEAM.</p> <p></p> <p>After Creating the team, the user is automatically logged in and redirected to their new Coralogix Team page.</p> <p>If the org admin does not allow subsequent users to join an existing team:</p> <ul> <li> <p>Option 3: In this case, the org admin chose to disable joining existing teams, but enable the option to allow the user to create new teams to be automatically connected to the organization.</p> </li> <li> <p>If they choose to create a new team, the Complete Your Account Setup page appears. They need to fill in their details, then click SAVE AND CONTINUE.</p> </li> </ul> <p></p> <ul> <li>The Create New Team page will appear. The user should enter a name for their new team and click CREATE NEW TEAM.</li> </ul> <p></p> <p>After Creating the team, the user is automatically logged in and redirected to their new Coralogix Team page.</p> <ul> <li>Option 4: In this case, the org admin chose to disable joining existing teams and also disable the option to allow the user to create new teams to be automatically connected to the organization. The user may contact their Org Admin to request to be added to a team as shown in the image below. If they do that, an email will be sent on their behalf and the org admin can chose what additional steps will be taken (e.g. create a new team for the user or invite them to one of the existing teams).</li> </ul> <p></p>"},{"location":"newoutput/managing-your-organization/#additional-resources","title":"Additional Resources","text":"Getting StartedGetting Started with CoralogixTeam and User ManagementManaging Your OrganizationMy TeamsManage AdminsQuota ManagerOrganization Settings"},{"location":"newoutput/managing-your-organization/#support","title":"Support","text":"<p>If you are new customer and an Org Admin was not automatically created for your organization, or if you are an existing customer wishing to enjoy Organization settings, contact Coralogix Support.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/managing-your-organization-manage-admins/","title":"Organization Admins","text":"<p>Use the Manage Admins page to view, add, and remove Organization Administrators belonging your organization.</p>"},{"location":"newoutput/managing-your-organization-manage-admins/#overview","title":"Overview","text":"<p>The Manage Admins page contains a list of all of the Organization Administrators (Org Admins) in your organization. It also contains a list of those users who are members of your organization, but not currently administrators.</p> <p>To help manage the organization, the Org Admin may add additional administrators with the same permissions and authorities. They can assign several users to be Org Admins and can unassign users who are currently assigned as Org Admins.</p> <p></p>"},{"location":"newoutput/managing-your-organization-manage-admins/#add-an-organization-administrator","title":"Add an Organization Administrator","text":"<p>STEP 1. Search for the member in the list of organization members.</p> <p>STEP 2. Click the ASSIGN USER icon next to their name.</p>"},{"location":"newoutput/managing-your-organization-manage-admins/#remove-an-organization-administrator","title":"Remove an Organization Administrator","text":"<p>STEP 1. Search for the member in the list of organization admins.</p> <p>STEP 2. Click the UNASSIGN USER icon next to their name.</p>"},{"location":"newoutput/managing-your-organization-manage-admins/#additional-resources","title":"Additional Resources","text":"DocumentationManaging Your OrganizationMy TeamsQuota ManagerOrganization Settings"},{"location":"newoutput/managing-your-organization-manage-admins/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/managing-your-organization-my-teams/","title":"Create & Manage Teams","text":"<p>As Organization Administrator, you can manage the teams in your organization on your My Teams page.</p>"},{"location":"newoutput/managing-your-organization-my-teams/#overview","title":"Overview","text":"<p>The My Teams page allows Organization Administrators (Org Admins) to manage their teams from a single convenient location.</p> <ul> <li> <p>Connect and disconnect teams from your organization.</p> </li> <li> <p>For each team, view the number of members it has, data usage quota, and retention.</p> </li> </ul> <p></p>"},{"location":"newoutput/managing-your-organization-my-teams/#connect-disconnect-existing-teams","title":"Connect &amp; Disconnect Existing Teams","text":"<p>To connect teams that you are a member of, but that are not yet categorized as part of your organization, click on the connect hyperlink symbol.</p> <p>To disconnect any teams that you do not want categorized as part of your organization, click on the disconnect hyperlink symbol.</p> <p></p>"},{"location":"newoutput/managing-your-organization-my-teams/#create-a-new-team","title":"Create a New Team","text":"<p>STEP 1. From the My Teams page, click CREATE NEW TEAM.</p> <p></p> <p>STEP 2. Enter a name for your new team and click NEXT.</p> <p></p> <p>STEP 3. Add units to your new team. Select an origin team to take units from and the amount of units to move to the new team.</p> <p>Note: The retention period of the new team will be identical to that of the team from which it received its units.</p> <p></p> <p>STEP 4. Click NEXT.</p> <p>STEP 5. Enter the email address of the person who should be the team administrator, then click CREATE.</p> <p></p> <p>Once the new team is created, you will be routed back to the My Teams page.</p> <p>An email invitation will be sent to the new team admin, and they will be come the first team admin of the new team, together with the org admin.</p>"},{"location":"newoutput/managing-your-organization-my-teams/#additional-resources","title":"Additional Resources","text":"DocumentationManaging Your OrganizationManage AdminsQuota ManagerOrganization Settings"},{"location":"newoutput/managing-your-organization-my-teams/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/managing-your-organization-organization-settings/","title":"Organization Domains","text":"<p>As Organization Administrator, you may manage your Organization Settings. Determine permissions for new users and which domains may connect to your organization.</p>"},{"location":"newoutput/managing-your-organization-organization-settings/#domain-manager","title":"Domain Manager","text":"<p>Add your organization\u2019s domain/s to automatically connect new users when they sign up for a Coralogix account using their work email account.</p> <p></p>"},{"location":"newoutput/managing-your-organization-organization-settings/#add-a-new-domain","title":"Add a New Domain","text":"<p>STEP 1. Click ADD NEW DOMAIN. You will be asked to verify the new domain.</p> <p>STEP 2. Input an email address in the new domain to which a confirmation email will be sent.</p> <p>STEP 3. Click SAVE.</p> <p>STEP 4. Check the email address which you entered for the confirmation email and click the link in the email.</p> <p>Notes:</p> <ul> <li> <p>The default domain is the domain of the email that the Organization Administrator used to create the organization.</p> </li> <li> <p>Until the email address has been confirmed, the domain will remain in a pending state and will be greyed out.</p> </li> <li> <p>Once the email address has been confirmed, the domain will become active.</p> </li> </ul>"},{"location":"newoutput/managing-your-organization-organization-settings/#additional-resources","title":"Additional Resources","text":"DocumentationManaging Your OrganizationMy TeamsManage AdminsQuota Manager"},{"location":"newoutput/managing-your-organization-organization-settings/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/managing-your-organization-quota-manager/","title":"Quota Management Across Organizations","text":"<p>Organization Administrators can use the Quota Manager page to view the amount of data consumed by their teams - collectively and individually - over a specific period and move units between them.</p> <p>Coralogix provides the Quota Manager dashboard to enable Org Admins better visibility of their organization\u2019s usage. It can help identify trends and allows the Org Admin to better utilize the organization\u2019s quota, promoting additional savings for our customers. By identifying teams that under or over-utilize their quota, Org Admins can easily shift units and help resolve these challenges.</p>"},{"location":"newoutput/managing-your-organization-quota-manager/#overview","title":"Overview","text":""},{"location":"newoutput/managing-your-organization-quota-manager/#units-usage","title":"Units Usage","text":"<p>The Quota Manager page displays the amount of data (either in Units or GB Sent) consumed by the teams in your organization over a specific time period (either 30 days, 90 days, or in the past calendar month, depending on which setting you select).</p> <ul> <li> <p>View units usage or GB sent by your organization collectively and by individual teams.</p> </li> <li> <p>View the number of units that make up your organization\u2019s total quota, how many teams are in your organization, and how many quota blocks occurred in the specified period.</p> </li> <li> <p>You can select whether to group the information by pillar (logs, traces and metrics), priority (high, medium and low), or none (which shows daily usage, high daily usage, and quota blocks).</p> </li> <li> <p>You can select one or more teams from the Team dropdown, to view only the usage details of those specific teams.</p> </li> <li> <p>In the Table view, you can also export the data for the selected teams to a CSV file using the Export to CSV button.</p> </li> </ul> <p></p>"},{"location":"newoutput/managing-your-organization-quota-manager/#team-overview","title":"Team Overview","text":"<p>View specific team information. The Team Overview section displays information either as a set of graphs or as a table of all the teams with sortable columns. You can sort both graphs and table by team name, quota, average usage, max daily usage, min daily usage, and quota blocks.</p> <p></p> <p>In the graph view, the usage each day is shown based on the Group by setting selected at the top of the screen:</p> <ul> <li> <p>Group by None: Shows normal usage, high usage (more than 80% of quota used) and quota blocked.</p> </li> <li> <p>Group by Pillar: Shows the 7-day average, as well as how many units or GB of logs, metrics and traces were sent.</p> </li> <li> <p>Group by Priority: Shows the 7-day average, as well as how many units or GB (depending on the view) were sent to high, medium and low priority.</p> </li> </ul> <p>A tooltip appears when you move the mouse over each day, showing the current quota, the average quota for the last 7 days, the quota used on that specific day, and the number of blocks that occurred that day.</p> <p>Note: It is possible for a team to get blocked multiple times in a single day for various reasons.</p> <p></p> <p>Additionally in the graph view, when viewing by Units, there is a thick horizontal quota line which shows what the team\u2019s quota was on any given day. This line changes when the quota is changed. This makes it easy to see when a team has exceeded their quota or if a team is always well under their daily quota. This can help when deciding to move units between teams to ensure efficient data usage.</p>"},{"location":"newoutput/managing-your-organization-quota-manager/#unit-transfer","title":"Unit Transfer","text":"<p>Move units between teams to ensure efficient data usage.</p> <p>Notes:</p> <ul> <li>Units may only be transferred between teams with the same retention periods.</li> </ul>"},{"location":"newoutput/managing-your-organization-quota-manager/#move-units-between-teams","title":"Move Units Between Teams","text":"<p>If the Org Admin decided to balance or shift units between the teams based on the dashboard insights, this can easily be done by using the Move Units option.</p> <p>STEP 1. Click MOVE UNITS in the right-hand side of the Team Overview section.</p> <p>STEP 2. Select the Origin Team (the team from which to move the units).</p> <p>STEP 3. Enter an amount of units to send.</p> <p>STEP 4. Select the Target Team (the team to which the units should be transferred).</p> <p></p> <p>Step 5. Click CALCULATE.</p> <p>A simple overview is displayed, helping the user understand the previous state of both teams and the proposed new state, prior to executing the move.</p> <p></p> <p>STEP 6. Click CONFIRM to complete the move.</p>"},{"location":"newoutput/managing-your-organization-quota-manager/#additional-resources","title":"Additional Resources","text":"DocumentationManaging Your OrganizationMy TeamsManage AdminsOrganization Settings"},{"location":"newoutput/managing-your-organization-quota-manager/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/metric-alerts-lucene/","title":"Metric Alerts using Lucene","text":"<p>Metric alerts allow Coralogix users to be notified directly to any email/emails or custom webhook on their metric data. Metrics can be sent to Coralogix in various ways (Prometheus,\u00a0MetricBeat,\u00a0Cloudwatch, and more) and be used for different purposes such as:</p> <ul> <li> <p>CPU Monitoring.which process is using more cpu.</p> </li> <li> <p>Memory usage.Is the instance configured with the right amount of memory to handle the load, etc.</p> </li> <li> <p>Backend frontend response time.How much time does it take the instance to respond or process a query.</p> </li> </ul> <p>Due to its simplicity and easiness of use Lucene is one of the languages that is used for Metric alerts.</p> <p>Metric Alerts can be created based on Prometheus metrics and metrics generated from log data using\u00a0Logs2Metrics.</p>"},{"location":"newoutput/metric-alerts-lucene/#create-alert","title":"Create Alert","text":"<p>1-To create a\u00a0Metric Alert, just navigate to\u00a0Alerts\u00a0Tab. Click\u00a0New Alert, Give your Alert a\u00a0name,\u00a0description, and\u00a0severity.</p> <p></p> <p>2. Choose\u00a0Metric Alert.</p> <p></p> <p>3. Choose the\u00a0Lucene\u00a0tab and add the query that you would like to trigger the alert on.</p> <p></p> <p>4.\u00a0Condition\u00a0\u2013 In this section you choose your alert condition.</p> <p>You can use aggregation by anything that you like from your logs. For instance, the\u00a0app\u00a0name,\u00a0subsystem, machine\u00a0id, or anything of your preference.</p> <p></p> <p>In this Alert here we are trying to be alerted when the Avg transaction time is more than 60 ms for more than 50% of 30 minutes Interval.</p> <p>After an alert is triggered, it will send a\u00a0webhook\u00a0or email or both to your defined destination, and appear under the\u00a0Insights\u00a0Tab \u2013 the name of the alert, the query you used, the graph to represent the alert, and the aggregation you have chosen.</p> <p>If you need assistance configuring this Alert please do not hesitate to contact Corlaogix support through our in-app chat or by sending us an email to\u00a0Support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/metric-alerts-promql/","title":"Metric Alerts","text":"<p>As part of Coralogix Alerting, metric alerts serve as your frontline defense, ensuring your systems and applications' uninterrupted performance, reliability, and security.</p>"},{"location":"newoutput/metric-alerts-promql/#overview","title":"Overview","text":"<p>Metric alerts are notifications triggered by predefined thresholds being met or exceeded for specific metrics in your Coralogix dashboard.</p> <p>Metric alerts are meticulously designed to monitor critical performance indicators surrounding infrastructure and other metrics. When specific thresholds or conditions are breached, these alerts act as our early warning system, instantly notifying our teams of potential issues requiring immediate attention. For instance, they proactively monitor server CPU utilization, response times, error rates, and resource utilization in cloud environments.</p> <p>Create PromQL alerts for standard metrics, such as Prometheus or\u00a0Cloudwatch metrics, or metrics hidden within your logs using\u00a0Events2Metrics.</p>"},{"location":"newoutput/metric-alerts-promql/#prerequisites","title":"Prerequisites","text":"<ul> <li>Metrics sent to Coralogix</li> </ul>"},{"location":"newoutput/metric-alerts-promql/#define-the-promql-query","title":"Define the PromQL Query","text":"<p>STEP 1. In your Coralogix toolbar, go to Alerts &gt; Alert Management. Click\u00a0ADD NEW ALERT.</p>"},{"location":"newoutput/metric-alerts-promql/#details","title":"Details","text":"<p>STEP 2. Set alert details: Name, Description, and Severity.</p> <p></p>"},{"location":"newoutput/metric-alerts-promql/#alert-type","title":"Alert Type","text":"<p>STEP 3. Select\u00a0Alert Type.</p> <p></p>"},{"location":"newoutput/metric-alerts-promql/#query","title":"Query","text":"<p>STEP 4. Add the\u00a0PromQL\u00a0query that you would like to trigger the alert.</p> <p></p> <p>Notes:</p> <ul> <li> <p>As you type your PromQL query, you will immediately get auto-complete suggestions.</p> </li> <li> <p>Aggregate using the value of your choice: app\u00a0name,\u00a0subsystem, machine\u00a0id, or otherwise. For instance, you might want to track a total exception count with a single application-wide metric and add metric labels to represent new code areas. If the exception counter was called <code>application_error_count</code> and it covered code area <code>x</code>, you can tack on a corresponding metric label.</p> </li> </ul> <pre><code>application_error_count{area=\"x\"}\n\n</code></pre> <ul> <li>Use the <code>by</code> aggregation operator to choose which dimensions (metric labels) to aggregate along and how to split your [alert notification groups]. For instance, the query\u00a0<code>sum by(instance)\u00a0(node_filesystem_size_bytes)</code>\u00a0returns the total <code>node_filesystem_size_bytes</code> for each instance.</li> </ul>"},{"location":"newoutput/metric-alerts-promql/#conditions","title":"Conditions","text":"<p>STEP 5. Define the conditions for which your alert will be triggered.</p> <p></p>"},{"location":"newoutput/metric-alerts-promql/#basic-conditions","title":"Basic Conditions","text":"<ul> <li> <p>Choose if your alert will be triggered if it is more or less than a certain value, or more than usual for a minimum threshold. When the query passes the value or minimum threshold in accordance with the conditions set, an alert will be triggered.</p> </li> <li> <p>Selecting the more-than-usual condition will trigger an alert when the number of matches is higher than normal and above a minimum threshold. Find out more here.</p> </li> <li> <p>Enter a percentage (for over x %) and timeframe (of the last x minutes). This determines how much of the timeframe you want to cross the threshold for the alert to trigger.</p> </li> <li> <p>Select the percentage (at least x %) of the timeframe that needs values for the alert to trigger.</p> </li> </ul> <p>Example:</p> <ul> <li>I determine that over 50% of my 10-minute timeframe needs to have the set value for the alert to trigger. If I reach the value for 5 out of the 10 data points, it will be not enough to trigger an alert, as it is not over 50%. If I reach the value for 6 out of the 10 data points, an alert will be triggered, as it is over 50%.</li> </ul>"},{"location":"newoutput/metric-alerts-promql/#percentage-values","title":"Percentage Values","text":"<ul> <li> <p>The percentage values setting is designed to disable the alert when there are not enough data points to consider the alert reliable. When the amount of data is under the set percentage, the alert will not trigger, regardless of the actual metric value and whether it is over or under a threshold.</p> </li> <li> <p>If the percentage is set to 0 and the query crosses the threshold once, an alert is triggered.</p> </li> <li> <p>If the percentage is set to 100, this means all of the time window values should cross the threshold. If at any point a value does not, an alert is triggered.</p> </li> <li> <p>This setting disappears when checking replace missing values with zeros, as it becomes irrelevant. Once missing values are replaced with zero, then there is a guarantee that 100% of the data exists.</p> </li> </ul>"},{"location":"newoutput/metric-alerts-promql/#replace-missing-values-with-0","title":"Replace Missing Values With 0","text":"<p>You have the option of replacing missing values with 0.</p> <p>Why replace missing values with 0? Sometimes data may have missing values as seen in the graph below. When you don\u2019t replace missing values with zero and leave them empty, the data you do have is considered to be 100% of the data.</p> <p>Let\u2019s say that you query for a time frame of 10 minutes. 6 data points have values and 4 data points have no values. If you haven\u2019t replaced the missing values with 0, the 6 minutes with values will be considered 100% of the timeframe. This can lead to false triggers.</p> <p></p>"},{"location":"newoutput/metric-alerts-promql/#manage-undetected-values","title":"Manage Undetected Values","text":"<p>If you are using the Less than condition, you will have the option to manage undetected values.</p> <p></p> <p>Undetected values occur when a permutation of a Less than alert stops being sent, causing multiple triggers of the alert (for every timeframe in which it was not sent).</p> <p>When you view an alert with undetected values, you have the option to retire these values manually, or select a time period after which undetected values will automatically be retired. You can also disable triggering on undetected values to immediately stop sending alerts when an undetected value occurs.</p>"},{"location":"newoutput/metric-alerts-promql/#preview-the-queried-metric","title":"Preview the Queried Metric","text":"<p>STEP 6. Expand Preview Alert to preview the queried metric and defined threshold over the past 24 hours. The preview is limited to a maximum of 100 time series.</p> <p></p>"},{"location":"newoutput/metric-alerts-promql/#notifications","title":"Notifications","text":"<p>STEP 7. Define Notification settings.</p> <p>In the notification settings, you have different options, depending on whether or not you are using the\u00a0Group By\u00a0condition.</p>"},{"location":"newoutput/metric-alerts-promql/#using-group-by","title":"Using Group By","text":"<p>When using\u00a0Group By\u00a0conditions, you will see the following options:</p> <ul> <li> <p>Trigger a single alert when at least one combination of\u00a0the group by values meets the condition. A single notification, aggregating all values matching an alert query and conditions, will be sent to your Coralogix Incidents screen.</p> </li> <li> <p>Trigger a separate alert for each combination that meets the condition. Multiple individual notifications for each Group By field value may be sent to your Coralogix Incidents Screen when query conditions are met. Select one or more keys \u2013 consisting of a subset of the fields selected in the alert conditions \u2013 in the drop-down menu. A separate notification will be sent for each key selected.</p> </li> <li> <p>The number of\u00a0Group By\u00a0permutations is limited to 1000. If there are more permutations, then only the first 1000 are tracked.</p> </li> </ul>"},{"location":"newoutput/metric-alerts-promql/#not-using-group-by","title":"Not Using Group By","text":"<p>When not using the\u00a0Group By\u00a0condition,\u00a0a single alert will be triggered\u00a0and sent to your\u00a0Incidents Screen\u00a0when the query meets the condition.</p> <p>You can define additional alert recipient(s) and notification channels in both cases by clicking\u00a0+ ADD WEBHOOK. Once you add a webhook, you can choose the parameters of your notification:</p> <ul> <li> <p>Notify Every. Sets the alert cadence. After an alert is triggered and a notification is sent, the alert will continue to work, but notifications will be suppressed for the duration of the suppression period.</p> </li> <li> <p>Notify When Resolved. Activate to receive an automatic update once an alert has ceased.</p> </li> </ul> <p>Notes:</p> <ul> <li> <p>Input Group By labels here as free text.</p> </li> <li> <p>The number of Group By permutations is limited to 1000. If there are more permutations, then only the first 1000 are tracked.</p> </li> </ul>"},{"location":"newoutput/metric-alerts-promql/#notification-parameters","title":"Notification Parameters","text":"<p>Both notification types allow you to choose the parameters of your notification.</p> <p>STEP 1. Notify Every. Sets the alert cadence. After an alert is triggered and a notification is sent, the alert will continue to work, but notifications will be suppressed for the duration of the suppression period.</p> <p>STEP 2. Notify when resolved. Activate to receive an automatic update once an alert has ceased.</p> <p>STEP 3. Define additional alert recipient(s) and notification channels by clicking + ADD WEBHOOK.</p>"},{"location":"newoutput/metric-alerts-promql/#view-your-triggered-alerts","title":"View Your Triggered Alerts","text":"<p>STEP 8. View your triggered alerts.</p>"},{"location":"newoutput/metric-alerts-promql/#incidents-screen","title":"Incidents Screen","text":"<p>Our\u00a0Incidents Screen\u00a0displays all of your triggered alert events within the Coralogix platform. View all those events that are currently triggered or those triggered within a specific time frame. With easy-to-use functionalities and the ability to drill down into events of interest, the feature ensures top-notch monitoring and analysis. Find out more here.</p>"},{"location":"newoutput/metric-alerts-promql/#alerts-map","title":"Alerts Map","text":"<p>Alerts Map\u00a0presents users with a visual representation of each alert status in real time. Grouping all of your alerts in a scalable, information-dense manner, this feature ensures optimal system monitoring. To access the\u00a0Alerts Map\u00a0feature, navigate to\u00a0Alerts &gt; Alert Map. \u00a0Find out more here.</p> <p></p>"},{"location":"newoutput/metric-alerts-promql/#faqs","title":"FAQs","text":""},{"location":"newoutput/metric-alerts-promql/#once-i-set-up-an-alert-how-long-will-it-take-to-activate-it","title":"Once I set up an alert, how long will it take to activate it?","text":"<p>When creating an alert, it takes 15 minutes before it triggers. In most cases, it will be faster.</p>"},{"location":"newoutput/metric-alerts-promql/#how-does-coralogix-define-step-intervals","title":"How does Coralogix define step intervals?","text":"<p>For timeframes up to 30 minutes, we define steps every 1 minute.</p> <p>For timeframes up to 12 hours, we define steps every 5 minutes.</p> <p>For timeframes over 12 hours, we define steps every 10 minutes.</p>"},{"location":"newoutput/metric-alerts-promql/#why-might-i-be-missing-values-in-my-query","title":"Why might I be missing values in my query?","text":"<p>This could result from late coming data, a lag of ingestion in a system when the alert triggered.</p>"},{"location":"newoutput/metric-alerts-promql/#how-can-i-avoid-false-triggers-due-to-missing-values","title":"How can I avoid false triggers due to missing values?","text":"<p>Sometimes data may have missing values as seen in the graph below. When you don\u2019t replace missing values with zero and leave them empty, the data you do have is considered to be 100% of the data.</p> <p>Let\u2019s say that you query for a time frame of 10 minutes. 6 data points have values and 4 data points have no values. If you haven\u2019t replaced the missing values with 0, the 6 minutes with values will be considered 100% of the timeframe. This can lead to false triggers. To avoid this, either replace missing values with 0 or set that at least 100% of the timeframe needs to have values for the alert to trigger. In the latter case, if certain points don\u2019t exist, an alert will not be triggered.</p>"},{"location":"newoutput/metric-alerts-promql/#how-can-i-avoid-false-triggers-as-a-result-of-missing-values","title":"How can I avoid false triggers as a result of missing values?","text":"<p>Sometimes data may have missing values, as seen in the graph below. When you don\u2019t replace missing values with zero and leave them empty, the data you do have is considered to be 100% of the data.</p> <p>Let\u2019s say that you query for a time frame of 10 minutes. 6 data points have values and 4 data points have no values. If you haven\u2019t replaced the missing values with 0, the 6 minutes with values will be considered 100% of the timeframe, leading to false triggers.</p> <p>To avoid this, either replace missing values with zero or set that at least 100% percent of the timeframe needs to have values for this alert to trigger.</p>"},{"location":"newoutput/metric-alerts-promql/#how-do-i-begin-debugging-an-alert","title":"How do I begin debugging an alert?","text":"<p>We strongly recommend viewing your metric in Grafana or your Custom Dashboard in real-time. By viewing the metric, you can see if there has been a lag in ingestion or sending.</p>"},{"location":"newoutput/metric-alerts-promql/#what-if-i-have-a-metric-where-zero-is-a-valid-value-in-a-timeframe-how-is-null-evaluated-in-that-scenario","title":"What if I have a metric where zero is a valid value in a timeframe? How is null evaluated in that scenario?","text":"<p>Our suggestion is to use PromQL function to return a value.</p>"},{"location":"newoutput/metric-alerts-promql/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/metrics-api/","title":"Metrics API","text":"<p>Coralogix provides a Metrics API that lets you query your hosted metrics easily.</p>"},{"location":"newoutput/metrics-api/#query-your-metrics","title":"Query Your Metrics","text":"<p>To use the Metrics API to query your metrics, first locate your:</p> <ul> <li> <p>Coralogix Logs Query Key. In your Coralogix toolbar, navigate to Data Flow &gt; API Keys.</p> </li> <li> <p>Coralogix domain</p> </li> </ul> <p>Each HTTP request should add The Logs Query Key as a Coralogix token. Your Coralogix domain will be used to construct the Metrics API endpoint specific to your account.</p> <p>The API request should contain the following:</p> <ul> <li> <p>Headers:</p> <ul> <li> <p>'Authorization: Bearer ' <li> <p>\u2018Content-type: application/json\u2018</p> </li> <li> <p>URL: Select the URL associated with your Coralogix domain.</p> </li> DomainURLcoralogix.uscx498.coralogixstg.wpengine.comcoralogixstg.wpengine.comeu2.coralogixstg.wpengine.comcoralogixsg.comhttps://ng-api-http.&lt;domain&gt;/metrics/api/v1coralogix.inhttps://ng-api-http.app.&lt;domain&gt;/metrics/api/v1"},{"location":"newoutput/metrics-api/#examples","title":"Examples","text":"<p>Be sure to update your domain and Log Query Key when using the following examples.</p>"},{"location":"newoutput/metrics-api/#server-instant-metric-query","title":"Server Instant Metric Query","text":"<p>Evaluates a server's instant metric at a specific moment in time.</p> <pre><code>curl --location --request GET '/api/v1/query?query=&lt;metric_name&gt;' \\\\\n--header 'Authorization: Bearer &lt;TOKEN&gt;' \\\\\n--header 'Content-Type: application/json' \\\\\n--data-raw ''\n</code></pre>"},{"location":"newoutput/metrics-api/#instant-metric-query","title":"Instant Metric Query","text":"<p>Evaluates an instant metric at a defined single point in time.</p> <pre><code>curl --location --request GET 'https://ng-api-http.app.&lt;domain&gt;/metrics/api/v1/query?query=&lt;metric_name&gt;&amp;time=2022-10-26T12:10:51.781Z' \\\\\n--header 'Authorization: Bearer &lt;TOKEN&gt;' \\\\\n--header 'Content-Type: application/json' \\\\\n--data-raw ''\n\n</code></pre>"},{"location":"newoutput/metrics-api/#range-query","title":"Range Query","text":"<p>Evaluates an instant metric over a range of time.</p> <pre><code>curl --location --request GET 'https://ng-api-http.app.&lt;domain&gt;/metrics/api/v1/query_range?query=&lt;metric_name&gt;&amp;start=2022-10-26T10:10:51.781Z&amp;end=2022-10-26T12:10:51.781Z' \\\\\n--header 'Authorization: Bearer &lt;TOKEN&gt;' \\\\\n--header 'Content-Type: application/json' \\\\\n--data-raw ''\n\n</code></pre>"},{"location":"newoutput/metrics-api/#additional-resources","title":"Additional Resources","text":"DocumentationPrometheus API Documentation"},{"location":"newoutput/metrics-api/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/metrics-cardinality/","title":"Metrics Cardinality","text":"<p>With Coralogix you can get your metrics cardinality (i.e. the number of elements in a set, or grouping, of metrics as defined by query criteria). Our API returns cardinality statistics by dates* in 4 categories:  </p> <p>1. seriesCountByMetricName: Provides a list of metrics names and their series count.</p> <p>2. seriesCountByLabelName: Provides a list of label names and their series count.</p> <p>3. seriesCountByLabelValuePair: Provides a list of label-value pairs and their series count.</p> <p>4. labelValueCountByLabelName: Provides a list of label names and their distinct value counts.  </p> <p>* Dates are in UTC.</p> <p> URL: Select the Metrics Cardinality API endpoint associated with your Coralogix domain.</p> <p>Method: GET</p> <p>Authorization: Use the \u201cAlerts, Rules and Tags API Key\u201d in the authorization header.</p> <p>Query parameters:</p> <ul> <li> <p>topN: Number of highest cardinality metrics to display in each category.</p> <ul> <li> <p>It should be higher than 0.</p> </li> <li> <p>10 by default.</p> </li> </ul> </li> <li> <p>fromDate: The oldest date that will be included into the statistics response.</p> <ul> <li> <p>It can\u2019t be in the future.</p> </li> <li> <p>It can\u2019t be after toDate.</p> </li> <li> <p>today by default.  </p> </li> </ul> </li> <li> <p>toDate: The most recent date that will be included into the statistics response.</p> <ul> <li> <p>It can\u2019t be in the future.</p> </li> <li> <p>It can\u2019t be before fromDate.</p> </li> <li> <p>today by default.</p> </li> </ul> </li> </ul> <p>Defaults will be used if all query parameters are omitted.</p> <p>Example #1: topN, fromDate, and toDate.</p> <pre><code>curl 'https://ng-api-http.coralogixstg.wpengine.com/api/metrics/cardinality?topN=2&amp;fromDate=2022-12-01&amp;toDate=2022-12-02' -H 'authorization: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n</code></pre> <p>Output:</p> <pre><code>[{\"date\":\"2022-12-02\",\"data\":{\"totalSeries\":120344,\"totalLabelValuePairs\":1603533,\"seriesCountByMetricName\":[{\"name\":\"apiserver_request_duration_seconds_bucket\",\"value\":31929},{\"name\":\"etcd_request_duration_seconds_bucket\",\"value\":7874}],\"seriesCountByLabelName\":[{\"name\":\"cluster\",\"value\":121290},{\"name\":\"job\",\"value\":90650}],\"seriesCountByLabelValuePair\":[{\"name\":\"cluster=onlineboutique\",\"value\":121290},{\"name\":\"job=kubernetes-apiservers\",\"value\":72421}],\"labelValueCountByLabelName\":[{\"name\":\"__name__\",\"value\":711},{\"name\":\"le\",\"value\":159}]}},{\"date\":\"2022-12-01\",\"data\":{\"totalSeries\":325782,\"totalLabelValuePairs\":5101308,\"seriesCountByMetricName\":[{\"name\":\"apiserver_request_duration_seconds_bucket\",\"value\":105285},{\"name\":\"etcd_request_duration_seconds_bucket\",\"value\":26405}],\"seriesCountByLabelName\":[{\"name\":\"cluster\",\"value\":405643},{\"name\":\"job\",\"value\":404865}],\"seriesCountByLabelValuePair\":[{\"name\":\"cluster=onlineboutique\",\"value\":405643},{\"name\":\"job=kubernetes-apiservers\",\"value\":239969}],\"labelValueCountByLabelName\":[{\"name\":\"__name__\",\"value\":712},{\"name\":\"name\",\"value\":200}]}}]\n</code></pre> <p>Above results in beautified JSON format:</p> <pre><code>[{\n    \"date\": \"2022-12-02\",\n    \"data\": {\n        \"totalSeries\": 120344,\n        \"totalLabelValuePairs\": 1603533,\n        \"seriesCountByMetricName\": [{\n            \"name\": \"apiserver_request_duration_seconds_bucket\",\n            \"value\": 31929\n        }, {\n            \"name\": \"etcd_request_duration_seconds_bucket\",\n            \"value\": 7874\n        }],\n        \"seriesCountByLabelName\": [{\n            \"name\": \"cluster\",\n            \"value\": 121290\n        }, {\n            \"name\": \"job\",\n            \"value\": 90650\n        }],\n        \"seriesCountByLabelValuePair\": [{\n            \"name\": \"cluster=onlineboutique\",\n            \"value\": 121290\n        }, {\n            \"name\": \"job=kubernetes-apiservers\",\n            \"value\": 72421\n        }],\n        \"labelValueCountByLabelName\": [{\n            \"name\": \"__name__\",\n            \"value\": 711\n        }, {\n            \"name\": \"le\",\n            \"value\": 159\n        }]\n    }\n}, {\n    \"date\": \"2022-12-01\",\n    \"data\": {\n        \"totalSeries\": 325782,\n        \"totalLabelValuePairs\": 5101308,\n        \"seriesCountByMetricName\": [{\n            \"name\": \"apiserver_request_duration_seconds_bucket\",\n            \"value\": 105285\n        }, {\n            \"name\": \"etcd_request_duration_seconds_bucket\",\n            \"value\": 26405\n        }],\n        \"seriesCountByLabelName\": [{\n            \"name\": \"cluster\",\n            \"value\": 405643\n        }, {\n            \"name\": \"job\",\n            \"value\": 404865\n        }],\n        \"seriesCountByLabelValuePair\": [{\n            \"name\": \"cluster=onlineboutique\",\n            \"value\": 405643\n        }, {\n            \"name\": \"job=kubernetes-apiservers\",\n            \"value\": 239969\n        }],\n        \"labelValueCountByLabelName\": [{\n            \"name\": \"__name__\",\n            \"value\": 712\n        }, {\n            \"name\": \"name\",\n            \"value\": 200\n        }]\n    }\n}]\n</code></pre> <p>Example #2: all query parameters omitted.</p> <pre><code>curl 'https://ng-api-http.coralogixstg.wpengine.com/api/metrics/cardinality?topN=&amp;fromDate=&amp;toDate=' -H 'authorization: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\n</code></pre> <p>Output:</p> <pre><code>[{\"date\":\"2022-12-02\",\"data\":{\"totalSeries\":121067,\"totalLabelValuePairs\":1612878,\"seriesCountByMetricName\":[{\"name\":\"apiserver_request_duration_seconds_bucket\",\"value\":32119},{\"name\":\"etcd_request_duration_seconds_bucket\",\"value\":7912},{\"name\":\"apiserver_response_sizes_bucket\",\"value\":5365},{\"name\":\"rest_client_request_duration_seconds_bucket\",\"value\":4943},{\"name\":\"apiserver_storage_list_duration_seconds_bucket\",\"value\":2758},{\"name\":\"apiserver_watch_events_sizes_bucket\",\"value\":2144},{\"name\":\"storage_operation_duration_seconds_bucket\",\"value\":2134},{\"name\":\"kubelet_runtime_operations_duration_seconds_bucket\",\"value\":1842},{\"name\":\"container_tasks_state\",\"value\":1449},{\"name\":\"container_blkio_device_usage_total\",\"value\":911}],\"seriesCountByLabelName\":[{\"name\":\"cluster\",\"value\":122013},{\"name\":\"job\",\"value\":121984},{\"name\":\"instance\",\"value\":121924},{\"name\":\"__name__\",\"value\":121067},{\"name\":\"le\",\"value\":71430},{\"name\":\"verb\",\"value\":47811},{\"name\":\"component\",\"value\":46996},{\"name\":\"version\",\"value\":44351},{\"name\":\"resource\",\"value\":42355},{\"name\":\"scope\",\"value\":42311}],\"seriesCountByLabelValuePair\":[{\"name\":\"cluster=onlineboutique\",\"value\":122013},{\"name\":\"job=kubernetes-apiservers\",\"value\":72862},{\"name\":\"instance=172.31.6.22:443\",\"value\":42526},{\"name\":\"component=apiserver\",\"value\":41181},{\"name\":\"version=v1\",\"value\":37223},{\"name\":\"beta_kubernetes_io_arch=amd64\",\"value\":28538},{\"name\":\"eks_amazonaws_com_capacityType=ON_DEMAND\",\"value\":24434},{\"name\":\"beta_kubernetes_io_os=linux\",\"value\":24410},{\"name\":\"eks_amazonaws_com_nodegroup=chen-eks2-nodes\",\"value\":24306},{\"name\":\"beta_kubernetes_io_instance_type=t3.large\",\"value\":20464}],\"labelValueCountByLabelName\":[{\"name\":\"__name__\",\"value\":711},{\"name\":\"le\",\"value\":159},{\"name\":\"type\",\"value\":147},{\"name\":\"name\",\"value\":146},{\"name\":\"resource\",\"value\":130},{\"name\":\"id\",\"value\":116},{\"name\":\"device\",\"value\":87},{\"name\":\"url\",\"value\":79},{\"name\":\"replicaset\",\"value\":77},{\"name\":\"kind\",\"value\":57}]}}]\n</code></pre> <p>Above results in beautified JSON format:</p> <pre><code>[{\n    \"date\": \"2022-12-02\",\n    \"data\": {\n        \"totalSeries\": 121067,\n        \"totalLabelValuePairs\": 1612878,\n        \"seriesCountByMetricName\": [{\n            \"name\": \"apiserver_request_duration_seconds_bucket\",\n            \"value\": 32119\n        }, {\n            \"name\": \"etcd_request_duration_seconds_bucket\",\n            \"value\": 7912\n        }, {\n            \"name\": \"apiserver_response_sizes_bucket\",\n            \"value\": 5365\n        }, {\n            \"name\": \"rest_client_request_duration_seconds_bucket\",\n            \"value\": 4943\n        }, {\n            \"name\": \"apiserver_storage_list_duration_seconds_bucket\",\n            \"value\": 2758\n        }, {\n            \"name\": \"apiserver_watch_events_sizes_bucket\",\n            \"value\": 2144\n        }, {\n            \"name\": \"storage_operation_duration_seconds_bucket\",\n            \"value\": 2134\n        }, {\n            \"name\": \"kubelet_runtime_operations_duration_seconds_bucket\",\n            \"value\": 1842\n        }, {\n            \"name\": \"container_tasks_state\",\n            \"value\": 1449\n        }, {\n            \"name\": \"container_blkio_device_usage_total\",\n            \"value\": 911\n        }],\n        \"seriesCountByLabelName\": [{\n            \"name\": \"cluster\",\n            \"value\": 122013\n        }, {\n            \"name\": \"job\",\n            \"value\": 121984\n        }, {\n            \"name\": \"instance\",\n            \"value\": 121924\n        }, {\n            \"name\": \"__name__\",\n            \"value\": 121067\n        }, {\n            \"name\": \"le\",\n            \"value\": 71430\n        }, {\n            \"name\": \"verb\",\n            \"value\": 47811\n        }, {\n            \"name\": \"component\",\n            \"value\": 46996\n        }, {\n            \"name\": \"version\",\n            \"value\": 44351\n        }, {\n            \"name\": \"resource\",\n            \"value\": 42355\n        }, {\n            \"name\": \"scope\",\n            \"value\": 42311\n        }],\n        \"seriesCountByLabelValuePair\": [{\n            \"name\": \"cluster=onlineboutique\",\n            \"value\": 122013\n        }, {\n            \"name\": \"job=kubernetes-apiservers\",\n            \"value\": 72862\n        }, {\n            \"name\": \"instance=172.31.6.22:443\",\n            \"value\": 42526\n        }, {\n            \"name\": \"component=apiserver\",\n            \"value\": 41181\n        }, {\n            \"name\": \"version=v1\",\n            \"value\": 37223\n        }, {\n            \"name\": \"beta_kubernetes_io_arch=amd64\",\n            \"value\": 28538\n        }, {\n            \"name\": \"eks_amazonaws_com_capacityType=ON_DEMAND\",\n            \"value\": 24434\n        }, {\n            \"name\": \"beta_kubernetes_io_os=linux\",\n            \"value\": 24410\n        }, {\n            \"name\": \"eks_amazonaws_com_nodegroup=chen-eks2-nodes\",\n            \"value\": 24306\n        }, {\n            \"name\": \"beta_kubernetes_io_instance_type=t3.large\",\n            \"value\": 20464\n        }],\n        \"labelValueCountByLabelName\": [{\n            \"name\": \"__name__\",\n            \"value\": 711\n        }, {\n            \"name\": \"le\",\n            \"value\": 159\n        }, {\n            \"name\": \"type\",\n            \"value\": 147\n        }, {\n            \"name\": \"name\",\n            \"value\": 146\n        }, {\n            \"name\": \"resource\",\n            \"value\": 130\n        }, {\n            \"name\": \"id\",\n            \"value\": 116\n        }, {\n            \"name\": \"device\",\n            \"value\": 87\n        }, {\n            \"name\": \"url\",\n            \"value\": 79\n        }, {\n            \"name\": \"replicaset\",\n            \"value\": 77\n        }, {\n            \"name\": \"kind\",\n            \"value\": 57\n        }]\n    }\n}]\n</code></pre> <p>Still have questions? If so, please reach out to us via our in-app chat for quick assistance.</p>"},{"location":"newoutput/metrics-usage/","title":"Metrics Usage","text":"<p>Coralogix now offers a Metrics Usage screen, complementing our metrics cardinality feature. It provides an intuitive visual representation of the metrics-related data surrounding metric statistics, which can be retrieved using an API.</p>"},{"location":"newoutput/metrics-usage/#metrics-usage-screen","title":"Metrics Usage Screen","text":"<p>View this data by navigating to Settings &gt; Metrics Usage.</p> <p></p> <p>Select the cardinality statistics of interest from the left-hand sidebar panel. The page will center on that particular section.</p> <p></p> <p>Each cardinality statistics section will have up to 1K entry lines.</p> <p>The % of used column refers to the percentage of what each metric will represent against the totality of all metrics.</p> <p></p> <p>In the search bar, seek any particular metric or label of interest, or input just a partial value and the results will be updated throughout all sections.</p> <p></p> <p>In the time frame menu, select one day of interest at a time, the time frame reference being UTC.</p> <p></p>"},{"location":"newoutput/metrics-usage/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/metrics-with-promql/","title":"Metrics with PromQL","text":"<p>As part of Coralogix's metrics offering, we support the querying of metric data through the most prevalent time-series query language - PromQL. This allows you to use Coralogix as your metrics backend and add it as a Prometheus data source to your Grafana instance.</p> <p>In this tutorial, you will learn how to do this and update existing dashboards without any change to the queries or syntax.</p>"},{"location":"newoutput/metrics-with-promql/#add-prometheus-metrics-to-grafana","title":"Add Prometheus metrics to Grafana","text":"<p>** Available for Grafana version 7 and up.</p> <p>Log in to Grafana and click on the gear icon, and then click add source.</p> <p></p> <ol> <li>Click the blue button ADD a Source.</li> <li>Search for Prometheus.</li> <li>Click select.</li> <li>Choose a name for your data source. You can give it any name you like.</li> <li>In the URL, please enter your end point (Choose your endpoint from the table below).</li> <li>Choose \"Skip TLS Verify\"</li> <li>Name the header: \"token\" and insert to the value your Coralogix Logs Query Key</li> <li>Set Query timeout to 300s</li> <li>Set HTTP Method to POST</li> </ol> <p></p>"},{"location":"newoutput/metrics-with-promql/#available-endpoints","title":"Available endpoints:","text":"<p>[table id=93 /]</p> <p>* For the metrics index the timestamp is \"timestamp\" instead of \"coralogix.timestamp\"</p>"},{"location":"newoutput/metrics-with-promql/#useful-commands-for-grafana-installed-on-docker-container","title":"Useful commands for Grafana installed on docker container","text":"<ul> <li>List all your docker containers</li> </ul> <pre><code>docker ps -a\n</code></pre> <ul> <li>Log in to your Grafana container</li> </ul> <pre><code>sudo docker exec -it \"Grafana container ID\" /bin/bash\n</code></pre> <ul> <li>Restart Grafana</li> </ul> <pre><code>docker restart \"Grafana container ID\"\n</code></pre> <p>Have any questions or need additional help? Reach out to our support team at support@coralogixstg.wpengine.com or via our 24/7 in-app chat!</p>"},{"location":"newoutput/microsoft-azure-functions/","title":"Microsoft Azure Functions","text":"<p>Coralogix provides seamless integration with Microsoft Azure\u00a0Cloud, so you can send your logs from anywhere and parse them according to your needs.</p>"},{"location":"newoutput/microsoft-azure-functions/#azure-resource-manager-deployments","title":"Azure Resource Manager Deployments","text":"<p>Coralogix provides several trigger strategies with any of the following automatic integrations using Azure resource manager (ARM) custom template deployments:</p> <ul> <li> <p>Event Hub</p> </li> <li> <p>Blob Storage</p> </li> <li> <p>Queue Storage</p> </li> <li> <p>Diagnostic Data</p> </li> </ul>"},{"location":"newoutput/microsoft-azure-functions/#optional-configurations","title":"Optional Configurations","text":"<p>In addition, we offer optional configurations for particular use-cases utilizing our Azure deployments. If you require resource monitoring in Azure storage accounts or Event Hubs that cannot be made public, deploy our function apps with virtual network (VNet) support.</p>"},{"location":"newoutput/microsoft-azure-functions/#terraform-modules","title":"Terraform Modules","text":"<p>Using our Terraform modules, you can easily install and manage Coralogix integrations with Azure services as modules in your infrastructure code:</p> <ul> <li> <p>Event Hub</p> </li> <li> <p>Blob Storage via Event Grid</p> </li> <li> <p>Queue Storage</p> </li> <li> <p>Diagnostic Data</p> </li> </ul>"},{"location":"newoutput/microsoft-azure-functions/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/microsoftentraid-logs/","title":"Microsoft Entra ID Logs","text":"<p>Collect Microsoft Entra ID (previously Azure Active Directory) audit, sign-in, and provisioning logs, and submit them to Coralogix for seamless integration.</p>"},{"location":"newoutput/microsoftentraid-logs/#overview","title":"Overview","text":"<p>Sign-in and audit logs comprise the activity logs behind many Microsoft Entra ID reports, which can be used to analyze, monitor, and troubleshoot activity in your tenant. Routing your activity logs to an analysis and monitoring solution provides greater insights into your tenant's health and security.</p> <p>Activity logs help you understand the behavior of users in your organization. There are three types of activity logs in Microsoft Entra ID:</p> <ul> <li> <p>Audit logs\u00a0include the history of every task performed in your tenant.</p> </li> <li> <p>Sign-in logs\u00a0capture the sign-in attempts of your users and client applications.</p> </li> <li> <p>Provisioning logs\u00a0provide information about users provisioned in your tenant through a third-party service.</p> </li> </ul> <p>This tutorial demonstrates how to collect Microsoft Entra ID audit and sign-in logs and submit them to Coralogix. It requires that you configure your Microsoft Entra ID Diagnostic Settings and leverage our Event Hub integration for the collection and submission of those logs to the Coralogix platform.</p>"},{"location":"newoutput/microsoftentraid-logs/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Microsoft Entra ID account with an active subscription [Note: To export sign-in data, you\u2019ll need a P1 or P2 Microsoft Entra ID license.]</p> </li> <li> <p>EventHub Namespace [Note: If your EventHub has restricted public access you will need to enable VNet support using these optional configuration steps.]</p> </li> </ul>"},{"location":"newoutput/microsoftentraid-logs/#audit-and-sign-in-log-export","title":"Audit and Sign-in Log Export","text":"<p>STEP 1. To configure audit and sign-in exports, navigate to your Microsoft Entra ID resource.</p> <p>STEP 2. Under Monitoring, click Diagnostic Settings.</p> <p>STEP 3. Click + Add diagnostic setting.</p> <p>STEP 4. In the Diagnostic Setting window, select your desired Categories and configure the Destination details to submit entries to your existing Event Hub.</p> <p></p>"},{"location":"newoutput/microsoftentraid-logs/#process-event-hub","title":"Process Event Hub","text":"<p>Now that your audit and sign-in log entries are being exported to your Event Hub, you\u2019ll need to deploy the Azure Event Hub integration to collect and submit the messages to the Coralogix platform.</p> <p>Deploy the integration via ARM template or Terraform.</p> <ul> <li> <p>ARM</p> <ul> <li> <p>Azure Event Hub ARM</p> </li> <li> <p>ARM Event Hub Integration Package</p> </li> </ul> </li> <li> <p>Terraform</p> <ul> <li>Azure Event Hub Terraform</li> </ul> </li> </ul>"},{"location":"newoutput/microsoftentraid-logs/#additional-resources","title":"Additional Resources","text":"DocumentationIntroduction to Microsoft Azure"},{"location":"newoutput/microsoftentraid-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/","title":"Migrate From OpenSearch to Coralogix Custom Dashboards","text":"<p>Coralogix Custom Dashboards provides you with the ideal dashboard experience, allowing you to create unlimited, personalized custom dashboards using our five visualizations: Data Table,\u00a0Line Chart,\u00a0Gauge,\u00a0Pie Chart\u00a0and\u00a0Bar Chart. Each visualization - supporting logs, metrics and spans - can be used to define and create a dashboard catered to your specific observability needs.</p> <p>The following tutorial demonstrates how to migrate from OpenSearch to Coralogix Custom Dashboards. Using a series of examples, it illustrates which OpenSearch dashboard parameters correlate with those in Coralogix Custom Dashboards.</p>"},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#data-tables","title":"Data Tables","text":"<p>Migrate a data table from your OpenSearch dashboard to your Coralogix Custom dashboard.</p>"},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#opensearch-dashboard","title":"OpenSearch Dashboard","text":""},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#custom-dashboards","title":"Custom Dashboards","text":""},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#migrate-parameters","title":"Migrate Parameters","text":"<p>For each section (1, 2, 3, or *), migrate the parameters from your OpenSearch dashboard to your Coralogix Custom dashboard.</p> Section OpenSearch Dashboard Parameter Coralogix Custom Dashboard Parameter Notes 1 Aggregation Aggregation 2 Split rows Group-by 3 Lucene query Query bar * --- Data Table: Event or Aggregation Data-table has two options: - Event: log grid - Aggregation: data table <p>For example, transfer the aggregation value in Section 1 of your OpenSearch Dashboard to the aggregation parameter value in Section 1 of your Coralogix Custom dashboard.</p> <p>For additional information on data tables, see the full tutorial on Data Tables in Custom Dashboards.</p>"},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#line-charts","title":"Line Charts","text":"<p>Migrate a line chart from your OpenSearch dashboard to your Coralogix custom dashboard.</p>"},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#opensearch-dashboard_1","title":"OpenSearch Dashboard","text":""},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#custom-dashboards_1","title":"Custom Dashboards","text":""},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#migrate-parameters_1","title":"Migrate Parameters","text":"<p>For each section (1, 2, or 3), migrate the parameters from your OpenSearch dashboard to your Coralogix Custom dashboard.</p> Section OpenSearch Dashboard Parameter Coralogix Custom Dashboard Parameter 1 Y-axis count Aggregation 2 Split-series Group by 2 Size Series per query 3 Lucene query Query bar <p>For example, transfer the Y-axis count values in Section 1 of your OpenSearch Dashboard to the aggregation parameter value in Section 1 of your Coralogix Custom dashboard.</p> <p>For additional information on line charts, see the full tutorial on Line Charts in Custom Dashboards.</p>"},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#pie-charts","title":"Pie Charts","text":"<p>Migrate a pie chart from your OpenSearch dashboard to your Coralogix Custom dashboard.</p>"},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#opensearch-dashboard_2","title":"OpenSearch Dashboard","text":""},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#custom-dashboards_2","title":"Custom Dashboards","text":""},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#migrate-parameters_2","title":"Migrate Parameters","text":"<p>For each section (1, 2, or 3), migrate the parameters from your OpenSearch dashboard to your Coralogix Custom dashboard.</p> Section OpenSearch Dashboard Parameter Coralogix Custom Dashboard Parameter 1 Slice size Aggregation 2 Split-series field Group by 2 Split-series (second level) Stack by 2 Size Max slices in chart (under Advanced) 3 Lucene query Query bar <p>For example, transfer the slice size value in Section 1 of your OpenSearch dashboard to the aggregation parameter value in Section 1 of your Coralogix Custom dashboard.</p> <p>For additional information on pie charts, see the full tutorial on Pie Charts in Custom Dashboards.</p>"},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#bar-charts","title":"Bar Charts","text":"<p>Migrate a bar chart from your OpenSearch Dashboard to your Coralogix Custom dashboard.</p>"},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#opensearch-dashboard_3","title":"OpenSearch Dashboard","text":""},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#custom-dashboards_3","title":"Custom Dashboards","text":""},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#migrate-parameters_3","title":"Migrate Parameters","text":"<p>For each section (1, 2, or 3), migrate the parameters from your OpenSearch dashboard to your Coralogix Custom dashboard.</p> Section OpenSearch Dashboard Parameter Coralogix Custom Dashboard Parameter Notes 1 Y-axis count Aggregation 2 Split-series field Group by If you apply the X-axis in OpenSearch Dashboard with a date histogram, you should choose X-axis as Time in Custom Dashboards (above the Group by field). 2 Size Max bars per chart (under Advanced) 2 Split-series (second level) Stack by 3 Lucene query Query bar <p>For example, transfer the Y-axis Count value in Section 1 of your OpenSearch dashboard to the aggregation parameter value in Section 1 of your Coralogix Custom dashboard.</p> <p>For additional information on bar charts, see the full tutorial on Bar Charts in Custom Dashboards.</p>"},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#gauges","title":"Gauges","text":"<p>Migrate a gauge from your OpenSearch dashboard to your Coralogix Custom dashboard.</p>"},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#opendashboard","title":"OpenDashboard","text":""},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#customdashboards","title":"CustomDashboards","text":""},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#migrate-parameters_4","title":"Migrate Parameters","text":"<p>For each section (1, 2, or 3), migrate the parameters from your OpenSearch dashboard to your Coralogix Custom dashboard.</p> Section OpenSearch Dashboard Parameter Coralogix Custom Dashboard Parameter 1 Aggregation Aggregation 2 Gauge Ranges Thresholds 2 Arc settings Arc settings 3 Lucene query Query bar <p>For example, transfer the aggregation value in Section 1 of your OpenSearch Dashboard to the aggregation parameter value in Section 1 of your Coralogix Custom dashboard.</p> <p>For additional information on gauges, see the full tutorial on Gauges in Custom Dashboards.</p>"},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#additional-resources","title":"Additional Resources","text":"DocumentationCustom DashboardsLine ChartsData TablesGaugesPie ChartsBar Charts"},{"location":"newoutput/migrating-from-opensearch-to-coralogix-custom-dashboards/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/mongodb-atlas/","title":"MongoDB Atlas","text":"<p>MongoDB Atlas is is a fully-managed cloud database that handles the complexity of deploying, managing, and healing your deployments on the cloud service provider of your choice.</p> <p>Deploy this integration to send your MongoDB Atlas metrics to you Coralogix account using the OpenTelemetry Collector.</p>"},{"location":"newoutput/mongodb-atlas/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Coralogix account</p> </li> <li> <p>MongoDB Atlas project</p> </li> <li> <p>Private and public keys created for your MongoDB Atlas\u00a0organization\u00a0or the\u00a0project\u00a0from which to send the data</p> </li> <li> <p>OpenTelemetry installed</p> </li> </ul>"},{"location":"newoutput/mongodb-atlas/#configuration","title":"Configuration","text":"<p>STEP 1. Create a configuration file\u00a0<code>/etc/otelcol-contrib/config.yaml</code>\u00a0with the following parameters:</p> <pre><code>receivers:\n  mongodbatlas:\n    # You can obtain the public key from the API Keys tab of the MongoDB Atlas Project Access Manager.\n    # This value is required.\n    public_key: &lt;&lt;YOUR-MONGODB-ATLAS-PUBLIC-KEY&gt;&gt;\n    # You can obtain the private key from the API Keys tab of the MongoDB Atlas Project Access Manager.\n    # This value is required.\n    private_key: &lt;&lt;YOUR-MONGODB-ATLAS-PRIVATE-KEY&gt;&gt;\n\nexporters:\n  logging:\n  prometheusremotewrite:\n    endpoint: &lt;&lt;coralogix_prometheusremotewrite_endpoint&gt;&gt;\n    headers:\n      Authorization: \"Bearer &lt;private key&gt;\"\n    external_labels:\n      environment: \"QA\"\nprocessors:\n  batch:\n\nextensions:\n  pprof:\n    endpoint: :1777\n  zpages:\n    endpoint: :55679\n  health_check:\n\nservice:\n  extensions: [health_check, pprof, zpages]\n  pipelines:\n    metrics:\n      receivers: [mongodbatlas]\n      processors: [batch]\n      exporters: [logging, prometheusremotewrite]\n\n</code></pre> <ul> <li> <p>Replace <code>&lt;&lt;YOUR-MONGODB-ATLAS-PUBLIC-KEY&gt;&gt;</code> with the public key to your MongoDB Atlas organization or project.</p> </li> <li> <p>Replace <code>&lt;&lt;YOUR-MONGODB-ATLAS-PRIVATE-KEY&gt;&gt;</code> with the private key to your MongoDB Atlas organization or project.</p> </li> <li> <p>Replace <code>&lt;&lt;coralogix_prometheusremotewrite_endpoint&gt;&gt;</code> with the Prometheus RemoteWrite endpoint associated with your Coralogix domain.</p> </li> </ul> <p>STEP 2. Restart your collector and view your metrics in your Coralogix dashboard.</p>"},{"location":"newoutput/mongodb-atlas/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/monitor-windows-server-using-otel/","title":"Monitoring  Windows Server using Otel & Prometheus","text":"<p>This tutorial demonstrates how to monitor Windows Server - including IIS and MSSQL - for logs, metrics, and traces using OpenTelemetry Collector and Prometheus Windows Exporter.</p>"},{"location":"newoutput/monitor-windows-server-using-otel/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>admin user in Windows with the ability to install services</p> </li> <li> <p><code>tar.gz</code> files unpacked</p> </li> </ul>"},{"location":"newoutput/monitor-windows-server-using-otel/#configuration","title":"Configuration","text":""},{"location":"newoutput/monitor-windows-server-using-otel/#windows-exporter","title":"Windows Exporter","text":"<p>STEP 1. Download the latest MSI from GitHub (0.22.0.msi as of 30 April '23).</p> <p>STEP 2. Open the command line and navigate to <code>C:\\Users\\Administrator\\Downloads</code> or the MSI folder.</p> <p>STEP 3. Select which exporter collectors you require - including MSSQL and IIS - from this verbose list: <code>cpu,cs,logical_disk,net,os,service,system,textfile,iis,mssql</code>.</p> <p>STEP 4. Open Command Prompt.</p> <p></p> <p>STEP 5. Run the following command to install the MSI with the collectors chosen:</p> <pre><code>msiexec /i windows_exporter-0.22.0-amd64.msi ENABLED_COLLECTORS=\"os,iis\u201d\n</code></pre>"},{"location":"newoutput/monitor-windows-server-using-otel/#opentelemetry-collector","title":"OpenTelemetry Collector","text":"<p>STEP 1. Download the latest tar.gz from GitHub (0.76.1.tar.gz as of 30 April '23).</p> <p>STEP 2. Unpack the content into a <code>C:\\\\cx-otel</code>. This may require you to install a program to unpack tar.gz files.</p> <p>STEP 3. Create a <code>config.yaml</code> based on the following configuration.</p> <pre><code>receivers:\n  filelog:\n    include: [ C:\\inetpub\\logs\\LogFiles\\*.* ]\n  prometheus:\n    config:\n      scrape_configs:\n        - job_name: 'windows_exporter'\n          scrape_interval: 15s\n          static_configs:\n            - targets: ['0.0.0.0:9182']\nprocessors:\n  resourcedetection:\n    detectors: [system]\n    system:\n      hostname_sources: [\"os\"]\n  batch:\n\nexporters:\n  coralogix:\n    metrics:\n      endpoint: \"ingress.coralogixstg.wpengine.com:443\"\n    logs:\n      endpoint: \"ingress.coralogixstg.wpengine.com:443\"\n    private_key: \"coralogix-api-key\"\n    application_name: \"WinServer\"\n    subsystem_name: \"Host\"\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [prometheus]\n      processors: [resourcedetection, batch]\n      exporters: [coralogix]\n    logs:\n      receivers: [filelog]\n      processors: [resourcedetection, batch]\n      exporters: [coralogix]\n</code></pre> <p>Notes:</p> <ul> <li> <p>The example configuration captures metrics from windows_exporter and logs from IIS. Modify as necessary.</p> </li> <li> <p>You are required to input the following variables:</p> <ul> <li> <p><code>endpoint</code>: Select the OpenTelemetry endpoint associated with your Coralogix domain</p> </li> <li> <p><code>private_key</code>: Your Coralogix Send-Your-Data API key</p> </li> <li> <p><code>application_name</code> &amp; <code>subsystem_name</code>: Application and subsystem names as they will appear in your Coralogix UI</p> </li> </ul> </li> </ul> <p>STEP 4. Install the collector as a service by running the following command:</p> <pre><code>sc.exe create cx-otelcol displayname=cx-otelcol start=delayed-auto binPath=\"C:\\cx-otel\\otelcol-contrib.exe --config C:\\cx-otel\\config.yaml\u201d\n</code></pre> <p>STEP 5. Run the service.</p> <pre><code>sc.exe start cx-otelcol \n</code></pre>"},{"location":"newoutput/monitor-windows-server-using-otel/#additional-configurations","title":"Additional Configurations","text":"<p>Configure Windows exporter dashboards and the OpenTelemetry Kubernetes Extension to monitor the collector itself.</p>"},{"location":"newoutput/monitor-windows-server-using-otel/#validation-testing","title":"Validation &amp; Testing","text":"<p>STEP 1. Open Services and check that both <code>windows_exporter</code> and <code>cx-otelcol</code> service are running.</p> <p>STEP 2. To validate which metrics exist in <code>windows_exporter</code>, navigate to http://localhost:9182/metrics.</p> <p>Notes:</p> <ul> <li> <p><code>windows_exporter.exe</code> is unsafe and may require a specific \u201cunblock\u201d in its settings.</p> </li> <li> <p>Both <code>windows_exporter</code> and <code>otelcol</code> can be run without services to check for logs in the command prompt.</p> </li> </ul> <p>STEP 3. Navigate to Grafana Explore to confirm that <code>windows_*</code> metrics are arriving in your Coralogix dashboard.</p>"},{"location":"newoutput/monitor-windows-server-using-otel/#additional-links","title":"Additional Links","text":"GithubOfficial OpenTelemetry Collector with Coralogix ExporterCoralogix Windows Exporter DashboardsTutorialsTail Sampling with Coralogix and OpenTelemetryBlogsHow to Configure the OTel Community Demo App to Send Telemetry Data to Coralogix"},{"location":"newoutput/monitor-windows-server-using-otel/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/multiple-queries-in-custom-dashboard-widgets/","title":"Multiple Queries in Custom Dashboards","text":"<p>Coralogix is bringing Full Stack Observability into your Custom Dashboards! With the new multiple query feature available in custom dashboard widgets, you can view queries from multiple data types (logs, metrics, traces) in a single chart.</p> <p>Each query has two sections that control its data. The query bar at the bottom of the screen lets you set the Lucene or PromQL query, and the right-hand side panel lets you select additional settings for each of your queries.</p> <p></p>"},{"location":"newoutput/multiple-queries-in-custom-dashboard-widgets/#configuration","title":"Configuration","text":"<p>You can add additional queries at any time.</p> <ul> <li> <p>To add a new query, click the + next to Query 1 in the query bar or click + Add Query towards the top of the right-hand side bar. When you add a new query it appears both in the bottom query bar and the right-hand side bar.</p> </li> <li> <p>Edit your query as needed using the query bar and the right-hand side panel.</p> </li> <li> <p>Switch between queries by selecting one from either the bottom query bar or using the right-hand side bar.</p> </li> <li> <p>To delete a query, either click the X on its tab in the bottom query bar, or click the trash icon in the right-hand side bar next to the query name.</p> </li> </ul>"},{"location":"newoutput/multiple-queries-in-custom-dashboard-widgets/#additional-resources","title":"Additional Resources","text":"DocumentationCustom Dashboards"},{"location":"newoutput/multiple-queries-in-custom-dashboard-widgets/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/nagios-coralogix/","title":"Nagios","text":"<p>Nagios is\u00a0an open-source monitoring system for computer systems. It was designed to run on the Linux operating system and can monitor devices running Linux, Windows, and Unix operating systems. Nagios software runs periodic status checks on critical parameters of application, network, and server resources.</p> <p>Both Nagios XI and Nagios Core have the ability to generate status checks, which arrive in the form of logs and metrics.</p> <p>Nagios supports forwarding status checks to another Nagios instances using NRDP protocol; Coralogix uses this protocol to collect logs and metrics from each status check that is sent.</p> <p>This tutorial demonstrates how to send your logs and metrics to Coralogix using Nagios.</p>"},{"location":"newoutput/nagios-coralogix/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Active Coralogix\u00a0account\u00a0with\u00a0metric bucket and a working Grafana dashboard for metrics</p> </li> <li> <p>Nagios Core installed</p> </li> <li> <p>Nagios XI installed</p> </li> </ul>"},{"location":"newoutput/nagios-coralogix/#configuration","title":"Configuration","text":""},{"location":"newoutput/nagios-coralogix/#nagios-core","title":"Nagios Core","text":"<p>STEP 1. Update the main configuration file nagios.cfg, usually located in: /usr/local/nagios/etc/, by adding the following:</p> <pre><code>obsess_over_hosts=1\nobsess_over_services=1\nochp_command=send_nrdp_host\nocsp_command=send_nrdp_service\n</code></pre> <p>STEP 2. Create (or update) the file commands.cfg, usually located in: /usr/local/nagios/etc/, and add it to nagios.cfg:</p> <pre><code>command_name send_nrdp_host\ncommand_line $USER1$/send_nrdp.php --url=&lt;clusterURL&gt;/nrdp/api/v1/&lt;appname&gt;/&lt;subsystem&gt; --token=&lt;privatekey&gt; --host=\"$HOSTNAME$\" --state=$HOSTSTATEID$ --output=\"$HOSTOUTPUT$|$HOSTPERFDATA$\"\n}\ndefine command{\ncommand_name send_nrdp_service\ncommand_line $USER1$/send_nrdp.php --url=&lt;clusterURL&gt;nrdp/v1/:application/:subsystem/nrdp/ --token=&lt;privatekey&gt; --host=\"$HOSTNAME$\" --service=\"$SERVICEDESC$\" --state=$SERVICESTATEID$ --output=\"$SERVICEOUTPUT$|$SERVICEPERFDATA$\"\n}\n\n</code></pre> <p>You are required to input 4 mandatory fields:</p> <ul> <li> <p>clusterURL: Coralogix\u00a0ingress endpoint\u00a0associated with your Coralogix domain with path changed to one of the following:</p> <ul> <li> <p><code>nrdp/v1/:application/:subsystem/nrdp/</code></p> </li> <li> <p><code>nrdp/v1/:application/:subsystem/</code></p> </li> </ul> </li> <li> <p>appName: Application name added to your metric attributes</p> </li> <li> <p>subsystemName: Subsystem name added to your metric attributes</p> </li> <li> <p>privateKey: Coralogix Send-Your-Data API key</p> </li> </ul>"},{"location":"newoutput/nagios-coralogix/#nagios-xi","title":"Nagios XI","text":"<p>Once Nagios Core is configured, configure Nagios XI.</p> <p>STEP 1. Select the admin tab in your navigation pane.</p> <p>STEP 2. In the left-hand side bar, click Outbound Transfers and input the following parameters:</p> <ul> <li> <p>Outbound check transfer: Enable.</p> </li> <li> <p>Global options: Change filter mode to include and filter /^.*/.</p> </li> <li> <p>NRDP: Enable.</p> </li> <li> <p>IP: (without HTTPS): Coralogix\u00a0ingress endpoint\u00a0associated with your Coralogix domain with path changed to\u00a0one of the following:</p> <ul> <li> <p><code>nrdp/v1/:application/:subsystem/nrdp/</code></p> </li> <li> <p><code>nrdp/v1/:application/:subsystem/</code></p> </li> </ul> </li> <li> <p>Type: HTTPS</p> </li> <li> <p>Token: Input Coralogix Send-Your-Data API key.</p> </li> <li> <p>Settings: Update as necessary.</p> </li> </ul>"},{"location":"newoutput/nagios-coralogix/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/node-js/","title":"Node.js OpenTelemetry Instrumentation","text":"<p>This tutorial demonstrates how to instrument Node.js applications to capture metrics and traces using OpenTelemetry and send them to Coralogix.</p>"},{"location":"newoutput/node-js/#overview","title":"Overview","text":"<p>Coralogix offers three ways to collect instrumentation data from your applications:</p> <ul> <li> <p>Automatic instrumentation without application code changes</p> </li> <li> <p>Manual instrumentation</p> </li> <li> <p>Instrumentation injection with K8s deployed applications</p> </li> </ul> <p>To configure your instrumentation to define, report, and monitor Coralogix Service Flows, you must use individual auto-instrumentation or manual instrumentation. The bundled auto-instrumentation method is not supported.</p>"},{"location":"newoutput/node-js/#auto-instrumentation","title":"Auto Instrumentation","text":"<p>Many popular Node.js packages have auto instrumentation libraries that allow your application to automatically instrument key elements of your code without modifying it.</p> <p>The Node.js packages that offer these automatic instrumentation libraries can be found here.</p> <p>If you need to collect instrumentation of other libraries or application code, you must manually instrument those portions.</p>"},{"location":"newoutput/node-js/#auto-instrumentation-using-instrumentation-libraries","title":"Auto Instrumentation using Instrumentation Libraries","text":"<p>Utilize instrumentation libraries individually or using a bundled package called auto-instrumentations-node. The bundled approach will load all currently available auto-instrumentation libraries into your application to ensure it captures all possible instrumentation. This is a great option for testing purposes, to see what is available to your application, but it does come with the overhead of increased application size and memory footprint. Once you\u2019ve identified which libraries best match your telemetry objectives, you can use the individual method to load the required packages.</p>"},{"location":"newoutput/node-js/#bundled-method","title":"Bundled Method","text":"<p>The bundled method is a fully automatic solution that doesn\u2019t require any JavaScript code to be written or modified. You\u2019ll first need to install the prerequisite packages below to use the bundled method.</p> <pre><code>npm install --save @opentelemetry/api\nnpm install --save @opentelemetry/auto-instrumentations-node\n\n</code></pre> <p>You then need to set environment variables to allow the auto-instrumentation registration to know what to do. Using this method, you can set almost every configuration option available in OTEL. For example, you set up the exporter to forward messages directly to Coralogix as follows:</p> <pre><code>export OTEL_TRACES_EXPORTER=\"otlp\"\nexport OTEL_EXPORTER_OTLP_PROTOCOL=\"grpc\"\nexport OTEL_EXPORTER_OTLP_COMPRESSION=\"gzip\"\nexport OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=\"&lt;https://ingress.coralogixstg.wpengine.com:443&gt;\"\nexport OTEL_EXPORTER_OTLP_HEADERS=\"Authorization=Bearer &lt;CXPrivateKey&gt;\"\nexport OTEL_RESOURCE_ATTRIBUTES=\"cx.application.name=AppName, cx.subsystem.name=SubName, service.namespace=YourAppNamespace\"\nexport OTEL_NODE_RESOURCE_DETECTORS=\"all\"\nexport OTEL_SERVICE_NAME=\"YourAppServiceName\"\n\n</code></pre> <p>These environment variables can be set in any manner available for your deployment type. However, they cannot be set within the application, as they need to be available to the auto-instrumentation library at startup. Full documentation of the environment variables can be found in the official documentation.</p> <p>Example: In the given instance, the variable OTEL_NODE_RESOURCE_DETECTORS is configured with the value 'all'. Certain resource types might be absent if you plan to execute the test application on your local machine, leading to errors. To address this, set OTEL_NODE_RESOURCE_DETECTORS to \"env,host,os,process\" to avoid encountering errors. You can locate the Resource detectors here.</p> <p>Lastly, you must execute your application and \u2014require the registration library. You can do so two ways, using an additional environment variable, or adding the requirement directly to your execution command:</p> <pre><code>export NODE_OPTIONS=\"--require @opentelemetry/auto-instrumentations-node/register\"\nnode YourApp.js\n\n</code></pre> <p>or</p> <pre><code>node --require @opentelemetry/auto-instrumentations-node/register YourApp.js\n\n</code></pre>"},{"location":"newoutput/node-js/#individual-method","title":"Individual Method","text":"<p>You\u2019ll first need to install the prerequisite packages below.</p> <pre><code>npm install --save @opentelemetry/api\nnpm install --save @opentelemetry/auto-instrumentations-node\nnpm install --save @coralogix/opentelemetry\n\n</code></pre> <p>If you don\u2019t want to use the bundled method, you can also create your instrumentation module and \u201c\u2014require\u201d it. This method is a bit more complicated, but it does give you more control as to which libraries you want to import and how you want to process the different data sources.</p> <p>Below is a short example of instrumentation for an \u201cexpress\u201d application. We\u2019ll create a new instrumentation.js file to avoid having to modify the base code of the application.</p> <pre><code>//instrumentation.js\n\n//load the auto-instrumentation libraries we want to use\nconst { HttpInstrumentation } = require('@opentelemetry/instrumentation-http');\nconst { ExpressInstrumentation } = require('@opentelemetry/instrumentation-express');\n\n//load some base OTEL modules\nconst opentelemetry = require(\"@opentelemetry/sdk-node\");\nconst { getNodeAutoInstrumentations } = require(\"@opentelemetry/auto-instrumentations-node\");\n\n//load an Exporter (OTLP to Coralogix ingress API)\nconst { OTLPTraceExporter } = require(\"@opentelemetry/exporter-trace-otlp-proto\");\n\n//transaction sampler\n**import {CoralogixTransactionSampler} from \"@coralogix/opentelemetry\";**\n\n// // Optional libraries for setting resource attributes via code\n// const { Resource } = require('@opentelemetry/resources');\n// const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');\n\n// // Optional libraries for metrics configuration\n// const { OTLPMetricExporter } = require(\"@opentelemetry/exporter-metrics-otlp-proto\");\n// const { PeriodicExportingMetricReader } = require('@opentelemetry/sdk-metrics');\n\n//Now we'll initialize the OTEL sdk:\nconst sdk = new opentelemetry.NodeSDK({\n  // // Optional resource configuration section\n  // resource: new Resource({\n  //   [SemanticResourceAttributes.SERVICE_NAME]: 'yourServiceName',\n  //   [SemanticResourceAttributes.SERVICE_VERSION]: '1.0',\n  // }),\n  // // Optional metrics configuration section\n  // metricReader: new PeriodicExportingMetricReader({\n  //   exporter: new OTLPMetricExporter({\n  //     concurrencyLimit: 1,\n  //   }),\n  // }),\n    **sampler: new CoralogixTransactionSampler(new AlwaysOnSampler()),**\n  instrumentations: [\n    new HttpInstrumentation(),\n    new ExpressInstrumentation(),\n  ],\n});\n\nsdk.start();\n\n</code></pre> <p>With this custom instrumentation.js, we\u2019ll initialize it as before using the same environment variables and load it using a required stanza:</p> <pre><code>node --require ./instrumentation.js YourApp.js\n\n</code></pre>"},{"location":"newoutput/node-js/#manual-instrumentation","title":"Manual Instrumentation","text":"<p>Manual instrumentation of your code may be beneficial when you need control of span creation, managing context propagation, or when you need to instrument code without existing auto-instrumentation libraries. You\u2019ll also need to do some manual instrumentation if you want to add support for logs, as log support is not native to the Node.js OTEL instrumentation framework. Examples of Log and Metrics support will be provided at a later date.</p> <p>Initially, you\u2019ll have to decide what trace provider you intend to use. Commonly, this would be the <code>BasicTracerProvider</code> from the <code>@opentelemetry/sdk-trace-base</code> module, but there are several other options available.</p> <p>In addition, you\u2019ll need to configure resource attribute tags manually, commonly done with the <code>SemanticResourceAttributes</code> from <code>@opentelemetry/semantic-conventions</code> though custom attributes can also be used.</p> <p>You\u2019ll have to select a span processor, based on your application load. In the provided example, We\u2019ll be using the <code>SimpleSpanProcessor</code> but <code>BatchSpanProcessor</code> is typically preferred.</p> <p>You can add in a trace sampling configuration, but by default, it will submit 100% of your traces. We do not use a trace sampling configuration in our example.</p> <p>Lastly, you\u2019ll need an exporter. We\u2019ll want to use the otlp+protobuf trace exporter as we used in our earlier examples. This will allow us to submit traces directly to the Coralogix backend. If you want to submit through an Opentelemetry Collector to leverage resource detection and other enrichments, that can be done using various trace exporters.</p> <p>Below is an example application from the OpenTelemetry-js repository, adjusted to support submission to Coralogix:</p> <pre><code>const opentelemetry = require('@opentelemetry/api');\nconst { Resource } = require('@opentelemetry/resources');\nconst { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');\nconst { BasicTracerProvider, ConsoleSpanExporter, SimpleSpanProcessor } = require('@opentelemetry/sdk-trace-base');\nconst { OTLPTraceExporter } = require(\"@opentelemetry/exporter-trace-otlp-grpc\");\nconst { diag, DiagConsoleLogger, DiagLogLevel } = require('@opentelemetry/api');\n\n**import {CoralogixTransactionSampler} from \"@coralogix/opentelemetry\";**\n\n// Diagnostic Logger, useful for troubleshooting\ndiag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG);\n\n// First we need to initialize a Trace Provider, for this example we're using BasicTraceProvider from the\n// &lt;https://www.npmjs.com/package/@opentelemetry/sdk-trace-base&gt; package.\n\nconst provider = new BasicTracerProvider({\n  // If submitting directly to Coralogix, set cx.application.name and cx.subsystem.name as Resource Attributes.\n  resource: new Resource({\n    [SemanticResourceAttributes.SERVICE_NAME]: 'nodejs-manual-instrumentation',\n    'cx.application.name': 'nodejs',\n    'cx.subsystem.name': 'manual',\n  }),\n  **sampler:  new CoralogixTransactionSampler(new AlwaysOnSampler()) // or your original sampler**\n  ,\n});\n\n// Create an OTLP/protobuf Exporter to send spans directly to Coralogix.\n// For production, obviously don't hardcode these values, but rather use an environment variables\nconst collectorOptions = {\n  timeoutMillis: 15000,\n  url: '&lt;https://ingress.coralogix.com:443/v1/traces&gt;',\n  headers: {\n    Authorization: 'Bearer cxtp_&lt;redacted&gt;',\n  },\n};\n\nconst exporter = new OTLPTraceExporter(collectorOptions);\n\n// Configure span processor to send spans to the exporter\nprovider.addSpanProcessor(new SimpleSpanProcessor(exporter));\n\n// Initialize the OpenTelemetry APIs to use the BasicTracerProvider bindings.\n\n// This registers the tracer provider with the OpenTelemetry API as the global\n// tracer provider. This means when you call API methods like\n// `opentelemetry.trace.getTracer`, they will use this tracer provider. If you\n// do not register a global tracer provider, instrumentation which calls these\n// methods will receive no-op implementations.\nprovider.register();\nconst tracer = opentelemetry.trace.getTracer('nodejs-manual-instrumentation');\n\n// Create a span. A span must be closed.\nconst parentSpan = tracer.startSpan('main');\nfor (let i = 0; i &lt; 2; i += 1) {\n  doWork(parentSpan);\n}\n// Be sure to end the span.\nparentSpan.end();\n\n// flush and close the connection.\nexporter.shutdown();\n\nfunction doWork(parent) {\n  // Start another span. In this example, the main method already started a\n  // span, so that'll be the parent span, and this will be a child span.\n  const ctx = opentelemetry.trace.setSpan(opentelemetry.context.active(), parent);\n  const span = tracer.startSpan('doWork', undefined, ctx);\n\n  // simulate some random work.\n  for (let i = 0; i &lt;= Math.floor(Math.random() * 40000000); i += 1) {\n    // empty\n  }\n\n  // Set attributes to the span.\n  span.setAttribute('SomeKey', 'SomeValue');\n\n  // Annotate our span to capture metadata about our operation\n  span.addEvent('invoking doWork');\n\n  span.end();\n}\n</code></pre> <p>Below is a list of the required libraries needed for this example:</p> <pre><code>\"dependencies\": {\n    \"@opentelemetry/api\": \"^1.7.0\",\n    \"@opentelemetry/exporter-trace-otlp-proto\": \"^0.47.0\",\n    \"@opentelemetry/resources\": \"^1.20.0\",\n    \"@opentelemetry/sdk-trace-base\": \"^1.20.0\",\n    \"@coralogix/opentelemetry\": \"^0.1.2\"\n\n\n</code></pre>"},{"location":"newoutput/node-js/#kubernetes-injection-auto-instrumentation","title":"Kubernetes Injection Auto Instrumentation","text":"<p>You may also implement auto-instrumentation using the OpenTelemetry Operator.</p> <p>The OpenTelemetry Operator supports injecting and configuring auto-instrumentation libraries for .NET, Java, Node.js, and Python services.</p>"},{"location":"newoutput/node-js/#service-flows","title":"Service Flows","text":"<p>For customers with existing functional Node.js OpenTelemetry instrumentation enabling data transmission to Coralogix using individual auto-instrumentation or manual instrumentation, this section guides reconfiguring the existing setup to define, report, and monitor Coralogix Service Flows.</p> <p>For new customers or those who haven't initiated Node.js OpenTelemetry instrumentation, follow our individual auto-instrumentation or manual instrumentation instructions above, which include the steps below. The bundled auto-instrumentation method is not supported.</p>"},{"location":"newoutput/node-js/#individual-auto-instrumentation","title":"Individual Auto-Instrumentation","text":"<p>Terminal command:</p> <pre><code>npm install --save @coralogix/opentelemetry\n</code></pre> <p>Add to the code:</p> <pre><code>import {CoralogixTransactionSampler} from \"@coralogix/opentelemetry\";\n</code></pre> <pre><code>sampler:  new CoralogixTransactionSampler(new AlwaysOnSampler()) // or your original sampler\n</code></pre>"},{"location":"newoutput/node-js/#manual-instrumentation_1","title":"Manual Instrumentation","text":"<p>Add to the package:</p> <pre><code>\"dependencies\": {\n\u00a0\u00a0\u00a0\u00a0\"@opentelemetry/api\": \"^1.7.0\",\n\u00a0\u00a0\u00a0\u00a0\"@opentelemetry/exporter-trace-otlp-proto\": \"^0.47.0\",\n\u00a0\u00a0\u00a0\u00a0\"@opentelemetry/resources\": \"^1.20.0\",\n\u00a0\u00a0\u00a0\u00a0\"@opentelemetry/sdk-trace-base\": \"^1.20.0\",\n\u00a0\u00a0\u00a0 \"@Coralogix/opentelemetry\": \"^0.1.2\"\n</code></pre> <p>Add to the code:</p> <pre><code>import {CoralogixTransactionSampler} from \"@coralogix/opentelemetry\";\n</code></pre> <pre><code>sampler:  new CoralogixTransactionSampler(new AlwaysOnSampler()) // or your original sampler\n</code></pre>"},{"location":"newoutput/node-js/#additional-resources","title":"Additional Resources","text":"DocumentationIntroduction to Application Performance Monitoring"},{"location":"newoutput/node-js/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/office-365-audit-logs/","title":"Office 365 Audit Logs","text":"<p>Knowing who does what and accesses which files, mailboxes,\u00a0 and so on in your Office 365 is crucial to the security of your environment. With Filebeat and Coralogix you will be able to audit these actions.</p> <p>The steps below will help you configure your office 365 to send audit logs to Coralogix.</p>"},{"location":"newoutput/office-365-audit-logs/#enable-auditing-in-office-365","title":"Enable Auditing in Office 365","text":"<p>Log in to your Office 365 and find the Admin tab:</p> <p></p> <p>A new window is going to open. Click Security Tab.</p> <p></p> <p>Look for Search in the left-hand menu, expand and click Audit Log Search.</p> <p></p> <p>If you have not enabled Audit logs a blue banner will be displayed on top of the page, click it to enable Audit logs.</p> <p></p>"},{"location":"newoutput/office-365-audit-logs/#create-an-app","title":"Create an app","text":"<p>Go to Azure Portal: https://portal.azure.com/</p> <p>Log in with the same credentials as your Office 365.</p> <p>Look for the Azure Active Directory in the left menu and click:</p> <p></p> <p>A new page will open. Click on Enterprise applications.</p> <p></p> <p>Click on create your own application. Name your application, select register an application to integrate with Azure AD, and hit save.</p> <p></p> <p>In the home menu, click on App registrations.</p> <p></p> <p>Under owned applications tab, select the app you have created. You will see the info needed for the FileBeat Office 365 module to allow authentication to your Office 365 and grabbing the audit logs.</p> <p>You also need certificates or a secret key that you should generate or upload under Client credentials on the same page.</p> <p></p> <p>Click on the API Permissions tab on the left, and add the permissions shown in the screenshot.</p> <p></p>"},{"location":"newoutput/office-365-audit-logs/#install-filebeat","title":"Install Filebeat","text":"<p>Find our full tutorial for installing Filebeat here: https://coralogixstg.wpengine.com/integrations/filebeat/</p> <p>Enable the o365 module and configure according to the following requirements.</p>"},{"location":"newoutput/office-365-audit-logs/#using-a-secret-key","title":"Using a Secret key","text":"<pre><code>audit:\n\nenabled: true\n\nvar.application_id: \"&lt;My Azure AD Application ID&gt;\"\n\nvar.tenants:\n\n- id: \"&lt;My Tenant ID&gt;\"\n\nname: \"mytenant.onmicrosoft.com\"\n\nvar.client_secret: \"&lt;My client secret&gt;\"\n</code></pre>"},{"location":"newoutput/office-365-audit-logs/#using-certificates","title":"Using certificates","text":"<pre><code>audit:\n\nenabled: true\n\nvar.application_id: \"&lt;My Azure AD Application ID&gt;\"\n\nvar.tenants: - id: \"&lt;My Tenant ID&gt;\" name: \"mytenant.onmicrosoft.com\"\n</code></pre> <pre><code>var.certificate: \"/path/to/certificate.pem\"\nvar.key: \"/path/to/private_key.pem\"\nvar.key_passphrase: \"my_passphrase\" # (optional) for encrypted keys\n</code></pre> <p>Tenant name is the primary domain that was initially created for Microsoft Office 365.</p> <p>Under Microsoft office 365 admin center.</p> <p>Click setup tab.</p> <p>Click Domains, on the left side you should see your Tenant name.</p> <p></p> <p>For more information about the o365 module in FileBeat: https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-o365.html</p> <p>Once done, restart FileBeat.</p> <p>It takes the office 365 Audit logs up to 24 hours to start showing on the Office 365 admin page.</p> <p>For additional assistance with this configuration, please contact our support team via our in-app chat!</p>"},{"location":"newoutput/okta-contextual-logs/","title":"Okta Contextual Logs","text":"<p>This tutorial demonstrates how to create a pulling integration with Okta to send your contextual data logs to Coralogix.</p>"},{"location":"newoutput/okta-contextual-logs/#overview","title":"Overview","text":"<p>Okta generates various logs that capture user authentication and authorization events, such as login attempts, user provisioning, and access management. These logs contain valuable information about user activities, security events, and system behavior within your Okta environment.</p> <p>Coralogix offers a pulling integration that ingests your Okta contextual data logs at specified intervals, allowing you to gain insights into system behavior within our platform and troubleshoot problems that arise.</p> <p>Benefits include:</p> <ul> <li> <p>Security Monitoring. Coralogix enables you to monitor user authentication and access events, detect suspicious activities, and identify potential security threats. Identify patterns, anomalies, and indicators of compromise so that you can respond swiftly to security incidents.</p> </li> <li> <p>Compliance and Auditing. By collecting and analyzing the context data logs, Coralogix helps you meet regulatory compliance requirements. It provides the ability to track and audit user activities, generate compliance reports, and ensure adherence to industry standards.</p> </li> <li> <p>Operational Insights. Our monitoring platform allows you to identify usage patterns, troubleshoot issues, track performance metrics, and optimize your Okta environment for improved efficiency.</p> </li> </ul>"},{"location":"newoutput/okta-contextual-logs/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Okta account</p> </li> <li> <p>New API key generated in Okta</p> </li> </ul>"},{"location":"newoutput/okta-contextual-logs/#configuration","title":"Configuration","text":"<p>STEP 1. In your Coralogix dashboard, navigate to Data Flow &gt; Contextual Data.</p> <p>STEP 2. In the Contextual Data section, locate Okta and click on ADD.</p> <p>STEP 3. Enter the integration details.</p> <ul> <li> <p>Integration Name</p> </li> <li> <p>Account Name (This will appear in your Coralogix UI as your subsystem name.)</p> </li> <li> <p>Okta Domain</p> </li> <li> <p>Okta API key</p> </li> </ul> <p></p> <p>STEP 4. Click CONNECT to trigger the integration. Your pulled Okta logs should appear in your Coralogix dashboard.</p> <p>STEP 5. [Optional] To enhance your monitoring capabilities, we highly recommend selecting the corresponding extension and deploying it.</p> <p>Learn more about our Extension Packages here.</p> <p></p>"},{"location":"newoutput/okta-contextual-logs/#additional-resources","title":"Additional Resources","text":"DocumentationOkta Audit LogsBlogOkta Log Insights with Coralogix"},{"location":"newoutput/okta-contextual-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/onelogin/","title":"OneLogin","text":"<p>OneLogin, a cloud-based identity and access management solution, streamlines user authentication and authorization processes for organizations. It features single sign-on (SSO) functionality, allowing users to access multiple applications using a single set of login credentials. OneLogin also offers multi-factor authentication options, such as SMS, email, and biometric verification, for enhanced security.</p> <p>Administrators can efficiently manage user accounts, permissions, and access rights through a centralized dashboard. OneLogin seamlessly integrates with existing directory services like Active Directory and LDAP. Security measures like encryption and threat detection ensure data protection while complying with industry standards such as GDPR and HIPAA. Overall, OneLogin helps organizations bolster security, streamline user access management, and enhance user experience across their IT infrastructure.</p> <p>Integration between OneLogin and Coralogix is facilitated via AWS EventBridge.</p>"},{"location":"newoutput/onelogin/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Active OneLogin account with admin access</p> </li> <li> <p>AWS account with permission to use Amazon EventBridge</p> </li> </ul>"},{"location":"newoutput/onelogin/#setup","title":"Setup","text":""},{"location":"newoutput/onelogin/#creating-an-event-webhook-for-amazon-eventbridge-from-onelogin-console","title":"Creating an Event Webhook for Amazon EventBridge from OneLogin Console","text":"<ol> <li> <p>Log in to the OneLogin Admin console.</p> </li> <li> <p>Navigate to Developers &gt; Webhooks &gt; New Webhook &gt; Event Webhook for Amazon EventBridge.</p> </li> <li> <p>Fill in the required fields and save.</p> </li> </ol> <p></p>"},{"location":"newoutput/onelogin/#associating-event-webhook-with-aws-eventbridge","title":"Associating Event Webhook with AWS EventBridge","text":"<ol> <li> <p>Go to your AWS account and navigate to Amazon EventBridge &gt; Partner Event Source.</p> </li> <li> <p>Locate the created webhook in the Partner Event Source section.</p> </li> <li> <p>If the status is inactive, select the webhook and click on the Associate with Event Bus tab.</p> </li> <li> <p>The partner event source becomes active, sending logs to the associated event bus.</p> </li> </ol> <p></p>"},{"location":"newoutput/onelogin/#creating-a-rule-for-event-destination","title":"Creating a Rule for Event Destination","text":"<ol> <li> <p>Under the associated bus, create a rule with the source as AWS events or EventBridge partner events.</p> </li> <li> <p>Select the destination as API destination.</p> </li> <li> <p>Choose Coralogix API destination. Find out more about creating an API destination for Coralogix with EventBridge here.</p> </li> </ol> <p>Once all steps are completed, navigate to the Event Bus and start discovery for the specific bus.</p>"},{"location":"newoutput/onelogin/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogix.com.</p>"},{"location":"newoutput/open-commerce-api/","title":"Open Commerce API","text":"<p>Coralogix provides an easy way to collect your Open Commerce OrderSearch API logs. The preferred and easiest integration method will be to use our app in the\u00a0AWS Serverless Application Repository.</p>"},{"location":"newoutput/open-commerce-api/#requirements","title":"Requirements","text":"<ul> <li>Your AWS user should have permissions to create lambdas and IAM roles.</li> </ul>"},{"location":"newoutput/open-commerce-api/#installation","title":"Installation","text":"<ul> <li> <p>Navigate to\u00a0the Application Page.</p> </li> <li> <p>Fill in the required parameters.</p> </li> <li> <p>Click Deploy.</p> </li> </ul> <p>Once the deployment is done the lambda will execute every X minutes based on the \u2018lambda schedule\u2019 parameter.</p>"},{"location":"newoutput/open-commerce-api/#parameters-and-descriptions","title":"Parameters and Descriptions","text":"VariableDescriptionApplication nameThe stack name of this application createdNotificationEmailFailure notification email addressApplicationNameApplication Name in CoralogixCoralogixRegionThe Coralogix region [EU1, EU2, US1, US2, AP1 (India), AP2 (Singapore)] associated with your Coralogix domainFunctionArchitectureLambda function architecture [x86_64, arm64]FunctionMemorySizeLambda function memory limitFunctionScheduleLambda function schedule in minutes, the function will be invoked each X minutes. After deploy, first invocation will be after X minutes.FunctionTimeoutLambda function timeout limitLogsToStdoutSend logs to stdout/cloudwatch. Possible values are\u00a0<code>True</code>,\u00a0<code>False</code>OcapiEndpointThe full endpoint to the orderSearch APIOcapiPasswordThe OCAPI password used for authentificationOcapiUsernameOCAPI username. used to get authenticated.PrivateKeyCoralogix Send-Your-Data API keySelectStatementThe select statement to be used in the query. Default to\u00a0<code>(*)</code>SubsystemNameSubsystem name in Coralogix"},{"location":"newoutput/opentelemetry/","title":"OpenTelemetry","text":"<p>OpenTelemetry\u00a0is a vendor-neutral, open-source observability framework for instrumenting, generating, collecting, and exporting telemetry data such as traces, metrics, and logs. Use OpenTelemetry\u2019s collection of APIs, SDKs, and tools to collect and export observability data from your environment to Coralogix.</p>"},{"location":"newoutput/opentelemetry/#instrumentation","title":"Instrumentation","text":"<p>Coralogix supports OpenTelemetry\u00a0to get telemetry data (traces, logs, and metrics) from your application as requests travel through its many services and other infrastructure.</p> <p>We assume you have already\u00a0instrumented your application\u00a0with OTel SDKs and set up a\u00a0receiver\u00a0for your data.</p> <p>If you\u2019ve never instrumented for observability, we provide some examples of automatic instrumentation for your applications in the languages below.</p> <ul> <li> <p>Java</p> </li> <li> <p>Python</p> </li> </ul>"},{"location":"newoutput/opentelemetry/#kubernetes-observability-using-opentelemetry","title":"Kubernetes Observability using OpenTelemetry","text":"<p>Coralogix offers Kubernetes Observability using OpenTelemetry for comprehensive Kubernetes and application observability. Using our OpenTelemetry Chart, the integration enables you to simplify the collection of logs, metrics, and traces from the running application in your pods to the cluster-level components of your Kubernetes cluster, while enabling our Kubernetes Dashboard.</p>"},{"location":"newoutput/opentelemetry/#configuration","title":"Configuration","text":"<p>We provide a few alternative scenarios for setting up OpenTelemetry and sending your data to Coralogix. Documentation related to the configuration and installation of the v0.64.0 release of OpenTelemetry Collector for different deployments can be found below.</p> <ul> <li> <p>Kubernetes Observability using OpenTelemetry</p> </li> <li> <p>OpenTelemetry using ECS - EC2</p> </li> <li> <p>OpenTelemetry using ECS - Fargate</p> </li> <li> <p>OpenTelemetry using Docker</p> </li> </ul>"},{"location":"newoutput/opentelemetry/#otel-metrics-best-practices","title":"Otel Metrics: Best Practices","text":"<p>OpenTelemetry metrics are broadly compatible with Coralogix dimensional metrics. We currently support OpenTelemetry metrics v1.x. All of the supported metric types include an independent set of associated attributes that map directly to dimensions customers can use to filter metric data during your query. While Coralogix now supports Delta Temporality in addition to Cumulative Temporality, we encourage the use of Cumulative Temporality as a best practice.</p>"},{"location":"newoutput/opentelemetry/#temporality","title":"Temporality","text":"<p>The OpenTelemetry Metrics\u00a0Data Model and\u00a0SDK are designed to support both Cumulative and Delta\u00a0Temporality. It is important to understand that temporality will impact how the SDK manages memory usage.</p> <p>The use of Cumulative Temporality for monotonic sums is common, exemplified by Prometheus. The use of Delta Temporality for metric sums is also common, exemplified by Statsd.</p>"},{"location":"newoutput/opentelemetry/#delta-temporality","title":"Delta Temporality","text":"<p>While Coralogix supports Delta Temporality in addition to Cumulative Temporality, we encourage using Cumulative Temporality as a best practice.</p> <p>Those customers using Delta Temporality should take note of the following configuration parameters in our Delta Cumulative Converter:</p> <ul> <li> <p>Coralogix supports monotonic sum and\u00a0histogram data points.</p> </li> <li> <p>Coralogix keeps all metrics in our data store for 24 hours. If values for the same metric arrive in intervals greater than 24 hours, they will be recorded, but treated as \u201cresets.\u201d</p> </li> <li> <p>Your data points may not always arrive consecutively. When a data point is missing, the time series with a missing data point will be delayed for 60 seconds. You can retry sending us missing data points during this window.</p> </li> <li> <p>If a second data point does not arrive after the first, your time series will restart from the third data point.</p> </li> <li> <p>Be cautious about properly setting up your retry or reset series, as the OTEL Exporter can potentially miss data points and may need to resend them.</p> </li> </ul>"},{"location":"newoutput/opentelemetry/#batch-sizing","title":"Batch Sizing","text":"<p>Coralogix recommends the default otel-integration chart settings for batch processors in all collectors:</p> <pre><code>  batch:\n    send_batch_size: 1024\n    send_batch_max_size: 2048\n    timeout: \"1s\"\n</code></pre> <p>This sizing ensures that the telemetry sent to the Coralogix backend is batched into larger requests, reducing networking overhead and improving performance.</p> <p>These settings impose a hard limit of 2048 units (spans, metrics, logs) on the batch size, balancing the recommended batch size and networking overhead.</p> <p>While you can adjust these settings to suit your requirements, considering the size limits enforced by Coralogix endpoints, currently set to a max of 10 MB after decompression, is essential.</p>"},{"location":"newoutput/opentelemetry/#limits-quotas","title":"Limits &amp; Quotas","text":"<ul> <li> <p>Coralogix places a hard limit of 10MB of data to our Otel endpoints, with a recommendation of 2MB.</p> </li> <li> <p>Metric names must be a maximum of 255 characters.</p> </li> <li> <p>Attribute keys for metric data must be a maximum of 255 characters.</p> </li> </ul> <p>Limits apply to single requests, regardless of timespan.</p>"},{"location":"newoutput/opentelemetry/#additional-resources","title":"Additional Resources","text":"GitHubOfficial RepositoryInstructional VideosIntegrate metrics into Coralogix using OpenTelemetry, Kubernetes &amp; HelmIntegrate logs into Coralogix using OpenTelemetry, Kubernetes &amp; HelmCapture Kubernetes logs, transform with Logs2Metrics and render with DataMapBlogsHow to Configure the OTel Community Demo App to Send Telemetry Data to CoralogixShip OpenTelemetry Data to Coralogix via Reverse Proxy (Caddy 2)"},{"location":"newoutput/opentelemetry/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/opentelemetry-collector-installation-ecs-fargate/","title":"OpenTelemetry using ECS Fargate","text":"<p>This tutorial demonstrates how to install and configure OpenTelemetry (OTel) Collector to send your traces on ECS Fargate to Coralogix.</p>"},{"location":"newoutput/opentelemetry-collector-installation-ecs-fargate/#installation-configuration","title":"Installation &amp; Configuration","text":"<p>1. In your AWS Console, go to AWS Systems Manager under Application Management.</p> <p>2. Click on Parameter Store.</p> <p>3. Create parameter. Specify the name of the new parameter <code>Coralogix-otel-col-config</code>.</p> <p></p> <p>4. Paste the OTel Collector configuration in the <code>value</code>.</p> <pre><code>extensions:\n\u00a0 health_check:\nreceivers:\n\u00a0 awsxray:\n\u00a0 \u00a0 endpoint: 0.0.0.0:2000\n\u00a0 \u00a0 transport: udp\n\u00a0 otlp:\n\u00a0 \u00a0 protocols:\n\u00a0 \u00a0 \u00a0 grpc:\n\u00a0 \u00a0 \u00a0 \u00a0 endpoint: 0.0.0.0:4317\n\u00a0 \u00a0 \u00a0 http:\n\u00a0 \u00a0 \u00a0 \u00a0 endpoint:\nprocessors:\n\u00a0 batch/traces:\n\u00a0 \u00a0 timeout: 5s\n\u00a0 \u00a0 send_batch_size: 256\n\u00a0 resourcedetection:\n\u00a0 \u00a0 detectors: [env, ec2, ecs]\n\u00a0 \u00a0 timeout: 5s\n\u00a0 \u00a0 override: true\nexporters:\n\u00a0 otlp/traces:\n\u00a0 \u00a0endpoint: &lt;CXOtelEndpoint&gt;\n\u00a0 \u00a0headers:\n\u00a0 \u00a0 \u00a0 Authorization: Bearer &lt;CXPrivateKey&gt;\n\u00a0 \u00a0 \u00a0 ApplicationName: \"&lt;CXApplicationName&gt;\"\n\u00a0 \u00a0 \u00a0 ApiName: \"&lt;CXSubsystem&gt;\"\n\u00a0 \u00a0 \u00a0 CX-Application-Name: \"&lt;CXApplicationName&gt;\"\n\u00a0 \u00a0 \u00a0 CX-Subsystem-Name: \"&lt;CXSubsystem&gt;\"\nservice:\n\u00a0 extensions: [health_check]\n\u00a0 pipelines:\n\u00a0 \u00a0 traces:\n\u00a0 \u00a0 \u00a0 receivers: [awsxray,otlp]\n\u00a0 \u00a0 \u00a0 processors: [resourcedetection,batch/traces]\n\u00a0 \u00a0 \u00a0 exporters: [otlp/traces]\n\n</code></pre> <p>5. Input the following environment variables:</p> <ul> <li> <p><code>CXDomain</code>: Coralogix\u00a0domain associated with your account</p> </li> <li> <p><code>`CXApplicationName`</code> &amp; <code>`CXSubsystem`</code>: ApplicationName and subsystem names</p> </li> <li> <p><code>CXPrivateKey</code>: Coralogix Send-Your-Data API key</p> </li> <li> <p><code>CXOtelEndpoint</code>: OpenTelemetry endpoint associated with your Coralogix domain</p> </li> </ul> <p></p> <p>6. Click Create Parameter.</p> <p>7. In your AWS Console , go to CloudFormation.</p> <p>8. Click Create Stack.</p> <p>9. Copy the following CloudFormation template to CloudFromation Designer. It installs the OTel Collector and defines the ECS task.</p> <pre><code>AWSTemplateFormatVersion: 2010-09-09\nDescription: 'Template to install AWS OTel Collector on ECS in Fargate mode'\nParameters:\n  IAMTaskRole:\n    Description: Task attached IAM role\n    Type: String\n    Default: CoralogixAWSOTelColTaskRoleECSFargate\n    ConstraintDescription: must be an existing IAM role which will be attached to EC2 instance.\n  IAMExecutionRole:\n    Description: Task Execution attached IAM role\n    Type: String\n    Default: CoralogixAWSOTelColExecutionRoleECSFargate\n    ConstraintDescription: must be an existing IAM role which will be attached to EC2 instance.\n  IAMPolicy:\n    Description: IAM Role attached IAM Policy\n    Type: String\n    Default: CoralogixAWSOTelColPolicyECSFargate\n    ConstraintDescription: Must be an existing IAM Managed Policy which will be attached to IAM Role.\n  ClusterName:\n    Type: String\n    Description: Enter the name of your ECS cluster from which you want to collect telemetry data\n  SecurityGroups:\n    Type: CommaDelimitedList\n    Description: The list of SecurityGroupIds in your Virtual Private Cloud (VPC)\n  Subnets:\n    Type: CommaDelimitedList\n    Description: The list of Subnets in your Virtual Private Cloud (VPC)\n  CoralogixAWSOTelColConfig:\n    Type: AWS::SSM::Parameter::Value&lt;String&gt;\n    Default: Coralogix-otel-col-config\n    Description: AWS SSM Parameter which contains OTel Collector config file\n  CoralogixReplicaServiceName:\n    Type: String\n    Default: Coralogix-aws-otel-col-svc-ecs-fargate\n    Description: ECS Service Name\nResources:\n  ECSTaskDefinition:\n    Type: 'AWS::ECS::TaskDefinition'\n    Properties:\n      Family: Coralogix-aws-otel-collector-fargate\n      TaskRoleArn: !Sub 'arn:aws:iam::${AWS::AccountId}:role/${IAMTaskRole}'\n      ExecutionRoleArn: !Sub 'arn:aws:iam::${AWS::AccountId}:role/${IAMExecutionRole}'\n      NetworkMode: awsvpc\n      ContainerDefinitions:\n        - Name: Coralogix-aws-otel-collector\n          Image: public.ecr.aws/aws-observability/aws-otel-collector:v0.25.0\n          LogConfiguration:\n            LogDriver: awslogs\n            Options:\n              awslogs-create-group: 'True'\n              awslogs-group: /ecs/aws-otel-collector\n              awslogs-region: !Ref 'AWS::Region'\n              awslogs-stream-prefix: ecs\n          Environment:\n            - Name: AOT_CONFIG_CONTENT\n              Value: !Ref CoralogixAWSOTelColConfig\n      RequiresCompatibilities:\n        - FARGATE\n      Cpu: 1024\n      Memory: 2048\n    DependsOn:\n      - ECSTaskRole\n      - ECSExecutionRole\n  ECSReplicaService:\n    Type: 'AWS::ECS::Service'\n    Properties:\n      TaskDefinition: !Ref ECSTaskDefinition\n      Cluster: !Ref ClusterName\n      LaunchType: FARGATE\n      SchedulingStrategy: REPLICA\n      DesiredCount: 1\n      ServiceName: !Ref CoralogixReplicaServiceName\n      NetworkConfiguration:\n        AwsvpcConfiguration:\n          AssignPublicIp: ENABLED\n          SecurityGroups: !Ref SecurityGroups\n          Subnets: !Ref Subnets\n  ECSTaskRole:\n    Type: 'AWS::IAM::Role'\n    Properties:\n      Description: Allows ECS tasks to call AWS services on your behalf.\n      AssumeRolePolicyDocument:\n        Version: 2012-10-17\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service: ecs-tasks.amazonaws.com\n            Action: 'sts:AssumeRole'\n      Policies:\n        - PolicyName: !Ref IAMPolicy\n          PolicyDocument:\n            Version: 2012-10-17\n            Statement:\n              - Effect: Allow\n                Action:\n                  - 'logs:PutLogEvents'\n                  - 'logs:CreateLogGroup'\n                  - 'logs:CreateLogStream'\n                  - 'logs:DescribeLogStreams'\n                  - 'logs:DescribeLogGroups'\n                  - 'xray:PutTraceSegments'\n                  - 'xray:PutTelemetryRecords'\n                  - 'xray:GetSamplingRules'\n                  - 'xray:GetSamplingTargets'\n                  - 'xray:GetSamplingStatisticSummaries'\n                  - 'ssm:GetParameters'\n                Resource: '*'\n      RoleName: !Ref IAMTaskRole\n  ECSExecutionRole:\n    Type: 'AWS::IAM::Role'\n    Properties:\n      Description: &gt;-\n        Allows ECS container agent makes calls to the Amazon ECS API on your\n        behalf.\n      AssumeRolePolicyDocument:\n        Version: 2012-10-17\n        Statement:\n          - Sid: ''\n            Effect: Allow\n            Principal:\n              Service: ecs-tasks.amazonaws.com\n            Action: 'sts:AssumeRole'\n      ManagedPolicyArns:\n        - 'arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy'\n        - 'arn:aws:iam::aws:policy/CloudWatchLogsFullAccess'\n        - 'arn:aws:iam::aws:policy/AmazonSSMReadOnlyAccess'\n      RoleName: !Ref IAMExecutionRole\n\n</code></pre> <p>10. Click Create Stack.</p> <p></p> <p></p> <p>11. Update ECS Cluster Name, Security Group Ids, Subnets in your VPC.</p> <p>12. Click Next.</p> <p></p> <p>12. Tick \u2018I acknowledge that AWS\u2026\u2019 and click Submit.</p>"},{"location":"newoutput/opentelemetry-collector-installation-ecs-fargate/#validation","title":"Validation","text":"<p>1. To validate that your stack was deployed, go to the CloudFormation Stacks console and check if the ECS-OTel-Collector-Task stack status is CREATE_COMPLETE.</p> <p>2. To validate that your deployment is properly running, go to the\u00a0ECS Console, select the proper region, and select the cluster you\u00a0used to deploy the AWS\u00a0OpenTelemetry\u00a0Collector. Navigate to the\u00a0Tasks tab and check if the task is running.</p> <p>3. To use OTEL to ship traces, click on the task and expand the\u00a0Containers\u00a0list. In the\u00a0Network &gt; Private IP\u00a0or\u00a0Public IP\u00a0sections, send telemetry data to these addresses on port 4317.</p>"},{"location":"newoutput/opentelemetry-collector-installation-ecs-fargate/#limits-quotas","title":"Limits &amp; Quotas","text":"<p>Coralogix places a hard limit of 10MB of data to our Otel endpoints, with a recommendation of 2MB.</p>"},{"location":"newoutput/opentelemetry-collector-installation-ecs-fargate/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/opentelemetry-custom-logs/","title":"OpenTelemetry Custom Logs","text":"<p>Send your custom logs to Coralogix using our OpenTelemetry-compatible endpoint.</p>"},{"location":"newoutput/opentelemetry-custom-logs/#overview","title":"Overview","text":"<p>Custom logs can be delivered directly to the Coralogix OpenTelemetry-compatible endpoint using any gRPC client or OpenTelemetry SDKs.</p> <p>The examples below guide you using gRPCurl and OpenTelemetry Java SDK.</p>"},{"location":"newoutput/opentelemetry-custom-logs/#prerequisites","title":"Prerequisites","text":"<p>If you are sending us your data using gRPCurl, you are required to have Git and gRPCurl installed.</p>"},{"location":"newoutput/opentelemetry-custom-logs/#data-model","title":"Data Model","text":"<p>The custom logs API implementation is based on the OpenTelemetry logging specification. This ensures that our logging implementation adheres to industry best practices and can seamlessly integrate with other components and tools in the OpenTelemetry ecosystem.</p>"},{"location":"newoutput/opentelemetry-custom-logs/#example-log","title":"Example Log","text":"<pre><code>{\n  \"resource_logs\": [\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"cx.application.name\",\n            \"value\": {\n              \"string_value\": \"my-test-application\"\n            }\n          },\n          {\n            \"key\": \"cx.subsystem.name\",\n            \"value\": {\n              \"string_value\": \"my-test-subsystem\"\n            }\n          }\n        ]\n      },\n      \"scope_logs\": [\n        {\n          \"scope\": {\n            \"name\": \"test\"\n          },\n          \"log_records\": [\n            {\n              \"time_unix_nano\": \"1665989944490035000\",\n              \"severity_number\": \"SEVERITY_NUMBER_WARN\",\n              \"severity_text\": \"WARN\",\n              \"body\": {\n                \"string_value\": \"Test log message\"\n              }\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n\n</code></pre>"},{"location":"newoutput/opentelemetry-custom-logs/#sending-data-with-grpcurl","title":"Sending Data With gRPCurl","text":"<p>gRPC is a modern way of calling APIs on top of HTTP/2. Similar to cURL, gRPCurl is a command-line tool used to communicate with gRPC services.</p> <p>Coralogix currently supports gRPC for its custom logs endpoint. REST APIs will be added in the future.</p> <p>Assuming the example in the data model is saved as <code>logs.json</code>, use the following command to send your data to Coralogix:</p> <pre><code># Clone OpenTelemetry protobuf definitions\ngit clone &lt;https://github.com/open-telemetry/opentelemetry-proto.git&gt;\n# Send logs to Coralogix \ngrpcurl -v -d @ \\\\\n  -rpc-header 'Authorization: Bearer &lt;send-your-data-api-key&gt;' \\\\\n  -proto opentelemetry-proto/opentelemetry/proto/collector/logs/v1/logs_service.proto \\\\\n  -import-path opentelemetry-proto \\\\\n  &lt;open-telemetry-endpoint&gt; \\\\\n  opentelemetry.proto.collector.logs.v1.LogsService/Export \\\\\n  &lt; logs.json\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>For <code>&lt;open-telemetry-endpoint&gt;</code>, input the Coralogix OpenTelemetry endpoint associated with your Coralogix domain.</p> </li> <li> <p>For the <code>&lt;send-your-data-api-key&gt;</code>, input your Coralogix Send-Your-Data API key.</p> </li> <li> <p>Set the <code>time_unix_nano</code> in the <code>logs.json</code> to a timestamp that is within the last 24 hours.</p> </li> </ul>"},{"location":"newoutput/opentelemetry-custom-logs/#sending-data-using-the-opentelemetry-java-sdk","title":"Sending Data Using the OpenTelemetry Java SDK","text":"<p>The example below guides you using OpenTelemetry Java SDK to send your custom logs to Coralogix. Others SDKs may also be used.</p> <p>STEP 1. Add to your maven pom.xml the following libraries:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n    &lt;artifactId&gt;opentelemetry-sdk-logs&lt;/artifactId&gt;\n  &lt;version&gt;&lt;!-- put a recent version of opentelemetry sdk here --&gt;&lt;version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n    &lt;artifactId&gt;opentelemetry-exporter-otlp-logs&lt;/artifactId&gt;\n  &lt;version&gt;&lt;!-- put a recent version of opentelemetry sdk here --&gt;&lt;version&gt;\n&lt;/dependency&gt;\n\n</code></pre> <p>STEP 2. Use the following code snippet to create and report a span:</p> <pre><code>SdkLoggerProvider loggerProvider =\n      SdkLoggerProvider.builder()\n        .addLogRecordProcessor(BatchLogRecordProcessor.builder(\n          OtlpGrpcLogRecordExporter.builder()\n            .setEndpoint(\"https://&lt;open-telemetry-endpoint&gt;\")\n            .addHeader(\"Authorization\", \"Bearer &lt;send-your-data-api-key&gt;\")\n            .build()\n        ).build())\n        .setResource(Resource.create(Attributes.of(\n          AttributeKey.stringKey(\"cx.application.name\"), \"my-test-application\",\n          AttributeKey.stringKey(\"cx.subsystem.name\"), \"my-test-subsystem\")))\n        .build();\n\n    Logger logger = loggerProvider.loggerBuilder(\"test\").build();\n\n    logger.logRecordBuilder()\n      .setSeverity(Severity.WARN)\n      .setSeverityText(\"WARN\")\n      .setBody(\"Test log message\")\n      .emit();\n\n    loggerProvider.forceFlush();\n\n</code></pre>"},{"location":"newoutput/opentelemetry-custom-logs/#limits-quotas","title":"Limits &amp; Quotas","text":"<p>Coralogix places a hard limit of 10MB of data to our OpenTelemetry endpoints, with a recommendation of 2MB.</p> <p>Limits apply to single requests, regardless of timespan.</p>"},{"location":"newoutput/opentelemetry-custom-logs/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Endpoints"},{"location":"newoutput/opentelemetry-custom-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/opentelemetry-custom-metrics/","title":"OpenTelemetry Custom Metrics","text":"<p>Coralogix provides a scalable Prometheus-compatible managed service for time-series data. Employ our custom metric endpoint, including serverless computing and quick cURL-like calls, to send counters, gauges, and histograms to Coralogix.</p>"},{"location":"newoutput/opentelemetry-custom-metrics/#overview","title":"Overview","text":"<p>Coralogix supports ingesting metrics in multiple ways. Our most common integrations are Prometheus &amp; OpenTelemetry, as well as metric-specific integrations such as CloudWatch metrics and AWS Kinesis Firehose. View a full list of available integrations here.</p> <p>This tutorial presents a series of use cases employing our custom metric endpoint, referred to as the OpenTelemetry endpoint, to send your data to Coralogix. The examples below employ gRPCurl and OpenTelemetry Java SDK.</p> <p>Coralogix metrics employs the Prometheus data model, wherein metrics sent can be in the form of counters, gauges, and histograms.</p>"},{"location":"newoutput/opentelemetry-custom-metrics/#prerequisites","title":"Prerequisites","text":"<p>If you are sending us your data using gRPCurl, you are required to have Git and gRPCurl installed.</p>"},{"location":"newoutput/opentelemetry-custom-metrics/#data-model","title":"Data Model","text":"<p>The custom logs API implementation is based on the OpenTelemetry metric specification. This ensures that our logging implementation adheres to industry best practices and can seamlessly integrate with other components and tools in the OpenTelemetry ecosystem.</p>"},{"location":"newoutput/opentelemetry-custom-metrics/#counter-gauge-example","title":"Counter &amp; Gauge Example","text":"<pre><code>{\n  \"resource_metrics\": {\n    \"resource\": {\n      \"attributes\": [\n        {\n          \"key\": \"cx.application.name\",\n          \"value\": {\n            \"string_value\": \"my-test-application\"\n          }\n        },\n        {\n          \"key\": \"cx.subsystem.name\",\n          \"value\": {\n            \"string_value\": \"my-test-subsystem\"\n          }\n        },\n        {\n          \"key\": \"service.name\",\n          \"value\": {\n            \"string_value\": \"my-test-service\"\n          }\n        }\n      ]\n    },\n    \"scope_metrics\": {\n      \"metrics\": [{\n        \"name\": \"grpc_sample_gauge1\",\n        \"gauge\": {\n          \"data_points\": [{\n            \"as_double\": 0.8,\n            \"start_time_unix_nano\": 1657079957000000000,\n            \"time_unix_nano\": 1657079957000000000\n          }]\n        }\n      },{\n        \"name\": \"grpc_sample_counter1\",\n        \"gauge\": {\n          \"data_points\": [{\n            \"as_int\": 100,\n            \"start_time_unix_nano\": 1657079957000000000,\n            \"time_unix_nano\": 1657079957000000000\n          }]\n        }\n      }]\n    }\n  }\n}\n\n</code></pre> <p>Notes:</p> <ul> <li>Currently both timestamps as well as <code>service.name</code> are mandatory.</li> </ul>"},{"location":"newoutput/opentelemetry-custom-metrics/#sending-data-with-grpcurl","title":"Sending Data With gRPCurl","text":"<p>gRPC is a modern way of calling APIs on top of HTTP/2. Similar to cURL, gRPCurl is a command-line tool used to communicate with gRPC services.</p> <p>Coralogix currently supports gRPC for its custom metrics endpoint.</p> <p>Assuming the example in the data model is saved as <code>metrics.json</code>, use the following command to send your data to Coralogix:</p> <pre><code># Clone OpenTelemetry protobuf definitions\ngit clone &lt;https://github.com/open-telemetry/opentelemetry-proto.git&gt;\n# Send metrics to Coralogix \ngrpcurl -v -d @ \\\\\n  -rpc-header 'Authorization: Bearer &lt;send-your-data-api-key&gt;' \\\\\n  -proto opentelemetry-proto/opentelemetry/proto/collector/metrics/v1/metrics_service.proto \\\\\n  -import-path opentelemetry-proto \\\\\n  &lt;open-telemetry-endpoint&gt; \\\\\n  opentelemetry.proto.collector.metrics.v1.MetricsService/Export \\\\\n  &lt; metrics.json\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>For <code>&lt;open-telemetry-endpoint&gt;</code>, input the Coralogix OpenTelemetry endpoint associated with your Coralogix domain.</p> </li> <li> <p>For the <code>&lt;send-your-data-api-key&gt;</code>, input your Coralogix Send-Your-Data API key.</p> </li> <li> <p>Set the <code>start_time_unix_nano</code> and the <code>time_unix_nano</code> in the <code>metrics.json</code> to a timestamp that is within the last 24 hours.</p> </li> </ul>"},{"location":"newoutput/opentelemetry-custom-metrics/#sending-data-using-the-opentelemetry-java-sdk","title":"Sending Data Using the OpenTelemetry Java SDK","text":"<p>The example below guides you using OpenTelemetry Java SDK to send your custom traces to Coralogix. Others SDKs may also be used.</p> <p>STEP 1. Add to your maven pom.xml the following libraries:</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n  &lt;artifactId&gt;opentelemetry-sdk-metrics&lt;/artifactId&gt;\n  &lt;version&gt;&lt;!-- put a recent version of opentelemetry sdk here --&gt;&lt;version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n  &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n  &lt;artifactId&gt;opentelemetry-exporter-otlp&lt;/artifactId&gt;\n  &lt;version&gt;&lt;!-- put a recent version of opentelemetry sdk here --&gt;&lt;version&gt;\n&lt;/dependency&gt;\n\n</code></pre> <p>STEP 2. Use this code snippet to generate a counter and a gauge:</p> <pre><code>SdkMeterProvider meterProvider =\n      SdkMeterProvider.builder()\n        .registerMetricReader(\n          PeriodicMetricReader.builder(\n            OtlpGrpcMetricExporter.builder()\n              .setEndpoint(\"https://&lt;open-telemetry-endpoint&gt;\")\n              .addHeader(\"Authorization\", \"Bearer &lt;send-your-data-api-key&gt;\")\n              .build()\n          ).build()\n        )\n        .setResource(Resource.create(Attributes.of(\n          ResourceAttributes.SERVICE_NAME, \"my-test-service\",\n          AttributeKey.stringKey(\"cx.application.name\"), \"my-test-application\",\n          AttributeKey.stringKey(\"cx.subsystem.name\"), \"my-test-subsystem\")))\n        .build();\n\n    Meter meter = meterProvider.meterBuilder(\"test\").build();\n\n    LongCounter counter = meter\n      .counterBuilder(\"otlp_test_counter1\")\n      .setDescription(\"Processed jobs\")\n      .build();\n    counter.add(100);\n\n    meter\n      .gaugeBuilder(\"otlp_test_gauge1\")\n      .buildWithCallback(measurement -&gt; {\n        measurement.record(0.8);\n      });\n    meterProvider.forceFlush();\n\n</code></pre> <p>Notes:</p> <ul> <li>Currently the <code>service.name</code> attribute is mandatory on each metric.</li> </ul>"},{"location":"newoutput/opentelemetry-custom-metrics/#limits-quotas","title":"Limits &amp; Quotas","text":"<ul> <li> <p>Coralogix places a hard limit of 10MB of data to our OpenTelemetry endpoints, with a recommendation of 2MB.</p> </li> <li> <p>Metric names must be a maximum of 255 characters.</p> </li> <li> <p>Attribute keys for metric data must be a maximum of 255 characters.</p> </li> </ul> <p>Limits apply to single requests, regardless of timespan.</p>"},{"location":"newoutput/opentelemetry-custom-metrics/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix EndpointsExternalGitHub"},{"location":"newoutput/opentelemetry-custom-metrics/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/opentelemetry-custom-traces/","title":"OpenTelemetry Custom Traces","text":"<p>Send your custom traces to Coralogix using our OpenTelemetry-compatible endpoint.</p>"},{"location":"newoutput/opentelemetry-custom-traces/#overview","title":"Overview","text":"<p>Custom traces can be delivered directly to the Coralogix OpenTelemetry-compatible endpoint using any gRPC client or OpenTelemetry SDKs.</p> <p>The examples below guide you using gRPCurl and OpenTelemetry Java SDK. This is an alternative to using the OpenTelemetry Collector.</p>"},{"location":"newoutput/opentelemetry-custom-traces/#prerequisites","title":"Prerequisites","text":"<p>If you are sending us your data using gRPCurl, you are required to have Git and gRPCurl installed.</p>"},{"location":"newoutput/opentelemetry-custom-traces/#data-model","title":"Data Model","text":"<p>The custom metric API implementation is based on the OpenTelemetry tracing specification. This ensures that our tracing implementation adheres to industry best practices and can seamlessly integrate with other components and tools in the OpenTelemetry ecosystem.</p>"},{"location":"newoutput/opentelemetry-custom-traces/#example-tracing-span","title":"Example Tracing Span","text":"<pre><code>{\n  \"resource_spans\": [\n    {\n      \"resource\": {\n        \"attributes\": [\n          {\n            \"key\": \"cx.application.name\",\n            \"value\": {\n              \"string_value\": \"my-test-application\"\n            }\n          },\n          {\n            \"key\": \"cx.subsystem.name\",\n            \"value\": {\n              \"string_value\": \"my-test-subsystem\"\n            }\n          },\n          {\n            \"key\": \"service.name\",\n            \"value\": {\n              \"string_value\": \"my-test-service\"\n            }\n          }\n        ]\n      },\n      \"scope_spans\": [\n        {\n          \"scope\": {\n            \"name\": \"test\"\n          },\n          \"spans\": [\n            {\n              \"trace_id\": \"d0XALf50O6rdjlMxreTJsA==\",\n              \"span_id\": \"YolJ2eFjJCs=\",\n              \"name\": \"test-span\",\n              \"kind\": \"SPAN_KIND_INTERNAL\",\n              \"start_time_unix_nano\": \"1665753217736722000\",\n              \"end_time_unix_nano\": \"1665753217737156416\",\n              \"status\": {}\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n\n</code></pre>"},{"location":"newoutput/opentelemetry-custom-traces/#sending-data-with-grpcurl","title":"Sending Data with gRPCurl","text":"<p>gRPC is a modern way of calling APIs on top of HTTP/2. Similar to cURL, gRPCurl is a command-line tool used to communicate with gRPC services.</p> <p>Coralogix currently supports gRPC for its custom traces endpoint. REST APIs will be added in the future.</p> <p>Assuming the example in the data model is saved as <code>traces.json</code>, use the following command to send your data to Coralogix:</p> <pre><code># Clone OpenTelemetry protobuf definitions\ngit clone &lt;https://github.com/open-telemetry/opentelemetry-proto.git&gt;\n# Send traces to Coralogix \ngrpcurl -v -d @ \\\\\n  -rpc-header 'Authorization: Bearer &lt;send-your-data-api-key&gt;' \\\\\n  -proto opentelemetry-proto/opentelemetry/proto/collector/trace/v1/trace_service.proto \\\\\n  -import-path opentelemetry-proto \\\\\n  &lt;open-telemetry-endpoint&gt; \\\\\n  opentelemetry.proto.collector.trace.v1.TraceService/Export \\\\\n  &lt; traces.json\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>For <code>&lt;open-telemetry-endpoint&gt;</code>, input the Coralogix OpenTelemetry endpoint associated with your Coralogix domain.</p> </li> <li> <p>For the <code>&lt;send-your-data-api-key&gt;</code>, input your Coralogix Send-Your-Data API key.</p> </li> <li> <p>Set the <code>start_time_unix_nano</code> and the <code>end_time_unix_nano</code> in the <code>traces.json</code> to a timestamp that is within the last 24 hours.</p> </li> </ul>"},{"location":"newoutput/opentelemetry-custom-traces/#sending-data-using-the-opentelemetry-java-sdk","title":"Sending Data Using the OpenTelemetry Java SDK","text":"<p>The example below guides you using OpenTelemetry Java SDK to send your custom traces to Coralogix. Others SDKs may also be used.</p> <p>STEP 1. Add the following libraries to your maven pom.xml:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n    &lt;artifactId&gt;opentelemetry-sdk-trace&lt;/artifactId&gt;\n  &lt;version&gt;&lt;!-- put a recent version of opentelemetry sdk here --&gt;&lt;version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n    &lt;artifactId&gt;opentelemetry-exporter-otlp&lt;/artifactId&gt;\n  &lt;version&gt;&lt;!-- put a recent version of opentelemetry sdk her--&gt;&lt;version&gt;\n&lt;/dependency&gt;\n\n</code></pre> <p>STEP 2. Use this code snippet to create and report a span:</p> <pre><code>SdkTracerProvider traceProvider =\n  SdkTracerProvider.builder()\n    .addSpanProcessor(BatchSpanProcessor.builder(\n      OtlpGrpcSpanExporter.builder()\n        .setEndpoint(\"https://&lt;open-telemetry-endpoint&gt;\")\n        .addHeader(\"Authorization\", \"Bearer &lt;send-your-data-api-key&gt;\")\n        .build()\n    ).build())\n    .setResource(Resource.create(Attributes.of(\n            ResourceAttributes.SERVICE_NAME, \"my-test-service\",\n            AttributeKey.stringKey(\"cx.application.name\"), \"my-test-application\",\n      AttributeKey.stringKey(\"cx.subsystem.name\"), \"my-test-subsystem\"\n        )))\n    .build();\n\nTracer tracer = traceProvider.tracerBuilder(\"test\").build();\n\nSpan span = tracer.spanBuilder(\"test-span\")\n  .setSpanKind(SpanKind.INTERNAL)\n  .startSpan();\nspan.end();\n\ntraceProvider.forceFlush();\n\n</code></pre>"},{"location":"newoutput/opentelemetry-custom-traces/#limits-quotas","title":"Limits &amp; Quotas","text":"<p>Coralogix places a hard limit of 10MB of data to our OpenTelemetry endpoints, with a recommendation of 2MB.</p> <p>Limits apply to single requests, regardless of timespan.</p>"},{"location":"newoutput/opentelemetry-custom-traces/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix EndpointsExternalGitHub"},{"location":"newoutput/opentelemetry-custom-traces/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/opentelemetry-using-docker/","title":"OpenTelemetry using Docker","text":"<p>This tutorial demonstrates configuring OpenTelemetry (OTEL) Collector to send your logs and metrics to Coralogix using Docker.</p>"},{"location":"newoutput/opentelemetry-using-docker/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed</li> </ul>"},{"location":"newoutput/opentelemetry-using-docker/#configuration","title":"Configuration","text":"<p>STEP 1. Create a configuration file. Copy this template file and save it as <code>config.yaml</code>.</p> <pre><code>receivers:\n  filelog:\n    start_at: beginning\n    include:\n      - /example.log\n    include_file_path: true\n    multiline: {line_start_pattern: \"\\\\n\"}\n    hostmetrics:\n        collection_interval: 30s\n        scrapers:\n          cpu:\n          memory:\nexporters:\n  coralogix:\n    domain: \"Domain\"\n    private_key: \"Private key\"\n    application_name: \"Application Name\"\n    subsystem_name: \"Subsystem Name\"\n    timeout: 30s\n\nservice:\n  pipelines:\n    logs:\n      receivers: [ filelog ]\n      exporters: [ coralogix ]\n    metrics:\n      receivers: [ hostmetrics ]\n      exporters: [ coralogix ]\n\n</code></pre> <p>Provide the following variables.</p> Variable Description Private Key Your Coralogix\u00a0Send-Your-Data API key Application Name The name of your\u00a0application, as it will appear in your Coralogix dashboard. For example, a company named\u00a0SuperData\u00a0might insert the\u00a0SuperData\u00a0string parameter. If SuperData wants to debug its test environment, it might use\u00a0SuperData\u2013Test. Subsystem Name The name of your\u00a0subsystem, as it will appear in your Coralogix dashboard. Applications often have multiple subsystems (ie. Backend Servers, Middleware, Frontend Servers, etc.). In order to help you examine the data you need, inserting the subsystem parameter is vital. Domain Your Coralogix domain <p>STEP 2. Save this log file as <code>example.log</code>.</p> <pre><code>2023-06-19 05:20:50 ERROR This is a test error message\n2023-06-20 12:50:00 DEBUG This is a test debug message\n2023-06-21 12:34:56 INFO This is a test info message\n\n</code></pre> <p>STEP 3. Pull a docker image and run the collector in a container. To load your custom configuration\u00a0<code>config.yaml</code>\u00a0and the log file <code>example.log</code> from your current working directory, mount the files as a volume into the container. Find out more here.</p> <pre><code>docker pull otel/opentelemetry-collector-contrib\ndocker run -d -v \"$(pwd)\"/config.yaml:/etc/otelcol-contrib/config.yaml -v \"$(pwd)\"/example.log:/example.log otel/opentelemetry-collector-contrib\n</code></pre>"},{"location":"newoutput/opentelemetry-using-docker/#validation","title":"Validation","text":"<p>Validate your configuration.</p>"},{"location":"newoutput/opentelemetry-using-docker/#logs","title":"Logs","text":"<p>In your Coralogix navigation pane, click LiveTrail &gt; Start to view your logs.</p> <p></p> <p></p>"},{"location":"newoutput/opentelemetry-using-docker/#metrics","title":"Metrics","text":"<p>STEP 1. Navigate to hosted Grafana view.</p> <p>STEP 2. In the left-hand panel, click Explore &gt; Metrics browser. Select the metrics that you would like to see.</p> <p></p>"},{"location":"newoutput/opentelemetry-using-docker/#additional-resources","title":"Additional Resources","text":"DocumentationOpenTelemetry"},{"location":"newoutput/opentelemetry-using-docker/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/opentelemetry-using-ecs-ec2/","title":"AWS ECS-EC2 using OpenTelemetry","text":"<p>This tutorial demonstrates how to deploy OpenTelemetry to AWS ECS-EC2 to facilitate the collection of logs, metrics, and traces.</p> <p>Telemetry is sent to Coralogix via the Coralogix Exporter, which allows for the use of enrichments such as dynamic <code>application</code> or <code>subsystem</code> name, defined using <code>application_name_attributes</code> and <code>subsystem_name_attributes</code>, respectively. Find out more here.</p>"},{"location":"newoutput/opentelemetry-using-ecs-ec2/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Coralogix account</p> </li> <li> <p>AWS account\u00a0with\u00a0AWS credentials configured</p> </li> <li> <p>ecs-cli</p> </li> <li> <p>aws-cli</p> </li> <li> <p>Coralogix distribution for OpenTelemetry (\u201dCDOT\u201d) ECS Service daemon, deployed using CloudFormation template or Terraform module</p> </li> </ul>"},{"location":"newoutput/opentelemetry-using-ecs-ec2/#image","title":"Image","text":"<p>This implementation utilises the Coralogix Opentelemetry Collector image coralogixrepo/coralogix-otel-collector. This image is an enhanced version of the official OpenTelemetry Contrib distribution, featuring custom components and extended configuration loading options, including environment variable support.</p> <p>Note the latest tag on this image is not updated, you must explicitly select a version when pulling. Tags can be found here.</p> <p>The image configuration utilizes the\u00a0otlp receiver\u00a0for both\u00a0HTTP (on 4318)\u00a0and\u00a0GRPC (on 4317). Data can be sent using either endpoint.</p>"},{"location":"newoutput/opentelemetry-using-ecs-ec2/#opentelemetry-configuration","title":"OpenTelemetry Configuration","text":"<p>The OpenTelemetry configuration for the agent is stored in a Base64 encoded environment variable and applied at runtime. This allows you to dynamically pass any configuration values you choose as a parameter to CloudFormation.</p> <p>The following configuration files work directly with the\u00a0coralogixrepo/coralogix-otel-collector\u00a0docker image for ECS:</p> <ul> <li> <p>logs</p> </li> <li> <p>metrics &amp; traces</p> </li> </ul> <p>Create other configurations by combining logs, metrics and/or traces.</p>"},{"location":"newoutput/opentelemetry-using-ecs-ec2/#deploy-a-new-ecs-cluster","title":"Deploy a New ECS Cluster","text":"<p>If you already have an existing ECS cluster, skip this step.</p> <p>Deploy a new cluster:</p> <pre><code>ecs-cli up --region &lt;region&gt; --keypair &lt;your-key-pair&gt; --cluster &lt;cluster-name&gt; --size &lt;no. of instances&gt; --capability-iam \n</code></pre> <p>Notes:</p> <ul> <li>The\u00a0<code>--keypair</code>\u00a0flag is not mandatory. However, if not supplied, you cannot connect to any of the instances in the cluster via SSH. Create a key pair using the command below:</li> </ul> <pre><code>aws ec2 create-key-pair --key-name MyKeyPair --query 'KeyMaterial' --output text &gt; MyKeyPair.pem\n</code></pre> <ul> <li> <p>The\u00a0<code>ecs-cli up</code>\u00a0command will leverage CloudFormation to create an ECS cluster.</p> </li> <li> <p>Default values will be used to create and configure a VPC and Subnets. These and other values can be controlled from\u00a0<code>ecs-cli</code>\u00a0using the following command:</p> </li> </ul> <pre><code>ecs-cli up --help\n</code></pre>"},{"location":"newoutput/opentelemetry-using-ecs-ec2/#deploy-otel-agent-ecs-task-definition-service","title":"Deploy OTEL Agent ECS Task Definition &amp; Service","text":"<p>Once an ECS cluster has been deployed, deploy a task definition to be used by ECS to create an ECS service to run OpenTelemetry.</p> <ul> <li> <p>An AWS\u00a0ECS task definition\u00a0serves as template defining a container configuration.</p> </li> <li> <p>An\u00a0ECS service\u00a0is a configuration item that defines and orchestrates how a task definition should be run.</p> </li> </ul> <p>Option 1: CloudFormation template</p> <ul> <li> <p>Deploy this\u00a0CloudFormation template, with the necessary parameters provided:</p> </li> <li> <p>CDOTImageVersion:\u00a0The Coralogix Open Telemetry Distribution Image Version/Tag. Take the most recent tag from\u00a0here if this is the first deployment.</p> </li> <li> <p>ClusterName: Name of an\u00a0existing\u00a0ECS Cluster</p> </li> <li> <p>CoralogixRegion:\u00a0Coralogix Region\u00a0associated with your Coralogix account</p> </li> <li> <p>ApplicationName\u00a0&amp;\u00a0SubsystemName:\u00a0Application and subsystem names\u00a0as they will appear in your Coralogix dashboard</p> </li> <li> <p>PrivateKey: Your\u00a0Coralogix Send-Your-Data API key</p> </li> </ul> <p>Once the template is deployed successfully, verify that the container is running:</p> <pre><code>ecs-cli ps --region &lt;region&gt; -c &lt;cluster name&gt;\n</code></pre> <p>Option 2: Terraform module</p> <ul> <li>This ECS EC2 Open Telemetry Agent Terraform module sets up the OTEL Agent as an ECS Daemon Service on a specified ECS cluster. See readme for usage instructions.</li> </ul>"},{"location":"newoutput/opentelemetry-using-ecs-ec2/#configure-the-application-container-with-the-location-of-the-otel-agent","title":"Configure the Application Container with the Location of the OTEL Agent","text":"<p>The Coralogix OTEL Collector is typically deployed as a Daemon Service Agent on each ECS container instance / EC2 instance. The OTEL Agent Task is connected using host networking, so it is accessible via the primary private IP address of the EC2 instance.</p> <p>Note: The steps in this section will not apply to Fargate Tasks. These will connect to an OTEL Collector Gateway service deployed in a Gateway architecture, or an OTEL \u201csidecar\u201d container. Daemon deployments are not supported in Fargate. Use a suitable OTEL service discovery approach.</p> <p>For ECS Tasks running on EC2, Tasks that are OTEL-instrumented will need to discover the IP address of an OTEL Agent to connect with. The discovery approach will differ based on which AWS ECS Task networking modes your ECS Tasks utilize. In each case, set the <code>[OTEL_EXPORTER_OTLP_ENDPOINT</code> environment variable](https://opentelemetry.io/docs/concepts/sdk-configuration/otlp-exporter-configuration/) to the address of the OTEL Agent.</p> <p>Your ECS Task should be configured to connect to the OTEL Collector daemon task listening at the primary IP of the EC2 host.</p> <ul> <li>Method 1: from EC2 instance metadata v1. One straightforward, simplest, and recommended way is to extract the primary IP of the EC2 host from the EC2 instance metadata v1 endpoint. Here we specify http protocol and gRPC protocol at port 4317. Adjust if required.</li> </ul> <pre><code>export OTEL_EXPORTER_OTLP_ENDPOINT=\"http://\"$( curl -s http://169.254.169.254/latest/meta-data/local-ipv4 )\":4317\"\n</code></pre> <ul> <li>Method 2: from EC2 instance metadata v2. As a variation, secure the same call with the EC2 instance metadata v2 security token.</li> </ul> <pre><code>TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"` &amp;&amp; curl -H \"X-aws-ec2-metadata-token: $TOKEN\" -v http://169.254.169.254/latest/meta-data/local-ipv4\n</code></pre> <ul> <li>Method 3: from ECS container metadata file. Alternately, if you have the ECS container metadata file enabled, the host private IP can be parsed from the file:</li> </ul> <pre><code>cat $ECS_CONTAINER_METADATA_FILE | jq -r .HostPrivateIPv4Address\n</code></pre> <p>Set variables during the application container entryPoint</p> <p>Update the application container to set <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> at startup.</p> <p>One approach is to override the ECS Task Definition\u2019s entryPoint. Prepend setting variables prior to calling the original application command. E.g.:</p> <pre><code>\"entryPoint\": [\n  \"sh\",\n  \"-c\",\n  \"export OTEL_EXPORTER_OTLP_ENDPOINT='http://'$( curl -s http://169.254.169.254/latest/meta-data/local-ipv4 )':4317'; &lt;Actual Startup Command&gt;\"\n]\n</code></pre> <p>Other approaches could include embedding commands in the Entrypoint script file itself, calling out to a helper script, calling an intermediate entry point wrapper script, etc. Given the wide variety of container configuration options, it is left to the customer to decide on the optimal ways to script and set environment variables.</p>"},{"location":"newoutput/opentelemetry-using-ecs-ec2/#configure-the-application-container-to-send-identifying-resource-attributes","title":"Configure the Application Container to Send Identifying Resource Attributes","text":"<p>Applications that are instrumented for OpenTelemetry, can mark their telemetry by specifying attribute name/value pairs in the <code>OTEL_RESOURCE_ATTRIBUTES</code> environment variable.</p> <p>If you decide on Container ID as the resource attribute identifier for telemetry, obtain container ID from ECS Container Metadata endpoint.</p> <pre><code>#!/bin/bash\n\n# Must run within an ECS Docker container\n# Requires jq cli tool\n\n# get container ID\ncontainerID=$(curl ${ECS_CONTAINER_METADATA_URI_V4} | jq '.DockerId' -r)\n\n# set env variable for resource attributes\nexport OTEL_RESOURCE_ATTRIBUTES=\"containerID=${containerID},$OTEL_RESOURCE_ATTRIBUTES\"\n\n</code></pre> <p>Like <code>OTEL_EXPORTER_OTLP_ENDPOINT</code>, the <code>OTEL_RESOURCE_ATTRIBUTES</code> environment variable should be set upon container startup.</p>"},{"location":"newoutput/opentelemetry-using-ecs-ec2/#additional-resources","title":"Additional Resources","text":"ExternalGitHub RepoFeaturesCoralogix APM FeaturesAPM using Amazon EC2"},{"location":"newoutput/opentelemetry-using-ecs-ec2/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/opentelemetry-using-kubernetes/","title":"OpenTelemetry Agent using Kubernetes","text":"<p>This guide shows you how to run the\u00a0v0.76.3\u00a0release of OpenTelemetry Collector in Kubernetes while deploying it in\u00a0<code>mode: daemonset</code>\u00a0to export your data to Coralogix. It assumes that you have already\u00a0instrumented your application\u00a0with OTel SDKs and set up a\u00a0receiver\u00a0for your data.</p> <p>Once configured, enjoy Coralogix\u00a0APM\u00a0features when using OpenTelemetry collector with a Kubernetes processor.</p>"},{"location":"newoutput/opentelemetry-using-kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Coralogix account</p> </li> <li> <p>Kubernetes installed, with the command-line tool\u00a0kubectl</p> </li> <li> <p>Helm installed and configured</p> </li> </ul>"},{"location":"newoutput/opentelemetry-using-kubernetes/#setup","title":"Setup","text":""},{"location":"newoutput/opentelemetry-using-kubernetes/#installation","title":"Installation","text":"<p>Coralogix has an\u00a0exporter\u00a0available for the OpenTelemetry Collector which allows you to forward trace and metric data from OpenTelemetry SDKs to Coralogix. The following section shows you how to easily install the exporter by adding it to your\u00a0OpenTelemetry Collector configuration.</p> <p>STEP 1: Install the Coralogix OpenTelemetry Agent Helm Chart Repository</p> <p>Open a new terminal and install the Coralogix OpenTelemetry Agent helm chart repository:</p> <pre><code>helm repo add coralogix-charts-virtual https://cgx.jfrog.io/artifactory/coralogix-charts-virtual\n\n</code></pre> <pre><code>helm repo update\n\n</code></pre> <p>STEP 2: Create a Secret to Protect your Private Key</p> <p>Create a\u00a0<code>secret</code>\u00a0with your Coralogix Send-Your-Data API key\u00a0called\u00a0<code>coralogix-keys</code> with the value\u00a0<code>PRIVATE_KEY</code>.</p> <p>Take this step in order to ensure that your private key remains protected and unexposed. While other methods exist for protecting this private information, we recommend creating a secret in this manner while running on a Kubernetes cluster.</p> <p>Your private key, as well as the Helm chart, should be saved in the same namespace.</p> <pre><code>export PRIVATE_KEY=&lt;Send-Your-Data API key&gt;\n\n</code></pre> <pre><code>export NAMESPACE=&lt;namespace&gt;\n\n</code></pre> <pre><code>kubectl create secret generic coralogix-keys -n $NAMESPACE --from-literal=PRIVATE_KEY=$PRIVATE_KEY\n\n</code></pre> <p>Hint: If you are interested in seeing the details of your\u00a0secret\u00a0called\u00a0<code>coralogix-keys</code>, run:</p> <pre><code>kubectl get secret coralogix-keys -o yaml -n $NAMESPACE\n\n</code></pre> <p>The created secret should look like this, where <code>PRIVATE_KEY</code> is the value taken from your Coralogix Send-Your-Data API key. Compare the <code>PRIVATE_KEY</code> in your <code>secret</code> to the Send-Your-Data API key on your UI to ensure proper configuration.</p> <pre><code>apiVersion: v1\ndata:\n  PRIVATE_KEY: &lt;encrypted-private-key&gt;\nkind: Secret\nmetadata:\n  name: coralogix-keys\n  namespace: &lt;namespace&gt;\ntype: Opaque\n\n</code></pre>"},{"location":"newoutput/opentelemetry-using-kubernetes/#configuration","title":"Configuration","text":"<p>STEP 1: Create a YAML-Formatted Override File</p> <p>Create a new YAML-formatted override file that defines certain\u00a0values for the OpenTelemetry Collector.</p> <p>In order to send your data to Coralogix, you are\u00a0required\u00a0to declare the <code>endpoint</code> variable into your file. Choose the OpenTelemetry endpoint for the domain associated with your Coralogix account. You have the option of sending logs, metrics, and / or traces to Coralogix. The example override file below includes all three.</p> <p>You are also required to specify the <code>mode</code>.</p> <ul> <li> <p>For high data traffic clusters and/or to leverage Coralogix APM capabilities, we recommend <code>daemonset</code> mode acting as an agent running on each node. Be aware that it consumes resources (e.g., CPU &amp; memory) from each node on which it runs.</p> </li> <li> <p>For development environments or clusters with minimal data, we recommend <code>deployment</code>\u00a0mode.</p> </li> </ul> <p>In order to collect the cluster name(s) to appear on your Coralogix Kubernetes Dashboard, add the OTel transform processor to your configuration, as in the example below.</p> <pre><code>#values.yaml\n---\nglobal:\n  domain: \"&lt;cx_domain&gt;\"\n  clusterName: \"&lt;your_cluster_name&gt;\"\nopentelemetry-collector:\n  mode: \"&lt;daemonset/deployment&gt;\"\n</code></pre> <p>Save this file as values.yaml.</p> <p>Here is information about other values for OpenTelemetry Collector. Making changes to these variables is optional.</p> <ul> <li> <p><code>extraEnvs</code>: Includes your Coralogix Send-Your-Data API key </p> </li> <li> <p><code>presets</code>: Built-in variables that automatically add processor and integration attributes</p> </li> <li> <p><code>config</code>: Option of sending logs, metrics, and / or tracing to Coralogix</p> </li> <li> <p><code>defaultApplicationName</code>\u00a0and\u00a0<code>defaultSubsystemName</code>: Logs, metrics, and traces emitted by this OpenTelemetry exporter are tagged in Coralogix with the default application and subsystem constants \u201cdefault\u201d and \u201cnodes\u201d, respectively.</p> </li> </ul> <p>STEP 2: Install the Associated OpenTelemetry Chart</p> <p>Install the associated OpenTelemetry chart with the release name of your choice.</p> <pre><code>export NAMESPACE=&lt;namespace&gt;\n\n</code></pre> <pre><code>helm upgrade --install otel-coralogix-agent coralogix-charts-virtual/opentelemetry-coralogix \\\n  -f values.yaml\n\n</code></pre> <p>Hint: Installing a new package requires two arguments: a release name that you pick and the name of the chart you want to install. You may choose any name that suits you; the example above adopts the release name\u00a0<code>otel-coralogix-agent</code>.</p> <p>STEP 3: Ensure all Pods are Running</p> <pre><code>kubectl get pods -o wide -n $NAMESPACE |grep otel-coralogix-agent\n\n</code></pre> <p>The STATUS of these pods should appear \"Running\". If this is not the case, see our Troubleshooting section below.</p> <p></p> <pre><code>kubectl get pods -o wide -n $NAMESPACE |grep otel-coralogix-agent\n\n</code></pre>"},{"location":"newoutput/opentelemetry-using-kubernetes/#validation","title":"Validation","text":"<p>In order to validate that you have set up your receiver correctly, set up a demo application in Node.js which generates traces. A prerequisite for doing so is creating an account with Docker. For support in other coding languages, click here.</p> <p>1. Download and unzip image.zip, which contains all files needed to create the test application image.</p> <p>2. Check the IP address of my OpenTelemetry collector pod to update\u00a0<code>tracing.js</code>:</p> <pre><code>kubectl get pods -o wide |grep otel\n\n</code></pre> <p>Below is an example. The string of numbers separated by periods -\u00a0172.17.0.3 - is known as the IP address.</p> <pre><code>otel-coralogix-agent-opentelemetry-collector-654d45fb7d-kfgj4 1/1 Running 0 42s 172.17.0.3 minikube &lt;none&gt; &lt;none&gt;\n\n</code></pre> <p>3. Update and save the\u00a0<code>tracing.js</code>\u00a0file with the IP address in line 18.</p> <pre><code>/* tracing.js */\n\n// Require dependencies\nconst opentelemetry = require(\"@opentelemetry/sdk-node\");\nconst { getNodeAutoInstrumentations } = require(\"@opentelemetry/auto-instrumentations-node\");\nconst { diag, DiagConsoleLogger, DiagLogLevel } = require('@opentelemetry/api');\n\n// For troubleshooting, set the log level to DiagLogLevel.DEBUG\ndiag.setLogger(new DiagConsoleLogger(), DiagLogLevel.INFO);\n\nconst {\n  OTLPTraceExporter,\n} = require(\"@opentelemetry/exporter-trace-otlp-http\");\n\nconst sdk = new opentelemetry.NodeSDK({\n  traceExporter: new OTLPTraceExporter({\n    // optional - url default value is &lt;http://localhost:4318/v1/traces&gt;\n    url: \"&lt;http://172.17.0.3:4318/v1/traces&gt;\",\n    // optional - collection of custom headers to be sent with each request, empty by default\n    headers: {},\n  }),\n  instrumentations: [getNodeAutoInstrumentations()],\n});\n\nsdk.start()\n</code></pre> <p>4. In order to push your docker file, log in to your Docker account:</p> <pre><code>docker login -u &lt;username&gt;\n\n</code></pre> <p>5. Run the following command:</p> <pre><code>docker build . -t &lt;your_docker_username&gt;/&lt;name_for_image&gt;\n\n</code></pre> <p>6. Push the image to your repository to make the file accessible from Kubernetes:</p> <pre><code>docker push &lt;your_docker_username&gt;/&lt;name_for_image&gt;\n\n</code></pre> <p>7. Deploy your application by running the following command:</p> <pre><code>kubectl run otel-test-app-pod --image &lt;your_docker_username&gt;/&lt;name_for_image&gt; --port 8080\n\n</code></pre> <p>8. Set the port forwarding:</p> <pre><code>kubectl port-forward otel-test-app-pod 8081:8080\n\n</code></pre> <p>9. Open\u00a0http://localhost:8081/\u00a0on your browser. \"Hello World\" text should appear.</p>"},{"location":"newoutput/opentelemetry-using-kubernetes/#demo","title":"Demo","text":"<p>In order to ensure that your data is being sent to Coralogix without being dependent on your own data, we recommend running a demo to validate your installation. We suggest doing so on non-production environments only.</p> <p>Note: When the demo is deployed, a new OpenTelemetry pod is created which sends traces to Coralogix. Rather than testing the pod that you created, it demonstrates how your application can be integrated with OpenTelemetry.</p> <p>1. In the same namespace in which the override file is saved, install the associated demo chart with the release name of your choice:</p> <pre><code>helm install my-otel-demo open-telemetry/opentelemetry-demo\n\n</code></pre> <p>2. Ensure all associated pods are running:</p> <pre><code>kubectl get pods -o wide |grep my-otel-dem\n\n</code></pre> <p>The STATUS of these pods should appear \"Running\". If this is not the case, see our Troubleshooting section below.</p> <p>3. Enable port forwarding so you can access the demo application from your browser:</p> <pre><code>kubectl port-forward svc/my-otel-demo-frontend 8080:8080\n\n</code></pre> <p>4. Open the following address in your browser:</p> <pre><code>http://localhost:8080\n\n</code></pre> <p>5. As you shop on the website, logs, metrics and/or traces will be sent to Coralogix, depending on which data you have chosen to send to us.</p>"},{"location":"newoutput/opentelemetry-using-kubernetes/#troubleshooting","title":"Troubleshooting","text":"<p>View data on your Coralogix dashboard</p> <p>Once the installation process is complete, you may not see your telemetry data appear in your Coralogix account. This may indicate either that the installation has failed due to some error or that your installation is successful, but your application has not been configured to send data to Coralogix via this container.</p> <p>In order to find the source of the problem, we recommend you take a number of steps, including\u00a0troubleshooting your Otel logs\u00a0and\u00a0running a demo. If the demo works successfully, the problem lies in the configuration of your application. If the demo fails to work, the problem lies in the configuration of the collector. You will find tools to solve both of these issues below.</p> <p>Troubleshoot your OTel logs</p> <p>1. Following installation, expect Kubernetes to run a pod with your chosen installation name. Ensure this is the case by running the following command:</p> <pre><code>kubectl get pods -o wide -n $NAMESPACE |grep otel-coralogix-agent\n\n</code></pre> <p>2. Locate and copy the full name of the OpenTelemetry collector agent in your list of pods. The pod should appear exactly once with the STATUS \"Running.\" If the STATUS is \"Pending\", rerun the command. The AGE appearing should be the time that has elapsed since your last helm upgrade.</p> <p>3. Once you have located this specific pod, use the default logging tool command\u00a0<code>kubectl logs</code>\u00a0for retrieving its logs. Running this command with the\u00a0<code>-follow</code>\u00a0flag streams logs from the specified resource, allowing you to live tail its logs from your terminal.</p> <pre><code>kubectl logs --follow &lt;paste full name of opentelemtry collector agent pod here&gt; -n &lt;namespace&gt;\n\n</code></pre> <p>Here is an example of the expected output of Steps 1-3:</p> <p></p> <p>Rerun this set of commands at any later stage as necessary.</p> <p>Validate your endpoints</p> <p>Validating your endpoints will allow you to test the connectivity in your domain structure.</p> <p>Ensure that your endpoint is correct:</p> <pre><code>nslookup &lt;endpoint name&gt;\n\n</code></pre> <p>Here is an example of the expected results for an account in the EU1 region:</p> <p></p> <p>If you receive an error, this may mean that you lack connectivity to your domain server.</p>"},{"location":"newoutput/opentelemetry-using-kubernetes/#limits-quotas","title":"Limits &amp; Quotas","text":"<p>Coralogix places a hard limit of 10MB of data to our Otel endpoints, with a recommendation of 2MB.</p> <p>Limits apply to single requests, regardless of timespan.</p>"},{"location":"newoutput/opentelemetry-using-kubernetes/#additional-resources","title":"Additional Resources","text":"GithubOfficial OpenTelemetry Collector with Coralogix ExporterInstructional VideosIntegrate metrics into Coralogix using OpenTelemetry, Kubernetes &amp; HelmIntegrate traces into Coralogix using OpenTelemetry, Kubernetes &amp; HelmIntegrate logs into Coralogix using OpenTelemetry, Kubernetes &amp; HelmCapture Kubernetes logs, transform with Logs2Metrics and render with DataMapFeaturesCoralogix APM featuresAPM using OpenTelemetry Collector with KubernetesTutorialsTail Sampling with Coralogix and OpenTelemetryBlogsHow to Configure the OTel Community Demo App to Send Telemetry Data to Coralogix"},{"location":"newoutput/opentelemetry-using-kubernetes/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/opsgenie-data-ingestion/","title":"Opsgenie Data Ingestion","text":"<p>Collect your Opsgenie alerts in the Coralogix platform using our automatic Contextual Data Integration Package. The package automatically generates a URL to be used when creating an Opsgenie webhook.</p>"},{"location":"newoutput/opsgenie-data-ingestion/#overview","title":"Overview","text":"<p>Opsgenie is a modern incident management and alerting platform that empowers DevOps, IT, and support teams to effectively respond to and resolve incidents in real-time. By centralizing alerts from various monitoring, ticketing, and communication tools, Opsgenie ensures that critical incidents are quickly detected, efficiently communicated, and appropriately addressed. Its comprehensive features include customizable alert routing, on-call scheduling, collaboration tools, and actionable insights, facilitating rapid incident resolution and minimizing downtime for organizations of all sizes.</p> <p>Sending your Opsgenie alerts to Coralogix streamlines alert management, enhances monitoring capabilities, and facilitates comprehensive incident analysis. By directing your Opsgenie alerts into Coralogix, you gain a centralized view of your alerting activities, enabling rapid incident detection, proactive troubleshooting, and data-driven decision-making. This integration empowers teams to optimize response workflows, strengthen system reliability, and ensure operational efficiency, leveraging Coralogix's analytics, alerts, and visualization tools to extract valuable insights from Opsgenie alerts and ensure a streamlined and resilient incident response process.</p>"},{"location":"newoutput/opsgenie-data-ingestion/#get-started","title":"Get Started","text":"<p>STEP 1.\u00a0In your navigation pane, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 2.\u00a0In the Contextual Data section, select Opsgenie and click\u00a0+ ADD.</p> <p></p> <p>STEP 3. Click ADD NEW.</p> <p>STEP 4.\u00a0Fill in the Integration Details:</p> <ul> <li> <p>Name.\u00a0Name your integration.</p> </li> <li> <p>Your API Key. Click\u00a0CREATE NEW KEY\u00a0to generate an API key and name it.</p> </li> <li> <p>Application Name and Subsystem Name. Enter an\u00a0application and subsystem name.</p> </li> </ul> <p></p> <p>STEP 5.\u00a0Click\u00a0GENERATE URL. The URL for the integration will be automatically created. Use this when creating a Opsgenie webhook.</p> <p></p>"},{"location":"newoutput/opsgenie-data-ingestion/#create-an-opsgenie-webhook","title":"Create an Opsgenie Webhook","text":"<p>Create an Opsgenie webhook using your URL.</p> <p>STEP 1.\u00a0Log in to your OpsGenie account and click on Settings.</p> <p>STEP 2. Scroll down to Integrations and click on Webhook. If you do not see \u201cwebhook\u201d, click on the integration list and search for it.</p> <p></p> <p>Once you chose webhook a new screen will pop up.</p> <p>STEP 3. Enter the URL generated by Coralogix in the Webhook URL field, and complete the form, then click Save.</p> <p></p> <p>To test your configuration, go under Alerts and configure a test alert.</p>"},{"location":"newoutput/opsgenie-data-ingestion/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/optimize-metrics-costs-in-coralogix-by-adjusting-your-scrape-interval/","title":"Optimize Metrics Costs in Coralogix by Adjusting Your Scrape Interval","text":"<p>Coralogix's metrics costs are influenced by the amount of metric data ingested, which is significantly impacted by the scrape interval settings of your OpenTelemetry Collector or Prometheus Agent. Adjusting your scrape intervals allows for more effective data ingestion management, leading to optimized costs and efficient usage.</p>"},{"location":"newoutput/optimize-metrics-costs-in-coralogix-by-adjusting-your-scrape-interval/#the-optimal-scrape-interval","title":"The Optimal Scrape Interval","text":""},{"location":"newoutput/optimize-metrics-costs-in-coralogix-by-adjusting-your-scrape-interval/#why-60-seconds-makes-sense","title":"Why 60 Seconds Makes Sense","text":"<p>After thorough analysis and consideration of various monitoring needs, it's evident that a 60-second scrape interval is the most balanced and universally applicable approach for a wide array of use cases. Here's why:</p> <ul> <li> <p>Cost-Effectiveness: A 60-second interval ensures you're not over-collecting data, which can quickly escalate costs without providing proportional value. This interval is long enough to capture meaningful trends and metrics, yet short enough to control data ingestion volumes \u2014 and thus costs.</p> </li> <li> <p>Data Quality and Relevance: For the vast majority of monitoring scenarios, metrics collected every minute provide a sufficient level of detail to observe system performance, identify issues, and make informed decisions. This frequency offers a practical balance, ensuring data is both manageable and meaningful.</p> </li> <li> <p>Reduced System Load: Collecting data at a more frequent interval can put unnecessary load on your systems and the data collection infrastructure. A 60-second interval minimizes this impact, preserving system resources for your core business operations.</p> </li> <li> <p>Streamlined Data Management: With data collected at a consistent and reasonable rate, storage, analysis, and management become significantly more straightforward. This efficiency allows your team to focus on leveraging data insights rather than handling them.</p> </li> </ul>"},{"location":"newoutput/optimize-metrics-costs-in-coralogix-by-adjusting-your-scrape-interval/#addressing-auto-scaling-use-cases","title":"Addressing Auto-Scaling Use Cases","text":"<p>While a 60-second scrape interval is recommended for most scenarios, auto-scaling environments present unique challenges that might necessitate a different approach. Specifically, the need for rapid scaling based on real-time demands requires a more nuanced strategy:</p> <ul> <li>Critical Metrics Focus: In auto-scaling scenarios, it's essential to identify which metrics most indicate the need to scale. Collecting these critical metrics at a shorter interval, while maintaining a 60-second interval for less sensitive data, offers a balanced solution. This approach ensures that the most current and relevant data inform your auto-scaling decisions without overwhelming your system with excessive metric collection.</li> </ul> <p>Adopting a dual-interval strategy allows you to enjoy the benefits of a balanced, cost-effective monitoring setup for general purposes while still catering to the specific needs of auto-scaling scenarios. This tailored approach optimizes your monitoring efficiency and cost, ensuring that you have the necessary data when it matters most, without incurring unnecessary expenses.</p>"},{"location":"newoutput/optimize-metrics-costs-in-coralogix-by-adjusting-your-scrape-interval/#adjusting-your-scrape-interval","title":"Adjusting Your Scrape Interval","text":"<p>OpenTelemetry Collector and Prometheus Agent Configuration: Review and adjust the <code>scrape_interval</code> setting in your configuration files to adopt the 60-second standard for general monitoring and consider shorter intervals for critical metrics affecting auto-scaling decisions.</p> <p>OpenTelemetry Collector:</p> <pre><code>  prometheus:\n    config:\n      scrape_configs: \n            - job_name: 'otel-collector'\n              scrape_interval: 60s\n              static_configs:\n              - targets: ['0.0.0.0:8888'] \n            - job_name: 'node-exporter'\n              scrape_interval: 15s\n              static_configs:\n              - targets: ['localhost:9100'] #Auto-Scaling/Critical metrics\n\n</code></pre> <p>This adjustment is foundational to achieving a more cost-effective and efficient monitoring setup.</p> <p>By thoughtfully setting your scrape intervals, you embrace best practices for most monitoring scenarios while accommodating the specialized needs of auto-scaling environments. This strategic approach ensures that your monitoring practices are cost-efficient and highly responsive to dynamic operational requirements.</p>"},{"location":"newoutput/optimize-metrics-costs-in-coralogix-by-adjusting-your-scrape-interval/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogix.com.</p>"},{"location":"newoutput/optional-configurations-microsoft-azure/","title":"Optional Configurations: Microsoft Azure","text":"<p>Coralogix offers optional configurations for particular use-cases utilizing our Azure deployments.</p>"},{"location":"newoutput/optional-configurations-microsoft-azure/#storage-accounts-event-hubs-with-restricted-public-access","title":"Storage Accounts &amp; Event Hubs With Restricted Public Access","text":"<p>If you require resource monitoring in Azure storage accounts or Event Hubs that cannot be made public, deploy our function apps with virtual network (VNet) support.</p>"},{"location":"newoutput/optional-configurations-microsoft-azure/#prerequisites","title":"Prerequisites","text":"<p>To enable the VNet feature, set the Function App Service Plan Type to 'Premium' during deployment. This will then enable the 'Networking' feature of your function app.</p>"},{"location":"newoutput/optional-configurations-microsoft-azure/#configuration","title":"Configuration","text":"<p>STEP 1. Open the VNet integration configuration of the function app in the left-hand Networking panel.</p> <p></p> <p>STEP 2. Select Add VNet and configure it to a VNet with access to the Storage Account or Event Hub.</p> <p></p> <p>STEP 3. Determine which virtual networks are permitted in the Networking section of the Blog storage account or Event Hub, as seen in the example below.</p> <p></p> <p>Notes:</p> <ul> <li> <p>If no virtual networks are aligned with the storage account or Event Hub, select Add new virtual network.</p> </li> <li> <p>If selecting an existing virtual network, be aware that it could be internet-restricted and prevent outbound messages to reach our API. When in doubt, create a new VNet for use by Coralogix integrations and the private storage account.</p> </li> </ul>"},{"location":"newoutput/optional-configurations-microsoft-azure/#cost-considerations","title":"Cost Considerations","text":"<ul> <li> <p>VNet configuration is not available for consumption (serverless) applications, which generally incur significantly less costs.</p> </li> <li> <p>Selecting the 'Premium' app service plan may incur significant costs, as it requires 24/7 resource payments (as opposed to a usage-based consumption plan).</p> </li> </ul>"},{"location":"newoutput/optional-configurations-microsoft-azure/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/organization-admin-console/","title":"Organization Admin Console","text":"<p>Coralogix supports multi-tenancy, allowing multiple teams to be connected under a single organization. Some companies prefer separate teams to isolate data based on the environment it originates from like: Dev, QA, or Production. While others prefer to isolate the data based on organizational units like: Infrastructure, Security, and Application. Coralogix allows you to associate multiple teams with an Organization.</p> <p>The Organization Admin console provides a single interface for the management of all teams associated with the organization.</p> <p>NOTE - Organization management features are only visible to users who have been assigned the role of Organization-admins</p>"},{"location":"newoutput/organization-admin-console/#creating-an-organization","title":"Creating an Organization","text":"<p>Organizations are created upon request. Please contact us through our in-app chat, or via email at support@coralogixstg.wpengine.com.</p> <p>Once your organization has been created, navigate to the Admin Console to connect your teams.</p>"},{"location":"newoutput/organization-admin-console/#admin-console","title":"Admin Console","text":"<p>The Admin Console is visible only to Organization-admins. It is accessible by ****going to the \u2018Account\u2019 menu.</p> <p></p>"},{"location":"newoutput/organization-admin-console/#my-teams","title":"My Teams","text":"<p>This screen lets admins select teams that will be connected to an Organization. Attaching or removing a team is done by toggling the \u201cConnect to Org\u201d switch.</p> <p></p> <p>Note: The user must be an admin on the team being connected.</p>"},{"location":"newoutput/organization-admin-console/#quota-manager","title":"Quota Manager","text":"<p>This view helps the Admins easily understand the data usage trends for the entire Organization. The overview graph shows you:</p> <ol> <li> <p>Total quota available for the entire organization</p> </li> <li> <p>Number of teams in the organization</p> </li> <li> <p>Daily usage and average 7-day usage</p> </li> <li> <p>Days with high data usage</p> </li> <li> <p>Number of times ingestion was blocked for all teams in your organization.</p> </li> </ol> <p>Note: High daily usage means more than 90% of the daily quota was used.</p> <p></p>"},{"location":"newoutput/organization-admin-console/#quota-usage-by-team","title":"Quota Usage by Team","text":"<p>The team overview section allows you to filter the data usage statistics by team. The report is available as either a graph or a table. The report shows data usage trends per team. You will see:</p> <ol> <li> <p>Daily quota available</p> </li> <li> <p>Retention period</p> </li> <li> <p>Daily usage and average 7-day usage</p> </li> <li> <p>High data usage warnings</p> </li> <li> <p>Number of times ingestion was blocked</p> </li> </ol> <p></p>"},{"location":"newoutput/organization-admin-console/#moving-units-between-teams","title":"Moving Units Between Teams","text":"<p>Over the course of time, you may need to optimize the quota allocation across your teams. After analyzing your data usage trends, you may discover that some teams get blocked frequently while others have excess quota allocation.</p> <p>The Quota Manager lets you move quota between teams with the same retention period. To access this feature, click on \u201cMove Units\u201d.</p> <p></p> <p>On the form presented, you will be required to select:</p> <ol> <li> <p>Origin - Quota will be deducted from this team</p> </li> <li> <p>Units - How many units (GB) to transfer from the Origin team</p> </li> <li> <p>Target - This team will receive quota units from Origin</p> </li> </ol> <p></p> <p>Approve the transfer to complete the process. The new quota will reflect approximately 5 minutes later.</p> <p>As always, if you have questions or suggestions, please contact us in the in-app chat or email us at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/otel-collector-for-k8s/","title":"Kubernetes Complete Observability: Basic Configuration","text":"<p>Coralogix\u2019s Kubernetes Complete Observability provides a comprehensive solution for full-stack observability in your Kubernetes environment.</p>"},{"location":"newoutput/otel-collector-for-k8s/#overview","title":"Overview","text":"<p>View all of your nodes, pods and cluster metrics, pod logs, Kubernetes events, and your distributed traces pipeline. Take advantage of our Kubernetes Dashboard using our pre-configured OpenTelemetry Collector.</p> <p>Utilizing OpenTelemetry, we ensure seamless and automated data collection from various components of your stack. This enables you to monitor infrastructure health and gain insights into application behavior and inter-service dependencies. Troubleshoot issues, optimize performance and manage your cluster more effectively with a 360-degree view of your Kubernetes ecosystem.</p>"},{"location":"newoutput/otel-collector-for-k8s/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Kubernetes (v1.24+)\u00a0installed, with the command-line tool\u00a0kubectl</p> </li> <li> <p>Helm\u00a0(v3.9+) installed and configured</p> </li> </ul> <p>Note! If you have previously installed the\u00a0Coralogix Exporter\u00a0or\u00a0Kubernetes Infrastructure Monitoring, they must be removed before proceeding with this integration.</p>"},{"location":"newoutput/otel-collector-for-k8s/#installation","title":"Installation","text":"<p>STEP 1. In your Coralogix toolbar, navigate to Data Flow &gt; Integrations.</p> <p>STEP 2. From the Integrations section, select Kubernetes Complete Observability.</p> <p></p> <p>STEP 3. On the Coralogix OpenTelemetry Collector integration page, click + SETUP COLLECTOR.</p> <p></p> <p>STEP 4. Enter a name for your integration.</p> <p>STEP 5. Enter one of your Send-Your-Data API keys or click CREATE NEW KEY to generate a new dedicated API key.</p> <p></p> <p>STEP 6. Click NEXT.</p> <p>STEP 7. Check the Helm version by using the <code>helm version</code> command. You are required to use Helm v3.9 or above.</p> <p>STEP 8. Add the Coralogix Helm repository to your Helm configuration by copying and running the command.</p> <p>Run the command <code>helm repo update</code> to update Helm's local repository cache.</p> <p>Click NEXT.</p> <p>STEP 9. OpenTelemetry Agent requires a\u00a0secret\u00a0called\u00a0<code>coralogix-keys</code>\u00a0with the\u00a0Send-Your-Data API key\u00a0created in STEP 5. It is defined as\u00a0<code>PRIVATE_KEY</code> inside the\u00a0same namespace\u00a0in which the chart is installed. If the secret is not present, create it by copying and running the command shown in the installer.</p> <p>STEP 10. Copy and run the <code>helm upgrade</code> command shown in the installer. Make sure you replace the <code>&lt;cluster name&gt;</code> with your Kubernetes cluster name.</p> <p>STEP 11. Mark the checkbox to confirm you have run the Helm command. Click COMPLETE.</p> <p></p>"},{"location":"newoutput/otel-collector-for-k8s/#limits-quotas","title":"Limits &amp; Quotas","text":"<ul> <li> <p>Coralogix places a\u00a0hard limit of 10MB\u00a0of data to our\u00a0OpenTelemetry Endpoints, with a\u00a0recommendation of 2MB.</p> </li> <li> <p>Metric names must be a maximum of 255 characters.</p> </li> <li> <p>Attribute keys for metric data must be a maximum of 255 characters.</p> </li> </ul>"},{"location":"newoutput/otel-collector-for-k8s/#next-steps","title":"Next Steps","text":"<p>Advanced configuration instructions can be found here.</p> <p>Validation instructions can be found here.</p>"},{"location":"newoutput/otel-collector-for-k8s/#additional-resources","title":"Additional Resources","text":"DocumentationGitHub RepositoryKubernetes Dashboard"},{"location":"newoutput/otel-collector-for-k8s/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/otel-lambda-auto-instrumentation/","title":"OpenTelemetry Lambda Auto Instrumentation","text":"<p>Coralogix offers <code>coralogix-*-wrapper-and-exporter-*</code> Lambda layers enable you to generate logs, metrics, and traces, providing insights into triggers, invocation times, and interconnections. These features work out-of-the-box, without requiring any modification of the monitored Lambda functions' code. Once the configuration is complete, view your data using our cutting-edge Serverless Monitoring feature.</p> <p>Notes:</p> <p>This integration is one of two options - complete and basic - for monitoring Lambda, a requirement for enjoying Serverless Monitoring. This tutorial demonstrates how to set up Lambda monitoring to get complete telemetry, including traces.</p> <p>To set up Lambda monitoring to get only basic telemetry, including logs, you are only required to set up the Coralogix AWS Lambda Telemetry Exporter. View the relevant documentation here.</p>"},{"location":"newoutput/otel-lambda-auto-instrumentation/#overview","title":"Overview","text":"<p>The Lambda instrumentation layers (<code>coralogix-*-wrapper-and-exporter-*</code>) contain modified versions of OpenTelemetry Lambda auto instrumentation wrappers, bundled together with the Coralogix AWS Lambda Telemetry Exporter.</p> <p>OpenTelemetry Lambda auto instrumentation wrappers instrument the handler function, AWS SDK, and other client libraries to produce traces out-of-the-box. They also provide a configured Otel SDK through which the function code can produce custom spans and metrics.</p> <p>The telemetry produced by the auto instrumentation wrapper is forwarded to the Coralogix AWS Lambda Telemetry Exporter, which supplements it with information obtained from the Lambda Telemetry API and sends it all to Coralogix. Once fully set up, the auto instrumentation will produce traces, as shown below. The traces will help you discover:</p> <ul> <li> <p>What invoked/triggered the lambda function</p> </li> <li> <p>What Lambda functions and other AWS services were called by the function</p> </li> <li> <p>How long did different parts of the invocation take</p> </li> </ul> <p></p>"},{"location":"newoutput/otel-lambda-auto-instrumentation/#set-up-lambda-monitoring","title":"Set Up Lambda Monitoring","text":"<p>Follow these steps in order to monitor AWS Lambda functions with Coralogix.</p> <p>STEP 1. Set up the Coralogix AWS resource metadata collection.</p> <p>STEP 2. Set up the OpenTelemetry Lambda auto instrumentation, as described in this tutorial. This is available for Node.js and Python (v3.8 or newer). For other runtimes, opt for the Coralogix AWS Lambda Telemetry Exporter setup to view basic telemetry.</p>"},{"location":"newoutput/otel-lambda-auto-instrumentation/#amazon-resource-name","title":"Amazon Resource Name","text":"<p>To deploy or update the Coralogix OpenTelemetry wrapper, select the Amazon Resource Name (ARN) corresponding to your AWS region, Lambda runtime and Architecture from the following lists.</p>"},{"location":"newoutput/otel-lambda-auto-instrumentation/#python-x86_64","title":"Python x86_64","text":"<pre><code>arn:aws:lambda:ap-south-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-north-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-west-3:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-west-2:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-west-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-northeast-3:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-northeast-2:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-northeast-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ca-central-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:sa-east-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-southeast-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-southeast-2:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-central-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:us-east-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:us-east-2:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:us-west-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:us-west-2:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:af-south-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-east-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-southeast-3:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-south-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:me-south-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-south-2:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-southeast-4:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-central-2:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-south-2:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:me-central-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:il-central-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ca-west-1:625240141681:layer:coralogix-python-wrapper-and-exporter-x86_64:7\n</code></pre> <p>Notes:</p> <ul> <li>Python v3.8 and higher is supported. We recommend migrating away from Python 3.7 as it is scheduled for deprecation by AWS in November 2023.</li> </ul>"},{"location":"newoutput/otel-lambda-auto-instrumentation/#python-arm64","title":"Python arm64","text":"<pre><code>arn:aws:lambda:ap-south-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-north-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-west-3:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-west-2:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-west-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-northeast-3:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-northeast-2:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-northeast-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:ca-central-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:sa-east-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-southeast-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-southeast-2:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-central-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:us-east-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:us-east-2:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:us-west-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:us-west-2:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:af-south-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-east-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-southeast-3:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-south-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:me-south-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-south-2:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-southeast-4:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-central-2:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-south-2:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:me-central-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\narn:aws:lambda:il-central-1:625240141681:layer:coralogix-python-wrapper-and-exporter-arm64:7\n</code></pre> <p>Notes:</p> <ul> <li>Python v3.8 and higher is supported. We recommend migrating away from Python 3.7 as it is scheduled for deprecation by AWS in November 2023.</li> </ul>"},{"location":"newoutput/otel-lambda-auto-instrumentation/#nodejs-x86_64","title":"Node.js x86_64","text":"<pre><code>arn:aws:lambda:ap-south-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-north-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-west-3:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-west-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-west-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-northeast-3:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-northeast-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-northeast-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ca-central-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:sa-east-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-southeast-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-southeast-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-central-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:us-east-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:us-east-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:us-west-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:us-west-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:af-south-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-east-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-southeast-3:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-south-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:me-south-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-south-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ap-southeast-4:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-central-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:eu-south-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:me-central-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:il-central-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\narn:aws:lambda:ca-west-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-x86_64:7\n</code></pre>"},{"location":"newoutput/otel-lambda-auto-instrumentation/#nodejs-arm64","title":"Node.js arm64","text":"<pre><code>arn:aws:lambda:ap-south-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-north-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-west-3:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-west-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-west-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-northeast-3:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-northeast-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-northeast-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:ca-central-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:sa-east-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-southeast-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-southeast-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-central-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:us-east-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:us-east-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:us-west-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:us-west-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:af-south-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-east-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-southeast-3:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-south-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:me-south-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-south-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:ap-southeast-4:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-central-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:eu-south-2:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:me-central-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\narn:aws:lambda:il-central-1:625240141681:layer:coralogix-nodejs-wrapper-and-exporter-arm64:7\n</code></pre>"},{"location":"newoutput/otel-lambda-auto-instrumentation/#java-x86_64-beta","title":"Java x86_64 (Beta)","text":"<pre><code>arn:aws:lambda:ap-south-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:eu-north-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:eu-west-3:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:eu-west-2:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:eu-west-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:ap-northeast-3:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:ap-northeast-2:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:ap-northeast-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:ca-central-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:sa-east-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:ap-southeast-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:ap-southeast-2:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:eu-central-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:us-east-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:us-east-2:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:us-west-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:us-west-2:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:af-south-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:ap-east-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:ap-southeast-3:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:eu-south-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:me-south-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:ap-south-2:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:ap-southeast-4:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:eu-central-2:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:eu-south-2:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:me-central-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:il-central-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\narn:aws:lambda:ca-west-1:625240141681:layer:coralogix-java-wrapper-and-exporter-x86_64:2\n\n</code></pre>"},{"location":"newoutput/otel-lambda-auto-instrumentation/#java-arm64-beta","title":"Java arm64 (Beta)","text":"<pre><code>arn:aws:lambda:ap-south-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:eu-north-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:eu-west-3:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:eu-west-2:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:eu-west-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:ap-northeast-3:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:ap-northeast-2:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:ap-northeast-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:ca-central-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:sa-east-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:ap-southeast-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:ap-southeast-2:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:eu-central-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:us-east-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:us-east-2:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:us-west-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:us-west-2:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:af-south-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:ap-east-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:ap-southeast-3:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:eu-south-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:me-south-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:ap-south-2:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:ap-southeast-4:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:eu-central-2:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:eu-south-2:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:me-central-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\narn:aws:lambda:il-central-1:625240141681:layer:coralogix-java-wrapper-and-exporter-arm64:2\n</code></pre>"},{"location":"newoutput/otel-lambda-auto-instrumentation/#configuration","title":"Configuration","text":"<p>In your AWS Management console, navigate to AWS Lambda.</p> <p>STEP 1. Access the Lambda function that you would like to monitor. Check the list of Layers. If you already have a <code>coralogix-aws-lambda-telemetry-exporter-*</code> layer added, remove it. In the following steps, you will install a <code>coralogix-*-wrapper-and-exporter-*</code> layer that already contains the telemetry exporter.</p> <p>STEP 2. Access the Lambda function that you would like to monitor. Select Layers &gt; Add a layer</p> <p></p> <p>STEP 3. Specify an ARN by pasting one from the list above &gt; Add</p> <p></p> <p>STEP 4. Access the Lambda function that you would like to monitor and configure the environment variables: Configuration &gt; Environment variables</p> <ul> <li> <p>Set <code>AWS_LAMBDA_EXEC_WRAPPER</code> = <code>/opt/otel-handler</code> to enable the auto-instrumentation wrapper.</p> </li> <li> <p>Set <code>CX_DOMAIN</code> to the Coralogix domain within which you\u2019ve set up your account. For example <code>coralogix.us</code> (do NOT add protocol, port, team name etc. to the domain).</p> </li> <li> <p>Set <code>CX_API_KEY</code> to the Coralogix Send-Your-Data API key.</p> </li> <li> <p>Set <code>CX_REPORTING_STRATEGY = REPORT_AFTER_INVOCATION</code></p> <ul> <li> <p>If the function is invoked frequently consider setting it to <code>LOW_OVERHEAD</code> instead.</p> </li> <li> <p>See AWS Lambda Telemetry Exporter documentation to learn more about reporting strategies.</p> </li> </ul> </li> </ul> <p>STEP 5. [Optional] Fine-tune the telemetry-exporter to match your preferences. See all available configuration options here.</p>"},{"location":"newoutput/otel-lambda-auto-instrumentation/#validation","title":"Validation","text":"<p>Assuming you\u2019ve already configured the Coralogix AWS resource metadata collection, you\u2019re ready to take advantage of the Serverless Monitoring feature. Validate that your setup works.</p> <p>STEP 1. Invoke the function.</p> <p>STEP 2. In your Coralogix navigation pane, click APM. Select the Serverless tab. Click on the function of interest.</p> <p>STEP 3. View Lambda invocations in the INVOCATIONS table.</p> <p>Note: It may take up to 1 minute for your telemetry to fully process.</p> <p></p> <p>STEP 4. Click on an invocation to drill down using our Spans View.</p> <p></p>"},{"location":"newoutput/otel-lambda-auto-instrumentation/#additional-resources","title":"Additional Resources","text":"DocumentationAWS Lambda Telemetry ExporterAWS Resource Metadata CollectionServerless MonitoringGitHubNode.js wrapperPython wrapper"},{"location":"newoutput/otel-lambda-auto-instrumentation/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/pagerduty-data-ingestion/","title":"PagerDuty Data Ingestion","text":"<p>Collect your PagerDuty alerts in the Coralogix platform using our automatic Contextual Data Integration Package. The package automatically generates a URL to be used when creating a PagerDuty webhook.</p>"},{"location":"newoutput/pagerduty-data-ingestion/#overview","title":"Overview","text":"<p>PagerDuty is a leading digital operations management platform designed to help businesses proactively manage and respond to critical incidents in real-time. By aggregating alerts from monitoring, cloud, and security tools, PagerDuty centralizes incident notifications and rapidly notifies the right individuals or teams, ensuring swift resolution. Its robust capabilities include automated alerting, on-call scheduling, customizable escalation policies, and rich analytics, enabling organizations to improve their incident response processes, reduce downtime, and enhance overall operational efficiency. PagerDuty's user-friendly interface and integrations with various third-party services make it a vital tool for modern IT, DevOps, and support teams seeking to maintain high system reliability and availability.</p> <p>Forwarding your PagerDuty alerts to Coralogix streamlines alert consolidation, augments monitoring capabilities, and expedites incident resolution. By routing your PagerDuty alerts into Coralogix, you achieve a unified view of your alerting and incident management activities, enabling swift anomaly detection, proactive troubleshooting, and informed decision-making. This integration empowers teams to optimize incident response workflows, enhance system reliability, and sustain operational effectiveness, utilizing Coralogix's analytics, alerts, and visualization tools to extract valuable insights from PagerDuty alerts and ensure an efficient and resilient incident management process.</p>"},{"location":"newoutput/pagerduty-data-ingestion/#get-started","title":"Get Started","text":"<p>STEP 1.\u00a0In your navigation pane, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 2.\u00a0In the Contextual Data section, select PagerDuty and click\u00a0+ ADD.</p> <p></p> <p>STEP 3. Click ADD NEW.</p> <p>STEP 4.\u00a0Fill in the Integration Details:</p> <ul> <li> <p>Name.\u00a0Name your integration.</p> </li> <li> <p>Your API Key. Click\u00a0CREATE NEW KEY\u00a0to generate an API key and name it.</p> </li> <li> <p>Application Name and Subsystem Name. Enter an\u00a0application and subsystem name.</p> </li> </ul> <p></p> <p>STEP 5.\u00a0Click\u00a0GENERATE URL. The URL for the integration will be automatically created. Use this when creating a Opsgenie webhook.</p> <p></p>"},{"location":"newoutput/pagerduty-data-ingestion/#create-a-pagerduty-webhook","title":"Create a PagerDuty Webhook","text":"<p>Create a PagerDuty webhook using your URL.</p> <p>STEP 1.\u00a0Log in to your PagerDuty\u00a0 account.</p> <p>STEP 2. Select Services and select Service directory to see your configured services.</p> <p></p> <p>STEP 3. Click the service you want to configure the webhook for and click Integrations.</p> <p>STEP 4. Click Add or manage extensions.</p> <p></p> <p>STEP 5. A new page will open. Click New extension.</p> <p></p> <p>STEP 6. Complete the form, entering the Coralogix generated URL in the URL field, and click Save.</p>"},{"location":"newoutput/pagerduty-data-ingestion/#example-log","title":"Example Log","text":"<pre><code>{\n   \"source_system\"  :  \"pagerduty\" ,\n   \"pagerduty\"  : {\n     \"event\"  :  \"incident.trigger\" ,\n     \"log_entries\"  : [\n      {\n         \"id\"  :  \"R0EYM81WSBZ27ACLRWOMSVVO77\" ,\n         \"type\"  :  \"trigger_log_entry\" ,\n         \"summary\"  :  \"Triggered through the website\" ,\n         \"self\"  :  \"https://api.pagerduty.com/log_entries/R0EYM81WSBZ27ACLRWOMSVVO77\" ,\n         \"html_url\"  :  \"https://coralogixtest.pagerduty.com/incidents/PRUIXXX/log_entries/R0EYM81WSBZ27ACLRWOMSVVO77\" ,\n         \"created_at\"  :  \"2021-02-22T08:44:45Z\" ,\n         \"agent\"  : {\n           \"id\"  :  \"P0G8XXX\" ,\n           \"type\"  :  \"user_reference\" ,\n           \"summary\"  :  \"John Doe\" ,\n           \"self\"  :  \"https://api.pagerduty.com/users/P0G8XXX\" ,\n           \"html_url\"  :  \"https://coralogixtest.pagerduty.com/users/P0G8XXX\" \n        },\n         \"channel\"  : {\n           \"type\"  :  \"web_trigger\" ,\n           \"summary\"  :  \"Outage\" ,\n           \"subject\"  :  \"Outage\" ,\n           \"details\"  :  \"Hey there, this is serious outage!\" ,\n           \"details_omitted\"  :  false ,\n           \"body_omitted\"  :  false \n        },\n         \"service\"  : {\n           \"id\"  :  \"PTIHXXX\" ,\n           \"type\"  :  \"service_reference\" ,\n           \"summary\"  :  \"web-server\" ,\n           \"self\"  :  \"https://api.pagerduty.com/services/PTIHXXX\" ,\n           \"html_url\"  :  \"https://coralogixtest.pagerduty.com/service-directory/PTIHXXX\" \n        },\n         \"incident\"  : {\n           \"id\"  :  \"PRUIXXX\" ,\n           \"type\"  :  \"incident_reference\" ,\n           \"summary\"  :  \"[#7] Outage\" ,\n           \"self\"  :  \"https://api.pagerduty.com/incidents/PRUIXXX\" ,\n           \"html_url\"  :  \"https://coralogixtest.pagerduty.com/incidents/PRUIXXX\" \n        },\n         \"teams\"  : [],\n         \"contexts\"  : [],\n         \"event_details\"  : {\n           \"description\"  :  \"Outage\" \n        }\n      }\n    ],\n     \"webhook\"  : {\n       \"endpoint_url\"  :  \"https://integrations.dev-shared.coralogix.net/v1/pagerduty/v1/events/98663b44-74e5-11eb-a35c-0a1196871111?appName=pagerfero&amp;amp;subSystem=stagging-incidents\" ,\n       \"name\"  :  \"Cora\" ,\n       \"description\"  :  null ,\n       \"webhook_object\"  : {\n         \"id\"  :  \"PTIHXXX\" ,\n         \"type\"  :  \"service_reference\" ,\n         \"summary\"  :  \"web-server\" ,\n         \"self\"  :  \"https://api.pagerduty.com/services/PTIHXXX\" ,\n         \"html_url\"  :  \"https://coralogixtest.pagerduty.com/service-directory/PTIHXXX\" \n      },\n       \"config\"  : {\n         \"referer\"  :  \"https://coralogixtest.pagerduty.com/services/PTIHXXX/integrations?service_profile=1\" \n      },\n       \"outbound_integration\"  : {\n         \"id\"  :  \"PJFWXXX\" ,\n         \"type\"  :  \"outbound_integration_reference\" ,\n         \"summary\"  :  \"Generic V2 Webhook\" ,\n         \"self\"  :  \"https://api.pagerduty.com/outbound_integrations/PJFWXXX\" ,\n         \"html_url\"  :  null \n      },\n       \"accounts_addon\"  :  null ,\n       \"id\"  :  \"PZNHXXX\" ,\n       \"type\"  :  \"webhook\" ,\n       \"summary\"  :  \"Cora\" ,\n       \"self\"  :  \"https://api.pagerduty.com/webhooks/PZNHXXX\" ,\n       \"html_url\"  :  null \n    },\n     \"incident\"  : {\n       \"incident_number\"  :  7 ,\n       \"title\"  :  \"Outage\" ,\n       \"description\"  :  \"Outage\" ,\n       \"created_at\"  :  \"2021-02-22T08:44:45Z\" ,\n       \"status\"  :  \"triggered\" ,\n       \"incident_key\"  :  \"a466d2a1d4a74932a3b980cb599e47ec\" ,\n       \"service\"  : {\n         \"id\"  :  \"PTIHXXX\" ,\n         \"name\"  :  \"web-server\" ,\n         \"description\"  :  \"Your first service - describe what this service is monitoring and any information that will help responders.\nFor example: What is the SLA of this service? Where are the runbooks for this service stored? What tier level is this service?\" ,\n         \"created_at\"  :  \"2021-02-09T16:00:23Z\" ,\n         \"updated_at\"  :  \"2021-02-09T16:00:23Z\" ,\n         \"status\"  :  \"critical\" ,\n         \"teams\"  : [],\n         \"alert_creation\"  :  \"create_alerts_and_incidents\" ,\n         \"addons\"  : [],\n         \"scheduled_actions\"  : [],\n         \"support_hours\"  :  null ,\n         \"last_incident_timestamp\"  :  \"2021-02-22T08:44:45Z\" ,\n         \"escalation_policy\"  : {\n           \"id\"  :  \"PLGAXXX\" ,\n           \"type\"  :  \"escalation_policy_reference\" ,\n           \"summary\"  :  \"Default\" ,\n           \"self\"  :  \"https://api.pagerduty.com/escalation_policies/PLGAXXX\" ,\n           \"html_url\"  :  \"https://coralogixtest.pagerduty.com/escalation_policies/PLGAXXX\" \n        },\n         \"incident_urgency_rule\"  : {\n           \"type\"  :  \"constant\" ,\n           \"urgency\"  :  \"high\" \n        },\n         \"acknowledgement_timeout\"  :  null ,\n         \"auto_resolve_timeout\"  :  null ,\n         \"alert_grouping\"  :  null ,\n         \"alert_grouping_timeout\"  :  null ,\n         \"alert_grouping_parameters\"  : {\n           \"type\"  :  null ,\n           \"config\"  :  null \n        },\n         \"integrations\"  : [\n          {\n             \"id\"  :  \"PXKGXXX\" ,\n             \"type\"  :  \"app_event_transform_inbound_integration_reference\" ,\n             \"summary\"  :  \"Coralogix\" ,\n             \"self\"  :  \"https://api.pagerduty.com/services/PTIHXXX/integrations/PXKGXXX\" ,\n             \"html_url\"  :  \"https://coralogixtest.pagerduty.com/services/PTIHXXX/integrations/PXKGXXX\" \n          }\n        ],\n         \"metadata\"  : {},\n         \"response_play\"  :  null ,\n         \"type\"  :  \"service\" ,\n         \"summary\"  :  \"web-server\" ,\n         \"self\"  :  \"https://api.pagerduty.com/services/PTIHXXX\" ,\n         \"html_url\"  :  \"https://coralogixtest.pagerduty.com/service-directory/PTIHXXX\" \n      },\n       \"assignments\"  : [\n        {\n           \"at\"  :  \"2021-02-22T08:44:45Z\" ,\n           \"assignee\"  : {\n             \"id\"  :  \"P0G8XXX\" ,\n             \"type\"  :  \"user_reference\" ,\n             \"summary\"  :  \"John Doe\" ,\n             \"self\"  :  \"https://api.pagerduty.com/users/P0G8XXX\" ,\n             \"html_url\"  :  \"https://coralogixtest.pagerduty.com/users/P0G8XXX\" \n          }\n        }\n      ],\n       \"assigned_via\"  :  \"escalation_policy\" ,\n       \"last_status_change_at\"  :  \"2021-02-22T08:44:45Z\" ,\n       \"first_trigger_log_entry\"  : {\n         \"id\"  :  \"R0EYM81WSBZ27ACLRWOMSVV111\" ,\n         \"type\"  :  \"trigger_log_entry_reference\" ,\n         \"summary\"  :  \"Triggered through the website\" ,\n         \"self\"  :  \"https://api.pagerduty.com/log_entries/R0EYM81WSBZ27ACLRWOMSVV111\" ,\n         \"html_url\"  :  \"https://coralogixtest.pagerduty.com/incidents/PRUIXXX/log_entries/R0EYM81WSBZ27ACLRWOMSVV111\" \n      },\n       \"alert_counts\"  : {\n         \"all\"  :  0 ,\n         \"triggered\"  :  0 ,\n         \"resolved\"  :  0 \n      },\n       \"is_mergeable\"  :  true ,\n       \"escalation_policy\"  : {\n         \"id\"  :  \"PLGAXXX\" ,\n         \"type\"  :  \"escalation_policy_reference\" ,\n         \"summary\"  :  \"Default\" ,\n         \"self\"  :  \"https://api.pagerduty.com/escalation_policies/PLGAXXX\" ,\n         \"html_url\"  :  \"https://coralogixtest.pagerduty.com/escalation_policies/PLGAXXX\" \n      },\n       \"teams\"  : [],\n       \"impacted_services\"  : [\n        {\n           \"id\"  :  \"PTIHXXX\" ,\n           \"type\"  :  \"service_reference\" ,\n           \"summary\"  :  \"web-server\" ,\n           \"self\"  :  \"https://api.pagerduty.com/services/PTIHXXX\" ,\n           \"html_url\"  :  \"https://coralogixtest.pagerduty.com/service-directory/PTIHXXX\" \n        }\n      ],\n       \"pending_actions\"  : [],\n       \"acknowledgements\"  : [],\n       \"basic_alert_grouping\"  :  null ,\n       \"alert_grouping\"  :  null ,\n       \"last_status_change_by\"  : {\n         \"id\"  :  \"PTIHUXXX\" ,\n         \"type\"  :  \"service_reference\" ,\n         \"summary\"  :  \"web-server\" ,\n         \"self\"  :  \"https://api.pagerduty.com/services/PTIHXXX\" ,\n         \"html_url\"  :  \"https://coralogixtest.pagerduty.com/service-directory/PTIHXXX\" \n      },\n       \"metadata\"  : {},\n       \"external_references\"  : [],\n       \"incidents_responders\"  : [],\n       \"responder_requests\"  : [],\n       \"subscriber_requests\"  : [],\n       \"urgency\"  :  \"high\" ,\n       \"id\"  :  \"PRUI111\" ,\n       \"type\"  :  \"incident\" ,\n       \"summary\"  :  \"[#7] Outage\" ,\n       \"self\"  :  \"https://api.pagerduty.com/incidents/PRUI111\" ,\n       \"html_url\"  :  \"https://coralogixtest.pagerduty.com/incidents/PRUI111\" ,\n       \"alerts\"  : []\n    },\n     \"id\"  :  \"36f99188-74ea-11eb-87ca-0242c0a81111\" ,\n     \"created_on\"  :  \"2021-02-22T08:44:45Z\" ,\n     \"account_features\"  : {\n       \"response_automation\"  :  true \n    },\n     \"account_id\"  :  \"PWLHXXX\" \n  }\n}\n\n</code></pre>"},{"location":"newoutput/pagerduty-data-ingestion/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/parse-json-field/","title":"Parse Json Field","text":"<p>Sometimes logs might be shipped escaped or stringified. With the Parse Json field rule, you can transform these logs to Json with no effort.</p> <p>Select the field that is escaped select then destination and you are all done.</p>"},{"location":"newoutput/parse-json-field/#configuration-steps","title":"Configuration steps.","text":"<ol> <li>Click on Data flow.</li> <li>Click on parsing rules and chose Parse Json field.</li> </ol> <p>3-Complete all the needed info, group name, rules matcher, rule's name etc.</p> <p></p> <ul> <li>Source field is the field you are trying to unescape.</li> <li>Merge into and Overwrite only apply if you are trying to overwrite or merge the data to an already existing field. If you are trying to create a new field these 2 options do not matter.</li> <li>Keep source field option keeps the source field and its content .</li> <li>Delete the source field it will remove the source field and its content.</li> </ul> <p>In the example below we are unescaping the log and making it in Json format.</p> <p></p> <p>Original Message:</p> <p></p> <p>Message after the rule:</p> <p></p> <p>If you chose a destination field that will cause a mapping exception, A message will pop to let you know that you will be creating an exception if you apply this rule.</p> <p></p>"},{"location":"newoutput/pay-as-you-go/","title":"Pay as you go","text":"<p>'Pay as you go' is an option that offers flexibility to ingest up to one time your contractual quota without commitment.</p> <p>Note: Keep in mind that data ingested as part of the 'Pay as you go' plan will be charged at a rate 25% higher than the standard plan.</p>"},{"location":"newoutput/pay-as-you-go/#activation","title":"Activation","text":"<p>To activate pay-as-you-go:</p> <p>1-On the right top corner, click your name.</p> <p>2-Click settings.</p> <p>3- On the left pan select 'Pay as you go'.</p> <p></p> <p>Click activate Pay as you go and you are all set.</p> <p></p> <p>If you have a custom plan, you will see the screen below, and you will need to contact Coralogix support to help you activate pay-as-you-go.</p> <p></p> <p>Note: It can take up to 5-min to deactivate your 'Pay as you go' plan.</p>"},{"location":"newoutput/perimeter-81-integration/","title":"Perimeter 81","text":"<p>Perimeter 81 is an Israeli cloud and network security company that develops secure remote networks for organizations based on zero-trust architecture. Its technology replaces legacy security appliances like VPNs and firewalls.</p> <p>Configure your Perimeter 81 data stream to an S3 bucket to have full visibility of your Perimeter 81 activity.</p>"},{"location":"newoutput/perimeter-81-integration/#prerequisites","title":"Prerequisites","text":"<p>1. AWS account with an S3 bucket for the Perimeter 81 logs destination</p> <p>2. AWS access with the ability to create and modify IAM resources</p> <p>3. Perimeter 81 account</p>"},{"location":"newoutput/perimeter-81-integration/#deployment","title":"Deployment","text":"<p>STEP 1. In AWS, create a new IAM policy with the following permissions:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n               \"s3:ListBucket\",\n               \"s3:GetBucketLocation\",\n               \"s3:PutObject\",\n               \"s3:GetObject\",\n               \"s3:DeleteObject\"\n            ],\n            \"Resource\": [\n              \"arn:aws:s3:::&lt;your_bucket_name&gt;\", \n              \"arn:aws:s3:::&lt;your_bucket_name&gt;/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>STEP 2. Create an IAM user with programmatic access and attach the policy created in STEP 1.</p> <p>STEP 3. In the Perimeter 81 platform, navigate to Settings &gt; Integrations.</p> <ul> <li> <p>Add a SIEM integration for S3.</p> </li> <li> <p>Paste the Access Key, Secret Access Key, and Bucket name.</p> </li> <li> <p>Click Validate.</p> </li> </ul> <p></p> <p>STEP 4. After the logs reach your S3 bucket, follow these instructions for AWS S3 Lambda log collection. Once the deployment is complete, every new gzipped log file placed in the Lambda is sent to Coralogix.</p>"},{"location":"newoutput/perimeter-81-integration/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/php-opentelemetry-instrumentation/","title":"PHP OpenTelemetry Instrumentation","text":"<p>This tutorial demonstrates how to instrument PHP applications to capture logs, metrics, and traces using\u00a0OpenTelemetry\u00a0and send them to Coralogix. It relies on a\u00a0Slim micro framework\u00a0application, but other web frameworks \u2013 such as WordPress, Symfony, or Laravel \u2013 can also be used.</p> <p>Instrumentation can be accomplished automatically or manually.\u00a0Automatic instrumentation\u00a0is the most efficient way to add instrumentation to PHP applications, injecting bytecode via an agent to capture telemetry.\u00a0Manual instrumentation\u00a0involves adding observability code to an app yourself. While this is more labor intensive, it provides the most control over the SDK configuration.</p>"},{"location":"newoutput/php-opentelemetry-instrumentation/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>A Coralogix\u00a0account, set up on the Coralogix\u00a0domain\u00a0corresponding to the region where your data is stored</p> </li> <li> <p>PHP 8.0+ for auto-instrumentation, and at least PHP 7.4 for manual-instrumentation.</p> </li> <li> <p>PECL - a repository for PHP Extensions.</p> </li> <li> <p>Composer - a dependency manager for PHP packages.</p> </li> </ul>"},{"location":"newoutput/php-opentelemetry-instrumentation/#extension-and-package-installation","title":"Extension and Package Installation","text":"<p>STEP 1. As the extension needs to be built from source, certain build tools are needed.</p> <pre><code>sudo apt-get install gcc make autoconf\n\n</code></pre> <p>STEP 2. Install the OpenTelemetry extension with\u00a0<code>PECL</code>.</p> <pre><code>pecl install opentelemetry\n\n</code></pre> <p>STEP 3. Add the extension to your\u00a0<code>php.ini</code>\u00a0file (e.g. /etc/php.in).</p> <pre><code>[opentelemetry]\nextension=opentelemetry.so\n\n</code></pre> <p>STEP 4. Verify that the extension is installed and enabled.</p> <pre><code>php --ri opentelemetry\n\n</code></pre> <p>STEP 5. Add the following packages to your application.</p> <pre><code>composer init \\\n  --no-interaction \\\n  --stability beta \\\n    --require slim/slim:\"^4\" \\\n  --require slim/psr7:\"^1\"\ncomposer config allow-plugins.php-http/discovery false\ncomposer update\n\n</code></pre> <p>Notes:</p> <ul> <li>If requested to \u201cDo you trust \"php-http/discovery\" to execute code\u2026\u201d, select No.</li> </ul>"},{"location":"newoutput/php-opentelemetry-instrumentation/#send-data-to-coralogix","title":"Send Data to Coralogix","text":"<p>Coralogix provides OTLP endpoints for various regions. Using the gRPC exporter can improve data transmission performance. Install the additional extensions via terminal.</p> <p>STEP 1. Install gRPC.</p> <pre><code>pecl install grpc\n\n</code></pre> <p>Notes:</p> <ul> <li>Installing gRPC make take several minutes, as the extension builds from source.</li> </ul> <p>STEP 2. Add the following extension to your\u00a0<code>php.ini</code>\u00a0file (e.g. /etc/php.in).</p> <pre><code>[grpc]\nextension=grpc.so\n\n</code></pre> <p>Notes:</p> <ul> <li>gRPC is a code generator for a remote procedure call (RPC) framework.</li> </ul> <p>STEP 3. Install the following packages.</p> <pre><code>composer require \\\n  open-telemetry/transport-grpc \\\n  open-telemetry/exporter-otlp \\\n  php-http/guzzle7-adapter\n\n</code></pre>"},{"location":"newoutput/php-opentelemetry-instrumentation/#automatic-instrumentation","title":"Automatic Instrumentation","text":"<p>Out of the box, zero-code instrumentation injects an OpenTelemetry PHP agent into the application and generates traces**.**</p> <p>STEP 1. Install Slim framework\u2019s auto-instrumentation library.</p> <pre><code>composer require \\\n    open-telemetry/sdk \\\n  open-telemetry/opentelemetry-auto-slim\n\n</code></pre> <p>Notes:</p> <ul> <li> <p><code>open-telemetry/sdk</code> provides an implementation of the OpenTelemetry API, and can be set up and configured in a number of ways.</p> </li> <li> <p><code>open-telemetry/opentelemetry-auto-slim</code>\u00a0is required to auto-instrument the\u00a0Slim micro framework.</p> </li> </ul> <p>STEP 2. The dependencies should be described in the\u00a0<code>composer.json</code>\u00a0file as shown below.</p> <pre><code>{\n    \"require\": {\n        \"slim/slim\": \"^4\",\n        \"slim/psr7\": \"^1\",\n                \"open-telemetry/transport-grpc\": \"^1.0\",\n        \"open-telemetry/exporter-otlp\": \"^1.0\",\n        \"php-http/guzzle7-adapter\": \"^1.0\"\n                \"open-telemetry/sdk\": \"^1.0\"\n                \"open-telemetry/opentelemetry-auto-slim\": \"^1.0@beta\",\n    },\n    \"minimum-stability\": \"beta\",\n    \"config\": {\n        \"allow-plugins\": {\n            \"php-http/discovery\": false\n        }\n    }\n}\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>To enable automatic instrumentation of other popular PHP frameworks, you need to install the respective \"auto\" packages via <code>composer</code>:</p> <ul> <li> <p>Laravel Framework - Contrib Repo <code>composer require open-telemetry/opentelemetry-auto-laravel</code></p> </li> <li> <p>Symfony Framework - Contrib Repo <code>composer require open-telemetry/opentelemetry-auto-symfony</code></p> </li> <li> <p>Wordpress Framework - Contrib Repo <code>composer require open-telemetry/opentelemetry-auto-wordpress</code></p> </li> </ul> </li> <li> <p>For more information about the various PHP library instrumentations, refer to the\u00a0OpenTelemetry PHP Registry.</p> </li> </ul> <p>STEP 3. Once you have installed the necessary extensions and packages, configure the instrumentation as environment variables to enable it. This follows the OpenTelemetry SDK Environment Variable Specification.</p> <pre><code>export OTEL_PHP_AUTOLOAD_ENABLED=true\nexport OTEL_TRACES_EXPORTER=otlp\nexport OTEL_METRICS_EXPORTER=none\nexport OTEL_LOGS_EXPORTER=none\nexport OTEL_EXPORTER_OTLP_PROTOCOL=grpc\nexport OTEL_EXPORTER_OTLP_ENDPOINT=\"https://&lt;coralogix_otel_endpoint&gt;\"\nexport OTEL_EXPORTER_OTLP_HEADERS=\"Authorization=Bearer &lt;CXPrivateKey&gt;\"\nexport OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;ServiceName&gt;,application.name=&lt;CXApplicationName&gt;,api.name=&lt;CXSubsystemName&gt;,cx.application.name=&lt;CXApplicationName&gt;,cx.subsystem.name=&lt;CXSubsystemName&gt;\n\n</code></pre> <ul> <li> <p><code>OTEL_PHP_AUTOLOAD_ENABLED</code>: For OpenTelemetry PHP, set to <code>true</code> to enable automatic instrumentation and globally register the SDK.</p> </li> <li> <p><code>OTEL_TRACES_EXPORTER</code>: As auto-instrumentation currently works on traces only, set the traces exporter to <code>otlp</code> to send it.</p> </li> <li> <p><code>OTEL_EXPORTER_OTLP_ENDPOINT</code>: Select the OpenTelemetry Endpoint associated with your Coralogix domain. Endpoint must have a protocol and fully qualified name (e.g. <code>https://ingress.coralogixstg.wpengine.com:443</code>).</p> </li> <li> <p><code>OTEL_EXPORTER_OTLP_HEADERS</code>: Input your Coralogix Send-Your-Data API key in the Authorization header for authentication.</p> </li> <li> <p><code>OTEL_RESOURCE_ATTRIBUTES</code>: Input the names of your application and subsystem. The <code>service.name</code> is used to name your service. These will be added to resource attributes:</p> <ul> <li> <p><code>service.name=&lt;ServiceName&gt;</code></p> </li> <li> <p><code>application.name=&lt;CXApplicationName&gt;</code></p> </li> <li> <p><code>api.name=&lt;CXSubSystemName&gt;</code></p> </li> <li> <p><code>cx.application.name=&lt;CXApplicationName&gt;</code></p> </li> <li> <p><code>cx.subsystem.name=&lt;CXSubSystemName&gt;</code></p> </li> </ul> </li> </ul> <p>STEP 4. Run the instrumentation.</p> <p>The <code>index.php</code> script below is a simple demo of a dice roll. It does not contain any code from the OpenTelemetry SDK.</p> <pre><code>&lt;?php\nuse Psr\\Http\\Message\\ResponseInterface as Response;\nuse Psr\\Http\\Message\\ServerRequestInterface as Request;\nuse Slim\\Factory\\AppFactory;\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n$app = AppFactory::create();\n\n$app-&gt;get('/rolldice', function (Request $request, Response $response) {\n    $result = random_int(1,6);\n    $response-&gt;getBody()-&gt;write(strval($result));\n    return $response;\n});\n\n$app-&gt;run();\n\n</code></pre> <p>STEP 5. Open a terminal window and run the instrumentation as a web server. Once the server is running, make HTTP calls to the application route using a browser or the\u00a0<code>curl</code>\u00a0command-line tool.</p> <pre><code>php -S localhost:8080\ncurl &lt;http://localhost:8080/rolldice&gt;\n\n</code></pre> <p>STEP 6. Explore your traces in the Coralogix app.</p> <p>In your Coralogix dashboard, click\u00a0Explore\u00a0&gt;\u00a0Tracing.\u00a0You should now see traces generated by your application.</p> <p>STEP 7. To add logging capabilities, the instrumentation code needs to be configured with a logger such as\u00a0<code>monolog</code>. To do this, add the required dependencies.</p> <pre><code>composer require \\\n  monolog/monolog \\\n  open-telemetry/opentelemetry-logger-monolog\n\n</code></pre> <p>STEP 8. Replace\u00a0<code>index.php</code>\u00a0script as follows. The command manually deploys the OpenTelemetry loggerProvider and attaches it to the OpenTelemetry Monolog Handler.</p> <pre><code>&lt;?php\nuse Monolog\\Logger;\nuse OpenTelemetry\\API\\Common\\Instrumentation\\Globals;\nuse OpenTelemetry\\Contrib\\Logs\\Monolog\\Handler;\nuse Psr\\Http\\Message\\ResponseInterface as Response;\nuse Psr\\Http\\Message\\ServerRequestInterface as Request;\nuse Psr\\Log\\LogLevel;\nuse Slim\\Factory\\AppFactory;\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n$loggerProvider = Globals::loggerProvider();\n$handler = new Handler(\n    $loggerProvider,\n    LogLevel::INFO\n);\n$monolog = new Logger('otel-php-monolog', [$handler]);\n\n$app = AppFactory::create();\n\n$app-&gt;get('/rolldice', function (Request $request, Response $response) use ($monolog) {\n    $result = random_int(1,6);\n    $response-&gt;getBody()-&gt;write(strval($result));\n    $monolog-&gt;info('dice rolled', ['result' =&gt; $result]);\n    return $response;\n});\n\n$app-&gt;run();\n\n</code></pre> <p>STEP 9. To enable traces and logs, configure the environment variables and set the respective telemetry exporters to use the\u00a0<code>otlp</code>\u00a0protocol.</p> <pre><code>export OTEL_PHP_AUTOLOAD_ENABLED=true\nexport OTEL_TRACES_EXPORTER=otlp\nexport OTEL_METRICS_EXPORTER=none\nexport OTEL_LOGS_EXPORTER=otlp\nexport OTEL_EXPORTER_OTLP_PROTOCOL=grpc\nexport OTEL_EXPORTER_OTLP_ENDPOINT=\"https://&lt;coralogix_otel_endpoint&gt;\"\nexport OTEL_EXPORTER_OTLP_HEADERS=\"Authorization=Bearer &lt;CXPrivateKey&gt;\"\nexport OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;ServiceName&gt;,application.name=&lt;CXApplicationName&gt;,api.name=&lt;CXSubsystemName&gt;,cx.application.name=&lt;CXApplicationName&gt;,cx.subsystem.name=&lt;CXSubsystemName&gt;\n\n</code></pre> <p>STEP 10. Start the built-in web server and call the \"rolldice\" route.</p> <pre><code>php -S localhost:8080\ncurl &lt;http://localhost:8080/rolldice&gt;\n\n</code></pre> <p>STEP 11. To view logs, click on the Explore &gt; Logs tab. The generated logs will be displayed.</p> <p></p>"},{"location":"newoutput/php-opentelemetry-instrumentation/#manual-instrumentation","title":"Manual Instrumentation","text":"<p>By adding observability code to an application using the OpenTelemetry PHP SDK, you gain greater control over which telemetry to provision and send. This approach also enables you to integrate OpenTelemetry with popular PHP libraries available in the community repository.</p>"},{"location":"newoutput/php-opentelemetry-instrumentation/#common-configuration","title":"Common Configuration","text":"<p>Before creating the respective providers for each type of observability, configure the Instrumentation SDK as environment variables using system <code>export</code> commands or within the PHP code using <code>putenv</code>. Set all observability exporters to <code>none</code> initially until manual instrumentation for each type is completed.</p> <pre><code>export OTEL_EXPORTER_OTLP_ENDPOINT=\"&lt;coralogix_otel_endpoint&gt;\"\nexport OTEL_TRACES_EXPORTER=none\nexport OTEL_METRICS_EXPORTER=none\nexport OTEL_LOGS_EXPORTER=none\nexport OTEL_EXPORTER_OTLP_PROTOCOL=grpc\nexport OTEL_EXPORTER_OTLP_HEADERS=\"Authorization=Bearer &lt;CXPrivateKey&gt;\"\nexport OTEL_RESOURCE_ATTRIBUTES=service.name=&lt;ServiceName&gt;,application.name=&lt;CXApplicationName&gt;,api.name=&lt;CXSubsystemName&gt;,cx.application.name=&lt;CXApplicationName&gt;,cx.subsystem.name=&lt;CXSubsystemName&gt;\n\n</code></pre> <ul> <li>Set all traces, metrics and logs (e.g. <code>OTEL_&lt;OBSERVABILITY&gt;_EXPORTER</code> ) to <code>none</code> first.</li> </ul>"},{"location":"newoutput/php-opentelemetry-instrumentation/#traces","title":"Traces","text":"<p>STEP 1: Set the <code>otlp</code> exporter for traces</p> <pre><code>export OTEL_TRACES_EXPORTER=otlp\n\n</code></pre> <p>STEP 2: Replace the <code>index.php</code> script with the following code. This code manually creates spans with additional information that provides insights to the trace telemetry.</p> <pre><code>&lt;?php\nuse OpenTelemetry\\SDK\\Trace\\TracerProviderFactory;\nuse Psr\\Http\\Message\\ResponseInterface as Response;\nuse Psr\\Http\\\\Message\\ServerRequestInterface as Request;\nuse Psr\\Log\\LogLevel;\nuse Slim\\Factory\\AppFactory;\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n//Initialize TraceProvider and Tracer\n$tracerFactory = new TracerProviderFactory();\n$tracerProvider = $tracerFactory-&gt;create();\n$tracer = $tracerProvider-&gt;getTracer('demo_trace');\n\n$app = AppFactory::create();\n\n$app-&gt;get('/rolldice', function (Request $request, Response $response) use ($tracer) {\n    //create spans\n    $span = $tracer-&gt;spanBuilder(\"roll_event\")-&gt;startSpan();    \n\n    $result = random_int(1,6);\n    $response-&gt;getBody()-&gt;write(strval($result));\n\n    //set attributes and events to span\n    $span-&gt;setAttribute(\"dice_roll\", $result);\n    $span-&gt;addEvent('rolled_event', [\n        /key:value\n                'roll_value' =&gt; $result,\n    ]);\n\n    //end span to send\n    $span-&gt;end();\n    return $response;\n});\n\n$app-&gt;run();\n\n//shutdown providers \n$tracerProvider-&gt;shutdown();\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>The <code>TracerProviderFactory</code> class from the SDK provides a way to create the necessary <code>TracerProvider</code> by using system environment variables as the default.</p> </li> <li> <p><code>getTracer()</code> creates instances of <code>$tracer</code> with a required name (and other attributes) to propagate Context information about the system.</p> </li> <li> <p>You can add information to spans using <code>setAttribute</code> and <code>addEvent</code> to provide additional details to the trace. Span Attributes store specific operation properties, while Span Events can carry zero or more Span Attributes, which are themselves key:value maps.</p> </li> <li> <p>It is necessary to call <code>end()</code> at the end of a any code logic to ensure that it gets sent.</p> </li> <li> <p>It is important to call <code>shutdown()</code> at the end of the instrumentation to allow the <code>TracerProvider</code> to do any necessary cleanup and export telemetry.</p> </li> </ul> <p>STEP 3: To generate traces, run the PHP web-server in the directory. Once the server is running, make HTTP calls to the application route using a browser or the <code>curl</code> command-line tool.</p> <pre><code>php -S localhost:8080\ncurl &lt;http://localhost:8080/rolldice&gt;\n\n</code></pre> <p>STEP 4: To view the traces generated by your demo rolldice application, go to your Coralogix dashboard and click on Explore &gt; Traces.</p> <p></p>"},{"location":"newoutput/php-opentelemetry-instrumentation/#metrics","title":"Metrics","text":"<p>STEP 1: Set the <code>otlp</code> exporter for metrics.</p> <pre><code>export OTEL_METRICS_EXPORTER=otlp\n\n</code></pre> <p>STEP 2: Replace the <code>index.php</code> script with the following code to add meter and metrics instrument. Instrumenting metrics via the OpenTelemetry SDK.</p> <pre><code>&lt;?php\nuse OpenTelemetry\\SDK\\Metrics\\MeterProviderFactory;\nuse Psr\\Http\\Message\\ResponseInterface as Response;\nuse Psr\\Http\\Message\\ServerRequestInterface as Request;\nuse Slim\\Factory\\AppFactory;\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n//Initialize Meters\n$meterFactory = new MeterProviderFactory();\n$meterProvider = $meterFactory-&gt;create();\n$meter = $meterProvider-&gt;getMeter('demo_meter');\n\n//Initialise Histogram\n$histogram = $meter-&gt;createHistogram('roll', 'num', 'The output of roll result');\n\n$app = AppFactory::create();\n\n$app-&gt;get('/rolldice', function (Request $request, Response $response) use ($histogram) {\n\n    $result = random_int(1,6);\n    $response-&gt;getBody()-&gt;write(strval($result));\n\n    //add to the metric counter\n    $histogram-&gt;record($result);\n\n    return $response;\n});\n\n$app-&gt;run();\n\n//shutdown providers\n$meterProvider-&gt;shutdown();\n\n</code></pre> <ul> <li> <p>The <code>MeterProviderFactory</code> class in the SDK provides a way to create the necessary <code>MeterProvider</code> by using system environment variables as the default.</p> </li> <li> <p>Calling <code>getMeter()</code> creates instances of <code>$meter</code> with a required name, and other optional attributes such as <code>version</code> and <code>schema_url</code>, to define the instrumentation scope of emitted telemetry. Meters are identified by these fields, and are considered identical if all identifying fields are equal.</p> </li> <li> <p>To create a histogram instrument, use the <code>createHistogram</code> function. Other types of instruments can also be created; see Meter Operations for details.</p> </li> <li> <p>Use <code>record()</code> to store data related to the meter instrument.</p> </li> <li> <p>Metrics are exported when either <code>forceFlush()</code> or <code>shutdown()</code> is called on the meter provider. It is important to call <code>shutdown()</code> at the end of the instrumentation to allow the <code>MeterProvider</code> to do any necessary cleanup and export telemetry.</p> </li> </ul> <p>STEP 3: To generate metrics, run the PHP web-server in the directory. Once the server is running, make HTTP calls to the application route using a browser or the <code>curl</code> command-line tool.</p> <pre><code>php -S localhost:8080\ncurl &lt;http://localhost:8080/rolldice&gt;\n\n</code></pre> <p>STEP 4: To view the metrics, access the Grafana Dashboard on Coralogix. Click on Explore, then select \u201croll_num_sum\u201d in the metrics browser. Next, use the roll_num_sum{} or any other relevant \u201croll_num\u201d metric and click on Run query.</p> <p></p>"},{"location":"newoutput/php-opentelemetry-instrumentation/#logs","title":"Logs","text":"<p>STEP 1: Set the <code>otlp</code> exporter for logs.</p> <pre><code>export OTEL_LOGS_EXPORTER=otlp\n\n</code></pre> <p>STEP 2: Install the following packages:</p> <p>As logging is a mature and well-established function, OpenTelemetry's Logger approach is not intended to be used directly but rather to be integrated into existing loggers, such as Monolog.</p> <pre><code>composer require \\\n  monolog/monolog \\\n  open-telemetry/opentelemetry-logger-monolog\n\n</code></pre> <ul> <li> <p>The Monolog package and OpenTelemetry's logger integration with Monolog are required.</p> </li> <li> <p>The OpenTelemetry handler, configured with an OpenTelemetry\u00a0<code>LoggerProvider</code>, is used to send Monolog\u00a0<code>LogRecord</code>s to OpenTelemetry.</p> </li> </ul> <p>STEP 3: Add logs via Monolog:</p> <p>Replace the <code>index.php</code> script with the following code.</p> <pre><code>&lt;?php\nuse Monolog\\Logger;\nuse OpenTelemetry\\Contrib\\Logs\\Monolog\\Handler as MonologHandler;\nuse OpenTelemetry\\SDK\\Logs\\LoggerProviderFactory;\n\nuse Psr\\Http\\Message\\ResponseInterface as Response;\nuse Psr\\Http\\Message\\ServerRequestInterface as Request;\nuse Psr\\Log\\LogLevel;\nuse Slim\\Factory\\AppFactory;\n\nrequire __DIR__ . '/vendor/autoload.php';\n\n//Initialize Logger\n$loggerFactory = new LoggerProviderFactory();\n$loggerProvider = $loggerFactory-&gt;create();\n$handler = new MonologHandler(\n    $loggerProvider,\n    LogLevel::DEBUG,\n);\n\n//Initialize Monolog\n$monolog = new Logger('logger', [$handler]);\n$monolog-&gt;info('hello world');\n\n$app = AppFactory::create();\n\n$app-&gt;get('/rolldice', function (Request $request, Response $response) use ($monolog) {\n\n    $result = random_int(1,6);\n    $response-&gt;getBody()-&gt;write(strval($result));\n\n    //log results\n    $monolog-&gt;debug('result rolled ' . strval($result));\n\n    return $response;\n});\n\n$app-&gt;run();\n\n//shutdown providers\n$loggerProvider-&gt;shutdown();\n\n</code></pre> <ul> <li> <p>The <code>LoggerProviderFactory</code> class in the SDK provides a way to create the necessary <code>LoggerProvider</code> by using system environment variables as the default.</p> </li> <li> <p>Thus <code>MonologHandler</code> is configured with the OpenTelemetry <code>LoggerProvider</code> to send log records from Monolog to OpenTelemetry by setting as the default handler for <code>$monolog</code>.</p> </li> <li> <p>Send logs at different levels (e.g. debug, info, error) with the <code>$monolog</code> logger.</p> </li> <li> <p>It is important to call <code>shutdown()</code> at the end of the instrumentation to allow the <code>LoggerProvider</code> to do any necessary cleanup and export telemetry.</p> </li> </ul> <p>STEP 3: To generate logs, run the PHP web-server in the directory. Once the server is running, make HTTP calls to the application route using a browser or the <code>curl</code> command-line tool.</p> <pre><code>php -S localhost:8080\ncurl &lt;http://localhost:8080/rolldice&gt;\n\n</code></pre> <p>STEP 4: To view the logs generated by your demo rolldice application, go to your Coralogix dashboard and click on Explore &gt; Logs and search logs.</p> <p></p>"},{"location":"newoutput/php-opentelemetry-instrumentation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"newoutput/php-opentelemetry-instrumentation/#php-extensions","title":"PHP Extensions","text":"<ul> <li> <p>To ensure that the <code>opentelemetry-beta</code>, <code>grpc</code>, and <code>protobuf</code> modules are installed and declared correctly in the <code>php.ini</code> file, run the command <code>php -m</code> to list all the loaded PHP modules.</p> </li> <li> <p>If the <code>grpc</code> module takes too long to install, it is because it compiles from source. However, building on Docker and using the docker-php-extension-installer can help create a base layer or image, so that all the modules do not need to be re-installed.</p> </li> </ul>"},{"location":"newoutput/php-opentelemetry-instrumentation/#telemetry-not-sending","title":"Telemetry Not Sending","text":"<p>If traces, metrics or logs are not appearing in Coralogix, use the following to check your instrumentation:</p> <ul> <li> <p>To verify that all OpenTelemetry environment variables are configured properly, use the <code>printenv</code> command in the terminal.</p> </li> <li> <p>Check that all dependencies are properly included in the <code>composer.json</code> file. You can run <code>composer show --installed</code> to see the list of installed packages.</p> </li> <li> <p>Check that telemetry is getting generated via the console by changing the respective exporters to <code>console</code>.</p> </li> </ul> <pre><code>export OTEL_LOGS_EXPORTER=console\nexport OTEL_TRACES_EXPORTER=console\nexport OTEL_METRICS_EXPORTER=console\n</code></pre>"},{"location":"newoutput/php-opentelemetry-instrumentation/#expected-output-of-traces","title":"Expected Output of Traces","text":"<pre><code>[\n  {\n    \"name\": \"GET /rolldice\",\n    \"context\": {\n      \"trace_id\": \"16d7c6da7c021c574205736527816eb7\",\n      \"span_id\": \"268e52331de62e33\",\n      \"trace_state\": \"\"\n    },\n    \"resource\": {\n      \"service.name\": \"__root__\",\n      \"service.version\": \"1.0.0+no-version-set\",\n      \"telemetry.sdk.name\": \"opentelemetry\",\n      \"telemetry.sdk.language\": \"php\",\n      \"telemetry.sdk.version\": \"1.0.0beta10\",\n      \"telemetry.auto.version\": \"1.0.0beta5\",\n      \"process.runtime.name\": \"cli-server\",\n      \"process.runtime.version\": \"8.2.6\",\n      \"process.pid\": 24435,\n      \"process.executable.path\": \"/bin/php\",\n      \"process.owner\": \"php\",\n      \"os.type\": \"darwin\",\n      \"os.description\": \"22.4.0\",\n      \"os.name\": \"Darwin\",\n      \"os.version\": \"Darwin Kernel Version 22.4.0: Mon Mar  6 20:59:28 PST 2023; root:xnu-8796.101.5~3/RELEASE_ARM64_T6000\",\n      \"host.name\": \"OPENTELEMETRY-PHP\",\n      \"host.arch\": \"arm64\"\n    },\n    \"parent_span_id\": \"\",\n    \"kind\": \"KIND_SERVER\",\n    \"start\": 1684749478068582482,\n    \"end\": 1684749478072715774,\n    \"attributes\": {\n      \"code.function\": \"handle\",\n      \"code.namespace\": \"Slim\\App\",\n      \"code.filepath\": \"/vendor/slim/slim/Slim/App.php\",\n      \"code.lineno\": 197,\n      \"http.url\": \"http://localhost:8080/rolldice\",\n      \"http.method\": \"GET\",\n      \"http.request_content_length\": \"\",\n      \"http.scheme\": \"http\",\n      \"http.status_code\": 200,\n      \"http.flavor\": \"1.1\",\n      \"http.response_content_length\": \"\"\n    },\n    \"status\": {\n      \"code\": \"Unset\",\n      \"description\": \"\"\n    },\n    \"events\": [],\n    \"links\": []\n  }\n]\n</code></pre> <p>For more information, please refer to the SDK Specification for Environment Variables and the OpenTelemetry PHP Documentation.</p>"},{"location":"newoutput/php-opentelemetry-instrumentation/#additional-resources","title":"Additional Resources","text":"ExternalSDK Specification for Environment VariablesOpenTelemetry PHP Documentation"},{"location":"newoutput/php-opentelemetry-instrumentation/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/pingsafe-using-fluentd/","title":"PingSafe","text":"<p>PingSafe is an industry-leading cloud security platform, which scans your cloud infrastructure from an attacker's lens. Security lapses are identified, prioritized, and auto-remediated to eliminate unwanted business impacts.</p> <p>This tutorial demonstrates how to undertake the PingSafe integration to Coralogix via webhook using Fluentd, allowing you to seamlessly send us your logs.</p>"},{"location":"newoutput/pingsafe-using-fluentd/#prerequisites","title":"Prerequisites","text":"<p>1. Server to install Fluentd</p> <p>2. Static public IP allocated to the server for initial configuration</p> <p>3. Active PingSafe account</p> <p>The webhook is configured to send data to the instance where Fluentd is installed, using the particular port configured in the Fluentd configuration file.</p>"},{"location":"newoutput/pingsafe-using-fluentd/#deployment","title":"Deployment","text":"<p>STEP 1. Install and configure Fluentd on your server. Additional information can be found here.</p> <p>STEP 2. Under /etc/td-agent/, edit the configuration file called td-agent.conf and replace the content with the following configuration:</p> <pre><code>&lt;system&gt;\n  log_level info\n&lt;/system&gt;\n\n&lt;source&gt;\n  @type http\n  @label @CORALOGIX\n  port 9880\n  bind 0.0.0.0\n  body_size_limit 32m\n  keepalive_timeout 10s\n&lt;/source&gt;\n\n&lt;label @CORALOGIX&gt;\n  &lt;filter **&gt;\n  @type record_transformer\n  @log_level warn\n  enable_ruby true\n  auto_typecast true\n  renew_record true\n  &lt;record&gt;\n    applicationName \"application_name\"\n    subsystemName \"subsystem_name\"\n    text ${record.to_json}\n  &lt;/record&gt;\n  &lt;/filter&gt;\n\n&lt;match **&gt;\n  @type http\n  @id http_to_coralogix\n  endpoint \"https://api.&lt;coralogix domain&gt;/logs/rest/singles\" \n  headers {\"private_key\":\"Your Coralogix account private key\"}\n  retryable_response_codes 503\n  error_response_as_unrecoverable false\n  &lt;buffer&gt;\n    @type memory\n    chunk_limit_size 10MB\n    compress gzip\n    flush_interval 1s\n    retry_max_times 5\n    retry_type periodic\n    retry_wait 2\n  &lt;/buffer&gt;\n  &lt;secondary&gt;\n    #If any messages fail to send they will be send to STDOUT for debug.\n    @type stdout\n  &lt;/secondary&gt;\n&lt;/match&gt;\n&lt;/label&gt;\n</code></pre> <p>Replace the values for:</p> <ul> <li> <p>applicationName &amp; subsystemName: Application and subsystem names to be displayed in your Coralogix dashboard</p> </li> <li> <p>endpoint: Replace  with your Coralogix domain <li> <p>private_key: Replace with your Coralogix Send-Your-Data API key</p> </li> <p>Save and configure your td-agent.conf file.</p> <p>STEP 3. Test the configuration:</p> <pre><code>td-agent --dry-run\n</code></pre> <p>STEP 4. Start the td-agent service:</p> <pre><code>sudo systemctl start td-agent.service\n</code></pre> <p>STEP 5. Check the status:</p> <pre><code>sudo systemctl status td-agent.service\n</code></pre> <p>STEP 6. To test the full flow, run this curl command:</p> <pre><code>curl --header \"Content-Type: application/json\" \\\n  --request POST \\\n  --data '{\"example_key\":\"example_value\"}' \\\n http://&lt;ip of the server&gt;:9880\n</code></pre>"},{"location":"newoutput/pingsafe-using-fluentd/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/plan-and-payments-management/","title":"Plan and Payments Management","text":"<p>Once you create your account you will need to set up your plan based on amount of logs you are going to send to Coralogix. To calculate a needed quota you need to decide which your data are high, medium, low priority. 1 GB purchased for frequent search (high priority) is equivalent to 2.5GB for monitoring (medium priority) and 6.5GB for compliance (Low priority). To learn more about TCO click here.</p> <p>To set your plan open Settings --&gt; Plan. Next choose the retention time from drop-down list and then click on the Select Plan button.</p> <p></p> <p>In the pop-up window click on the Update button if you need more than 1GB per day:</p> <p></p> <p>Put the quantity for GB Ingested per day. The calculation per month is done automatically at the Total section:</p> <p></p> <p>Click the Confirm button to apply the changes. You will get back to the earlier popup window where other information should be provided. Once the form is filled, click the Confirm button.</p> <p>You will see Account details where you can check your current plan, update your plan and your credit card details. In the Invoices section you can review and download your invoices and check the payment status.</p> <p>Example:</p> <p></p>"},{"location":"newoutput/pod-host/","title":"Pod & Host","text":"<p>Our application performance monitoring (APM) provides you with all logs relevant to a particular span context, granting a full picture of the services that power your applications.</p> <p>Use our newest layers of observability \u2013\u00a0<code>**POD**</code>\u00a0and\u00a0<code>**HOST**</code>\u00a0\u2013 to:</p> <ul> <li> <p>Instantly view all of your pod and host metrics, including resource consumption and associated network information</p> </li> <li> <p>Compare metrics within a specific pod and across pods from a specific service</p> </li> <li> <p>Correlate between Kubernetes spans, logs, and metrics for a specific pod and/or host</p> </li> <li> <p>Troubleshoot log span errors</p> </li> <li> <p>Annotate deployment tags based on span context</p> </li> </ul>"},{"location":"newoutput/pod-host/#concepts","title":"Concepts","text":"<p>Spans and traces form the basis of your Coralogix APM journey.</p> <p>Using this telemetry data, Coralogix allows you to observe\u00a0application resource consumption\u00a0and\u00a0infrastructure resource consumption\u00a0using two new observability layers.</p> POD application resource consumption Key factors that impact response times and throughput of applications HOST infrastructure resource consumption Usage of IT resources, systems, and processes"},{"location":"newoutput/pod-host/#access-pod-host-features","title":"Access Pod &amp; Host Features","text":"<p>STEP 1. In your navigation pane, click Explore &gt; Tracing. Click on the trace of interest.</p> <p></p> <p>STEP 2. Click VIEW RESOURCES \u2192 in your right-hand tool bar.</p> <p></p> <p>STEP 3. Click on the\u00a0<code>POD</code>\u00a0and\u00a0<code>HOST</code>\u00a0tabs, in addition to\u00a0<code>RELATED LOGS</code>\u00a0and\u00a0<code>SPAN LOGS</code>.</p> <p></p> <p></p>"},{"location":"newoutput/pod-host/#additional-resources","title":"Additional Resources","text":"DocumentationApplication Performance MonitoringDistributed TracingBlogOne Click Visibility: Coralogix Expands APM Capabilities to Kubernetes"},{"location":"newoutput/pod-host/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/prometheus/","title":"Prometheus","text":"<p>Prometheus is currently the leading tool for metric collection, it's easy to integrate and easy to use.</p> <p>Still, short-term retention is a big struggle faced by Prometheus users.</p> <p>Coralogix helps you overcome this struggle by providing you a way to automatically ship your metrics into your Coralogix account and store them long-term without making complex changes to your Prometheus architecture.</p> <p>Enjoy our new APM features when using OpenTelemetry collector with a Kubernetes processor and Prometheus.</p>"},{"location":"newoutput/prometheus/#send-prometheus-metrics-to-coralogix","title":"Send Prometheus Metrics to Coralogix","text":"<p>In order to send metrics to Coralogix, select a Prometheus RemoteWrite endpoint URL for the domain associated with your Coralogix account.</p>"},{"location":"newoutput/prometheus/#using-a-yaml-file","title":"Using a YAML File","text":"<p>Under root level add remote_write using the following format:</p> <pre><code>remote_write:\n- url: &lt;endpoint&gt;\n  name: '&lt;customer_name&gt;'\n  remote_timeout: 120s\n  bearer_token: '&lt;Send_Your_Data_API_key&gt;'\n</code></pre>"},{"location":"newoutput/prometheus/#using-prometheus-operator","title":"Using Prometheus Operator","text":"<p>Prometheus Operator provides Kubernetes native deployment.</p> <p>You can add remoteWrite in the following format (note the camel case):</p> <pre><code>remoteWrite:\n    url: &lt;endpoint&gt;\n    name: '&lt;customer_name&gt;'\n    remoteTimeout: 120s\n    bearerToken: '&lt;Send_Your_Data_API_key&gt;'\n</code></pre> <p>Note:</p> <ul> <li>The Prometheus operator must be in\u00a0v0.59.0 at least\u00a0in order to support the agent mode.</li> </ul>"},{"location":"newoutput/prometheus/#guidelines-parameters","title":"Guidelines &amp; Parameters","text":"<ul> <li> <p><code>url</code>: query params that send metric information to Coralogix. These params contain appLabelName, subSystemLabelName and severityLabelName. The parameters you should provide are the\u00a0keys\u00a0you choose to send to Coralogix, they are mapped to metric labels.</p> </li> <li> <p><code>name</code>: The name of the time series. This attribute was added after Prometheus v2.15.0. If added before, it will produce an error.</p> </li> <li> <p><code>remoteTimeout</code>: Timeout for requests to the remote write endpoint.</p> </li> <li> <p><code>bearerToken</code>: Input your Coralogix Send-Your-Data API key.</p> </li> <li> <p>Severities values that are valid in Coralogix are as follows: debug, verbose, info, warning, error, and critical. The default severity when leaving <code>severityLabelName</code> empty is info.</p> </li> </ul>"},{"location":"newoutput/prometheus/#server-responses","title":"Server Responses","text":"<p>The following is a list of possible server responses.</p> <ul> <li> <p><code>HTTP 200</code>: The protobuf file is valid and contains Prometheus Metadata.</p> </li> <li> <p><code>HTTP 201</code>: The protobuf file is valid and processed by Coralogix.</p> </li> <li> <p><code>HTTP 204</code>: The protobuf file is valid, but the Prometheus.WriteRequest is empty.</p> </li> <li> <p><code>HTTP 400</code>: The REST API request itself was malformed. Check the query params.</p> </li> <li> <p><code>HTTP 401</code>: The customer is unauthorized. Verify your private key.</p> </li> </ul>"},{"location":"newoutput/prometheus/#grafana-visibility","title":"Grafana Visibility","text":"<p>Coralogix enables you to integrate and view your metrics from Grafana as well.</p> <p></p> <p>To connect Grafana to your account you can follow this tutorial: https://coralogixstg.wpengine.com/tutorials/grafana-plugin/</p>"},{"location":"newoutput/prometheus/#coralogix-visibility","title":"Coralogix Visibility","text":"<p>To view the metrics in your Coralogix dashboard:</p> <ol> <li> <p>Navigate to 'Grafana Explore'</p> </li> <li> <p>Choose 'Metrics' as a data source.</p> </li> <li> <p>Expand 'Metrics browser.'</p> </li> <li> <p>Select a metric.</p> </li> </ol> <p></p> <p>Enabling Metrics Archive in Coralogix</p> <p>Learn more on how to enable your S3 archive for metrics: Connect S3 Archive</p>"},{"location":"newoutput/prometheus/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/prometheus-agent/","title":"Prometheus Agent","text":"<p>Send your data to Coralogix using Prometheus Agent, a new operational mode of running Prometheus, built directly into the Prometheus binary. The agent mode optimizes the remote write use case configuring the Prometheus instance while disabling some of Prometheus' usual features - querying and alerting.</p> <p>Use agent mode to:</p> <ul> <li> <p>Focus on scraping metrics and sending them to Coralogix, with querying and alerting disabled</p> </li> <li> <p>Enable easier horizontal scalability for ingestion compared to server-mode</p> </li> <li> <p>Increase efficiency by replacing local storage with a customized TSDB WAL</p> </li> <li> <p>Improve scalability by enabling horizontal scalability for ingestion</p> </li> </ul>"},{"location":"newoutput/prometheus-agent/#prerequisites","title":"Prerequisites","text":"<p>1. Sign up for a Coralogix account. Set up your account on the Coralogix domain corresponding to the region within which you would like your data stored.</p> <p>2. Access your Coralogix Send-Your-Data API key.</p> <p>3. Install and configure Prometheus Operator. Prometheus Agent collects ServiceMonitors and PodMonitors, which are enabled only when using the Prometheus Operator.</p> <p>Note:</p> <ul> <li>The Prometheus operator must be in v0.59.0 at least in order to support the agent mode.</li> </ul>"},{"location":"newoutput/prometheus-agent/#configuration-installation","title":"Configuration &amp; Installation","text":"<p>STEP 1. Create a secret private key.</p> <p>Create a Kubernetes\u00a0<code>secret</code>\u00a0for your Coralogix Send-Your-Data API key\u00a0called\u00a0<code>coralogix-keys</code> with the value\u00a0<code>PRIVATE_KEY</code>.</p> <p>Take this step in order to ensure that your private key remains protected and unexposed. While other methods exist for protecting this private information, we recommend creating a secret in this manner while running on a Kubernetes cluster.</p> <p>Your private key, as well as the Helm chart, should be saved in the same namespace.</p> <pre><code>kubectl create secret generic coralogix-keys \\\\\n  --from-literal=PRIVATE_KEY=&lt;private-key&gt;\n\n</code></pre> <p>The created secret should look like this:</p> <pre><code>apiVersion: v1\ndata:\n  PRIVATE_KEY: &lt;encrypted-private-key&gt;\nkind: Secret\nmetadata:\n  name: coralogix-keys\n  namespace: &lt;the-release-namespace&gt;\ntype: Opaque\n\n</code></pre> <p>STEP 2. Configure your Coralogix endpoints.</p> <p>Select a Prometheus RemoteWrite endpoint for the domain associated with your Coralogix account.</p> <p>STEP 3. Add the Coralogix Helm charts repository.</p> <p>Add the Coralogix Helm charts repository to the local repos list with the following command:</p> <pre><code>helm repo add coralogix-charts-virtual https://cgx.jfrog.io/artifactory/coralogix-charts-virtual\n\n</code></pre> <p>To get the updated Helm charts from the added repository, run:</p> <pre><code>helm repo update\n\n</code></pre> <p>STEP 4. Create an override.yaml file.</p> <p>Create an override.yaml file, which includes the following:</p> <pre><code>---\n# override.yaml:\nprometheus:\n  prometheusSpec:\n    remoteWrite:\n    - authorization:\n        credentials:\n          name: coralogix-keys\n          key: PRIVATE_KEY\n      name: prometheus-agent-coralogix\n      queueConfig:\n        capacity: 2500\n        maxSamplesPerSend: 1000\n        maxShards: 200\n      remoteTimeout: 120s\n      url: &lt;YOUR_ENDPOINT&gt;\n\n\n</code></pre> <p>STEP 5. Install the chart.</p> <pre><code>helm upgrade prometheus-agent coralogix-charts-virtual/prometheus-agent-coralogix \\\n  --install \\\n  --namespace=&lt;your-namespace&gt; \\\n  --create-namespace \\\n  -f override.yaml\n\n</code></pre>"},{"location":"newoutput/prometheus-agent/#best-practices","title":"Best Practices","text":"<p>By default, the Prometheus agent uses ephemeral volumes, which is not suitable for a production environment.</p> <p>For a production environment, we highly recommended defining a persistent volume to avoid data loss between restarts. This is done by using the specification available on the values file.</p> <p>Note:</p> <p>If you do not specify the\u00a0<code>storageClassName</code>,\u00a0Kubernetes will use the default storage class available on the cluster.</p> <pre><code>prometheus:\n  prometheusSpec:\n    storageSpec:\n      volumeClaimTemplate:\n        spec:\n          accessModes: [\"ReadWriteOnce\"]\n          resources:\n            requests:\n              storage: 50Gi\n\n</code></pre>"},{"location":"newoutput/prometheus-agent/#limits-quotas","title":"Limits &amp; Quotas","text":"<p>Coralogix places the following limits on endpoints:</p> <ul> <li> <p>A hard limit of 10MB of data to our OpenTelemetry endpoint, with a recommendation of 2MB</p> </li> <li> <p>A hard limit of 2411724 bytes of data to our Prometheus RemoteWrite endpoint, with a recommendation for any amount less than this limit.</p> </li> </ul> <p>Limits apply to single requests, regardless of timespan.</p>"},{"location":"newoutput/prometheus-agent/#additional-resources","title":"Additional Resources","text":"DocumentationPrometheus Operator"},{"location":"newoutput/prometheus-agent/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/prometheus-alertmanager/","title":"Prometheus Alertmanager Data Ingestion","text":"<p>Collect your Prometheus alerts in the Coralogix platform using our automatic Contextual Data Integration Package. The package automatically generates a URL to be used when creating a Prometheus webhook.</p>"},{"location":"newoutput/prometheus-alertmanager/#overview","title":"Overview","text":"<p>Prometheus is an open-source monitoring and alerting toolkit initially developed by SoundCloud. It is widely used in modern cloud-native environments to monitor the health and performance of applications and infrastructure. Prometheus collects time-series data from various sources through a pull model and stores it in a time-series database. With its flexible query language, Prometheus allows users to explore and analyze metrics data, enabling proactive monitoring and troubleshooting. Additionally, it features a powerful alerting system that can be configured to send notifications when predefined conditions are met, empowering teams to respond promptly to potential issues and maintain the reliability of their systems. As a key component of the Cloud Native Computing Foundation (CNCF) landscape, Prometheus has become a fundamental tool for monitoring in containerized and distributed architectures.</p> <p>Sending your Prometheus alerts to Coralogix streamlines alert aggregation, enhances monitoring capabilities, and facilitates comprehensive incident analysis. By directing your Prometheus alerts into Coralogix, you gain a centralized view of your alerting activities, enabling rapid incident detection, proactive troubleshooting, and data-driven decision-making. This integration empowers teams to optimize response workflows, strengthen system reliability, and ensure operational efficiency, leveraging Coralogix's analytics, alerts, and visualization tools to extract valuable insights from Prometheus alerts and ensure a streamlined and resilient incident response process.</p>"},{"location":"newoutput/prometheus-alertmanager/#get-started","title":"Get Started","text":"<p>STEP 1.\u00a0In your navigation pane, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 2.\u00a0In the Contextual Data section, select Prometheus and click\u00a0+ ADD.</p> <p></p> <p>STEP 3. Click ADD NEW.</p> <p>STEP 4.\u00a0Fill in the Integration Details:</p> <ul> <li> <p>Name.\u00a0Name your integration.</p> </li> <li> <p>Your API Key. Click\u00a0CREATE NEW KEY\u00a0to generate an API key and name it.</p> </li> <li> <p>Application Name and Subsystem Name. Enter an\u00a0application and subsystem name.</p> </li> </ul> <p></p> <p>STEP 5.\u00a0Click\u00a0GENERATE URL. The URL for the integration will be automatically created. Use this when creating a Prometheus webhook.</p> <p></p>"},{"location":"newoutput/prometheus-alertmanager/#create-a-prometheus-webhook","title":"Create a Prometheus Webhook","text":"<p>Create a Prometheus webhook using your Coralogix generated URL.</p> <p>STEP 1.\u00a0Make sure you have Prometheus configured with Alertmanager.</p> <p>STEP 2. Add some rules in your <code>alert_file.yml</code> (you can add as many rules as you like).</p> <p>STEP 3. Make sure to confirm your rules are working properly by using:</p> <pre><code>     Promtool check rules  /path to your rules.yml file.\n</code></pre> <p></p> <p>STEP 4. Configure your alertmanager to trigger an alert once any of the rules in your <code>rules.yml</code> file has been satisfied.</p> <p>STEP 5. Open <code>alertmanager.yml</code> (it is usually located under <code>/etc/alertmanger/alertmanager.yml</code>, however the path depends on your installation).</p> <p></p> <p>STEP 6. Add the following to the file:</p> <pre><code>route:\n  receiver: Coralogix\n\nreceivers:\n  - name: Coralogix\n    webhook_configs:\n      - url: Coralogix-url\n        send_resolved: true\n</code></pre> <p>STEP 6. Replace <code>Coralogix-url</code> with the Coralogix generated URL from the previous section.</p> <p>STEP 7. Use amtool to check the alert manager config and make sure it passes. Your alert manager will not start if this config is not correct.</p> <p>If the config passes you will get a success message with details about what you have configured in your <code>alertmanger.yml</code>.</p> <p></p>"},{"location":"newoutput/prometheus-alertmanager/#example-log","title":"Example Log","text":""},{"location":"newoutput/prometheus-alertmanager/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/prometheus-operator/","title":"Prometheus Operator","text":"<p>Prometheus Operator\u00a0provides easy way to operate end-to-end Kubernetes cluster monitoring with Prometheus and exporters. Use it to collect, process, and aggregate metrics from applications in your Kubernetes cluster and send them to Coralogix. This guide shows you how to run Prometheus Operator in Kubernetes to export your data to Coralogix.</p> <p>Our Helm chart is open source and you are welcome to review and make suggestions for improvements in our Integrations repository.</p>"},{"location":"newoutput/prometheus-operator/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p><code>Kubernetes</code>\u00a01.20+ with Beta APIs enabled.</p> </li> <li> <p><code>Helm</code>\u00a02.9+ Package Manager installed (For installation instructions please visit\u00a0Get Helm!).</p> </li> </ul>"},{"location":"newoutput/prometheus-operator/#setup","title":"Setup","text":""},{"location":"newoutput/prometheus-operator/#create-a-namespace-in-our-example-we-will-use-monitoring","title":"Create a Namespace (in our example, we will use:\u00a0monitoring):","text":"<pre><code>kubectl create namespace monitoring\n</code></pre>"},{"location":"newoutput/prometheus-operator/#create-secret","title":"Create Secret","text":"<p>Your Send-Your-Data API key can be found in the Coralogix UI in the top of the screen under\u00a0Data Flow\u00a0\u2013&gt;\u00a0API Keys\u00a0\u2013&gt;\u00a0Send Your Data</p> <pre><code>kubectl create secret generic coralogix-keys \\\n        -n monitoring \\\n        --from-literal=PRIVATE_KEY=&lt;send-your-data-API-key&gt;\n</code></pre> <p>The created secret will look as such:</p> <pre><code>apiVersion: v1\ndata:\n  PRIVATE_KEY: &lt;encrypted-private-key&gt;\nkind: Secret\nmetadata:\n  name: coralogix-keys\n  namespace: monitoring\ntype: Opaque \n</code></pre>"},{"location":"newoutput/prometheus-operator/#add-the-helm-chart-repo","title":"Add the Helm Chart Repo","text":"<pre><code>helm repo add coralogix-charts-virtual https://cgx.jfrog.io/artifactory/coralogix-charts-virtual &amp;&amp;\nhelm repo update\n</code></pre>"},{"location":"newoutput/prometheus-operator/#create-an-overrideyml-file","title":"Create An override.yml File","text":"<p>Choose the correct Coralogix Domain according to your account, the domain table can be found here.</p> <pre><code>#override.yaml:\n---\nglobal:\n  endpoint: \"https://ingress.&lt;Coralogix Domain&gt;/prometheus/v1\"\n</code></pre>"},{"location":"newoutput/prometheus-operator/#deploy-the-chart","title":"Deploy the Chart","text":"<pre><code>helm upgrade --install prometheus-coralogix coralogix-charts-virtual/prometheus-operator-coralogix \\\n  --namespace=monitoring \\\n  -f override.yaml\n</code></pre>"},{"location":"newoutput/prometheus-operator/#delete-the-deployment","title":"Delete the deployment","text":"<pre><code>helm uninstall prometheus-coralogix \\\n--namespace=monitoring\n</code></pre>"},{"location":"newoutput/prometheus-server/","title":"Prometheus Server","text":"<p>Prometheus is currently the leading tool for metric collection, it's easy to integrate and easy to use.</p> <p>Still, short-term retention is a big struggle faced by Prometheus users.</p> <p>Coralogix helps you overcome this struggle by providing you a way to automatically ship your metrics into your Coralogix account and store them long-term without making complex changes to your Prometheus architecture.</p> <p>New! You can now enjoy our new APM features when using OpenTelemetry collector with a Kubernetes processor and Prometheus.</p>"},{"location":"newoutput/prometheus-server/#send-prometheus-metrics-to-coralogix","title":"Send Prometheus Metrics to Coralogix","text":""},{"location":"newoutput/prometheus-server/#endpoint","title":"Endpoint","text":"<p>Select a Prometheus RemoteWrite endpoint for the domain associated with your Coralogix account.</p>"},{"location":"newoutput/prometheus-server/#yaml-file","title":"YAML File","text":"<p>Under root level add remote_write using the following format:</p> <pre><code>remote_write:\n- url: &lt;Remote_write URL&gt;\n  name: '&lt;customer_name&gt;'\n  remote_timeout: 120s\n  bearer_token: '&lt;Send_Your_Data_private_key&gt;'\n</code></pre>"},{"location":"newoutput/prometheus-server/#using-prometheus-operator-","title":"Using Prometheus operator -","text":"<p>Prometheus Operator provides Kubernetes native deployment.</p> <p>You can add remoteWrite in the following format (note the camel case):</p> <pre><code>remoteWrite:\n    url: &lt;Remote_write URL&gt;\n    name: '&lt;customer_name&gt;'\n    remoteTimeout: 120s\n    bearerToken: '&lt;Send_Your_Data_private_key&gt;'\n</code></pre>"},{"location":"newoutput/prometheus-server/#guidelines-","title":"Guidelines -","text":"<ul> <li> <p>URL \u2013 query params that send metric information to Coralogix. These params contain appLabelName, subSystemLabelName and severityLabelName. The parameters you should provide are the\u00a0keys\u00a0you choose to send to Coralogix, they are mapped to metric labels.</p> </li> <li> <p>Name \u2013 The name of the time series.</p> </li> <li> <p>Remote timeout - Timeout for requests to the remote write endpoint.</p> </li> <li> <p>Bearer Token - This token identifies you into your Coralogix account.\u00a0 Access your Send-Your-Data API key.</p> </li> <li> <p>Severities values that are valid in Coralogix are Debug, Verbose, Info, Warning, Error, and Critical. The default severity when leaving severityLabelName empty is Info.</p> </li> </ul>"},{"location":"newoutput/prometheus-server/#server-responses-","title":"Server responses -","text":"<ul> <li> <p><code>HTTP 200</code>: The protobuf file is valid and contains Prometheus Metadata.</p> </li> <li> <p><code>HTTP 201</code>: The protobuf file is valid and processed by Coralogix.</p> </li> <li> <p><code>HTTP 204</code>: The protobuf file is valid, but the Prometheus.WriteRequest is empty.</p> </li> <li> <p><code>HTTP 400</code>: The REST API request itself was malformed. Check the query params.</p> </li> <li> <p><code>HTTP 401</code>: The customer is unauthorized. Verify your Send-Your-Data API key.</p> </li> </ul>"},{"location":"newoutput/prometheus-server/#grafana-visibility","title":"Grafana Visibility","text":"<p>Coralogix enables you to integrate and view your metrics from Grafana as well.</p> <p></p> <p>To connect Grafana to your account you can follow this tutorial: https://coralogixstg.wpengine.com/tutorials/grafana-plugin/</p>"},{"location":"newoutput/prometheus-server/#coralogix-visibility","title":"Coralogix Visibility","text":"<p>To view the metrics in Coralogix:</p> <ol> <li> <p>Navigate to 'Grafana Explore'</p> </li> <li> <p>Choose 'Metrics' as a data source.</p> </li> <li> <p>Expand 'Metrics browser.'</p> </li> <li> <p>Select a metric.</p> </li> </ol> <p></p> <p>Enabling Metrics Archive in Coralogix</p> <p>Learn more on how to enable your S3 archive for metrics: Connect S3 Archive</p>"},{"location":"newoutput/prometheus-server/#limits-quotas","title":"Limits &amp; Quotas","text":"<p>Coralogix places the following limits on endpoints:</p> <ul> <li> <p>A hard limit of 10MB of data to our OpenTelemetry endpoint, with a recommendation of 2MB</p> </li> <li> <p>A hard limit of 2411724 bytes of data to our Prometheus RemoteWrite endpoint, with a recommendation for any amount less than this limit</p> </li> </ul> <p>Limits apply to single requests, regardless of timespan.</p>"},{"location":"newoutput/prometheus-server/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/query-s3-coralogix-archive-bucket-cx-data/","title":"Access CX-Data Directly","text":"<p>This guide explains how to query your S3 Coralogix archive bucket (<code>cx-data</code>) using a third-party framework with the standard Apache Parquet reader provided by the relevant framework and required schema.</p>"},{"location":"newoutput/query-s3-coralogix-archive-bucket-cx-data/#folder-structure","title":"Folder Structure","text":"<p><code>cx-data</code> is stored in standard hive-like partitions, with the following partition fields:</p> <ul> <li> <p><code>team_id=&lt;team-id&gt;</code>: Coralogix Team ID</p> </li> <li> <p><code>dt=YYYY-MM-DD</code>: Date of the data in UTC</p> </li> <li> <p><code>hr=HH</code>: Hour of the data in UTC</p> </li> </ul> <p>These fields can be defined as virtual columns inside the framework, serving as filters in a query.</p> <p>Note:</p> <ul> <li> <p>Both <code>dt</code> and <code>hr</code> are based on the event timestamp.</p> </li> <li> <p>The <code>team_id=&lt;team-id&gt;</code> partition allows reusing the same bucket and prefix to write data from multiple Coralogix teams and query them all in one query.</p> </li> </ul>"},{"location":"newoutput/query-s3-coralogix-archive-bucket-cx-data/#fields","title":"Fields","text":"<p>Each Apache Parquet file has three fields with data as JSON-formatted strings:</p> <ul> <li> <p><code>src_obj__event_metadata</code>: JSON object containing metadata related to the event</p> </li> <li> <p><code>src_obj__event_labels</code>: JSON object containing the labels of the event (such as the Coralogix applicationName and subsystemName)</p> </li> <li> <p><code>src_obj__user_data</code>: JSON object containing actual event data</p> </li> </ul>"},{"location":"newoutput/query-s3-coralogix-archive-bucket-cx-data/#examples","title":"Examples","text":"<p>Below is an example of each of the 3 payload fields.</p> <p><code>src_obj__event_metadata</code>:</p> <pre><code>{\n  \"timestamp\": \"2022-03-28T08:50:57.946\",\n  \"severity\": \"Debug\",\n  \"priorityclass\": \"low\",\n  \"logid\": \"some-uuid\"\n}\n\n</code></pre> <p><code>src_obj__event_labels</code>:</p> <pre><code>{\n  \"applicationname\": \"some-app\",\n  \"subsystemname\": \"some-subsystem\",\n  \"category\": \"some-category\",\n  \"classname\": \"some-class\",\n  \"methodname\": \"some-method\",\n  \"computername\": \"some-computer\",\n  \"threadid\": \"some-thread-id\",\n  \"ipaddress\": \"some-ip-address\"\n}\n\n</code></pre> <p><code>src_obj__user_data</code>:</p> <pre><code>{\n  \"_container_id\": \"0f099482cf3b507462020e9052516554b65865fb761af8e076735312772352bf\",\n  \"host\": \"ip-10-1-11-144\",\n  \"short_message\": \"10.1.11.144 - - [28/Mar/2022:08:50:57 +0000] \\\\\"GET /check HTTP/1.1\\\\\" 200 16559 \\\\\"-\\\\\" \\\\\"Consul Health Check\\\\\" \\\\\"-\\\\\"\"\n}\n\n</code></pre>"},{"location":"newoutput/query-s3-coralogix-archive-bucket-cx-data/#reading-cx-data-files-using-a-standard-framework","title":"Reading <code>cx-data</code> Files Using a Standard Framework","text":""},{"location":"newoutput/query-s3-coralogix-archive-bucket-cx-data/#pandas","title":"Pandas","text":"<p>Loading <code>cx-data</code> files in Pandas can be done using the <code>read_parquet</code> method:</p> <pre><code>import pandas as pd\n\n# Notice that only the three payload columns are passed eventually to read_parquet()\n\ncx_columns = [\n  'src_obj__event_metadata',\n  'src_obj__event_labels',\n  'src_obj__user_data'\n]\n\ndf = pd.read_parquet('s3://.../myfile.parquet',columns = cx_columns)\n\n# The dataframe `df` contains all the data needed for further processing.\n\n</code></pre> <p>Here is the output of <code>df.info()</code> showing the expected schema of the DataFrame:</p> <pre><code>print(df.info())\n\n### Output\n\nRangeIndex: 161050 entries, 0 to 161049\nData columns (total 3 columns):\n #   Column                   Non-Null Count   Dtype\n--- ------ -------------- -----\n 0   src_obj__event_metadata  161050 non-null  object\n 1   src_obj__event_labels    161050 non-null  object\n 2   src_obj__user_data       161050 non-null  object\ndtypes: object(3)\n\n\n</code></pre>"},{"location":"newoutput/query-s3-coralogix-archive-bucket-cx-data/#athena","title":"Athena","text":"<p>To use <code>cx-data</code> directly in Athena, you\u2019ll need to create an <code>EXTERNAL</code> table as follows:</p> <pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS \"my_table\" (\n    `src_obj__event_labels` STRING,\n    `src_obj__event_metadata` STRING,\n    `src_obj__user_data` STRING,\n)\nPARTITIONED BY (`team_id` string, `dt` string, `hr` string)\nROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\nSTORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\nLOCATION 's3://&lt;bucket&gt;/parquet/v1/'\n\n</code></pre> <p>After creating the external table, table partitions should be added to limit the scope of queries, reducing costs and improving performance.</p> <p>Table partitions can be added manually:</p> <pre><code>ALTER TABLE \"my_table\" ADD PARTITION (team_id='23333', dt='2022-05-30', hr='01');\n\n# Additional ALTER TABLE ... ADD PARTITION statements according to which dates and hours are needed\n\n</code></pre> <p>Table partitions can also be added automatically:</p> <pre><code>MSCK REPAIR TABLE \"my_table\"\n\n</code></pre> <p>Note! This command scans all of the files in the table location, which may be a time-consuming task when the amount of partitions detected is large. It may be easier to add the relevant new partitions manually. One option to expedite automatic partitioning is to limit the scope of the data by specifying a specific <code>team_id</code> and <code>dt</code> in the table definition <code>LOCATION</code> (e.g. modify the <code>CREATE EXTERNAL TABLE</code> command to include the <code>team_id=&lt;team-id&gt;/dt=&lt;date&gt;/</code> subpath). This restricts the partition auto-detection to the relevant dates.</p> <p>After adding the relevant partitions, a query can be executed on the table. Accessing a specific field inside one of the payload fields should be done using the Athena function <code>json_extract_scalar</code>, as in the following example:</p> <pre><code>SELECT\n  json_extract_scalar(src_obj__event_metadata,'$.severity') severity,\n  json_extract_scalar(src_obj__event_labels,'$.applicationName') app_name,\n  json_extract_scalar(src_obj__user_data,'$.my_obj.my_message_field') message_field\nFROM \"my_table\"\n\n</code></pre>"},{"location":"newoutput/query-s3-coralogix-archive-bucket-cx-data/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up. Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/querying-coralogix-with-sql/","title":"Querying Coralogix with SQL","text":"<p>The Coralogix JDBC driver allows you to investigate your log data using SQL queries with your favorite database tool. With the Coralogix JDBC driver, you can quickly get started on performing SQL queries against the data already stored in your Coralogix account.</p> <p>JDBC, which stands for \"Java Database Connectivity\", is a common standard for database drivers, and many popular querying tools support it. In this tutorial, you'll find instructions on using the Coralogix JDBC driver with two popular tools - DataGrip and DBeaver.</p>"},{"location":"newoutput/querying-coralogix-with-sql/#getting-started","title":"Getting Started","text":"<p>Follow these steps to set up the connection to Coralogix:</p> <ol> <li> <p>Download the latest driver.</p> </li> <li> <p>Follow the steps below for client-specific instructions. Other clients should have a similar procedure for installing the driver.</p> </li> <li> <p>Test the connection using a simple query: <code>SELECT * FROM logs LIMIT 50</code></p> </li> </ol>"},{"location":"newoutput/querying-coralogix-with-sql/#datagrip","title":"DataGrip","text":"<ol> <li> <p>Click on\u00a0<code>+</code> icon in the Database menu and choose\u00a0<code>Driver</code></p> </li> <li> <p>Into\u00a0<code>Name</code>\u00a0field write\u00a0<code>Coralogix</code></p> </li> <li> <p>Click on\u00a0<code>+</code>\u00a0under\u00a0<code>Driver Files</code>\u00a0and pick the driver file you downloaded earlier (Getting Started step #1 above)</p> </li> <li> <p>Open\u00a0<code>Class</code>\u00a0picker and pick <code>org.opensearch.jdbc.Driver</code></p> </li> <li> <p>Click on\u00a0<code>Apply</code>\u00a0then\u00a0<code>OK</code></p> </li> <li> <p>Click on\u00a0<code>+</code>\u00a0icon in the database menu -&gt; choose\u00a0<code>Data Source</code>\u00a0-&gt; choose\u00a0<code>Coralogix</code></p> </li> <li> <p>Copy your <code>apiKey</code> from\u00a0<code>`Data Flow`\u00a0-&gt;\u00a0`API Keys` -&gt;\u00a0`Logs Query Key`</code> in the Coralogix UI and replace  with this value in the JDBC URL below as per your Team's cluster location. <li> <p>In the\u00a0<code>General</code>\u00a0tab change the url to:  </p> <p><code>jdbc:opensearch://https://ng-api-http.coralogixstg.wpengine.com</code>/sql/ If your account is in\u00a0Europe (ends in .com)   <p><code>jdbc:opensearch://https://ng-api`-http`.app.coralogix.in</code>/sql/ If your account is in\u00a0India (ends in .in)   <p><code>jdbc:opensearch://https://ng-api`-http`.coralogix.us</code>/sql/ If your account is in\u00a0the US (ends in .us)   <p><code>jdbc:opensearch://https://ng-api`-http`.eu2.coralogixstg.wpengine.com</code>/sql/ If your account is in\u00a0Europe (ends in <code>.eu2.coralogixstg.wpengine.com</code>)   <p><code>jdbc:opensearch://https://ng-api`-http`.coralogixsg.com</code>/sql/ If your account is in\u00a0Singapore (ends in <code>.coralogixsg.com</code>)   <li> <p>Click on\u00a0<code>Apply</code>\u00a0then\u00a0<code>OK</code></p> </li> <li> <p>Congratulations! You are ready to query your logs using DataGrip</p> </li>"},{"location":"newoutput/querying-coralogix-with-sql/#dbeaver","title":"DBeaver","text":"<ol> <li> <p>In the top menu select <code>Database</code>\u00a0-&gt;\u00a0<code>Driver manager</code>\u00a0and click the <code>New</code>\u00a0button</p> </li> <li> <p>In\u00a0<code>Driver Name</code>\u00a0field write\u00a0<code>Coralogix</code></p> </li> <li> <p>Click the Libraries tab</p> </li> <li> <p>Click\u00a0<code>Add File</code>\u00a0button and pick the driver file you downloaded earlier (Getting Started step #1 above)</p> </li> <li> <p>Click\u00a0the <code>Find Class</code>\u00a0button. It should show you\u00a0<code>org.opensearch.jdbc.Driver</code>\u00a0in\u00a0the <code>Driver Class</code>\u00a0field, click on it</p> </li> <li> <p>Click <code>OK</code></p> </li> <li> <p>Click Close</p> </li> <li> <p>Click <code>Database/New Database Connection</code> (make sure that All is selected)</p> </li> <li> <p>Type\u00a0<code>coralogix</code> into the search box and choose\u00a0<code>Coralogix</code>\u00a0driver, and click <code>Next</code></p> </li> <li> <p>Copy your <code>apiKey</code> from\u00a0<code>`Data Flow`\u00a0-&gt;\u00a0`API Keys` -&gt;\u00a0`Logs Query Key`</code> in the Coralogix UI and replace  with this value in the JDBC URL below as per your Team's cluster location. <li> <p>Set\u00a0<code>JDBC URL</code>\u00a0to:\u00a0  </p> <p><code>jdbc:opensearch://https://ng-api`-http`.coralogixstg.wpengine.com</code>/sql/ If your account is in\u00a0Europe (ends in .com)   <p><code>jdbc:opensearch://https://ng-api`-http`.app.coralogix.in</code>/sql/ If your account is in\u00a0India (ends in .in)   <p><code>jdbc:opensearch://https://ng-api`-http`.coralogix.us</code>/sql/ If your account is in\u00a0the US (ends in .us)   <p><code>jdbc:opensearch://https://ng-api`-http`.eu2.coralogixstg.wpengine.com</code>/sql/ If your account is in\u00a0Europe (ends in <code>.eu2.coralogixstg.wpengine.com</code>)   <p><code>jdbc:opensearch://https://ng-api`-http`.coralogixsg.com</code>/sql/ If your account is in\u00a0Singapore (ends in <code>.coralogixsg.com</code>)   <li> <p>Click Test Connection to ensure it all works</p> </li> <li> <p>Click OK</p> </li> <li> <p>Congratulations! The <code>Coralogix</code>\u00a0connection in the\u00a0<code>Database Navigator</code>\u00a0has just been created</p> </li>"},{"location":"newoutput/querying-coralogix-with-sql/#tableau","title":"Tableau","text":"<ol> <li> <p>Download the cx_sql_jdbc.taco\u00a0connector file, and copy it to:</p> </li> <li> <p>Windows:\u00a0<code>C:\\Users\\%USERNAME%\\Documents\\My Tableau Repository\\Connectors</code></p> </li> <li> <p>MacOS:\u00a0<code>~/Documents/My Tableau Repository/Connectors</code></p> </li> <li> <p>Place the OpenSearch JDBC driver (jar file) downloaded earlier (Getting Started step #1 above) into:</p> </li> <li> <p>Windows:\u00a0<code>C:\\Program Files\\Tableau\\Drivers</code></p> </li> <li> <p>MacOS:\u00a0<code>~/Library/Tableau/Drivers</code></p> </li> <li> <p>Run the Tableau Desktop with the command line flag\u00a0<code>-DDisableVerifyConnectorPluginSignature=true</code>:</p> </li> <li> <p>Windows:\u00a0<code>C:\\Program Files\\Tableau\\Tableau 2022.1\\bin\\tableau.exe\" -DDisableVerifyConnectorPluginSignature=true</code></p> </li> <li> <p>MacOS:\u00a0<code>open -n /Applications/Tableau\\ Desktop\\ 2022.1.app --args -DDisableVerifyConnectorPluginSignature=true</code> </p> <p>(Adjust the command line according to the Tableau version you have. You can create a shortcut or a script to simplify the above step).</p> </li> <li> <p>Copy your <code>apiKey</code> from\u00a0<code>`Data Flow`\u00a0-&gt;\u00a0`API Keys` -&gt;\u00a0`Logs Query Key`</code> in the Coralogix UI and replace  with this value in the JDBC URL below as per your Team's cluster location. <li> <p>Open Tableau, and select to a Server -&gt; Coralogix by Coralogix, and set the\u00a0<code>JDBC URL</code>\u00a0to:\u00a0  </p> <p><code>`jdbc:opensearch://https://`ng-api`-http`.coralogixstg.wpengine.com</code>/sql/ If your account is in\u00a0Europe (ends in .com)   <p><code>`jdbc:opensearch://https://`ng-api`-http`.app.coralogix.in</code>/sql/ If your account is in\u00a0India (ends in .in)   <p><code>jdbc:opensearch://https://ng-api`-http`.coralogix.us</code>/sql/ If your account is in\u00a0the US (ends in .us)   <p><code>jdbc:opensearch://https://ng-api`-http`.eu2.coralogixstg.wpengine.com</code>/sql/ If your account is in\u00a0Europe (ends in <code>.eu2.coralogixstg.wpengine.com</code>)   <p><code>jdbc:opensearch://https://ng-api`-http`.coralogixsg.com</code>/sql/ If your account is in\u00a0Singapore (ends in <code>.coralogixsg.com</code>) <li> <p>Congratulations! You are ready to query your logs using Tableau  </p> </li>"},{"location":"newoutput/querying-coralogix-with-sql/#the-sql-data-model","title":"The SQL Data Model","text":"<p>The SQL data model exposed by the Coralogix SQL interface currently includes just one table: <code>logs</code>. This table contains all the log records currently stored in Coralogix. More tables might be exposed in the future for other, distinct data types stored in Coralogix.</p> <p>For convenience, you may query logs for specific application names and subsystems through the table name: querying the table <code>logs.production.billing</code>\u00a0will query for logs from the <code>production</code>\u00a0application and <code>billing</code>\u00a0subsystem.</p> <p>The field names of your log records are mapped to column names. Nested field names are mapped to column names using their full path with the path elements concatenated by dots (<code>.</code>). The types of the table fields correspond to the types specified in the fields' mapping. Here's a short example of how a log record document is mapped to the tabular format.</p> <p>This document:</p> <pre><code>{\n  \"kubernetes\": {\n    \"container_name\": \"some_service\"\n  },\n  \"log\": \"Starting up\"\n}\n</code></pre> <p>Will be emitted as this result-set:</p> <pre><code>kubernetes.container_name | log\n---------------------------------------------\nsome_service              | Starting up\n</code></pre> <p>In addition, every row available in the <code>logs</code>\u00a0table contains a <code>coralogix</code>\u00a0object with standard metadata; for example:</p> <ul> <li> <p><code>coralogix.timestamp</code></p> </li> <li> <p><code>coralogix.metadata.applicationName</code></p> </li> <li> <p><code>coralogix.metadata.subsystemName</code></p> </li> </ul>"},{"location":"newoutput/querying-coralogix-with-sql/#available-queries","title":"Available queries","text":"<p>The vast majority of standard SQL functionality works with the Coralogix SQL interface: selecting fields, using <code>WHERE</code>\u00a0clauses for filtering, aggregations with <code>GROUP BY</code>\u00a0and <code>HAVING</code>\u00a0clauses and so forth. There are two \u00a0exceptions to this: joins and set queries (<code>UNION / MINUS / INTERSECT</code>) are currently unsupported. It is possible that support for these constructs will be added in the future.</p> <p>Beyond standard SQL functionality, a number of full-text search constructs are supported:</p>"},{"location":"newoutput/querying-coralogix-with-sql/#match-query","title":"Match Query","text":"<p>The <code>MATCH_QUERY</code> function can be used to apply a full-text search to a specific field. For example:</p> <pre><code>SELECT text, coralogix.metadata.severity\n FROM logs\n WHERE text = MATCH_QUERY('healthcheck')\n</code></pre> <p>This query will fetch the <code>text</code>\u00a0and <code>severity</code>\u00a0fields of all log records whose body contains the word <code>healthcheck</code>.</p>"},{"location":"newoutput/querying-coralogix-with-sql/#query-string","title":"Query String","text":"<p>More complex predicates can be expressed using Query Strings with the <code>QUERY</code>\u00a0function, for example:</p> <pre><code>SELECT COUNT(*), coralogix.metadata.processName\nFROM logs\nWHERE QUERY('coralogix.metadata.applicationName:production AND coralogix.metadata.subsystemName:billing')\nGROUP BY coralogix.metadata.processName\n</code></pre> <p>This query will count the number of logs from the <code>production</code>\u00a0application and <code>billing</code>\u00a0subsystem and group them by their <code>processName</code>\u00a0value.</p>"},{"location":"newoutput/querying-coralogix-with-sql/#additional-resources","title":"Additional Resources","text":"APIDirect Archive Query HTTP APIExternalOpenDistro SQLRefer to this documentation for further references on supported SQL features."},{"location":"newoutput/querying-coralogix-with-sql/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/queue-storage-microsoft-azure-functions/","title":"Queue Storage: Microsoft Azure Resource Manager (ARM)","text":"<p>Coralogix provides seamless integration with Azure cloud, allowing you to send your logs from anywhere and parse them according to your needs.</p> <p>Deploy the Azure Queue Storage integration to send Coralogix your JSON-formatted queue messages using the ARM template below.</p>"},{"location":"newoutput/queue-storage-microsoft-azure-functions/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure account with an active subscription</li> </ul>"},{"location":"newoutput/queue-storage-microsoft-azure-functions/#azure-resource-manager-template-deployment","title":"Azure Resource Manager Template Deployment","text":"<p>Sign into your Azure account and deploy the Queue Storage integration by clicking here.</p> <p></p> <p>Notes:</p> <ul> <li> <p>The Azure Queue Storage integration allows parsing of queue messages in JSON format.</p> </li> <li> <p>Other format messages will not be processed and submitted to the Coralogix platform.</p> </li> </ul>"},{"location":"newoutput/queue-storage-microsoft-azure-functions/#fields","title":"Fields","text":"FieldDescriptionSubscriptionAzure subscription for which you wish to deploy the integration.Must be the same as the monitored storage account.Resource GroupResource group for which you wish to deploy the integrationCoralogix RegionRegion associated with your Coralogix domainCustom URLCustom URL associated with your Coralogix account. Ignore if you do not have a custom URL.Coralogix Private KeyCoralogix Send-Your-Data API keyCoralogix ApplicationMandatory metadata field sent with each log and helps to classify itCoralogix SubsystemMandatory metadata field sent with each log and helps to classify itStorage Account NameName of the storage account containing the storage queue. Must be of StorageV2 (general purpose V2) type.Storage Account Resource GroupName of the storage account resource group containing the storage queue to be monitoredStorage Queue NameName of the Storage Queue to be monitoredFunction App Service Plan TypeType of the function app service plan. Choose 'Premium' if you need VNet support."},{"location":"newoutput/queue-storage-microsoft-azure-functions/#optional-configuration-options","title":"Optional Configuration Options","text":"<p>If your Storage Queue belongs to a restricted storage account, review this\u00a0optional configuration\u00a0documentation\u00a0to learn about VNet support options.</p>"},{"location":"newoutput/queue-storage-microsoft-azure-functions/#additional-resources","title":"Additional Resources","text":"GithubQueue Storage DocumentationMicrosoft Azure Functions Manual IntegrationsBlob StorageEvent Hub"},{"location":"newoutput/queue-storage-microsoft-azure-functions/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/quota-management/","title":"Quota Management","text":"<p>Manage team quotas in your Coralogix account using the following tutorial.</p>"},{"location":"newoutput/quota-management/#overview","title":"Overview","text":"<p>The quota management API allows team administrators to view and distribute quotas across teams they manage. Allocate and move quotas across teams for any period to:</p> <ul> <li> <p>Contend with unexpected spikes in logs shipping to one team</p> </li> <li> <p>Better handle resources when a team is underutilizing its quota</p> </li> <li> <p>Meet new or changing capacity requirements of your teams\u00a0</p> </li> <li> <p>Save on cost by more efficiently using quotas</p> </li> </ul>"},{"location":"newoutput/quota-management/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>The latest version of the Coralogix CLI</p> </li> <li> <p>Teams API key (Access this in Account settings &gt; API Access)</p> </li> <li> <p>Team ID (Access this in Account settings &gt; Send Your Logs)</p> </li> <li> <p>Team administrator (admin) role granted to the person moving quotas between teams (He / she must be team admin in both the team receiving and giving the quota)</p> </li> </ul>"},{"location":"newoutput/quota-management/#limitations-constraints","title":"Limitations &amp; Constraints","text":"<ul> <li> <p>The minimum quota that can be transferred is 0.001 GB.</p> </li> <li> <p>A quota cannot be moved to a trial account.</p> </li> <li> <p>Quotas can only be moved between teams with the same retention period.</p> </li> <li> <p>Quotas cannot be transferred from a team if the remaining daily quota is less than the quota being moved. In this case, transfer will be possible once the daily quota resets at 00:00 UTC</p> </li> <li> <p>You need to be admin on the teams you are trying to move quota between.</p> </li> <li> <p>Quota allocations will take a maximum of 15 minutes to take effect.</p> </li> </ul>"},{"location":"newoutput/quota-management/#environment-variables","title":"Environment Variables","text":"<p>The following table displays environment variables supported by commands.</p> <p>[table id=72 /]</p>"},{"location":"newoutput/quota-management/#get-quota","title":"Get Quota","text":""},{"location":"newoutput/quota-management/#command","title":"Command","text":"<p>This command returns the daily quota (GB) for the teamId being queried:</p> <p><code>cxctl account get-quota</code></p>"},{"location":"newoutput/quota-management/#example","title":"Example","text":"<p>[table id=70 /]</p> <p>Options</p> <p>[table id=68 /]</p> <p>*Use eu if account top level domain is .com, in, if it is .in, and us if it is .us.</p>"},{"location":"newoutput/quota-management/#move-quota","title":"Move Quota","text":""},{"location":"newoutput/quota-management/#command_1","title":"Command","text":"<p>This command moves quota (GB) from one team to another:</p> <p><code>cxctl account move-quota</code></p> <p>You may use the Alerts, Rules &amp; Tags API key for any of the teams between whom you are moving quotas.</p>"},{"location":"newoutput/quota-management/#example_1","title":"Example","text":"<p>[table id=71 /]</p> <p>Options</p> <p>[table id=69 /]</p> <p>*Use eu if account top level domain is .com, in, if it is .in, and us if it is .us.</p>"},{"location":"newoutput/quota-management/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/ratio-alerts/","title":"Ratio Alerts","text":"<p>Ratio alerts allow you to easily calculate a ratio between two log queries and trigger an alert when the ratio reaches a set threshold.</p>"},{"location":"newoutput/ratio-alerts/#feature","title":"Feature","text":"<p>Use this feature to monitor:</p> <ul> <li> <p>Operational Health. Monitor the number of outgoing responses to incoming requests or the ratio of specific error codes to the overall number of errors.</p> </li> <li> <p>Marketing. Monitor the ratio between traffic from specific regions to overall traffic following regional campaigns.</p> </li> <li> <p>Security. Monitor the ratio of denied requests, specific admin operations, or requests originating from blocked network domains compared to all requests.</p> </li> </ul>"},{"location":"newoutput/ratio-alerts/#create-a-ratio-alert","title":"Create a Ratio Alert","text":"<p>STEP 1. In the navigation pane, click Alerts &gt; Alert Management. Click NEW ALERT on the top-right area of the UI.</p> <p></p>"},{"location":"newoutput/ratio-alerts/#details","title":"Details","text":"<p>STEP 2. Define the Alert Details.</p> <ul> <li> <p>Please enter:</p> <ul> <li> <p>Alert Name.</p> </li> <li> <p>Alert Description.</p> </li> <li> <p>Alert Severity. Choose from one of four options: Info, Warning, Error, Critical.</p> </li> <li> <p>Labels. Define a new label or choose from an existing one. Nest a label using <code>key:value</code>.</p> </li> <li> <p>Set as Security Alert. Check this option to create an alert related to Coralogix Security solutions.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"newoutput/ratio-alerts/#alert-type","title":"Alert Type","text":"<p>STEP 3. Select RATIO Alert Type.</p> <p></p>"},{"location":"newoutput/ratio-alerts/#define-queries","title":"Define Queries","text":"<p>STEP 4. Define Query 1 &amp; Query 2.</p> <ul> <li> <p>Create a meaningful name (Alias) for your query, as it will appear in your alert notifications</p> </li> <li> <p>Input a new query. Using the available RegEx cheat sheet for support.</p> </li> <li> <p>Filter by Application, Subsystem and Severity.</p> </li> </ul> <p></p> <p></p>"},{"location":"newoutput/ratio-alerts/#additional-query-examples","title":"Additional Query Examples","text":"<ul> <li> <p>Example 1</p> <ul> <li> <p>Query1: <code>status:504</code></p> </li> <li> <p>Query2:\u00a0<code>_exists_:status</code></p> </li> <li> <p>Result: Finds the ratio between error code 504 and the overall number of response codes received. Higher-than-usual ratios may indicate operational issues.</p> </li> </ul> </li> <li> <p>Example 2</p> <ul> <li> <p>Query1:\u00a0<code>NOT client_addr:/172\\\\.[0-9]{1,3}\\\\.[0-9]{1,3}\\\\.[0-9]{1,3}/</code></p> </li> <li> <p>Query2:\u00a0<code>_exists_:client_addr</code></p> </li> <li> <p>Result: Assume addresses outside 172.xxx.xxx.xxx are restricted. An abnormal ratio of restricted traffic to all traffic may indicate an attack.</p> </li> </ul> </li> <li> <p>Example 3</p> <ul> <li> <p>Query1:\u00a0request_status:success</p> </li> <li> <p>Query2: response_status:rejectrequest</p> </li> <li> <p>Result: Calculates how many requests were not answered successfully out of all successful requests. A higher-than-usual ratio may indicate operational issues.</p> </li> </ul> </li> </ul>"},{"location":"newoutput/ratio-alerts/#conditions","title":"Conditions","text":"<p>STEP 5. Set the Conditions to trigger the alert.</p> <p></p> <p>An alert will trigger when the count of the entries matching the alert definition will be more / less than the chosen threshold (the ratio chosen in the Query1/Query2 drop-down list). Hit count will present the actual number of entries that match within the selected time window.</p>"},{"location":"newoutput/ratio-alerts/#group-by","title":"Group By","text":"<ul> <li> <p>Group By alerts by aggregating one or more values into a histogram. An alert is triggered whenever the condition threshold is met for a specific aggregated value within the specified timeframe.</p> </li> <li> <p>If using 2 values for Group By, matching logs will first be aggregated by the parent field (ie. region), then by the child field (ie. pod_name). An alert will fire when the threshold meets the unique combination of both parent and child. Only logs that include the Group By fields will be included in the count.</p> </li> </ul>"},{"location":"newoutput/ratio-alerts/#trigger-on-infinity","title":"Trigger on Infinity","text":"<p>Choose whether you would like to be alerted on Infinity. The infinity value is met when the value of the second query is 0. In that case, the ratio result will be infinite.</p>"},{"location":"newoutput/ratio-alerts/#notifications","title":"Notifications","text":"<p>STEP 6. Define Notification settings.</p> <p>In the notification settings, you have different options, depending on whether or not you are using the\u00a0Group By\u00a0condition.</p>"},{"location":"newoutput/ratio-alerts/#using-group-by","title":"Using Group By","text":"<p>When using\u00a0Group By\u00a0conditions, you will see the following options:</p> <ul> <li> <p>Trigger a single alert when at least one combination of\u00a0the group by values meets the condition. A single notification, aggregating all values matching an alert query and conditions, will be sent to your Coralogix Incidents screen.</p> </li> <li> <p>Trigger a separate alert for each combination that meets the condition. Multiple individual notifications for each Group By field value may be sent to your Coralogix Incidents screen when query conditions are met. Select one or more Keys \u2013 consisting of a subset of the fields selected in the alert conditions \u2013 in the drop-down menu. A separate notification will be sent for each Key selected.</p> </li> <li> <p>The number of\u00a0Group By\u00a0permutations is limited to 1000. If there are more permutations, then only the first 1000 are tracked.</p> </li> </ul>"},{"location":"newoutput/ratio-alerts/#not-using-group-by","title":"Not Using Group By","text":"<p>When not using the\u00a0Group By\u00a0condition,\u00a0a single alert will be triggered\u00a0and sent to your\u00a0Incidents Screen\u00a0when the query meets the condition.</p> <p>You can define additional alert recipient(s) and notification channels in both cases by clicking\u00a0+ ADD WEBHOOK. Once you add a webhook, you can choose the parameters of your notification:</p> <ul> <li> <p>Notify Every. Sets the alert cadence. After an alert is triggered and a notification is sent, the alert will continue to work, but notifications will be suppressed for the duration of the suppression period.</p> </li> <li> <p>Notify When Resolved. Activate to receive an automatic update once an alert has ceased.</p> </li> </ul>"},{"location":"newoutput/ratio-alerts/#schedule","title":"Schedule","text":"<p>STEP 7. Set a Schedule. Limit triggering to specific days &amp; times.</p> <p></p>"},{"location":"newoutput/ratio-alerts/#notification-content","title":"Notification Content","text":"<p>STEP 8. Define Notification Content.</p> <ul> <li> <p>Choose a specific JSON key or keys to include in the alert notification.</p> </li> <li> <p>Leave blank to view the full log text.</p> </li> </ul> <p></p>"},{"location":"newoutput/ratio-alerts/#finalize-alert","title":"Finalize Alert","text":"<p>STEP 9. Click CREATE ALERT on the upper-right side of the screen.</p> <p>After saving your alert, it may take up to 15 minutes for the alert to be active in the cluster.</p>"},{"location":"newoutput/ratio-alerts/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/real-user-monitoring/","title":"Real User Monitoring","text":"<p>Coralogix's Real User Monitoring (RUM) is an advanced monitoring solution that provides unparalleled visibility into your application's frontend performance, all from the perspective of real users in real time. With RUM, you can gain a comprehensive understanding of how your application performs across a variety of browsers, devices, and networks, allowing you to quickly detect issues that might be affecting your users' experience.</p> <p>RUM is designed to be as seamless as possible, correlating every user journey with backend metrics, traces, logs, and network performance data. This means that you can quickly identify the root cause of any problems and resolve them with cross-stack context, reducing your mean time to resolution and improving your overall service quality. What\u2019s more, this feature is powered by our\u00a0Streama\u00a9 technology, allowing your data to run on the Coralogix monitoring pipeline, at a third of the cost, without prior indexing.</p> <p>Whether you're looking to optimize your app's performance, reduce downtime, or simply improve your user experience, RUM provides the tools and insights you need to optimize engagement with your application.</p>"},{"location":"newoutput/real-user-monitoring/#what-is-real-user-monitoring","title":"What is Real User Monitoring?","text":"<p>RUM is a web performance monitoring technique that collects data on how real users interact with a website or web application. It tracks metrics such as page load times, network requests, and user interactions to provide insights into the actual user experience. By integrating a JavaScript snippet into web pages, RUM captures data from users' browsers, allowing website owners to identify performance issues, optimize user experience, and make data-driven decisions for improving their website's overall performance.</p> <p>RUM allows you to optimize your website\u2019s overall performance using four types of functionalities for monitoring web and mobile applications.</p> <ul> <li> <p>Error Tracking. Monitor and analyze errors that occur on the client-side during real user interactions with your website or web application. Identify and understand issues that users encounter over time and versions to enhance the overall user experience. Find out more here.</p> </li> <li> <p>User Sessions. Investigate each and every user session - a user journey on your web or mobile application, including page views and associated telemetry. A user session typically starts when a user visits a website or performs a specific action and ends when the user either closes the browser window, navigates to a different website, or remains inactive for a specified period. Understand how users are engaging with your site over time and how performance metrics might change within a single session.</p> </li> <li> <p>Page Performance. Measure and analyze how a web page or web application performs from the perspective of real users. Inspect page load time and rendering speed, which reflect the users' experience while accessing the site.</p> </li> <li> <p>RUM Analytics. Collect real user analytics - including page views, unique visitors, geographic distribution, and user interaction with your application (page visited, resources loaded, clicks, interactions, feature usage, and errors).</p> </li> </ul> <p>RUM is available for Monitoring and Compliance priority-level data, in addition to Frequent Search.</p>"},{"location":"newoutput/real-user-monitoring/#how-streama-enables-optimized-rum","title":"How Streama Enables Optimized RUM","text":"<p>RUM is powered by our\u00a0Streama\u00a9 technology, which enables error tracking as part of the streaming process,\u00a0without prior indexing. In other words, we process your data first and delay storage and indexing until all important decisions have been made. Use this technology to rapidly track thousands of components and proactively troubleshoot errors that you have monitored.</p> <p></p>"},{"location":"newoutput/real-user-monitoring/#how-can-i-use-rum","title":"How Can I Use RUM?","text":"<p>RUM is valuable for a wide range of companies and individuals who operate websites or web applications. By gaining insights into real user experiences, these entities can make data-driven decisions to optimize their digital platforms and achieve their business objectives. Check out some of these use-cases below.</p> <ul> <li> <p>Error Tracking. An e-commerce fashion retailer uses RUM to track user interactions on its website. The RUM tool detects that some users encounter a JavaScript error when trying to add items to their cart. The retailer promptly investigates and finds that the error is caused by a recent code update. With the insights provided by RUM, the retailer rolls back the problematic code and deploys a fix, ensuring a smooth shopping experience for all users.</p> </li> <li> <p>User Sessions. A game development company leverages RUM to gain insights into player behavior. The RUM tool reveals that players tend to drop off at a certain level in the game. By analyzing user sessions, the company\u2019s developer discovers that the level is too challenging for many players, leading to disengagement. Armed with this knowledge, the developer tweaks the level design to strike a better balance, resulting in improved player retention and more positive reviews.</p> </li> <li> <p>Page Performance. An electronics retailer relies on RUM to monitor their website's performance. The RUM tool highlights that users on mobile devices experience slower load times on the checkout page. Using these RUM insights, the retailer identifies that an overly complex checkout form is causing delays. They simplify the form, leading to faster load times and reduced cart abandonment.</p> </li> </ul>"},{"location":"newoutput/real-user-monitoring/#getting-started","title":"Getting Started","text":"<ul> <li> <p>Set up and configure\u00a0Real User Monitoring\u00a0via the\u00a0RUM Integration Package\u00a0to hit the ground running with\u00a0our various RUM features. The package includes automatic configuration of the RUM Browser SDK, as well as upload of your source maps. Once configured, all network requests and errors in your system will be captured and sent to Coralogix.</p> </li> <li> <p>Get familiar with Error Tracking. Use our dedicated user manual to track your errors and engage with our user-friendly UI. To access the Error Tracking screen in your Coralogix toolbar, navigate to Explore &gt; Error Tracking.</p> </li> </ul>"},{"location":"newoutput/real-user-monitoring/#additional-resources","title":"Additional Resources","text":"DocumentationRUM Integration PackageBrowser SDK Installation GuideError TrackingError Tracking: User Manual"},{"location":"newoutput/real-user-monitoring/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/real-user-monitoring-cli/","title":"RUM  CLI","text":"<p>The Coralogix RUM CLI is a command-line interface tool that simplifies the process of uploading source maps for your applications to the Coralogix Real User Monitoring service. This CLI tool provides an easy and efficient way to authenticate with the Coralogix API, specify the application and release information, and traverse a folder to upload the relevant source map files.</p>"},{"location":"newoutput/real-user-monitoring-cli/#install-the-rum-cli","title":"Install the RUM CLI","text":"<p>STEP 1. Open a terminal or command prompt.</p> <p>STEP 2. Run the following command to install the CLI globally:</p> <pre><code>npm i @coralogix/rum-cli\n\n</code></pre> <p>STEP 3. Once the installation is complete, you can use the CLI by running the\u00a0<code>coralogix-rum-cli</code>\u00a0command in your terminal.</p> <p>STEP 3. With each code deployment, run this script in order to upload your source map using the Coralogix CLI tool. This allows us to visualize your errors in the Coralogix app.</p> <ul> <li> <p>Open a terminal or command prompt.</p> </li> <li> <p>Run the following command to install the CLI globally.</p> </li> </ul> <pre><code>npm install @coralogix/rum-cli\n\n</code></pre> <p>STEP 4. Once the installation complete, use the CLI by running the\u00a0<code>coralogix-rum-cli</code>\u00a0command in your terminal.</p>"},{"location":"newoutput/real-user-monitoring-cli/#additional-resources","title":"Additional Resources","text":"DocumentationReal User MonitoringRUM Integration PackageRUM Browser SDK Installation GuideError TrackingError Tracking: User Manual"},{"location":"newoutput/real-user-monitoring-cli/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/recording-rules-api/","title":"Recording Rules API","text":"<p>Recording rules allow you to pre-process and derive new time series from existing ones. The rules are defined in a configuration file and are executed in the background at regular intervals.</p> <p>Coralogix provides an API that allows you to manage your recording rules.</p>"},{"location":"newoutput/recording-rules-api/#overview","title":"Overview","text":"<p>A recording rule is defined by:</p> <ol> <li> <p>a name</p> </li> <li> <p>a query expression</p> </li> <li> <p>an optional set of labels to add to the resulting time series.</p> </li> </ol> <p>The query expression is used to select the time series that the rule will operate on, and can include aggregation functions such as sum, min, max, etc. When the rule is executed, it will evaluate the query expression over a range of time, specified in the configuration file as evaluation_interval, and create a new time series for each unique combination of label values resulting from the evaluation.</p> <p>Example:</p> <p>Suppose you have a metric called request_duration_seconds that represents the duration of HTTP requests made to your web server, and you want to track the average request duration over the past minute. You could define a recording rule like this:</p> <pre><code>groups:\n  - name: \n      rules:\n        - record: service:request_duration_seconds:avg1m\n          expr: avg(request_duration_seconds{namespace=\"frontend\"}[1m]) by (service)\n\n</code></pre> <p>This rule would create a new time series called service:request_duration_seconds:avg1_m for each unique value of the service label, with the value of the time series being the average of the _request_duration_seconds time series over the past minute.</p>"},{"location":"newoutput/recording-rules-api/#recording-rules-api","title":"Recording Rules API","text":"<p>Coralogix provides an API that allows you to manage your recording rules. In order to use the Recording Rules API, you will need:</p> <ol> <li> <p>An authentication key from Data Flow \u2013&gt; API Keys -&gt; Alerts, Rules, and Tags API key</p> </li> <li> <p>The API endpoint associated with your Coralogix domain.</p> </li> </ol> Coralogix Region Endpoint US1 https://ng-api-grpc.coralogix.us:443/ US2 https://ng-api-grpc.cx498.com:443 EU1 https://ng-api-grpc.coralogixstg.wpengine.com:443/ EU2 https://ng-api-grpc.eu2.coralogixstg.wpengine.com:443/ AP1 (IN) https://ng-api-grpc.app.coralogix.in:443/ AP2 (SG) https://ng-api-grpc.coralogixsg.com:443/ <p>NOTE:</p> <ol> <li> <p>The Alerts, Rules, and Tags API keys are visible only for admin users.</p> </li> <li> <p>Rules that depend on other recording-rules are not supported.</p> </li> <li> <p>Importing a new configuration file will replace the existing recording-rules. Therefore, new configuration must contain all your rules, not just updates/changes.</p> </li> <li> <p>Alerting rules are not supported.</p> </li> </ol> <p>The actions supported by the API are listed below. These examples require that you have the grpcurl tool installed in your environment. Also note that these examples use the EU1 endpoint</p>"},{"location":"newoutput/recording-rules-api/#save-rulegroups","title":"Save RuleGroups","text":"<p>This action will Create a new rule-group. In case the rule-group exists, it will be overwritten.</p> <pre><code>grpcurl -H \"Authorization: Bearer &lt;ALERTS_RULES_TAGS_API_KEY&gt;\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 rule_manager.groups.RuleGroups.Save &lt;&lt;EOF\n{\n  \"name\": \"FrontendRuleGroup\",\n  \"interval\": 60,\n  \"limit\": \"0\",\n  \"rules\": [\n    {\n      \"record\": \"service:request_duration_seconds:avg1h\",\n      \"expr\": \"avg(request_duration_seconds{namespace=\\\"frontend\\\"}[1h]) by (service)\",\n      \"labels\": {}\n    }\n  ]\n}\nEOF\n\n</code></pre> <p>An empty response is returned on success.</p> <pre><code>{}\n\n</code></pre>"},{"location":"newoutput/recording-rules-api/#fetch-rule-groups","title":"Fetch rule-groups","text":"<p>This action will retrieve the specified rule-group.</p> <pre><code>grpcurl -H \"Authorization: Bearer &lt;ALERTS_RULES_TAGS_API_KEY&gt;\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 rule_manager.groups.RuleGroups.Fetch &lt;&lt;EOF\n{\n  \"name\": \"HTTPRuleGroup\"\n}\nEOF\n\n</code></pre> <p>A single rule-group is returned on success</p> <pre><code>{\n  \"ruleGroup\": {\n    \"name\": \"HTTPRuleGroup\",\n    \"interval\": 60,\n    \"limit\": \"0\",\n    \"rules\": [\n      {\n        \"record\": \"job:http_requests_total:sum\",\n        \"expr\": \"sum(rate(http_requests_total{namespace=\\\"frontend\\\"}[5m])) by (job)\"\n      }\n    ]\n  }\n}\n\n</code></pre>"},{"location":"newoutput/recording-rules-api/#list-rule-groups","title":"List rule-groups","text":"<p>This action will list all existing rule-groups.</p> <pre><code>grpcurl -H \"Authorization: Bearer &lt;ALERTS_RULES_TAGS_API_KEY&gt;\" ng-api-grpc.coralogixstg.wpengine.com:443 rule_manager.groups.RuleGroups.List\n\n</code></pre> <p>A list of rule-groups is returned on success.</p> <pre><code>{\n  \"ruleGroups\": [{\n  \"name\": \"FrontendRuleGroup\",\n  \"interval\": 60,\n  \"limit\": \"0\",\n  \"rules\": [\n    {\n      \"record\": \"service:request_duration_seconds:avg1m\",\n      \"expr\": \"avg(request_duration_seconds{namespace=\\\"frontend\\\"}[1m]) by (service)\"\n    }]\n  }, {\n  \"name\": \"HTTPRuleGroup\",\n  \"interval\": 60,\n  \"limit\": \"0\",\n  \"rules\": [{\n    \"labels\": {},\n    \"record\": \"job:http_requests_total:sum\",\n    \"expr\": \"sum(rate(http_requests_total{}[5m])) by (job)\"\n    }]      \n  }]\n}\n\n</code></pre>"},{"location":"newoutput/recording-rules-api/#delete-rule-groups","title":"Delete rule-groups","text":"<p>This action will delete the specified rule-groups.</p> <pre><code>grpcurl -H \"Authorization: Bearer &lt;ALERTS_RULES_TAGS_API_KEY&gt;\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 rule_manager.groups.RuleGroups.Delete &lt;&lt;EOF\n{\n  \"name\": \"HTTPRuleGroup\"\n}\nEOF\n\n</code></pre> <p>An empty response is returned on success.</p> <pre><code>{}\n\n</code></pre>"},{"location":"newoutput/recording-rules-api/#additional-resources","title":"Additional Resources","text":"<p>Coralogix Terraform Provider - Recording Rules</p> <p>Prometheus Recording Rules</p>"},{"location":"newoutput/recording-rules-api/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/recordingrules/","title":"Recording Rules","text":"<p>Recording rules allow you to preprocess and derive new time series from existing ones. By recording a new metric time series, you can simplify complex and resource-intensive PromQL queries into a leaner and more quickly queried metric. The newly recorded metric can be used in various dashboard visualizations and delivers high-performance analytics.</p>"},{"location":"newoutput/recordingrules/#overview","title":"Overview","text":"<p>Create recording rules using your Coralogix UI or import existing rules from a YAML file using our Recording Rules API. These rules are executed in the background at a regular interval specified in the rule group.</p> <p>A recording rule is defined by:</p> <ul> <li> <p>a name</p> </li> <li> <p>a query expression</p> </li> <li> <p>an optional set of labels to add to the resulting time series</p> </li> </ul> <p>The query expression is used to select the time series upon which the rule will operate. It may include aggregation functions such as sum, min, max, etc. When a rule is executed, it evaluates the query expression over a range of time specified in the configuration file as <code>evaluation_interval</code>. It then creates a new time series for each unique combination of label values resulting from the evaluation.</p>"},{"location":"newoutput/recordingrules/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Coralogix account set up to send metrics</p> </li> <li> <p>S3 bucket set up and defined</p> </li> </ul>"},{"location":"newoutput/recordingrules/#add-a-new-rule","title":"Add a New Rule","text":"<p>Add a new recording rule using your Coralogix UI.</p> <p>STEP 1. In your navigation pane, click Data Flow &gt; Recording Rules to view the existing list of recording rules.</p> <p></p> <p>STEP 2. Click + ADD NEW GROUP.</p> <p></p> <p>STEP 3. Select the rule set for the new group. Either select from the Select set for this group dropdown, or click + ADD NEW SET to add the rule group to a new rule set.</p> <p>STEP 4. Enter a name for the group and select the Evaluation interval (how frequently the rule is run) and Series limit (the number of permutations the rule can create) for the new group.</p> <p>Notes:</p> <ul> <li> <p>The default setting for the Evaluation interval is 30 seconds (the minimum is 15 seconds). Note that the evaluation interval set in the rule must be greater than the time it will actually take to evaluate. If the frequency is shorter than the actual evaluation time, the recording rule will fail.</p> </li> <li> <p>The default setting for the Series limit is 0, which means \u201cno limit\u201d.</p> </li> </ul> <p>STEP 5. Enter a Name for the metric and enter the Expression as a PromQL query.</p> <p></p> <p>STEP 6. [Optional] Add Labels and Values to the rule.</p> <p>STEP 7. You may add additional rules for the rule group by repeating Steps 1-4 for each additional rule.</p> <p>STEP 8. Click Save Rule Group at the top of the screen.</p>"},{"location":"newoutput/recordingrules/#import-rules-from-a-yaml-file","title":"Import Rules From a YAML File","text":"<p>To import rules from an existing YAML file:</p> <p>STEP 1. In your navigation pane, click Data Flow &gt; Recording Rules to view the existing list of recording rules.</p> <p></p> <p>STEP 2. Click IMPORT YAML.</p> <p>STEP 3. Select the YAML file to import and click Open.</p> <p>If the upload is successful, the imported rules appear in the Recording Rules page.</p> <p>If the upload fails for any reason, an error message appears with the reason for the failure.</p>"},{"location":"newoutput/recordingrules/#additional-resources","title":"Additional Resources","text":"DocumentationRecording Rules API"},{"location":"newoutput/recordingrules/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/relative-graphs/","title":"Relative Graphs","text":"<p>Time-series graphs are a great way to visualize your search results. Identifying trends and spotting outliers is much easier on a graph.</p> <p>Relative graphs on Coralogix allow you to compare the current search result with a single time-frame in the past. You gain the ability to perform a time-series comparison of any key in your logs, when used together with the \u201cGroup-by key\u201d.</p> <p>Examples of graphs that you can plot include:</p> <ul> <li>Counts of exceptions with a past time-frame.</li> <li>Events such failed login attempts with past time-frame.</li> </ul>"},{"location":"newoutput/relative-graphs/#getting-started","title":"Getting started:","text":"<ul> <li>Navigate to the log screen. Perform a search to display the first graph of results.</li> <li>Click on the \u201cCompare to\u201d drop-down. Select your preferred time-frame from comparison.</li> </ul> <ul> <li>Hovering your mouse over the graph displays a tool-tip. The tool-tip shows a count of values in the \u201cGroup-by key\u201d</li> </ul>"},{"location":"newoutput/relative-graphs/#notes","title":"Notes:","text":"<ul> <li>Your team's retention period determines how much time-shifted data is available.</li> <li>Compare to feature is not available for archive queries.</li> </ul>"},{"location":"newoutput/relative-time-series-graphs/","title":"Relative Time Series Graphs","text":"<p>The top graph in the Logs screen supports the comparison of the current query with any baseline in the past.</p> <p>This feature allows you to quickly determine the current state of your logs, and compare it to the state of logs previously received matching the same query criteria. Relative Time Series Graphs are a great way to visualize your search results and allow for easier identification of trends and outliers.</p> <p>For the comparison, the historical time-shift can be set to seconds, minutes, hours, or days.</p> <p>Examples of graphs that you could plot include:</p> <ul> <li>Counts of exceptions within a given time-frame.</li> <li>Events such as failed login attempts within a given time-frame.</li> </ul> <p>With this feature, trends are easily identifiable. It is possible to determine if more/less/no errors are being received - Has an error trend previously seen in the logs been corrected? Or on the contrary, has a new error trend been identified?</p>"},{"location":"newoutput/relative-time-series-graphs/#video-tutorial","title":"[Video Tutorial]","text":"<p>Let's illustrate how this feature works using the following example:  </p> <p>1. We are interested in analyzing logs with Error and Critical priority for the frontend subsystem for the last 24 hours: </p> <p>2. Following are the resulting logs:</p> <p></p> <p>3. At the 3:00 AM mark we show a total of 5,006 logs with Error severity, and 4,988 logs with Critical severity for the current day. Let's compare this to the same time a week ago:</p> <p></p> <p>Please note that the comparison criteria can be set to:</p> <ul> <li>The Day Before</li> <li>The Week Before</li> <li>The Month Before</li> <li>A Custom Time  </li> </ul> <p>4. Upon selecting the desired comparison baseline, the graph will automatically refresh. Following are the comparison results:</p> <p></p> <p>The tooltip in the image above at the 3:00 AM mark, shows the baseline behavior on the left (from 7 days ago), the current behavior in the middle, and the difference between them on the right under the \"Delta column\". If the value under investigation has decreased for the comparison period it will show a negative number, which is not the case in this example.</p> <p>5. The Custom Time criteria can be set by clicking \u201cCustom Time\u201d and entering the desired time criteria.</p> <p></p> <p>And clicking 'apply' to display the comparison results:</p> <p></p> <p>6. Finally, it is also possible to perform a time-series comparison for any key in your logs, when used with the \u201cGroup graph by key\u201d option. For example:</p> <p></p> <p></p> <p>Notes:</p> <ul> <li>Your team\u2019s retention period determines how much time-shifted data is available.</li> <li>The \"Compare to\" feature is not yet available for archive queries.</li> </ul> <p>For any questions, please visit us via our in-app chat. We are always here to help!</p>"},{"location":"newoutput/rest-api-bulk/","title":"Coralogix REST API /bulk","text":"<p>Send your logs using our Rest API <code>/bulk</code> endpoint.</p>"},{"location":"newoutput/rest-api-bulk/#endpoint-url","title":"Endpoint URL","text":"<p>Create an endpoint URL by inputting your Coralogix account\u00a0domain as follows: https://ingress./logs/v1/bulk."},{"location":"newoutput/rest-api-bulk/#schema","title":"Schema","text":""},{"location":"newoutput/rest-api-bulk/#endpoint-details","title":"Endpoint Details","text":"URLhttps://ingress.&lt;domain&gt;/logs/v1/bulkHTTP MethodPOSTContent-Typeapplication/jsonHeaderAuthorization: Bearer &lt;Send-Your-Data API key&gt; <ul> <li> <p>We recommend sending logs in batches to minimize network calls.</p> </li> <li> <p>Group logs from the same application, subsystem, and computer under a single API call using the \u201clogEntries\u201d property.</p> </li> <li> <p>The API is limited to a message size of 2MB which is approximately 3,000 medium-sized logs.</p> </li> <li> <p>If you are using Ajax or a similar technology, you may need to send the data with JSON.stringify()).</p> </li> </ul>"},{"location":"newoutput/rest-api-bulk/#post-body","title":"POST Body","text":"Required Property Name Property Type Notes Yes applicationName string usually used to separate environments Yes subsystemName string usually used to separate components computerName string Yes logEntries array of logs"},{"location":"newoutput/rest-api-bulk/#log","title":"Log","text":"Required Property Name Property Type Notes timestamp number UTC milliseconds since 1970 (supports sub millisecond via a floating point) Yes severity number 1 \u2013 Debug, 2 \u2013 Verbose, 3 \u2013 Info, 4 \u2013 Warn, 5 \u2013 Error, 6 \u2013 Critical Yes text string hiResTimestamp string UTC nanoseconds since 1970(supports millisecond, microsecond and nanosecond) className string methodName string threadId string category string"},{"location":"newoutput/rest-api-bulk/#example","title":"Example","text":"<pre><code>curl --location --request POST '&lt;https://ingress.&lt;domain&gt;/logs/v1/bulk&gt;' \\\\\n  --header 'Content-Type: application/json' \\\\\n  --header 'Authorization: Bearer &lt;Send-Your-Data API key&gt;' \\\\\n  --data-raw '{\n    \"applicationName\": \"*insert desired application name*\",\n    \"subsystemName\": \"*insert desired subsystem name*\",\n    \"computerName\": \"*insert computer name*\",\n    \"logEntries\": [\n      {\n            \"timestamp\": 1675148539123.342,\n        \"severity\": 3,\n        \"text\": \"this is a normal text message\",\n        \"category\": \"cat-1\",\n        \"className\": \"class-1\",\n        \"methodName\": \"method-1\",\n        \"threadId\": \"thread-1\"\n      }, {\n        \"hiResTimestamp\": \"1675148539789123123\",\n        \"severity\": 5,\n        \"text\": \"{\\\\\"key1\\\\\":\\\\\"val1\\\\\",\\\\\"key2\\\\\":\\\\\"val2\\\\\",\\\\\"key3\\\\\":\\\\\"val3\\\\\",\\\\\"key4\\\\\":\\\\\"val4\\\\\"}\",\n        \"category\": \"DAL\",\n        \"className\": \"UserManager\",\n        \"methodName\": \"RegisterUser\",\n        \"threadId\": \"a-352\"\n    }]}'\n\n</code></pre> <ul> <li> <p>If you are sending a JSON payload, we suggest escaping it and then inserting it into the text field, which should be sent as string.</p> </li> <li> <p>If <code>timestamp</code> is present, use milliseconds or microseconds. If not, we will inject the UTC time at the time the request is received.</p> </li> </ul>"},{"location":"newoutput/rest-api-bulk/#coralogix-dashboard","title":"Coralogix Dashboard","text":"<p>View your logs, with all metadata fields, in your Coralogix dashboard.</p> <p></p>"},{"location":"newoutput/rest-api-bulk/#support","title":"Support","text":"<p>Need help? Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/rest-api-singles/","title":"Coralogix REST API /singles","text":"<p>Send your logs using the Coralogix REST API /singles.</p>"},{"location":"newoutput/rest-api-singles/#endpoint-url","title":"Endpoint URL","text":"<p>Input your Coralogix domain into the following endpoint URL:\u00a0<code>https://ingress.&lt;domain&gt;/logs/v1/singles</code>.</p>"},{"location":"newoutput/rest-api-singles/#schema","title":"Schema","text":""},{"location":"newoutput/rest-api-singles/#endpoint-details","title":"Endpoint Details","text":"URLhttps://ingress.&lt;domain&gt;/logs/v1/singlesHTTP MethodPOSTContent-Typeapplication/jsonHeaderAuthorization: Bearer &lt;Send-Your-Data API key&gt; <ul> <li> <p>We recommend sending logs in batches to minimize network calls.</p> </li> <li> <p>The API is limited to a message size of 2MB which is approximately 3,000 medium-sized logs.</p> </li> <li> <p>If you are using Ajax or a similar technology, you may need to send the data with JSON.stringify()).</p> </li> </ul>"},{"location":"newoutput/rest-api-singles/#post-body","title":"POST Body","text":"<p>An array of JSON objects which contain:</p> RequiredProperty NameProperty TypeNotetimestampnumberUTC milliseconds since 1970 (supports sub-millisecond via a floating point)YesapplicationNamestringusually used to separate environmentsYessubsystemNamestringusually used to separate componentscomputerNamestringseveritynumber1 \u2013 Debug, 2 \u2013 Verbose, 3 \u2013 Info, 4 \u2013 Warn, 5 \u2013 Error, 6 \u2013 CriticalcategorystringCategory fieldclassNamestringClass fieldmethodNamestringMethod fieldthreadIdstringThread ID fieldhiResTimestampstringUTC nanoseconds since 1970(supports millisecond, microsecond and nanosecond)Yestextstring/jsonevent log"},{"location":"newoutput/rest-api-singles/#example","title":"Example","text":"<pre><code>curl --location --request POST 'https://ingress.&lt;domain&gt;/logs/v1/singles' \\\n  --header 'Content-Type: application/json' \\\n  --header 'Authorization: Bearer &lt;Send-Your-Data API key&gt;' \\\n  --data-raw '[{\n      \"applicationName\": \"*insert desired application name*\",\n      \"subsystemName\": \"*insert desired subsystem name*\",\n      \"computerName\": \"*insert computer name*\",\n      \"severity\": 3,\n      \"text\": \"this is a normal text message\",\n      \"category\": \"cat-1\",\n      \"className\": \"class-1\",\n      \"methodName\": \"method-1\",\n      \"threadId\": \"thread-1\",\n      \"timestamp\": 1675148539123.342\n    }, {\n      \"applicationName\": \"*insert desired application name*\",\n      \"subsystemName\": \"*insert desired subsystem name*\",\n      \"computerName\": \"*insert computer name*\",\n      \"hiResTimestamp\": \"1675148539789123123\",\n      \"severity\": 5,\n      \"text\": \"{\\\"key1\\\":\\\"val1\\\",\\\"key2\\\":\\\"val2\\\",\\\"key3\\\":\\\"val3\\\",\\\"key4\\\":\\\"val4\\\"}\",\n      \"category\": \"DAL\",\n      \"className\": \"UserManager\",\n      \"methodName\": \"RegisterUser\",\n      \"threadId\": \"a-352\"\n    }]'\n</code></pre> <p>Note:</p> <ul> <li> <p>If you are sending a JSON payload note, we suggest escaping it and then inserting it into the text field, which should be sent as a string.</p> </li> <li> <p>If <code>timestamp</code> is present, use milliseconds or microseconds. If not, we will inject the UTC time at the time the request is received.</p> </li> </ul>"},{"location":"newoutput/rest-api-singles/#coralogix-dashboard","title":"Coralogix Dashboard","text":"<p>View your logs, with all metadata fields, in your Coralogix dashboard.</p> <p></p>"},{"location":"newoutput/rest-api-singles/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/role-based-access-control-logs/","title":"Role-Based Access Control: Logs","text":"<p>As part of our team and user management options, role-based access control (RBAC) allows account administrators to grant some or all team members specific application and subsystem data permissions and action permissions. You can also assign team users into multiple groups with different action permissions to any subset of users.</p> <p>User roles are determined when a user is initially invited to a Team on the Invites page. You may change users' roles by either assigning them to a different group from the Team Members\u00a0page.</p> <p></p> <p>Or add the user to the group by choosing him or her in the Members option. You can also change permissions to an entire group by changing the role in the Select Role option.</p> <p></p> <p>In Manage application scope, you may choose specific applications that will be visible to the group across Coralogix. For example, in the AWS people group, choosing the application \"AWS\" will give access to aws-related information only to the users who are members of the AWS people group (access to view aws logs in the Logs view, aws logs in the LiveTail view, aws-related alerts, AWS traces etc.). You can also choose a different Filter type to include several applications which start, end or include a search term.</p> <p></p> <p>The same goes for Manage subsystem scope.</p> <p>Note: If Application and Subsystem are not specified then users have access to all applications and subsystems.</p>"},{"location":"newoutput/role-based-access-control-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/role-based-access-control-traces/","title":"Role-Based Access Control: Traces","text":"<p>As part of our team and user management options, *r*ole-based access control (RBAC) allows account administrators to grant some or all team members specific application and subsystem data permissions, as well as action permissions. You can also assign team users into multiple groups with different action permissions to any sub-set of users. As indicated previously, by default 3 groups are created: Admins, Users, and Read-only.</p> <p>User roles are determined when a user is initially invited to a Team (in the Invites page). You may change users roles by either assigning them to a different group from the Team Members\u00a0page:</p> <p></p> <p>Or by adding the user to the group by choosing him in the Members option. You can also change permissions to an entire group by changing the role in the Select Role option.</p> <p></p> <p>In Manage application scope you may choose specific applications that will be visible to the group across Coralogix. For example, in the AWS people group, choosing application \"AWS\" will give access to aws-related information only to the users who are members of the AWS people group (access to view aws logs in the Logs view, aws logs in the LiveTail view, aws-related alerts, AWS traces etc.). You can also choose a different Filter type to include several applications which start, end or include a search term.</p> <p></p> <p>The same goes for Manage subsystem scope.</p> <p>Notes:</p> <ul> <li> <p>If application and subsystem are not specified, then users have access to all applications and subsystems.</p> </li> <li> <p>In case a user does not have the permission to view a specific subsystem, the relevant trace will be marked as N\\A(out of scope) and the relevant spans will not be visible (grayed out) for him (only latency will be shown).</p> </li> </ul>"},{"location":"newoutput/role-based-access-control-traces/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/roles-permissions/","title":"Create Roles & Permissions","text":""},{"location":"newoutput/roles-permissions/#overview","title":"Overview","text":"<p>Coralogix offers a flexible access management system that allows you to customize the level at which you control your users' access to Coralogix resources and associated actions.</p> <p>Organizations can access system roles with predefined permissions assigned to users based on their job functions. Create custom roles to combine permissions into new ones while maintaining security and compliance standards for flexibility. Permissions attached to a custom role apply to all resources of a particular resource type.</p> <p>Organizations and users that need maximum flexibility can control user data scope by limiting access to particular applications and subsystems.</p>"},{"location":"newoutput/roles-permissions/#role-based-access-control","title":"Role-Based Access Control","text":""},{"location":"newoutput/roles-permissions/#rbac-model","title":"RBAC Model","text":"<p>Roles categorize users and define their account permissions, determining which actions they may perform and on which resources.</p> <p>By default, Coralogix offers seven predefined system roles. You can create custom roles\u00a0to customize between your users and their permissions by adding permissions to existing system roles.</p> <p>By granting permissions to roles, any user who is associated with that role receives that permission. When users are associated with multiple roles, they receive all the permissions granted to each of their roles. The more roles a user is associated with, the more access they have within a Coralogix account.</p>"},{"location":"newoutput/roles-permissions/#group-membership","title":"Group Membership","text":"<p>Users enjoy role-based access based on their membership in one or more groups, each assigned particular roles. Find out more here.</p>"},{"location":"newoutput/roles-permissions/#role-types","title":"Role Types","text":""},{"location":"newoutput/roles-permissions/#system-roles","title":"System Roles","text":"<p>Coralogix offers seven predefined system roles.</p> System Role Description Platform Admin Platform admins manage all teams, user access, and settings. They can also manage and configure all Coralogix resources. They have full access to billing information and can revoke API keys. They have management and read-only access to organization settings. Data Admin Data admins can view and modify all Coralogix monitoring features. They may not view and manage organization settings and are limited in their access to team settings. Observability Lead Observability leads can view all Coralogix monitoring features but are limited in their management and configuration permissions. They may not view and manage organization settings and are limited in their access to team settings. Standard User Read-only users do not have access to make changes within Coralogix. This comes in handy when you'd like to share specific read-only views with a client or when a member of one team needs to share a dashboard or other resource with someone outside their team. Read-Only User Read-only users do not have access to make changes within Coralogix. This comes in handy when you\u2019d like to share specific read-only views with a client, or when a member of one team needs to share a dashboard or other resource with someone outside their team. No Access User Users have no permissions. Use this system role as a baseline to create custom roles with limited permissions. Security User Security users can perform daily security tasks and manage related configurations, such as incident investigation, alert configuration, security posture visibility, and extension deployment."},{"location":"newoutput/roles-permissions/#custom-roles","title":"Custom Roles","text":"<p>Custom roles allow you to specify and control the permissions, privileges, and access levels of different user groups within the platform. Rooted in our predefined system roles, custom roles enable you to tailor access and responsibilities to your team members' specific needs and responsibilities.</p> <p>When creating custom roles, one may exclusively add permissions to system roles.</p> <p>A complete list of permissions available for custom roles can be found below.</p>"},{"location":"newoutput/roles-permissions/#legacy-roles","title":"Legacy Roles","text":"<p>Legacy roles are predefined system roles from our sunsetted role management system. Existing customers must migrate the legacy roles assigned to their users to system and custom roles to prevent role loss. Full instructions can be found here.</p>"},{"location":"newoutput/roles-permissions/#create-a-custom-role","title":"Create a Custom Role","text":"<p>STEP 1. From the Coralogix toolbar, click on the user icon in the top right-hand corner.</p> <p></p> <p>STEP 2. Click Settings.</p> <p>STEP 3. In the left-hand menu, select Roles.</p> <p></p> <p>STEP 4. Click + ADD NEW.</p> <p>STEP 5. Enter a name and description for the new role.</p> <p></p> <p>STEP 6. Select an existing system role upon which to base the new custom role. The list of permissions associated with the system role will be displayed.</p> <p></p> <p>STEP 7. Mark the checkboxes next to the permissions you want to add to your custom role. PERMISSIONS ADDED will appear to the right of the PERMISSION GROUPS list.</p> <p></p> <p>STEP 8. Click CREATE.</p> <p>STEP 9. Your new custom role will appear in the list of roles, along with additional contextual information:</p> <ul> <li> <p>Users. The number of users with this role.</p> </li> <li> <p>Groups. The groups that have been assigned this role.</p> </li> <li> <p>Role Type. Specifies whether the role is SYSTEM, CUSTOM, or LEGACY. If it is a Custom Role, the system role upon which it has been modeled will also be displayed.</p> </li> </ul> <p>View, edit, or delete the custom role from the Actions column anytime.</p> <p></p>"},{"location":"newoutput/roles-permissions/#compare-roles","title":"Compare Roles","text":"<p>Scroll down to compare between system roles in the Compare Roles section.</p> <p>Click on multiple roles of interest to compare and contrast their permissions. Compare all permissions for all roles you would like to compare or exclusively between those unique to one or more roles.</p> <p></p>"},{"location":"newoutput/roles-permissions/#additional-resources","title":"Additional Resources","text":"DocumentationTeamsGroups"},{"location":"newoutput/roles-permissions/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to contact us\u00a0via our in-app chat\u00a0or by emailing support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/rum-integration-package/","title":"RUM Integration Package","text":"<p>Set up and configure Real User Monitoring\u00a0via the RUM Integration Package to hit the ground running with\u00a0our various RUM features. The package includes automatic configuration of the RUM Browser SDK, as well as upload of your source maps. Once configured, all network requests and errors in your system will be captured and sent to Coralogix.</p>"},{"location":"newoutput/rum-integration-package/#overview","title":"Overview","text":"<p>The RUM Integration Package includes automatic configuration of the RUM Browser SDK, as well as upload of your application source maps. Once configured, all network requests and errors in your system will be captured and sent to Coralogix.</p>"},{"location":"newoutput/rum-integration-package/#rum-browser-sdk","title":"Rum Browser SDK","text":"<p>As part of our\u00a0Real User Monitoring\u00a0(RUM) toolkit, Coralogix offers multi-faceted\u00a0Error Tracking,\u00a0enabled by our\u00a0RUM Browser SDK. Integrated into the front end of web applications, this light-weight code tool detects and captures errors that arise within users\u2019 browsers, including JavaScript runtime errors, unhandled exceptions, network errors, and application (custom logic) errors. The SDK collects essential error information and additional contextual data, such as browser details and URLs, and securely sent it to our platform through logs for further analysis.</p>"},{"location":"newoutput/rum-integration-package/#source-maps","title":"Source Maps","text":"<p>Take advantage of this integration package to upload source maps for your applications to the Coralogix RUM service.</p> <p>Source map files are files commonly used in web development to facilitate the debugging process of minified or transpiled code. When JavaScript or CSS code is minified or transformed into a more compact form for production, it becomes challenging to trace errors back to the original source code due to the loss of meaningful variable names, line numbers, and structure. Source map files address this issue by providing a mapping between the minified code and its original, human-readable source code. This enables developers to debug efficiently by allowing browsers and debugging tools to display accurate error messages and stack traces based on the original code, aiding in the identification and resolution of issues in the development process.</p>"},{"location":"newoutput/rum-integration-package/#installation","title":"Installation","text":"<p>STEP 1. From your Coralogix toolbar, navigate to Data Flow &gt; Integrations.</p> <p>STEP 2. Select the Real User Monitoring Integration Package from the list of integrations.</p> <p></p> <p>STEP 3. Click + ADD NEW.</p> <p></p> <p>STEP 4. Enter a name for your integration.</p> <p>STEP 5. Click CREATE NEW KEY to create an API key for your RUM integration. This key will be used in the following steps.</p> <p>STEP 6. Click NEXT.</p> <p></p> <p>STEP 7. Integrate the Browser SDK into your frontend application using the code snippets generated by the integration.</p> <p>STEP 8. Click FINISH to end the installation here, or [Recommended] click SET UP SOURCE MAP to upload your source map and gain full data observability and make the best of the error tracking tool.</p> <p></p> <p>STEP 9. Copy and run the setup source map command. Click FINISH.</p> <p></p> <p>The integration is completed and appears under the list of integrations.</p> <p>Clicking on the integration name expands the row to show you the version, date deployed, whether or not a source map was uploaded and whether or not data is being received.</p>"},{"location":"newoutput/rum-integration-package/#additional-resources","title":"Additional Resources","text":"DocumentationReal User MonitoringRUM Browser SDK"},{"location":"newoutput/rum-integration-package/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/rum-source-maps/","title":"Source Maps","text":"<p>Take advantage of our Real User Monitoring (RUM) CLI to easily upload the source maps for your applications to the Coralogix RUM service.</p>"},{"location":"newoutput/rum-source-maps/#overview","title":"Overview","text":"<p>Source map files are files commonly used in web development to facilitate the debugging process of minified or transpiled code. When JavaScript or CSS code is minified or transformed into a more compact form for production, it becomes challenging to trace errors back to the original source code due to the loss of meaningful variable names, line numbers, and structure. Source map files address this issue by providing a mapping between the minified code and its original, human-readable source code. This enables developers to debug efficiently by allowing browsers and debugging tools to display accurate error messages and stack traces based on the original code, aiding in the identification and resolution of issues in the development process.</p> <p>Take advantage of our RUM CLI to easily upload the source maps for your applications to the Coralogix RUM service.</p> <p>Information on how to automatically upload your source maps using the RUM Integration Package can be found here.</p>"},{"location":"newoutput/rum-source-maps/#install-the-rum-cli","title":"Install the RUM CLI","text":"<p>STEP 1. Open a terminal or command prompt.</p> <p>STEP 2. Run the following command to install the CLI globally:</p> <pre><code>npm i @coralogix/rum-cli\n\n</code></pre> <p>STEP 3. Once the installation is complete, you can use the CLI by running the\u00a0<code>coralogix-rum-cli</code>\u00a0command in your terminal.</p>"},{"location":"newoutput/rum-source-maps/#commands","title":"Commands","text":"<p>Here are some example commands for using the Coralogix RUM CLI:</p>"},{"location":"newoutput/rum-source-maps/#upload-source-maps","title":"Upload Source Maps","text":"<pre><code>coralogix-rum-cli upload-source-maps -k &lt;privateKey&gt; -a &lt;application&gt; -v &lt;version&gt; -f &lt;folderPath&gt; -e &lt;env&gt; -c &lt;commitHash&gt; -n &lt;repoName&gt; -o &lt;orgName&gt;\n\n</code></pre>"},{"location":"newoutput/rum-source-maps/#display-help","title":"Display Help","text":"<pre><code>coralogix-rum-cli upload-source-maps --help\n\n</code></pre>"},{"location":"newoutput/rum-source-maps/#uploading-source-maps","title":"Uploading Source Maps","text":"<p>Three options exist for uploading source maps.</p> <ul> <li> <p>Upload source maps during the onboarding process.</p> </li> <li> <p>Upload source maps from the Coralogix RUM user interface. [Recommended if you didn\u2019t upload during the onboarding process]</p> </li> <li> <p>Upload source maps manually using the Coralogix RUM CLI (explained in this document).</p> </li> </ul> <p>When uploading source maps using the RUM CLI, use the following options:</p> Option Description <code>k, --private-key &lt;privateKey&gt;</code> Your Alerts, Rules and Tags API Key to authenticate with the Coralogix API <code>a, --application &lt;application&gt;</code> Name of the application <code>-v, --version &lt;version&gt;</code> The application version Must match the version used by the RUM SDK <code>f, --folder-path &lt;folderPath&gt;</code> Path to the folder containing the source maps. <code>e, --env &lt;env&gt;</code> Your environment <code>c, --commit-hash &lt;commitHash&gt;</code> GitHub commit hash (optional) <code>n, --repo-name &lt;repoName&gt;</code> GitHub repository name (optional) <code>o, --org-name &lt;orgName&gt;</code> GitHub organization name (optional) <code>h, --help</code> Display help"},{"location":"newoutput/rum-source-maps/#upload-source-maps-using-a-script","title":"Upload Source Maps Using a Script","text":"<p>To simplify the process of uploading source maps using the Coralogix RUM CLI, you can create a bash script that automates the task. Follow these steps to set up the script for CI and non-CI integrations.</p>"},{"location":"newoutput/rum-source-maps/#ci-integration","title":"CI Integration","text":"<p>STEP 1. Create a new file named\u00a0<code>upload-source-maps.sh</code>\u00a0and open it for editing.</p> <p>STEP 2. Copy and paste the following script into the file, replacing the placeholder values with your actual information:</p> <pre><code>#! /usr/bin/env bash\n\n# Replace these values with your actual information\nREPO_NAME=\"your-repo-name\"\nORG_NAME=\"your-github-username\"\nAPPLICATION=\"your-application-name\"\nENV=\"your-environment\"\nSOURCE_MAPS_PATH=\"your-source-maps-path\"\nPRIVATE_KEY=\"your-coralogix-private-key\"\nVERSION=\"your-application-version\"\n\n# Get the commit hash using git rev-parse\nCOMMIT_HASH=$(git rev-parse HEAD)\n\n# Run Coralogix RUM CLI to upload source maps\ncoralogix-rum-cli upload-source-maps -k \"$PRIVATE_KEY\" -a \"$APPLICATION\" -v \"$VERSION\" -f \"$SOURCE_MAPS_PATH\" -e \"$ENV\" -c \"$COMMIT_HASH\" -n \"$REPO_NAME\" -o \"$ORG_NAME\"\n\n</code></pre> <p>Note: The <code>$RELEASE_ID</code> must align with the content of the version field in your <code>CoralogixRum.init()</code> configuration in Coralogix SDK for Browsers.</p>"},{"location":"newoutput/rum-source-maps/#non-ci-integration","title":"Non-CI Integration","text":"<p>STEP 1. Open a terminal window.</p> <p>STEP 2. Run the following command:</p> <pre><code>coralogix-rum-cli upload-source-maps -k \"$PRIVATE_KEY\" -a \"$APPLICATION\" -v \"$VERSION\" -f \"$SOURCE_MAPS_PATH\" -e \"$ENV\" -c \"$COMMIT_HASH\" -n \"$REPO_NAME\" -o \"$ORG_NAME\"\n\n</code></pre> <p>Note: The <code>$VERSION</code> must align with the content of the version field in your <code>CoralogixRum.init()</code> configuration in Coralogix SDK for Browsers.</p>"},{"location":"newoutput/rum-source-maps/#available-environments","title":"Available Environments","text":"<p>The Coralogix RUM CLI supports the following environments:</p> <ul> <li> <p>AP1</p> </li> <li> <p>AP2</p> </li> <li> <p>EU1</p> </li> <li> <p>EU2</p> </li> <li> <p>US1</p> </li> </ul> <p>Please use the appropriate environment value when specifying the\u00a0<code>-e, --env\u00a0option</code>.</p>"},{"location":"newoutput/rum-source-maps/#optional-github-information","title":"Optional GitHub Information","text":"<p>Use this GitHub information to add context to your source maps.</p> Option Description <code>c, --commit-hash &lt;commitHash&gt;</code> This option allows you to provide the GitHub commit hash associated with the source maps. Including this hash can help in tracking down issues specific to a particular commit. <code>n, --repo-name &lt;repoName&gt;</code> By specifying the GitHub repository name, you're indicating the repository where the source code is hosted. This information aids in correlating source maps with the correct repository. <code>o, --org-name &lt;orgName&gt;</code> This option enables you to input your GitHub organization user, helping to attribute the source maps to the appropriate organization user. Providing these GitHub-related options can enhance the accuracy of source map management and issue tracking within Coralogix RUM. However, they are not mandatory and can be omitted if not applicable."},{"location":"newoutput/rum-source-maps/#limitations","title":"Limitations","text":"<p>The source map folder size limit is 100MB and should not be exceeded.</p>"},{"location":"newoutput/rum-source-maps/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Real User MonitoringCoralogix SDK for BrowsersExternal LinksCoralogix RUM CLI Library"},{"location":"newoutput/rum-source-maps/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/running-opentelemetry-as-a-cli-application/","title":"Running OpenTelemetry as a CLI Application","text":"<p>This tutorial demonstrates how to configure OpenTelemetry (OTEL) Collector to send your logs and metrics to Coralogix when running OpenTelemetry as a CLI application or service.</p>"},{"location":"newoutput/running-opentelemetry-as-a-cli-application/#prerequisites","title":"Prerequisites","text":"<ul> <li>OpenTelemetry CLI application or service installed</li> </ul> <p>Notes:</p> <p>To use the Coralogix Exporter, ensure that you have installed opentelemetry-collector-contrib. You can download the relevant version for your OS using this link.</p>"},{"location":"newoutput/running-opentelemetry-as-a-cli-application/#configuration","title":"Configuration","text":"<p>STEP 1. Create a configuration file. Copy this template file and save it as <code>config.yaml</code>.</p> <pre><code>receivers:\n  filelog:\n    start_at: beginning\n    include:\n      - /example.log\n    include_file_path: true\n    multiline: {line_start_pattern: \"\\\\n\"}\n    hostmetrics:\n        collection_interval: 30s\n        scrapers:\n          cpu:\n          memory:\nexporters:\n  coralogix:\n    domain: \"Domain\"\n    private_key: \"Private key\"\n    application_name: \"Application Name\"\n    subsystem_name: \"Subsystem Name\"\n    timeout: 30s\n\nservice:\n  pipelines:\n    logs:\n      receivers: [ filelog ]\n      exporters: [ coralogix ]\n    metrics:\n      receivers: [ hostmetrics ]\n      exporters: [ coralogix ]\n\n</code></pre> <p>Provide the following variables.</p> Variable Description Private Key Your Coralogix\u00a0Send-Your-Data API key Application Name The name of your\u00a0application, as it will appear in your Coralogix dashboard. For example, a company named\u00a0SuperData\u00a0might insert the\u00a0SuperData\u00a0string parameter. If SuperData wants to debug its test environment, it might use\u00a0SuperData\u2013Test. Subsystem Name The name of your\u00a0subsystem, as it will appear in your Coralogix dashboard. Applications often have multiple subsystems (ie. Backend Servers, Middleware, Frontend Servers, etc.). In order to help you examine the data you need, inserting the subsystem parameter is vital. Domain Your Coralogix domain <p>STEP 2. Save this log file as <code>example.log</code>.</p> <pre><code>2023-06-19 05:20:50 ERROR This is a test error message\n2023-06-20 12:50:00 DEBUG This is a test debug message\n2023-06-21 12:34:56 INFO This is a test info message\n\n</code></pre> <p>STEP 3. Run the example file. Choose the command that mirrors your installation type.</p> <ul> <li>OTEL installed as an application</li> </ul> <pre><code>./otelcol-contrib --config config.yaml\n\n</code></pre> <ul> <li>OTEL installed as a service</li> </ul> <pre><code>otelcol-contrib --config config.yaml\n\n</code></pre>"},{"location":"newoutput/running-opentelemetry-as-a-cli-application/#validation","title":"Validation","text":"<p>Validate your configuration.</p>"},{"location":"newoutput/running-opentelemetry-as-a-cli-application/#logs","title":"Logs","text":"<p>In your Coralogix navigation pane, click LiveTrail &gt; Start to view your logs.</p> <p></p> <p></p>"},{"location":"newoutput/running-opentelemetry-as-a-cli-application/#metrics","title":"Metrics","text":"<p>STEP 1. Navigate to hosted Grafana view.</p> <p>STEP 2. In the left-hand panel, click Explore &gt; Metrics browser. Select the metrics that you would like to see.</p> <p></p>"},{"location":"newoutput/running-opentelemetry-as-a-cli-application/#additional-resources","title":"Additional Resources","text":"DocumentationOpenTelemetry"},{"location":"newoutput/running-opentelemetry-as-a-cli-application/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/salesforce-commerce-cloud/","title":"Salesforce Commerce Cloud","text":"<p>Coralogix provides an easy way to collect your Salesforce logs. The preferred and easiest integration method will be to use our app in the AWS Serverless Application Repository.</p>"},{"location":"newoutput/salesforce-commerce-cloud/#requirements","title":"Requirements","text":"<ul> <li>Your AWS user should have permissions to create lambdas and IAM roles.</li> </ul>"},{"location":"newoutput/salesforce-commerce-cloud/#installation","title":"Installation","text":"<ul> <li> <p>Navigate to Application Page.</p> </li> <li> <p>Fill in the required parameters.</p> </li> <li> <p>Click Deploy.</p> </li> </ul> <p>Once the deployment is done the lambda will execute every X minutes based on the 'lambda schedule' parameter.</p>"},{"location":"newoutput/salesforce-commerce-cloud/#parameters-and-descriptions","title":"Parameters and Descriptions","text":"Variable Description Application Name The stack name of this application created via AWS CloudFormation NotificationEmail Failure notification email address ApplicationName Application Name in Coralogix CoralogixRegion The region associated with your Coralogix domain FunctionArchitecture Our Function supports x86_64 or arm64 FunctionMemorySize Max Memory for the function itself. FunctionSchedule The lambda function schedule invoke in minutes FunctionTimeout Lambda function timeout PrivateKey Your Coralogix account Send-Your-Data API key SfccEndpoints Salesforce commerce cloud endpoints to check logs in, in a DOMAIN/PATH format. Multiple can be inserted with a comma in between. SfccPassword Salesforce commerce cloud password used for authentication SfccUsername Salesforce commerce cloud username used for authentication SubsystemName Subsystem name in Coralogix"},{"location":"newoutput/saml-management-via-cli/","title":"SAML Management (via CLI)","text":"<p>The Coralogix CLI tool allows management of SAML SSO configuration by admin users. Actions supported on the CLI include viewing, initializing, activating, and deactivating SAML configuration. This capability makes it possible for SAML integration to be automated using scripts or other provisioning tools.</p> <p>This tutorial will guide you on how to manage the SAML integration using the CLI tool.</p> <p>Notes:</p> <ul> <li> <p>If you intend to follow this integration with our SCIM integration, delete any existing users before the SCIM integration is applied. If necessary, leave one admin user.</p> </li> <li> <p>Upon completion of the SCIM integration, recreate all users through SCIM.</p> </li> </ul>"},{"location":"newoutput/saml-management-via-cli/#getting-started","title":"Getting started:","text":"<ol> <li> <p>Install the latest version of the Coralogix CLI</p> </li> <li> <p>Teams API key (Fetch this from Account -&gt; Settings -&gt; API access)</p> </li> <li> <p>User must have an admin role.</p> </li> </ol>"},{"location":"newoutput/saml-management-via-cli/#environment-variables","title":"Environment variables:","text":"<p>[table id=76 /] Note: When the environment variable is set --api-key (-k) becomes an optional argument when using the tool.</p>"},{"location":"newoutput/saml-management-via-cli/#commands","title":"Commands:","text":""},{"location":"newoutput/saml-management-via-cli/#details","title":"details","text":"<p>This command displays SAML configuration for:: your Team.</p>"},{"location":"newoutput/saml-management-via-cli/#activate","title":"activate","text":"<p>This command will activate SAML on your Team.</p>"},{"location":"newoutput/saml-management-via-cli/#deactivate","title":"deactivate","text":"<p>This command will deactivate SAML on your Team.</p>"},{"location":"newoutput/saml-management-via-cli/#init","title":"init","text":"<p>This command will initialize SAML on Coralogix with metadata file from the IdP.</p> <p>Note: Initializing SAML does not activate it. For SSO authentication to work, SAML needs to be activated (using activate command).</p>"},{"location":"newoutput/saml-management-via-cli/#add-entity-id","title":"add-entity-id","text":"<p>This command adds your team-id to the SP Entity URL.\u00a0</p> <p>This will help uniquely identify the Coralogix SP on the IdP (required when you are configuring SAML for multiple teams with the same Identity Provider).</p>"},{"location":"newoutput/saml-management-via-cli/#remove-entity-id","title":"remove-entity-id","text":"<p>This command removes team-id from the SP Entity URL</p>"},{"location":"newoutput/saml-management-via-cli/#generate-provisioning-token","title":"generate-provisioning-token","text":"<p>This command generates the provisioning token</p>"},{"location":"newoutput/saml-management-via-cli/#remove-provisioning-token","title":"remove-provisioning-token","text":"<p>This command removes the provisioning token</p>"},{"location":"newoutput/saml-management-via-cli/#examples","title":"Examples:","text":"<p>Note: Examples below assume the api-key is provided as an environment variable.</p> <p>[table id=77 /]</p> <p>Options</p> <p>[table id=78 /]</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/","title":"Security Traffic Analyzer (STA) Dashboards","text":"<p>When you install the Coralogix Security Traffic Analyzer (STA) for the first time (and optionally in subsequent installations too) the following dashboards will be automatically added to your account:</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#baby-domains","title":"Baby Domains","text":"<p>Displays young domains that were created in the last three months that were accessed by the monitored servers. Normally, this dashboard should be empty, for the purpose of this example we have changed the condition to show connection to domains that were created in the past 30 years just so that you'll get a feeling of what it looks like:</p> <p></p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#zeek","title":"Zeek","text":""},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#connections","title":"Connections","text":"<p>Displays information about connections observed by the STA. Includes information such as protocols (e.g. TCP, UDP, ICMP), ports, connection states (e.g. SYN, ACK, PSH, FIN), geographic locations and more.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#dns","title":"DNS","text":"<p>Displays information about DNS queries and answers detected in the monitored traffic. Includes information such as queries, DNS servers, response codes, geographic locations, protocols, domain names, domains creation date, domains expiration dates, domains NLP scores (help determine if a domain name is machine generated) and more.</p> <p></p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#ftp","title":"FTP","text":"<p>Displays information about FTP connections detected in the monitored traffic. Includes information such as FTP commands, file paths.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#files","title":"Files","text":"<p>Displays information about files that were detected in the network traffic in any protocol. Includes information such as file hashes, MIME types, source IPs.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#http","title":"HTTP","text":"<p>Displays information about HTTP connections, includes information such as hostnames, web methods, URIs, amount of bytes transferred, HTTP referrers, user-agents, MIME types.</p> <p></p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#dhcp","title":"DHCP","text":"<p>Displays information about DHCP connections observed in the traffic.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#irc","title":"IRC","text":"<p>Displays information about IRC connections observed in the traffic.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#mysql","title":"MySQL","text":"<p>Displays information about MySQL connections found in the traffic. Includes information such as database names, queries, user names, tables and more.</p> <p></p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#rdp","title":"RDP","text":"<p>Displays information about Remote Desktop Protocol (RDP) connections found in the monitored traffic.</p> <p></p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#sip","title":"SIP","text":"<p>Displays information about SIP connections (VOIP) found in the monitored traffic.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#smb","title":"SMB","text":"<p>Displays information about SMB connections found in the monitored traffic.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#smtp","title":"SMTP","text":"<p>Displays information about Simple Mail Transmission Protocol (SMTP) connections found in the monitored traffic.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#snmp","title":"SNMP","text":"<p>Displays information about SNMP connections found in the monitored traffic.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#ssh","title":"SSH","text":"<p>Displays information about SSH connections found in the monitored traffic.</p> <p></p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#ssltls","title":"SSL/TLS","text":"<p>Displays information about SSL/TLS certificates detected in the traffic. Includes information such as certificates details (issuer, CN, validation status)</p> <p></p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#software","title":"Software","text":"<p>Displays information about software types of servers and clients as detected in the observed traffic by specific protocol analyzers. Includes information such as user agents, SSH clients and servers, database servers.</p> <p></p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#tunnels","title":"Tunnels","text":"<p>Displays information about tunnelled connections as detected in the monitored traffic.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#x509","title":"X.509","text":"<p>Displays information about X.509 certificates that have traversed the network.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#notices","title":"Notices","text":"<p>Displays security issues that were detected by Zeek.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#weird","title":"Weird","text":"<p>Displays possible security issues that were detected by Zeek. Issues presented here not necessarily mean that there\u2019s a security issue with your network but it is an indication that something might need to be investigated.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#nids-network-intrusion-detection-system","title":"NIDS (Network Intrusion Detection System)","text":"<p>Displays security issues that were detected by Suricata.</p> <p></p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#hids-host-intrusion-detection-system","title":"HIDS (Host Intrusion Detection System)","text":"<p>Displays information from Wazuh agents (if installed). This information includes actions performed on the monitored instance and their related MITRE ATT&amp;CK framework correlated tactics, Vulnerabilities detected and more.</p> <p></p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#connections_1","title":"Connections","text":""},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#destination-sum-of-total-bytes","title":"Destination - Sum of Total Bytes","text":"<p>Displays a world map with colored dots that their size represents the sum of total bytes transferred to that geographic region.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#destination-top-connection-duration","title":"Destination - Top Connection Duration","text":"<p>Displays a world map with colored dots that their size represents the top connection duration to that geographic region.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#source-originator-bytes","title":"Source - Originator Bytes","text":"<p>Displays a world map with colored dots that their size represents the sum of originator bytes transferred from that geographic region.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#source-responder-bytes","title":"Source - Responder Bytes","text":"<p>Displays a world map with colored dots that their size represents the sum of responder bytes transferred from that geographic region.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#source-sum-of-total-bytes","title":"Source - Sum of Total Bytes","text":"<p>Displays a world map with colored dots that their size represents the sum of total bytes transferred from that geographic region.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#source-top-connection-duration","title":"Source - Top Connection Duration","text":"<p>Displays a world map with colored dots that their size represents the top connection duration from that geographic region.</p> <p></p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#top-source-ips","title":"Top Source IPs","text":"<p>Displays a pie chart of the top source IP addresses with some details.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#total-bytes","title":"Total Bytes","text":"<p>Displays total number of bytes transferred by some aggregations such as: by source IP, by destination IP, by Connection, by destination port.</p>"},{"location":"newoutput/security-traffic-analyzer-sta-dashboards/#frequency-analysis","title":"Frequency Analysis","text":"<p>Displays statistics regarding the NLP based score on all domains observed in the traffic.</p> <p></p>"},{"location":"newoutput/send-log-outbound-webhooks/","title":"Send Log Outbound Webhooks","text":"<p>Enhance your observability workflows by sending real-time event notifications and log data to Coralogix. With this webhook, you can easily receive logs in Coralogix, automate responses to critical events, and improve your organization's incident management and alerting processes.</p>"},{"location":"newoutput/send-log-outbound-webhooks/#create-a-send-log-webhook","title":"Create a Send Log Webhook","text":"<p>STEP 1. From the Coralogix toolbar, navigate to Data Flow &gt; Outbound Webhooks.</p> <p>STEP 2. In the Outbound Webhooks section, click SEND LOG WEBHOOK.</p> <p></p> <p>STEP 3. Click + ADD NEW.</p> <p></p> <p>STEP 4. Enter a memorable name for your webhook that will enable you to easily identify this webhook later when attaching it to one of your alerts.</p> <p>Note: The URL and UUID fields are auto-populated.</p> <p>STEP 5. Click NEXT.</p> <p>STEP 6. [Optional] Edit the message body that will be sent with the webhook message. The fields contained within the <code>logEntries</code> key will be displayed in the log.</p> <p></p> <p>STEP 7. Click TEST &amp; SAVE.</p> <p>The system creates a log on the Coralogix Explore page in the dashboard to check that your configuration is valid. If the log is created successfully, a confirmation message is displayed.</p> <p>STEP 8. Configure your alert notifications once the configuration is confirmed and the webhook is in place.</p>"},{"location":"newoutput/send-log-outbound-webhooks/#additional-resources","title":"Additional Resources","text":"DocumentationConfigure Alert Notifications for Outbound Webhooks"},{"location":"newoutput/send-log-outbound-webhooks/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/send-logs-using-aws-kinesis-data-firehose/","title":"Send Logs using AWS Kinesis Data Firehose","text":"<p>Amazon Kinesis Data Firehose\u00a0delivers real-time streaming data to destinations like Amazon Simple Storage Service (Amazon S3), Amazon Redshift, or Amazon OpenSearch Service (successor to Amazon Elasticsearch Service), and now supports delivering streaming data to Coralogix. There is no limit on the number of delivery streams, so it can be used for retrieving data from multiple AWS services.</p> <p>Coralogix is an AWS Partner Network (APN) Advanced Technology Partner with AWS\u00a0 Competencies in DevOps. The platform enables you to easily explore and analyze logs to gain deeper insights into the state of your applications and AWS infrastructure. Analyze all of your AWS service logs while storing only those you need. Generate metrics from aggregated logs to uncover and alert on trends in your AWS services.</p>"},{"location":"newoutput/send-logs-using-aws-kinesis-data-firehose/#setup-options","title":"Setup Options","text":"<ul> <li> <p>Automated Integration Package (Recommended). Streamline ingesting and analyzing logs from your AWS resources using our automated\u00a0integration packages.</p> </li> <li> <p>Manual Integration.\u00a0Alternatively, use our\u00a0manual integration below.</p> </li> <li> <p>Terraform. Install and manage the Firehose Logs integration with AWS services as modules in your infrastructure code.</p> </li> <li> <p>CloudFormation. Install our\u00a0AWS Kinesis Firehose Logs with CloudFormation Template\u00a0and incorporate the configurations and settings via CloudFormation to automate your Firehose logs collection setup and management.</p> </li> </ul>"},{"location":"newoutput/send-logs-using-aws-kinesis-data-firehose/#automated-integration-packages","title":"Automated Integration Packages","text":"<p>Streamline your setup process using our automated integration depending on the integration type (e.g. CloudWatch, WAF). The selected package lets you preconfigure and deploy a template, replicating the manual setup</p>"},{"location":"newoutput/send-logs-using-aws-kinesis-data-firehose/#integration-options","title":"Integration Options","text":"<p>Choose one of the integration types based on the format of the logs that are being sent:</p> <ul> <li> <p>AWS CloudWatch (JSON) - CloudWatch logs store records of events and data generated by various AWS resources, applications, and services. These logs provide insights into system behavior, errors, and operational performance.</p> </li> <li> <p>AWS CloudTrail (CloudWatch) - CloudTrail provides comprehensive records of actions performed within your Amazon Web Services (AWS) account. This includes API calls, configuration changes, and user activities.</p> </li> <li> <p>Web Application Firewall (WAF) - WAF logs are detailed records of web traffic and security events generated by AWS Web Application Firewall. These logs contain critical information about incoming requests, such as IP addresses, request types, response codes, and potential security threats or attacks.</p> </li> <li> <p>Amazon Elastic Kubernetes Service (EKS) Fargate - EKS Fargate logs are the records of events, activities, and information generated by containers running within the EKS Fargate environment. These logs provide essential insights into the operation and performance of your containerized applications.</p> </li> <li> <p>Generic Logs - This integration provides a wider array of data sources, including <code>Default</code> and <code>RawText</code> types. Its generic nature allows you to select the source of your logs flexibly. You can also define the application and subsystem names as dynamic extractions during the setup, accommodating various AWS resources and services.</p> </li> </ul>"},{"location":"newoutput/send-logs-using-aws-kinesis-data-firehose/#configuration","title":"Configuration","text":"<p>STEP 1. From your Coralogix toolbar, navigate to\u00a0Data Flow\u00a0&gt;\u00a0Integrations.</p> <p>STEP 2.\u00a0In the Integrations section, search \u201cFirehose\u201d and select one of the following:</p> <ul> <li> <p>AWS CloudWatch Logs via Firehose</p> </li> <li> <p>AWS CloudTrail Logs via Firehose</p> </li> <li> <p>AWS EKS Fargate Logs via Firehose</p> </li> <li> <p>AWS WAF Logs via Firehose</p> </li> <li> <p>AWS Generic Logs via Firehose</p> </li> </ul> <p>STEP 3.\u00a0Click\u00a0ADD NEW.</p> <p>STEP 4.\u00a0Input your integration details.</p> <ul> <li> <p>Integration Name.\u00a0Enter a name for your integration. This will be used as a stack name in CloudFormation.</p> </li> <li> <p>API Key. Enter your\u00a0Send-Your-Data API key\u00a0or click\u00a0CREATE A NEW KEY\u00a0to create a new API key for the integration.</p> </li> <li> <p>Application Name.\u00a0Enter an\u00a0application name. The default name is AWS.</p> </li> <li> <p>Subsystem Name.\u00a0Enter a\u00a0subsystem name. The default name</p> </li> <li> <p>Kineses Stream ARN. [Optional] Enter the ARN of the Kinesis stream if using Amazon Kinesis Data Streams as a source for logs.</p> </li> <li> <p>Input Source [for Generic Logs Integration]. The data source in AWS Kinesis Data Firehose determines the\u00a0<code>integrationType</code>\u00a0parameter value. For extracting the <code>applicationName</code>\u00a0and\u00a0<code>subsystemName</code> values dynamically, an explanation is in the Manual Setup: Step 4 and a reference table in the Dynamic Values Table section of this document.</p> </li> <li> <p>AWS Region.\u00a0Select your AWS region from the dropdown menu.</p> </li> <li> <p>AWS PrivateLink (Advanced Settings). [Optional] Enabling AWS PrivateLink is\u00a0recommended\u00a0to ensure a secure and private connection between your VPCs and AWS services. Find out more\u00a0here.</p> </li> </ul> <p>STEP 6.\u00a0Click\u00a0NEXT.</p> <p>STEP 7.\u00a0Review the instructions for your integration. Click\u00a0CREATE CLOUDFORMATION.</p> <p>STEP 8.\u00a0You will be rerouted to the AWS website. Verify that all of the auto pre-populated values are correct. Click\u00a0Create Stack.</p> <p>STEP 9.\u00a0Return to the Coralogix application, where you will find instructions on configuring the log delivery from the selected input source to AWS Kinesis Data Firehose (if relevant).</p> <p>Notes:</p> <p>If you provide a Kinesis Stream ARN, Coralogix assumes that the data is in the stream and does not provide any additional instructions. It is the user\u2019s responsibility to deliver data to the stream. In place of the instructions, you will see a message that prompts the user to confirm the integration.</p> <p>STEP 10. Click\u00a0COMPLETE\u00a0to close the module.</p> <p>STEP 11.\u00a0[Optional] Deploy the\u00a0extension package\u00a0of your choice to complement your integration needs. We offer the following extensions for data originating from CloudTrail and WAF:</p> <ul> <li> <p>AWS CloudTrail</p> </li> <li> <p>AWS WAF</p> </li> </ul> <p>STEP 12.\u00a0View the logs by navigating to\u00a0Explore\u00a0&gt;\u00a0Logs\u00a0in your Coralogix toolbar. Find out more\u00a0here.</p>"},{"location":"newoutput/send-logs-using-aws-kinesis-data-firehose/#manual-setup","title":"Manual Setup","text":""},{"location":"newoutput/send-logs-using-aws-kinesis-data-firehose/#configuration_1","title":"Configuration","text":"<p>STEP 1. Navigate to the Kinesis Data Firehose console and choose \u2018Create delivery stream\u2019.</p> <p>STEP 2. Under \u2018Choose source and destination\u2019:</p> <ul> <li> <p>Source: Choose Direct PUT</p> </li> <li> <p>Destination: Choose Coralogix</p> </li> <li> <p>Delivery stream name: Fill in the desired stream name.</p> </li> </ul> <p>STEP 3. Scroll down to \u2018Destination settings\u2019:</p> <ul> <li> <p>HTTP endpoint URL: Choose a URL based on your Coralogix\u00a0domain and region.</p> </li> <li> <p>Private key: Enter your Coralogix\u00a0Send Your Data \u2013 API key.</p> </li> <li> <p>Content encoding: Select GZIP.</p> </li> <li> <p>Retry duration: Choose 300 seconds.</p> </li> </ul> <p>STEP 4. Scroll down to \u2018Parameters\u2019. This section allows you to add and configure additional parameters surrounding the Coralogix platform.</p> <p>The following parameters are available:</p> Parameter Description applicationName A comma-separated list of application name sources applicationNameDefault Deprecated subsystemName A comma-separated list of subsystem name sources subsystemNameDefault Deprecated integrationType Data structure: - CloudWatch_JSON: Data from CloudWatch log groups - WAF: CloudWatch_CloudTrail - EksFargate - Default - RawText: Use for VPC flow logs dynamicMetadata Deprecated <p>A name source can be a literal string (something), a quoted string (\u201csomething\u201d), or a field reference (e.g. <code>${logGroup}</code>). Sources in a list are evaluated in order, variables without a value are skipped. For example, <code>${applicationName}, MyApp</code> for a Default integration will use <code>applicationName</code> field if available, otherwise it will default to MyApp.</p> <p>The Dynamic Values Table section in this document serves as a reference for setting the applicationName and subsystemName dynamically.</p> <p>Notes:</p> <ul> <li> <p>By default, your delivery stream name will be used as \u2018applicationName\u2019 and ARN as \u2018subsystemName\u2019.</p> </li> <li> <p>To override the associated \u2018applicationName\u2019 or \u2018subsystemName\u2019, add a new parameter with the desired value.</p> <ul> <li> <p>Key: \u2018applicationName\u2019 , value \u2013 \u2018new-app-name\u2019</p> </li> <li> <p>Key: \u2018subsystemName\u2019 , value \u2013 \u2018new-subsystem-name\u2019</p> </li> </ul> </li> <li> <p>The source of the data in Firehose determines the \u2018integrationType\u2019 parameter value:</p> <ul> <li> <p>For CloudWatch logs, use <code>CloudWatch_JSON</code>.</p> </li> <li> <p>For CloudTrail logs in CloudWatch, use <code>CloudWatch_CloudTrail</code>.</p> </li> <li> <p>For logs coming from EKS Fargate using our guide, use <code>EksFargate</code>.</p> </li> <li> <p>For data sources matching the Coralogix\u00a0log ingestion format, use <code>Default</code> .</p> </li> <li> <p>For other data sources, use <code>RawText</code>. This moves all the text to\u00a0<code>text</code>\u00a0field of log, adds severity of\u00a0<code>Info</code>, and generates a current timestamp. All further parsing of these logs should be done using parsing rules.</p> </li> <li> <p>For logs coming from AWS WAF, use <code>WAF</code>. This\u00a0requires\u00a0configuration on\u00a0WAF\u00a0as follows:</p> </li> </ul> </li> </ul> <p></p> <p></p> <p></p> <ul> <li> <p>Without adding the \u2018integrationType\u2019 parameter, the <code>Default</code> integration type is selected.</p> </li> <li> <p>For integration of type <code>Default</code>, the logs should be structured according to our\u00a0REST API rules.</p> </li> </ul> <p>STEP 5. Set up a recovery bucket (recommended). Enabling source data backup ensures that the data can be recovered if the record processing transformation does not produce the desired results.</p> <p></p> <p>STEP 6. Scroll down to \u2018Backup settings\u2019:</p> <ul> <li> <p>Source record backup in Amazon S3: We suggest selecting\u00a0Failed data only.</p> </li> <li> <p>S3 backup bucket: Choose an existing bucket or create a new one.</p> </li> <li> <p>Buffer hints, compression, encryption: Leave these fields as is.</p> </li> </ul> <p>STEP 7. Review your settings and select\u00a0Create delivery stream.</p> <p>Logs subscribed to your delivery stream will be immediately sent and available for analysis within Coralogix.</p>"},{"location":"newoutput/send-logs-using-aws-kinesis-data-firehose/#data-source-configuration","title":"Data Source Configuration","text":"<p>Below are a couple of ways to connect your data source to Firehose.</p>"},{"location":"newoutput/send-logs-using-aws-kinesis-data-firehose/#cloudwatch-logs","title":"CloudWatch Logs","text":"<p>To send your logs to Coralogix, create a subscription filter inside your CloudWatch log group.</p> <p>First, create a new role in IAM for your Cloudwatch log group to allow sending data to Firehose.</p> <p>Go to the IAM console and choose \u2018Roles\u2019 under \u2018Access management\u2019.</p> <p>Click on \u2018Create role\u2019 on the right.</p> <p>Under \u2018Trusted entity type\u2019 choose \u2018Custom trust policy\u2019 and insert this policy.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"logs.&lt;region_code&gt;.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n\n</code></pre> <p>Note: Change <code>&lt;region_code&gt;</code> into your AWS region. e.g <code>us-east-1</code></p> <p>In \u2018Add permissions\u2019, click \u2018Create policy\u2019.</p> <p>On the opened window, click on the \u2018JSON\u2019 tab and insert this policy:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"firehose:PutRecord\",\n                \"firehose:PutRecordBatch\"\n            ],\n            \"Resource\": [\n                \"&lt;firehose_ARN&gt;\"\n            ]\n        }\n    ]\n}\n\n</code></pre> <p>Note: Change <code>&lt;firehose_ARN&gt;</code> to your Firehose Amazon Resource Name (ARN).</p> <p>After creating the policy, go back to the role creation page and click on the refresh button.</p> <p>Pick the newly created policy.</p> <p>Give your role a name and create it.</p> <p>After creating the role, go to the Cloudwatch console and choose \u2018Logs groups\u2019 under the \u2018Logs\u2019 side menu.</p> <p>Create a new subscription filter for the relevant log group \u2013 <code>&lt;Your_log_group&gt;</code> -&gt; \u2018Subscription filters\u2019 -&gt; \u2018Create Kinesis Firehose subscription filter\u2019.</p> <p>Under \u2018Choose destination\u2019:</p> <ul> <li> <p>For \u2018Destination account\u2019 choose \u2018Current account\u2019</p> </li> <li> <p>For \u2018Kinesis Firehose delivery stream\u2019 choose the created firehose delivery stream</p> </li> </ul> <p>Scroll down to \u2018Grant permission\u2019:</p> <ul> <li>For \u2018Select an existing role\u2019 choose the role created above</li> </ul> <p>Scroll down and click on \u2018Start streaming\u2019.</p> <p>Logs coming to your Cloudwatch log group will also be directed to Firehose.</p> <p>Note: Use the correct integration type inside your Firehose configuration.</p>"},{"location":"newoutput/send-logs-using-aws-kinesis-data-firehose/#kinesis-data-stream","title":"Kinesis Data Stream","text":"<p>To start sending your Kinesis data stream logs to Coralogix, connect the Data stream to Firehose.</p> <p>Go to the Kinesis Data Stream console and choose \u2018Create data stream\u2019.</p> <p>Under \u2018Data stream configuration\u2019:</p> <ul> <li>Data stream name: Enter the name of the data stream</li> </ul> <p>Scroll down to \u2018Data stream capacity\u2019:</p> <ul> <li>Capacity mode: Choose \u2018On-demand\u2019</li> </ul> <p>After that scroll down and click on \u2018Create data stream\u2019.</p> <p>Note: to connect a kinesis data stream to a firehose delivery stream the delivery stream must use <code>Amazon kinesis data streams</code> as its source instead of <code>Direct PUT</code> .</p>"},{"location":"newoutput/send-logs-using-aws-kinesis-data-firehose/#terraform-module-setup","title":"Terraform\u00a0Module Setup","text":"<p>Using\u00a0Coralogix Terraform modules, you can easily install and manage Coralogix integrations with AWS services as modules in your infrastructure code. Our open-source modules are available on our\u00a0GitHub\u00a0and in the\u00a0Terraform Registry. Visit our full AWS Kinesis Firehose Terraform Module documentation for more info.</p> <p>For logs, install\u00a0AWS Kinesis Data Firehose\u00a0by adding this declaration to your Terraform project:</p> <pre><code>module \"cloudwatch_firehose_coralogix_logs\" {\n  source                         = \"coralogix/aws/coralogix//modules/firehose-logs\"\n  private_key                    = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXX\"\n  firehose_stream                = \"coralogix-firehose-logs\"\n  coralogix_region               = \"Europe\"\n  integration_type_logs          = \"Default\"\n  source_type_logs               = \"DirectPut\"\n}\n\n</code></pre>"},{"location":"newoutput/send-logs-using-aws-kinesis-data-firehose/#dynamic-values-table","title":"Dynamic Values Table","text":"<p>For application name\u00a0and/or\u00a0subsystem name\u00a0to be set dynamically in relation to their\u00a0<code>integrationType</code>'s resource fields (e.g. CloudWatch_JSON's loggroup name, EksFargate's k8s namespace). The source's variable can be mapped as a string literal with pre-defined values:</p> Field Source\u00a0<code>Var</code> Expected Parameter Integration Type Notes applicationName\u00a0field in logs applicationName ${applicationName} Default Needs to be supplied in the log to be used subsystemName\u00a0field in logs subsystemName ${subsystemName} Default Needs to be supplied in the log to be used CloudWatch LogGroup name logGroup ${logGroup} CloudWatch_JSONCloudWatch_CloudTrail Supplied by AWS <code>kubernetes.namespace_name</code>\u00a0field kubernetesNamespaceName ${kubernetesNamespaceName} EksFargate Supplied by the default configuration <code>kubernetes.container_name</code>\u00a0field kubernetesContainerName ${kubernetesContainerName} EksFargate Supplied by the default configuration name part of the\u00a0log.webaclId\u00a0field webAclName ${webAclName} WAF Supplied by AWS <p>The expected parameter for the field must be exact with no extra characters. Characters may, however, be exchanged - for instance, <code>subsystemName</code> parameter with value <code>${applicationName}</code> or )</p> <p>Note:\u00a0<code>RawText</code>\u00a0integrationType does not support dynamic values.</p>"},{"location":"newoutput/send-logs-using-aws-kinesis-data-firehose/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogix.com.</p>"},{"location":"newoutput/send-your-data-api-key/","title":"Send-Your-Data API Key","text":"<p>To send your data to Coralogix, you are required to access and use your unique Coralogix Send-Your-Data API key.</p> <p>This tutorial demonstrates how to access the default Send-Your-Data API key associated with your Coralogix account, as well as to add unlimited Send-Your-Data API keys with advanced security settings.</p> <p>Note! Customers wishing to enjoy unlimited Send-Your-Data API keys with advanced security settings should reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/send-your-data-api-key/#feature","title":"Feature","text":"<p>To send your data to Coralogix, you are required to access and use your unique Send-Your-Data API key. This sensitive information is required to send us your telemetry data securely and allows us to authenticate the identity of those sending it.</p> <p>Coralogix offers its customers the option of creating multiple Send-Your-Data API keys with advanced security settings, allowing you to:</p> <ul> <li> <p>Minimize security vulnerabilities</p> </li> <li> <p>Utilize different keys across different systems, deployment methods, and teams</p> </li> <li> <p>Avoid the need to revoke one 'Send Your Data' API key in all of your systems, as well as the associated downtime, in the event of exposure</p> </li> </ul> <p>Our recommended best practice is to create multiple keys for your organization with all keys enjoying our advanced security settings.</p> <p>Notes:</p> <ul> <li> <p>Multiple Send-Your-Data API keys with advanced security systems are supported by our Send-Your-Data Management API.</p> </li> <li> <p>This feature supports sending data using Logstash, Fluent Bit, Vector, Fluentd, OpenTelemetry, Prometheus, Prometheus Server, Prometheus Agent, Prometheus Operator, Metrics Cardinality, Grafana API, Telegraf, Telegraf Operator, Zabbix, Nagios, StatsD, Custom Metrics, RabbitMQ Metrics,</p> </li> <li> <p>At present, this feature does not support\u00a0Windows Event logs with Winlogbeat, \u00a0Beats: Filebeat, and Prometheus Alertmanager.</p> </li> </ul>"},{"location":"newoutput/send-your-data-api-key/#manage-send-your-data-api-keys","title":"Manage Send-Your-Data API Keys","text":"<p>The following section demonstrates how to (i) view and create Send-Your-Data API keys and (ii) set up your advanced security settings.</p>"},{"location":"newoutput/send-your-data-api-key/#view-and-create-send-your-data-api-keys","title":"View and Create Send-Your-Data API Keys","text":"<p>STEP 1. In your Coralogix account, open the\u00a0Data Flow\u00a0dropdown menu in the toolbar &gt;\u00a0API Keys.</p> <p></p> <p>STEP 2. Under the heading Send your Data, you will see your default Send-Your-Data API key, which you can view and copy, as well as any other keys that you have created.</p> <p></p> <p>STEP 3. Click +NEW KEY to add a Send-Your-Data API key.</p> <p>STEP 4. Input a key name and click CREATE.</p> <p>STEP 5. The system will generate your key. Click VIEW or DOWNLOAD.</p> <p>STEP 6. Click DONE.</p> <p>Your new key will now appear with all of your existing keys under the Send your Data heading with its name and date of creation. View, copy, activate, deactivate and even delete it on your Coralogix UI as necessary.</p> <p>Notes:</p> <ul> <li> <p>Once a key is generated, only its name can be changed.</p> </li> <li> <p>If a key is deactivated and reactivated, it is reactivated with the same value.</p> </li> <li> <p>Your default key can never be deleted.</p> </li> </ul>"},{"location":"newoutput/send-your-data-api-key/#advanced-security-settings","title":"Advanced Security Settings","text":"<p>Send-Your-Data API keys created after the API Key Security Settings have been activated will be encrypted, without the possibility of viewing them after their creation.</p> <p>STEP 1. To enjoy advanced security settings, navigate to your personal account &gt; Settings.</p> <p>STEP 2. In the left-hand sidebar, select API Key Security Settings. Activate or deactivate as required.</p> <p>Notes:</p> <ul> <li> <p>Only account administrators have permissions to activate these settings.</p> </li> <li> <p>Once these settings are in place, they will apply to new keys generated after this time.</p> </li> </ul> <p>STEP 3. Open the\u00a0Data Flow\u00a0dropdown menu in the toolbar &gt; API Keys.</p> <p>STEP 4. Under the Send your Data heading, you will see your default 'Send Your Data' API key, as well as any other additional keys that you have generated. View the names and dates of keys, as well as the value of those keys created prior to the activation of the advanced security settings. You will not have the option to view and copy the value of any new keys created after the API Key Security Settings have been activated.</p> <p>Note:</p> <ul> <li>When generating a new key once the API Key Security Settings have been activated, you will have a one time opportunity to view and copy it on your UI upon its creation. You will be required to download it as a text file to be save locally.</li> </ul>"},{"location":"newoutput/send-your-data-api-key/#additional-resources","title":"Additional Resources","text":"DocumentationSend-Your-Data Management API"},{"location":"newoutput/send-your-data-api-key/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/send-your-data-management-api/","title":"Send-Your-Data Management API","text":"<p>Coralogix provides an API that allows you to manage your Send-Your-Data API keys.</p>"},{"location":"newoutput/send-your-data-management-api/#prerequisites","title":"Prerequisites","text":"<ul> <li>Coralogix Alerts, Rules and Tags AP Key. Access this in your navigation pane by clicking Data Flow &gt; API Keys.</li> </ul> <ul> <li>Select a Management API Endpoint.</li> </ul>"},{"location":"newoutput/send-your-data-management-api/#create-a-new-send-your-data-api-key","title":"Create a New 'Send Your Data' API Key","text":"<pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.apikeys.v1.ApiKeysManagementService/CreateKey &lt;&lt;EOF\n{\n  \"key_name\": \"my-api-key\",\n  \"roles\": [{\n    \"id\": 8\n  }],\n  \"owner\": {\n    \"team_id\": {\n      \"id\": 000000\n    }\n  }\n}\nEOF\n\n</code></pre>"},{"location":"newoutput/send-your-data-management-api/#request-args","title":"Request Args","text":"Field Description Key Name Name of the key Roles Defines what roles this key will be assigned to. The only value that should be used is role id 8, which is a SendData role. Owner Defines who can be the owner of the key. The only available value at present is the team_id."},{"location":"newoutput/send-your-data-management-api/#response","title":"Response","text":"<p>The response will be a message with information about the created key.</p> <pre><code>{\n  \"apiKey\": {\n    \"keyId\": {\n      \"value\": \"c8d84762-9aff-4f90-93ef-1762757e0c2c\"\n    },\n    \"keyName\": \"my-api-key\",\n    \"isKeyActive\": true,\n    \"keyValue\": {\n      \"value\": \"&lt;key_value&gt;\"\n    },\n    \"dateCreated\": \"2023-03-30T10:00:58Z\"\n  }\n}\n\n</code></pre> Field Description Key Id ID of the key Key Name Assigned name to the key Is Key Active Defines if key is in active state Key Value Value of the key. For securely stored keys, this is the only time it will be visible. Date Created Creation date"},{"location":"newoutput/send-your-data-management-api/#rename-a-send-your-data-api-key","title":"Rename a 'Send Your Data' API Key","text":"<pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.apikeys.v1.ApiKeysManagementService/RenameKey &lt;&lt;EOF\n{\n    \"key_id\": {\n        \"value\": \"f0e68232-8bcc-4932-8e39-b320d74df9b1\"\n    },\n    \"new_key_name\": \"my-api-key-new-name\"\n}\nEOF\n\n</code></pre>"},{"location":"newoutput/send-your-data-management-api/#request-args_1","title":"Request args","text":"Field Description Key Id ID of the key New Key Name New name that will be correlated with the ke"},{"location":"newoutput/send-your-data-management-api/#response_1","title":"Response","text":"<p>The response will be an empty message.</p>"},{"location":"newoutput/send-your-data-management-api/#activate-a-send-your-data-api-key","title":"Activate a 'Send Your Data' API Key","text":"<pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.apikeys.v1.ApiKeysManagementService/SetActive &lt;&lt;EOF\n{\n  \"active\": false,\n  \"key_id\": {\n    \"value\": \"f0e68232-8bcc-4932-8e39-b320d74df9b1\"\n  }\n}\nEOF\n\n</code></pre>"},{"location":"newoutput/send-your-data-management-api/#request-args_2","title":"Request args","text":"Field Description Active Defines if key should be active or not Key Id ID of the key"},{"location":"newoutput/send-your-data-management-api/#response_2","title":"Response","text":"<p>The response will be an empty message.</p>"},{"location":"newoutput/send-your-data-management-api/#delete-a-send-your-data-api-key","title":"Delete a 'Send-Your-Data' API Key","text":"<pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.apikeys.v1.ApiKeysManagementService/DeleteKey &lt;&lt;EOF\n{\n  \"key_id\": {\n    \"value\": \"f0e68232-8bcc-4932-8e39-b320d74df9b1\"\n  }\n}\nEOF\n\n</code></pre>"},{"location":"newoutput/send-your-data-management-api/#request-args_3","title":"Request args","text":"Field Description Key Id ID of the key"},{"location":"newoutput/send-your-data-management-api/#response_3","title":"Response","text":"<p>The response will be an empty message.</p>"},{"location":"newoutput/send-your-data-management-api/#get-a-send-your-data-api-key","title":"Get a 'Send Your Data' API Key","text":"<pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.apikeys.v1.ApiKeysManagementService/GetKeys &lt;&lt;EOF\n{\n  \"role_id\": {\n    \"id\": 8\n  },\n  \"owner\": {\n    \"team_id\": {\n      \"id\": 000000\n    }\n  }\n}\nEOF\n\n</code></pre>"},{"location":"newoutput/send-your-data-management-api/#request-args_4","title":"Request args","text":"Field Description Role id Defines role for which associated keys will be returned Owner Defines who is the owner of the keys"},{"location":"newoutput/send-your-data-management-api/#response_4","title":"Response","text":"<p>The response will be an message with list of all API :</p> <pre><code>{\n  \"apiKeys\": [\n    {\n      \"keyId\": {\n        \"value\": \"4863c538-2a47-4091-b90e-7088836ab569\"\n      },\n      \"keyName\": \"T4\",\n      \"isKeyActive\": true,\n      \"dateCreated\": \"2023-03-29T08:56:09Z\"\n    },\n    {\n      \"keyId\": {\n        \"value\": \"4ff895e6-7ef7-4828-97bc-7fa82e8ca47a\"\n      },\n      \"keyName\": \"T1\",\n      \"isKeyActive\": true,\n      \"keyValue\": {\n        \"value\": \"cxtp_HRiXteLAhcxPQJY1ZPfYGdOhdQEd73\"\n      },\n      \"dateCreated\": \"2023-03-29T07:38:42Z\"\n    }\n  ]\n}\n\n</code></pre> Field Description Key Id ID of the key Key Name Assigned name to the key Is Key Active Defines if key is in active state Key Value Optional value of the key. If key is stored securely, this field will not be sent. Date Created Creation date"},{"location":"newoutput/send-your-data-management-api/#get-a-send-your-data-api-key-by-id","title":"Get a 'Send Your Data' API Key by ID","text":"<pre><code>grpcurl -H \"Authorization: Bearer &lt;YOUR-API-KEY&gt;\" -d @ &lt;CORALOGIX-DOMAIN&gt; com.coralogix.apikeys.v1.ApiKeysManagementService/GetKeyById &lt;&lt;EOF\n{\n  \"key_id\": {\n    \"value\": \"4ff895e6-7ef7-4828-97bc-7fa82e8ca47a\"\n  }\n}\nEOF\n\n</code></pre>"},{"location":"newoutput/send-your-data-management-api/#request-args_5","title":"Request args","text":"Field Description Key ID ID of the key"},{"location":"newoutput/send-your-data-management-api/#response_5","title":"Response","text":"<p>The response will be an message with API key information:</p> <pre><code>{\n  \"apiKey\": {\n    \"keyId\": {\n      \"value\": \"4ff895e6-7ef7-4828-97bc-7fa82e8ca47a\"\n    },\n    \"keyName\": \"key1\",\n    \"isKeyActive\": true,\n    \"keyValue\": {\n      \"value\": \"&lt;key_value&gt;\"\n    },\n    \"dateCreated\": \"2023-03-29T07:38:42Z\"\n  }\n}\n\n</code></pre> Field Description Key Id ID of the key Key Name Assigned name to the key Is Key Active Defines if key is in active state Key Value Optional value of the key. If key is stored securely, this field will not be sent. Date Created Creation date"},{"location":"newoutput/send-your-data-management-api/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/sentinelone/","title":"SentinelOne","text":"<p>SentinelOne\u00a0protects computers, endpoints, and data with anti-malware and anti-exploit protection. The SentinelOne agent continually receives intelligence updates from SentinelOne servers with a lightweight agent and offers minimal to no impact on your work.</p> <p>This tutorial demonstrates how to seamlessly send SentinelOne logs to Coralogix. The integration requires sending your logs to an interceptive server and then forwarding them from the server to Coralogix.</p>"},{"location":"newoutput/sentinelone/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Admin access\u00a0to your Coralogix account</p> </li> <li> <p>Admin access to your SentinelOne account</p> </li> <li> <p>Server (e.g. EC2 machine) that will host the OpenTelemetry log shipper</p> </li> <li> <p>OpenTelemetry\u00a0installed</p> </li> </ul>"},{"location":"newoutput/sentinelone/#create-tls-certificates","title":"Create TLS Certificates","text":"<p>STEP 1. On the EC2 server, create a folder to hold all certificates.</p> <pre><code>sudo mkdir /etc/certificates &amp;&amp; cd /etc/certificates\n\n</code></pre> <p>STEP 2. Create your CA certificate. For FQDN, input the server\u2019s public IP address.</p> <pre><code>openssl genrsa -out RootCA.key 2048\nopenssl req -x509 -new -nodes -key RootCA.key -sha256 -days 1024 -out RootCA.pem\n\n</code></pre> <p>STEP 3. Create a custom OpenSSL configuration file.</p> <pre><code>sudo vim custom_ssl.conf\n\n</code></pre> <p>STEP 4. Replace\u00a0CN\u00a0and\u00a0alt_names\u00a0with your server IP/domain name.</p> <pre><code>[req]\ndistinguished_name = req_distinguished_name\nx509_extensions = v3_req\nprompt = no\n[req_distinguished_name]\nCN=&lt;SERVER_IP&gt;\n[v3_req]\nkeyUsage = keyEncipherment, dataEncipherment\nextendedKeyUsage = serverAuth\nsubjectAltName = @alt_names\n[alt_names]\nDNS.1 = &lt;SERVER_IP&gt;\n</code></pre> <p>STEP 5. Generate a SentinelOne key &amp; certificate signing request.</p> <pre><code>openssl genrsa -out SentinelOne.key 2048\nopenssl req -new -key SentinelOne.key -out SentinelOne.csr -config custom_ssl.conf\n\n</code></pre> <p>STEP 6. Generate SentinelOne certificate based on our own CA certificate.</p> <pre><code>openssl x509 -req -in SentinelOne.csr -CA RootCA.pem -CAkey RootCA.key -CAcreateserial -out SentinelOne.pem -days 1024 -sha256\n\n</code></pre> <p>STEP 7. Add read permissions to all of the certificates.</p> <pre><code>sudo chmod +r /etc/certificates/*\n\n</code></pre> <p>STEP 8. Create a configuration file for OpenTelemetry, while modifying the following variables.</p> VariableDescriptiondomainCoralogix Domain\u00a0associated with your Coralogix accountprivate_keyCoralogix Send-Your-Data API keymessage_formatSyslog message format (rfc3164/rfc5424) <pre><code>receivers:\n  syslog:\n    tcp:\n      listen_address: \"0.0.0.0:514\"\n      tls:\n        cert_file: \"/etc/certificates/RootCA.pem\"\n        key_file: \"/etc/certificates/RootCA.key\"\n        ca_file: \"/etc/certificates/RootCA.pem\"\n    protocol: rfc5424\n    operators: \n      - type: syslog_parser\n        protocol: &lt;**message_format&gt;**\n        parse_from: body\n        parse_to: body\n            - type: remove\n        field: attributes\nexporters:\n  coralogix:\n    domain: \"coralogixstg.wpengine.com\"\n    private_key: \"your private key\"\n    application_name: \"syslog-application\"\n    subsystem_name: \"syslog-subsystem\"\n    timeout: 30s\nservice:\n  pipelines:\n    logs:\n      receivers: [ syslog ]\n      exporters: [ coralogix ]\n\n</code></pre> <p>STEP 9. Save and run the OpenTelemetry file.</p>"},{"location":"newoutput/sentinelone/#forward-logs-to-the-syslog-server","title":"Forward Logs to the Syslog Server","text":"<p>Before proceeding, we\u00a0recommended\u00a0contacting SentinelOne to receive the IP addresses, specific to your SentinelOne Account, over which SentinelOne will be sending data, and to provide the relevant permissions to those IPs in your EC2 instance security group. This will ensure the\u00a0principle of least privilege.</p> <p>STEP 1. Navigate to the\u00a0Integrations\u00a0panel under\u00a0Settings\u00a0in the SentinelOne platform.</p> <ul> <li> <p>Types. Choose\u00a0SYSLOG.</p> </li> <li> <p>IP. Insert the IP address of the server created previously.</p> </li> <li> <p>Port. Insert the desired port. The default port is the same port chosen in the Logstash configuration file.</p> </li> <li> <p>Formatting. Select\u00a0CEF2.</p> </li> </ul> <p></p> <ul> <li>Check the\u00a0TLS\u00a0checkbox and upload the SentinelOne certificates previously created.</li> </ul> <p></p> <ul> <li>Click\u00a0Test.</li> </ul> <p>STEP 2. Save the SYSLOG integration.</p>"},{"location":"newoutput/sentinelone/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/serilog/","title":"SeriLog","text":"<p>Coralogix customers with Microsoft .NET applications using the Serilog logging library, can send logs to Coralogix using OpenTelemetry (OTEL).</p> <p>The following steps guide you through configuring a Serilog logger in a C# .NET application to send logs to Coralogix, either through an OTEL Collector [recommended] or directly to a Coralogix endpoint.</p>"},{"location":"newoutput/serilog/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Coralogix account</p> </li> <li> <p>.NET application that uses Serilog logging</p> </li> <li> <p>A running OTEL Collector configured to your Coralogix account (recommended)</p> </li> </ul> <p>\ud83d\udca1 OTEL Collector vs Direct to Coralogix. It is highly recommended to send telemetry via your own OTEL Collector instead of directly to the Coralogix endpoint. An architecture using OTEL Collectors provides the advantages of batching, reliability, and advanced functionality. For instructions on configuring an OTEL Collector to Coralogix, see the Coralogix OpenTelemetry page.</p>"},{"location":"newoutput/serilog/#set-up-a-c-net-application-with-serilog","title":"Set up a C# .NET application with Serilog","text":"<p>The following console commands create a simple \u201cHello World\u201d console app and add NuGet packages for Serilog and various Sinks. If you have an existing application using Serilog, you will only need to add the Serilog.Sinks.OpenTelemetry package.</p> <pre><code>dotnet new console\ndotnet run\ndotnet add package Serilog\ndotnet add package Serilog.Sinks.Console\ndotnet add package Serilog.Sinks.OpenTelemetry\n</code></pre>"},{"location":"newoutput/serilog/#configure-logging-to-the-serilog-otel-sink","title":"Configure Logging to the Serilog OTEL Sink","text":"<p>The Serilog Sink for OpenTelemetry is the key to enabling the .NET application to send logs to an OTEL Collector.</p>"},{"location":"newoutput/serilog/#programmatic-configuration-approach","title":"Programmatic Configuration Approach","text":"<p>In the following example, Serilog is configured to send logs to an OpenTelemetry sink at a specified network endpoint and logs a simple text message. If required, update the OTEL collector endpoint accordingly for your environment.</p> <pre><code>using System;\nusing Serilog;\nusing Serilog.Sinks.OpenTelemetry;\nusing Serilog.Debugging;\ninternal class Program\n{\n    static void Main(string[] args)\n    {\n        Log.Logger = new LoggerConfiguration()\n            .WriteTo.OpenTelemetry(endpoint: \"http://127.0.0.1:4317\")\n            .WriteTo.Console()\n            .MinimumLevel.Debug()\n            .CreateLogger();\n        Serilog.Debugging.SelfLog.Enable(Console.Error);\n        Log.Information(\"Hello, world!\");\n    }\n}\n</code></pre> <p>Alternatively, while an OTEL Collector is the recommended architecture, you may choose to send send telemetry directly to your Coralogix account instead. To do so, adjust the Logger creation statement as follows, specifying additional options for your Coralogix endpoint, and additional headers for your Coralogix API key, Application Name, and Subsystem Name. Substitute your values accordingly.</p> <pre><code>        Log.Logger = new LoggerConfiguration()\n                     .WriteTo.OpenTelemetry(options =&gt; { \n                       options.Endpoint = \"https://ingress.coralogixstg.wpengine.com/\";\n                       options.Headers = new Dictionary&lt;string, string&gt; {\n                         {\"Authorization\", \"Bearer &lt;REPLACE_WITH_YOUR_API_KEY&gt;\"},\n                         {\"CX-Application-Name\", \"serilog-demo\"},\n                         {\"CX-Subsystem-Name\", \"serilog-demo\"},\n                       };\n                     })\n                     .WriteTo.Console()\n                     .MinimumLevel.Debug()\n                     .CreateLogger();\n</code></pre>"},{"location":"newoutput/serilog/#declarative-configuration-approach","title":"Declarative Configuration Approach","text":"<p>Serilog can be alternately configured using a configuration file. The following is an appsettings.json configuration file configuring an OpenTelemetry sink to send logs to a configured OpenTelemetry Collector agent endpoint. Edit the OpenTelemetry endpoint if required.</p> <pre><code>{\n  \"Logging\": {\n    \"LogLevel\": {\n      \"Default\": \"Debug\",\n      \"Microsoft.AspNetCore\": \"Warning\"\n    }\n  },\n  \"AllowedHosts\": \"*\",\n  \"Serilog\": {\n    \"MinimumLevel\": {\n      \"Default\": \"Debug\",\n      \"Override\": {\n        \"Microsoft\": \"Error\",\n        \"Microsoft.AspNetCore\": \"Warning\",\n        \"System\": \"Error\"\n      },\n      \"Using\": [ \"Serilog.Sinks.Console\", \"Serilog.Sinks.OpenTelemetry\"]\n    },\n    \"WriteTo\": [\n      {\n        \"Name\": \"Console\"\n      },\n      {\n        \"Name\": \"OpenTelemetry\",\n        \"Args\": { \"endpoint\": \"http://127.0.0.1:4317/\" }\n      }\n    ]\n  }\n}\n</code></pre> <p>The OpenTelemetry collector agent should be network-accessible, and configured to export telemetry to your Coralogix account.</p> <p>\ud83d\udca1 Declarative configuration currently requires an OTEL collector. Currently, Serilog does not support sending OTEL headers required by the Coralogix endpoint. If you wish to use the latter option, please use the programmatic configuration approach.</p>"},{"location":"newoutput/serilog/#verify-logging","title":"Verify logging","text":"<p>Run the application.</p> <pre><code>dotnet run\n</code></pre> <p>Verify logs appear correctly at the Coralogix web console.</p>"},{"location":"newoutput/serilog/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/serilog-net-opentelemetry/","title":"SeriLog","text":"<p>Coralogix customers with Microsoft .NET applications using the Serilog logging library, can send logs to Coralogix using OpenTelemetry (OTEL).</p> <p>The following steps guide you through configuring a Serilog logger in a C# .NET application to send logs to Coralogix, either through an OTEL Collector [recommended] or directly to a Coralogix endpoint.</p>"},{"location":"newoutput/serilog-net-opentelemetry/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Coralogix account</p> </li> <li> <p>.NET application that uses Serilog logging</p> </li> <li> <p>A running OTEL Collector configured to your Coralogix account (recommended)</p> </li> </ul> <p>\ud83d\udca1 OTEL Collector vs Direct to Coralogix. It is highly recommended to send telemetry via your own OTEL Collector instead of directly to the Coralogix endpoint. An architecture using OTEL Collectors provides the advantages of batching, reliability, and advanced functionality. For instructions on configuring an OTEL Collector to Coralogix, see the Coralogix OpenTelemetry page.</p>"},{"location":"newoutput/serilog-net-opentelemetry/#set-up-a-c-net-application-with-serilog","title":"Set up a C# .NET application with Serilog","text":"<p>The following console commands create a simple \u201cHello World\u201d console app and add NuGet packages for Serilog and various Sinks. If you have an existing application using Serilog, you will only need to add the Serilog.Sinks.OpenTelemetry package.</p> <pre><code>dotnet new console\ndotnet run\ndotnet add package Serilog\ndotnet add package Serilog.Sinks.Console\ndotnet add package Serilog.Sinks.OpenTelemetry\n</code></pre>"},{"location":"newoutput/serilog-net-opentelemetry/#configure-logging-to-the-serilog-otel-sink","title":"Configure Logging to the Serilog OTEL Sink","text":"<p>The Serilog Sink for OpenTelemetry is the key to enabling the .NET application to send logs to an OTEL Collector.</p>"},{"location":"newoutput/serilog-net-opentelemetry/#programmatic-configuration-approach","title":"Programmatic Configuration Approach","text":"<p>In the following example, Serilog is configured to send logs to an OpenTelemetry sink at a specified network endpoint and logs a simple text message. If required, update the OTEL collector endpoint accordingly for your environment.</p> <pre><code>using System;\nusing Serilog;\nusing Serilog.Sinks.OpenTelemetry;\nusing Serilog.Debugging;\ninternal class Program\n{\n    static void Main(string[] args)\n    {\n        Log.Logger = new LoggerConfiguration()\n            .WriteTo.OpenTelemetry(endpoint: \"http://127.0.0.1:4317\")\n            .WriteTo.Console()\n            .MinimumLevel.Debug()\n            .CreateLogger();\n        Serilog.Debugging.SelfLog.Enable(Console.Error);\n        Log.Information(\"Hello, world!\");\n    }\n}\n</code></pre> <p>Alternatively, while an OTEL Collector is the recommended architecture, you may choose to send send telemetry directly to your Coralogix account instead. To do so, adjust the Logger creation statement as follows, specifying additional options for your Coralogix endpoint, and additional headers for your Coralogix API key, Application Name, and Subsystem Name. Substitute your values accordingly.</p> <pre><code>        Log.Logger = new LoggerConfiguration()\n                     .WriteTo.OpenTelemetry(options =&gt; { \n                       options.Endpoint = \"https://ingress.coralogixstg.wpengine.com/\";\n                       options.Headers = new Dictionary&lt;string, string&gt; {\n                         {\"Authorization\", \"Bearer &lt;REPLACE_WITH_YOUR_API_KEY&gt;\"},\n                         {\"CX-Application-Name\", \"serilog-demo\"},\n                         {\"CX-Subsystem-Name\", \"serilog-demo\"},\n                       };\n                     })\n                     .WriteTo.Console()\n                     .MinimumLevel.Debug()\n                     .CreateLogger();\n</code></pre>"},{"location":"newoutput/serilog-net-opentelemetry/#declarative-configuration-approach","title":"Declarative Configuration Approach","text":"<p>Serilog can be alternately configured using a configuration file. The following is an appsettings.json configuration file configuring an OpenTelemetry sink to send logs to a configured OpenTelemetry Collector agent endpoint. Edit the OpenTelemetry endpoint if required.</p> <pre><code>{\n  \"Logging\": {\n    \"LogLevel\": {\n      \"Default\": \"Debug\",\n      \"Microsoft.AspNetCore\": \"Warning\"\n    }\n  },\n  \"AllowedHosts\": \"*\",\n  \"Serilog\": {\n    \"MinimumLevel\": {\n      \"Default\": \"Debug\",\n      \"Override\": {\n        \"Microsoft\": \"Error\",\n        \"Microsoft.AspNetCore\": \"Warning\",\n        \"System\": \"Error\"\n      },\n      \"Using\": [ \"Serilog.Sinks.Console\", \"Serilog.Sinks.OpenTelemetry\"]\n    },\n    \"WriteTo\": [\n      {\n        \"Name\": \"Console\"\n      },\n      {\n        \"Name\": \"OpenTelemetry\",\n        \"Args\": { \"endpoint\": \"http://127.0.0.1:4317/\" }\n      }\n    ]\n  }\n}\n</code></pre> <p>The OpenTelemetry collector agent should be network-accessible, and configured to export telemetry to your Coralogix account.</p> <p>\ud83d\udca1 Declarative configuration currently requires an OTEL collector. Currently, Serilog does not support sending OTEL headers required by the Coralogix endpoint. If you wish to use the latter option, please use the programmatic configuration approach.</p>"},{"location":"newoutput/serilog-net-opentelemetry/#verify-logging","title":"Verify logging","text":"<p>Run the application.</p> <pre><code>dotnet run\n</code></pre> <p>Verify logs appear correctly at the Coralogix web console.</p>"},{"location":"newoutput/serilog-net-opentelemetry/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/serverless-integration-deployment-container-microsoft-azure-functions/","title":"Serverless Integration Deployment Container: Microsoft Azure Functions","text":"<p>Coralogix provides a seamless integration with\u00a0<code>Azure</code>\u00a0cloud so you can send your logs from anywhere and parse them according to your needs. The following tutorial demonstrates a Coralogix Azure serverless integration deployment using Docker.</p>"},{"location":"newoutput/serverless-integration-deployment-container-microsoft-azure-functions/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>An Azure account with an active subscription</p> </li> <li> <p>Docker</p> </li> </ul>"},{"location":"newoutput/serverless-integration-deployment-container-microsoft-azure-functions/#installation","title":"Installation","text":"<p>STEP 1. Clone this repo:</p> <pre><code>$ git clone https://github.com/coralogix/coralogix-azure-deploy.git\n$ cd coralogix-azure-deploy\n</code></pre> <p>STEP 2. Modify the requisite .vars file for your intended integration deployment:</p> <pre><code>BlobStorage.vars\nStorageQueue.vars\nEventHub.vars\n</code></pre> <p>STEP 3. Log in to a Dockerhub registry.</p> <p>STEP 4. Run the following command:</p> <pre><code>$ docker run -it --rm --platform linux/amd64 --env-file &lt;integration&gt;.vars coralogixrepo/azure-functions-deploy\n</code></pre> <p>STEP 5. The Azure CLI will request authorization. You'll need to follow the prompt and authenticate through your browser with a provided code.</p>"},{"location":"newoutput/serverless-integration-deployment-container-microsoft-azure-functions/#additional-resources","title":"Additional Resources","text":"GithubCoralogix Azure serverless integration deployment containerMicrosoft Azure Functions Manual IntegrationsEvent HubBlob StorageQueue Storage"},{"location":"newoutput/serverless-integration-deployment-container-microsoft-azure-functions/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/serverless-monitoring/","title":"Serverless Monitoring","text":"<p>Our Serverless Monitoring feature provides customers using the Coralogix AWS Lambda Telemetry Exporter with the ability to better control and understand your Lambda servers on both macro and granular levels.</p> <p></p>"},{"location":"newoutput/serverless-monitoring/#feature","title":"Feature","text":"<p>For each Lambda function, view its triggers, the AWS account to which the function belongs, region, runtime, invocations, errors, timeouts, last invocation, average latency, and whether the function is out of memory.</p>"},{"location":"newoutput/serverless-monitoring/#prerequisites","title":"Prerequisites","text":"<p>In order to monitor AWS Lambda functions using our Serverless Monitoring feature, you are required to follow these steps.</p> <p>STEP 1. Set up Coralogix AWS Resource Metadata Collection.</p> <p>STEP 2. You have two options for Lambda monitoring; choose one.</p> <ul> <li> <p>Set up Lambda monitoring to get\u00a0full telemetry, including traces. This is available for Node.js and Python (v3.8 or newer). View the relevant documentation\u00a0here.</p> </li> <li> <p>Set up Lambda monitoring to get only basic telemetry, including logs, or to select other runtimes. With this option, you are only required to set up the Coralogix AWS Lambda Telemetry Exporter. View the relevant documentation\u00a0here.</p> </li> </ul>"},{"location":"newoutput/serverless-monitoring/#access-the-serverless-tab","title":"Access the Serverless Tab","text":"<p>STEP 1. From your navigation pane, click APM. Select the Serverless tab.</p> <p>STEP 2. Select the timeframe for which you want to view information.</p> <p>STEP 3. Select a function to view the drill down.</p>"},{"location":"newoutput/serverless-monitoring/#drill-down","title":"Drill Down","text":"<p>Clicking on a specific function provides you with more in-depth information about it.</p> <p></p> <p>The drill down shows the metadata of the Lambda function being examined, including several graphs, as well as the breakdown of when the function was triggered and how.</p>"},{"location":"newoutput/serverless-monitoring/#invocations","title":"Invocations","text":"<p>The INVOCATIONS graph presents an aggregative breakdown of how often the function was triggered. Each bar in the graph shows the number of invocations, errors, and timeouts for a segment of time (the exact length of time is calculated dynamically depending on the time range shown).</p>"},{"location":"newoutput/serverless-monitoring/#cold-start-frequency","title":"Cold Start Frequency","text":"<p>The COLD START FREQUENCY graph presents you the frequency of cold starts over time. Cold starts occur when the function is triggered and has to start from scratch (in contrast with invocations where the function is woken up from a sleep state).</p>"},{"location":"newoutput/serverless-monitoring/#duration","title":"Duration","text":"<p>The DURATION graph presents you with the length of time a function ran. It includes both invoked Lambdas and cold starts and allows you to see the time taken for each.</p>"},{"location":"newoutput/serverless-monitoring/#memory","title":"Memory","text":"<p>The MEMORY graph presents how much memory was used by the triggered Lambda function.</p>"},{"location":"newoutput/serverless-monitoring/#related-data","title":"Related Data","text":"<p>Clicking on a specific invocation of the Lambda function brings you to the Related Data page. This page shows the Lambda function together with the chain of actions that occurred before and after it was triggered.</p> <p></p> <p>The left-hand pane presents the trace view, while the right-hand pane presents information about the Lambda function (such as triggers, resources, events, logs, tags, etc.).</p>"},{"location":"newoutput/serverless-monitoring/#additional-resources","title":"Additional Resources","text":"DocumentationAWS Lambda Auto InstrumentationCoralogix AWS Lambda Telemetry ExporterCoralogix AWS Resource Metadata CollectionAWS OpenTelemetry Wrappers"},{"location":"newoutput/serverless-monitoring/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/service-catalog/","title":"Service Catalog","text":"<p>The Service Catalog offers a centralized, data-rich resource for managing and optimizing the services within your system. It provides a holistic view of service health, enabling better decision-making and faster issue resolution, ultimately improving the performance and reliability of your entire system.</p>"},{"location":"newoutput/service-catalog/#service-catalog-overview","title":"Service Catalog Overview","text":"<p>The Service Catalog provides a complete list of services you have in your system, displaying the health of each service. The catalog shows service type, number of requests received by the service, and error rate and latency for received requests.</p> <p></p> <ul> <li> <p>Use the search bar, search by service, or any other parameter.</p> </li> <li> <p>Select the timeframe for which you want to view your services.</p> </li> <li> <p>Use dimensions to filter your services. Dimensions help you filter your services by adding new labels to a metric, allowing you to filter the services shown according to the tags you define.</p> </li> </ul>"},{"location":"newoutput/service-catalog/#prerequisites","title":"Prerequisites","text":"<ul> <li>Coralogix Application Performance Monitoring (APM) installed and configured</li> </ul>"},{"location":"newoutput/service-catalog/#access-the-service-catalog","title":"Access the Service Catalog","text":"<p>STEP 1. In your Coralogix toolbar, navigate to APM. Click on the Service Catalog tab.</p> <p>STEP 2. Select the timeframe for which you want to view information.</p> <p>STEP 3. Select a service to view the service drill-down.</p>"},{"location":"newoutput/service-catalog/#filter-services-using-dimensions","title":"Filter Services using Dimensions","text":"<p>Creating a dimension involves adding a new label to a metric, allowing you to filter the services shown using the tags you define.</p> <p>STEP 1. Click { } Add Dimension on the upper right-hand corner of the Service Catalog tab.</p> <p></p> <p>STEP 2. Enter a filter name and select a span tag from the dropdown menu to pair the data source with the dimension.</p> <p></p> <p>STEP 3. To add additional filters, click { } ADD DIMENSION and repeat STEP 2.</p> <p>STEP 4. Click ADD DIMENSIONS.</p> <p>STEP 5. Once you have created one or more dimensions, the Dimensions toolbar will appear above the Service Catalog.</p> <p></p> <p>To filter the services using a specific dimension, choose from the dimensions bar at the top and select the results you wish to see.</p> <p></p> <p>Notes:</p> <ul> <li>When you enter a specific service with dimensions selected, the service drill-down will remain filtered by the desired dimension.</li> </ul>"},{"location":"newoutput/service-catalog/#limitations","title":"Limitations","text":"<ul> <li> <p>Dimensions create metrics from spans and are therefore considered part of your quota. Use a maximum of 5 dimensions, each of which can filter up to 10k labels (cardinality).</p> </li> <li> <p>Only team admins have permission to create dimensions.</p> </li> </ul>"},{"location":"newoutput/service-catalog/#service-drill-down","title":"Service Drill-Down","text":"<p>The Service Drill-Down displays more detailed information about the specific service selected.</p> <p></p> <p>The drill-down includes details of which service you are viewing and all the details given on the main service catalog page. It includes visualizations and additional information that changes depending on your viewing tab.</p> <p>The service drill-down includes the following tabs:</p> <ul> <li> <p>Overview</p> </li> <li> <p>Flows</p> </li> <li> <p>Operations</p> </li> <li> <p>SLO</p> </li> <li> <p>Resources</p> </li> <li> <p>Logs</p> </li> <li> <p>Map</p> </li> </ul>"},{"location":"newoutput/service-catalog/#overview","title":"Overview","text":"<p>The Overview tab gives a summary of the service.</p> <p>The widgets in this tab give you a broad overview of the service for the timeframe selected in the top bar.</p> <p>The Overview widgets include:</p> <ul> <li> <p>SLO.\u00a0An overview of the current SLOs, how many are okay, how many are breached, and how many are not available.</p> </li> <li> <p>Average Latency.\u00a0Shows the average latency for the current service.</p> </li> <li> <p>Throughput.\u00a0Shows the throughput for the current service.</p> </li> <li> <p>Error Percentage.\u00a0Displays the percentage of errors in relation to the total number of requests.</p> </li> <li> <p>Requests and Errors.\u00a0Shows a graph with the number of requests and errors for the service.</p> </li> <li> <p>Error Percentage for Top 5 Requests.\u00a0Displays the percentage of errors in the top five incoming service requests.</p> </li> <li> <p>Apdex Score.\u00a0Displays the Apdex (Application Performance Index) score over the selected timeframe. The Apdex score is a standardized metric used to measure and quantify user satisfaction with the response time of software applications. For more information about Apdex, including defining the threshold, view our\u00a0Apdex Score tutorial.</p> </li> <li> <p>Highest Consumption.\u00a0Shows the five operations with the highest consumption.</p> </li> <li> <p>Latency. Shows a graph with the service\u2019s P99, P75, P50, and Average latency.</p> </li> <li> <p>Map.\u00a0Shows a mini version of the service map.</p> </li> </ul>"},{"location":"newoutput/service-catalog/#flows","title":"Flows","text":"<p>The Service Flows tab allows you to rapidly investigate the radius of the impact of different services in your system over time.</p> <p>Use it to:</p> <ul> <li> <p>Investigate the performance of each service flow by breaking it down into its constituent operations.</p> </li> <li> <p>Gain a granular understanding of how each sub-flow, a collection of related operations, affects the performance of the entire service flow over time.</p> </li> <li> <p>Rapidly identify and troubleshoot the subflows causing performance issues over time.</p> </li> </ul> <p>Find out more here.</p>"},{"location":"newoutput/service-catalog/#operations","title":"Operations","text":"<p>The Operations tab presents incoming, outgoing, and internal requests for your service through various spans. Select which request type you would like to view in the dropdown menu in the upper right-hand corner.</p> <p>At the top of the page, three charts are shown displaying the displaying service operations for each of the following:</p> <ul> <li> <p>Time Consumption</p> </li> <li> <p>Throughput</p> </li> <li> <p>Error Rate</p> </li> </ul>"},{"location":"newoutput/service-catalog/#incoming-requests","title":"Incoming Requests","text":"<p>View the requests the service receives \u2013 in the form of server and consumer spans.</p> <p>For each operation, view operation type, method, time consumed, percentage of errors caused by the operation, and what percentage of the operation comprised the total number of operations. These are all shown for the timeframe and dimensions selected.</p> <p>View a deeper drill-down of each operation by clicking on an operation row or a series.</p> <p>The deep drill-down shows the time when the operation occurred, the operation type, the service for which the operation was taken, the duration of the operation, and how many errors it generated. It also shows the Throughput, Error Rate, and Latency graphs for that specific operation.</p>"},{"location":"newoutput/service-catalog/#outgoing-requests","title":"Outgoing Requests","text":"<p>View operations that the service requested from other services, in the form of client and producer spans.</p> <p>For each operation, you can view the operation type, method, P95 latency, percentage of total requests, percentage of errors caused by the operation, and the time consumed by the operation. These are all shown for the timeframe selected in the top bar.</p> <p>You can see a deeper drill-down of each operation by clicking on an operation row.</p>"},{"location":"newoutput/service-catalog/#internal-requests","title":"Internal Requests","text":"<p>View operations internal to the service with internal spans.</p> <p>For each internal operation, you can view the operation type, method, P95 latency, percentage of total requests, percentage of errors caused by the operation, and the time consumed by the operation. These are all shown for the timeframe selected in the top bar.</p> <p>You can see a deeper drill-down of each operation by clicking on an operation row.</p>"},{"location":"newoutput/service-catalog/#slo","title":"SLO","text":"<p>The SLO tab provides a view of the service level objectives for the service.</p> <p>For any service, the SLO presents error percentages, and if that amount is within the acceptable range of errors for the service. If the percentage of errors is higher than allowed, an SLO breach occurs.</p> <p>Notes:</p> <ul> <li> <p>Only team admins can add new SLOs.</p> </li> <li> <p>New SLOs take at least seven days for their computation window to complete.</p> </li> <li> <p>Before it is complete, the SLO will show incomplete data.</p> </li> </ul> <p>To begin tracking SLOs, add a new SLO to the system.</p> <p>STEP 1. From the SLO tab of a Service Catalog drill-down or the main Service Catalog tab, click + ADD NEW SLO.</p> <p>STEP 2. Select the service or services to apply the SLO to, from the Service dropdown.</p> <p>STEP 3. Select the SLO type: Error or Latency.</p> <p>STEP 4. Enter a name and optional description for the SLO.</p> <p>STEP 5. Select the filters and threshold for your SLO.</p> <p></p> <p>STEP 6. Select the percentage and the period for which the SLO is valid. For example, 90% for 7 days means that the SLO is valid as long as the error rate over seven days is no higher than 90%.</p> <p></p> <p>STEP 7. Click ADD NEW.</p>"},{"location":"newoutput/service-catalog/#resources","title":"Resources","text":"<p>The Resources tab presents resources used by the service.</p> <p>The resources in this tab present CPU utilization, memory used (bytes), and network usage (bytes) for the timeframe selected in the top bar.</p>"},{"location":"newoutput/service-catalog/#logs","title":"Logs","text":"<p>The Logs tab presents all related logs for the selected service.</p> <p>On the right-hand side of the logs tab, click OPEN LOG QUERY to open a new tab with the logs open in your Coralogix Explore Screen.</p> <p>Set up Correlation Mapping to allow your system to identify the fields in a log that are related to the service. The feature does this by mapping a single key to one or more replacement keys in the service\u2019s logs.</p> <p>STEP 1. Click Setup Correlation on the right-hand side of the logs tab.</p> <p>STEP 2. Select the replacement logs key from the dropdown menu.</p> <p></p> <p>STEP 3. Click UPDATE CORRELATIONS.</p> <p>You can see a deeper drill-down of each operation by clicking on an operation row.</p>"},{"location":"newoutput/service-catalog/#map","title":"Map","text":"<p>The Map tab displays the service map centered on a selected service.</p> <p></p> <p>Services that send requests to the selected service are shown on the left.</p> <p>Services that receive requests from the select service are displayed on the right.</p> <p>The latency for each is presented on the line between the services, the thickness of which changes according to the latency. The thicker the line, the greater the latency.</p> <p>Where multiple services have an error rate greater than 0%, the service with the highest error rate is encircled in red.</p> <p>Hovering over a service shows a tooltip with the service's throughput, error rate, and average duration in relation to the central service.</p> <p>Clicking on a service brings up a context menu with the option to view the Service Overview, its errors, traces, or related logs.</p>"},{"location":"newoutput/service-catalog/#dynamic-view","title":"Dynamic View","text":"<p>The Map view dynamically changes depending on the size of your screen. On larger screens, you can see the throughput, error rate, and SLO status in a box with the service name. All of the information presented relates to the central service, except for SLO status, which is provided for each service.</p> <p></p> <p>On smaller screens, the service name is shown to the side of a circular icon, and the rest of the information moves into a tooltip.</p> <p></p> <p></p>"},{"location":"newoutput/service-catalog/#additional-resources","title":"Additional Resources","text":"DocumentationApplication Performance Monitoring (APM)Apdex Score"},{"location":"newoutput/service-catalog/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/service-flows/","title":"Service Flows","text":"<p>Purpose-built for microservices-based environments, the Coralogix Service Flows feature allows you to rapidly investigate the radius of the impact of different services over time and troubleshoot issues immediately as part of Application Performance Monitoring.</p> <p>To enjoy the feature, contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p> <p></p>"},{"location":"newoutput/service-flows/#overview","title":"Overview","text":"<p>Use Service Flows to:</p> <ul> <li> <p>Investigate the performance of each service flow by breaking it down into its constituent operations.</p> </li> <li> <p>Gain a granular understanding of how each sub-flow, a collection of related operations, affects the performance of the entire service flow over time.</p> </li> <li> <p>Rapidly identify and troubleshoot the sub-flows causing performance issues over time.</p> </li> </ul>"},{"location":"newoutput/service-flows/#concepts","title":"Concepts","text":"<p>A user visits an e-commerce website and clicks on a product to view its details. This action initiates a service flow. The flow begins when the user's request reaches a service and ends when the service sends back the product details as a response. During this flow, several operations occur: the service might query a database to retrieve the product information, call external services for current pricing, and execute internal logic to prepare the content for display. Each of these operations represents a sub-flow. The service flow breakdown shows how these sub-flows collectively contribute to the request's overall response time and performance.</p>"},{"location":"newoutput/service-flows/#service-flow","title":"Service Flow","text":"<p>Simply put, a service flow denotes a singular logical unit of work in a software application. More precisely, it encompasses the function and method calls constituting that unit of work. Each flow consists of a root span, an operation that serves as its entry point and triggers all other related operations.</p>"},{"location":"newoutput/service-flows/#operation","title":"Operation","text":"<p>Every service flow may contain hundreds, if not thousands of spans, grouped per operation name. An operation is a logical entity consolidating all spans with the same action type.</p>"},{"location":"newoutput/service-flows/#sub-flow","title":"Sub-Flow","text":"<p>A sub-flow is a collection of related operations, the constituent functions that break down the various service operations into a service flow and collectively structure its performance. Examples include external service calls and database calls. Each sub-flow provides insights into its performance and how it affects the flow over time.</p> <p>Coralogix\u2019s APM captures and monitors the performance metrics associated with each sub-flow, making up the service flow. This information helps identify bottlenecks, optimize code, and ensure a smooth and efficient user experience during the entire purchase process.</p>"},{"location":"newoutput/service-flows/#service-flows-v-distributed-tracing","title":"Service Flows v. Distributed Tracing","text":"<p>What is the difference between Service Flows and Distributed Tracing?</p> <p>The primary distinction between Service Flows and Distributed Tracing features is the analysis context and depth. Service Flows examine the entire sequence of events over time within a service. This method thoroughly explains how different parts of the flow interact and evolve, providing deep insights into the internal dynamics of service requests.</p> <p>While Distributed Tracing is adept at capturing snapshots of individual operations at specific moments, it does not effectively identify long-term bottlenecks within the overall flow. Service Flows addresses this by delivering a detailed analysis that helps pinpoint and comprehend the underlying causes of performance issues across the entire service flow.</p> <p>In essence, the Service Flows feature enhances the capabilities of Distributed Tracing by examining the collective impact of operations within a service flow, thus offering a more comprehensive view of their influence on the flow's performance over time.</p>"},{"location":"newoutput/service-flows/#instrumentation","title":"Instrumentation","text":"<p>Users must configure custom instrumentation to define, report, and monitor service flows. Doing so connects all sub-flows or service operations to a particular service flow.</p> <p>To add sub-flows to a service flow, use\u00a0custom instrumentation for any of the following languages:</p> <ul> <li> <p>Node.js</p> </li> <li> <p>Java</p> </li> <li> <p>Python</p> </li> <li> <p>Golang</p> </li> </ul>"},{"location":"newoutput/service-flows/#monitoring-service-flows","title":"Monitoring Service Flows","text":"<p>The Service Flow Screen lets you identify service flows\u00a0that may be good candidates for fine-tuning performance problems or resolving errors. It presents a high-level overview of the selected app's service flows, presented as operations.</p> <p></p> <p>STEP 1. To view information about your app's service flows, navigate to APM &gt; Service Flows from your Coralogix toolbar.</p> <p>STEP 2. Select the Flow Type and filter to view the service flows of interest.</p> <ul> <li> <p>Flow types include:</p> <ul> <li> <p>Web Flow: Responds to external requests</p> </li> <li> <p>Pub-Sub Flow: No expected responses (e.g., message queues)</p> </li> <li> <p>Internal Flow: Self-triggered flows (e.g., shutdown operations)</p> </li> </ul> </li> <li> <p>Once you have defined your view, the service flows' response time, throughput, and error rate will be displayed visually.</p> </li> <li> <p>A grid will display all of the service flows matching your filter specifications. For each, you will be presented with average response time, P95 response time, average throughput (RPM), and error rate (per minute).</p> </li> </ul> <p>STEP 3. Clicking on a service flow for further investigation will transfer you to the Sub-Flow Screen.</p>"},{"location":"newoutput/service-flows/#monitoring-sub-flows","title":"Monitoring Sub-Flows","text":"<p>The Sub-Flow Screen summarizes the performance of the succeeding operations of the flow root operation, the entry point for your service flow.</p> <p></p>"},{"location":"newoutput/service-flows/#visualize-sub-flow-performance","title":"Visualize Sub-Flow Performance","text":"<p>View the number of flow requests, errors, and maximum response times for your service sub-flows. Performance is presented in bar and line charts for the time frame chosen.</p>"},{"location":"newoutput/service-flows/#contextualize-sub-flows","title":"Contextualize Sub-Flows","text":"<p>Each row of the Sub-Flow Summary presents a sub-flow, a collection of related operations, with its performance over time.</p> <p>Hover over a sub-flow to view the exact placement of its constituent spans within a trace in Gantt view. This allows you to pinpoint the sub-flow with the greatest impact on the flow behavior over a specific timeframe. As spans do not appear as part of a trace linearly, you can move between spans to investigate each span and its connections within the flow further.</p> <p>Hover over a sub-flow and click on the diagonal arrow to access an action menu for each sub-flow: drill down, obtain a trace link, export to JSON, view the raw span, or see the entire trace map.</p>"},{"location":"newoutput/service-flows/#investigate-spans-traces","title":"Investigate Spans &amp; Traces","text":"<p>Click on the Sub-Flow Drill-Down to investigate a sub-flow further.</p> <p></p> <p>Under the Sub-flow Spans tab, you will be presented with a full list of spans that comprise the particular sub-flow, each with contextual data.</p> <p>To focus on particular spans, click on a point in time on the graphs at the top of the page, presenting average span response times, requests, and errors within a sub-flow.</p> <p>Under the Gantt and Map tabs, select to view a trace from the dropdown menu. Sort traces using the following categories: most recent, max errors, max duration, above P95 and P99.</p> <p></p>"},{"location":"newoutput/service-flows/#additional-resources","title":"Additional Resources","text":"DocumentationIntroduction to Application Performance Monitoring"},{"location":"newoutput/service-flows/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/service-map/","title":"Service Map","text":"<p>The Service Map fully visualizes your system architecture, breaking down your application into all its constituent\u00a0services and drawing the observed dependencies between them in real-time based on your distributed tracing.</p> <p></p>"},{"location":"newoutput/service-map/#setup","title":"Setup","text":"<p>Service Map requires that Coralogix Application Performance Monitoring (APM) be installed and configured.</p>"},{"location":"newoutput/service-map/#span-attributes","title":"Span Attributes","text":"<p>Certain services require the following attributes to be defined for them to be identified in the Service Map.</p>"},{"location":"newoutput/service-map/#aws-s3","title":"AWS S3","text":"Attribute Type Description <code>span.kind</code> string Set to <code>CLIENT</code>, as it's an external call <code>rpc.system</code> string Set to <code>aws-api</code> <code>rpc.service</code> string Contains S3 or tag with prefix <code>aws.s3</code> <code>aws.s3.bucket</code> string The name of the S3 will be retrieved from this attribute. If missing, S3 will be used <p>We also identify AWS S3 spans based on the specific S3 icon.</p>"},{"location":"newoutput/service-map/#kafka","title":"Kafka","text":"Attribute Type Description <code>span.kind</code> string In one match (call), Kafka can be either a destination or source service. Set to <code>PRODUCER</code> if Kafka is the destination service. Set to <code>CONSUMER</code> if Kafka is the source service. <code>messaging.system</code> string Set to <code>kafka</code> <code>messaging.destination</code> or <code>messaging.destination.name</code> string Name of the topic and will represent the name in the Coralogix UI: \u201ckafka topic \u201d <p>We also identify Kafka spans based on the specific Kafka icon.</p>"},{"location":"newoutput/service-map/#rabbitmq","title":"RabbitMQ","text":"Attribute Type Description <code>span.kind</code> string In one match (call), RabbitMQ can be either a destination or source service. Set to PRODUCER if Kafka is the destination service. Set to CONSUMER if Kafka is the source service. <code>messaging.system</code> string Set to <code>rabbitmq</code> <code>messaging.destination</code> or <code>messaging.destination.name</code> string Name of the topic and will represent the name in the UI: \u201cRabbitMQ \u201d <p>We also identify RabbitMQ spans based on the specific RabbitMQ icon.</p>"},{"location":"newoutput/service-map/#redis","title":"Redis","text":"Attribute Type Description <code>span.kind</code> string Set to <code>CLIENT</code>, as it's an external call <code>db.system</code> string Set to <code>redis</code> <code>db.name</code> + <code>db.redis.database_index</code> string This will be the name of the database. If one of the attributes is missing, the one that is included will be used."},{"location":"newoutput/service-map/#how-it-works","title":"How It Works","text":"<p>The Service Map offers insights into your services and their well-being, effectively cutting through extraneous information to pinpoint problematic areas.</p>"},{"location":"newoutput/service-map/#access-the-service-map","title":"Access the Service Map","text":"<p>To access this feature, navigate to APM &gt; Service Map in your Coralogix toolbar.</p>"},{"location":"newoutput/service-map/#visualize-traces","title":"Visualize Traces","text":"<p>View a visualization of your traces, organized into clusters. In the left-hand sidebar, filter by LANGUAGE, PROVIDER, ENVIRONMENT, DATABASE, and/or ERROR TYPE.</p>"},{"location":"newoutput/service-map/#identify-service-dependencies","title":"Identify Service Dependencies","text":"<p>The service map presents a comprehensive overview of a service's dependencies, encompassing connections in various environments. Hover over and click on a service point to view all of its associated information. Search for a service using the search function.</p> <p></p>"},{"location":"newoutput/service-map/#additional-resources","title":"Additional Resources","text":"DocumentationApplication Performance Monitoring (APM)Distributed TracingBlogOne Click Visibility: Coralogix Expands APM Capabilities to Kubernetes"},{"location":"newoutput/service-map/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/service-removal-api/","title":"Service Removal gRPC API","text":""},{"location":"newoutput/service-removal-api/#overview","title":"Overview","text":"<p>We are introducing the APM Service Removal API for customers who want to regularly maintain their APM Service Catalog. Even if services no longer exist on your side, the catalog in Coralogix lists all previously imported services indefinitely. With the Service Removal API, you can manually remove one or more unused services from your Coralogix subscription.</p> <p>Here is what you can expect after a service is deleted:</p> <ul> <li> <p>The service is removed from the service Catalog list.</p> </li> <li> <p>The service is removed from the dropdown in the New SLO modal.</p> </li> <li> <p>Dimensions based on the service\u2019s span tags/process tags are removed from the \u2018Dimensions\u2019 dropdown.</p> </li> <li> <p>The service is removed from the service map, and any connections on the map are adjusted accordingly.</p> </li> <li> <p>The Dependency map is updated. If other services had dependencies on the deleted service, these connections are also adjusted in the UI.</p> </li> </ul>"},{"location":"newoutput/service-removal-api/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>API Key for Alerts, Rules &amp; Tags to successfully authenticate.</p> </li> <li> <p>Management API Endpoint that corresponds with your Coralogix subscription.</p> </li> <li> <p>Administrator permissions to manage your services.</p> </li> </ul>"},{"location":"newoutput/service-removal-api/#api-endpoints","title":"API Endpoints","text":"<p>This reference document lists example requests and responses using\u00a0gRPCurl. The following calls accept arguments as JSON in the request body and return results as JSON in the response body. A complete\u00a0list of Management Endpoints\u00a0is available here.</p>"},{"location":"newoutput/service-removal-api/#authentication","title":"Authentication","text":"<p>Coralogix API uses API keys to authenticate requests. You can view and manage your API keys\u00a0from the Data Flow tab in Coralogix. You need to use this API key in the Authorization request header to successfully connect.</p> <p>Example:</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\"\n\n</code></pre> <p>Then, use one of our designated\u00a0Management Endpoints\u00a0to structure your header.</p> <pre><code>-d @ ng-api-grpc.coralogixstg.wpengine.com:443\n\n</code></pre> <p>For the Service Removal API, the service name will be\u00a0<code>ApmServiceService</code>.</p> <pre><code>com.coralogixapis.apm.services.v1.ApmServiceService/\n\n</code></pre> <p>The complete request header should look like this:</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.apm.services.v1.ApmServiceService/\n\n</code></pre>"},{"location":"newoutput/service-removal-api/#listapmservices","title":"ListApmServices","text":"<p>Lists all the services available in the APM Service Catalog. In this example, you are listing information for the\u00a0<code>browser-api-service</code>\u00a0,\u00a0<code>browser-ingress</code>, and\u00a0<code>cdn-ingress</code>\u00a0services. Here you will find the\u00a0<code>id</code>\u00a0field associated with each service.</p> <p>REQUEST</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.apm.services.v1.ApmServiceService/ListApmServices &lt;&lt;EOF\n{\n}\nEOF\n\n</code></pre> <p>RESPONSE</p> <pre><code>{\n  \"services\": [\n    {\n      \"id\": \"5ae06ace-6643-4ad9-80e5-e2de3875cc99\",\n      \"name\": \"browser-api-service\",\n      \"type\": \"web\",\n      \"workloads\": [\n        \"browser-api-service\"\n      ],\n      \"sloStatusCount\": {},\n      \"technology\": \"\"\n    },\n    {\n      \"id\": \"2937b289-9e8e-422f-8958-676d75e50f7a\",\n      \"name\": \"browser-ingress\",\n      \"type\": \"web\",\n      \"workloads\": [\n        \"browser-ingress\"\n      ],\n      \"sloStatusCount\": {},\n      \"technology\": \"rust\"\n    },\n    {\n      \"id\": \"c2192929-683d-405a-866e-e4e922137d36\",\n      \"name\": \"cdn-ingress\",\n      \"type\": \"web\",\n      \"workloads\": [\n        \"cdn-ingress\"\n      ],\n      \"sloStatusCount\": {},\n      \"technology\": \"rust\"\n    }\n  ]\n}\n\n</code></pre>"},{"location":"newoutput/service-removal-api/#getapmservice","title":"GetApmService","text":"<p>Retrieves specific information for a given service from the catalog. In this example, you are retrieving information pertaining the\u00a0<code>ws-tracing</code>\u00a0service from the catalog list. You need to provide the service\u00a0<code>id</code>.</p> <p>REQUEST</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.apm.services.v1.ApmServiceService/GetApmService &lt;&lt;EOF\n{\n    \"id\": \"46f9cc9c-cf83-4575-adb7-3c62345bb499\"\n}\nEOF\n\n</code></pre> <p>RESPONSE</p> <pre><code>{\n  \"service\": {\n    \"id\": \"46f9cc9c-cf83-4575-adb7-3c62345bb499\",\n    \"name\": \"ws-tracing\",\n    \"type\": \"web\",\n    \"workloads\": [\n      \"ws-tracing\",\n      \"ws-tracing-graph-test\"\n    ],\n    \"sloStatusCount\": {\n      \"ok\": \"0\",\n      \"breach\": \"0\",\n      \"notAvailable\": \"1\"\n    },\n    \"technology\": \"mysql\"\n  }\n}\n\n</code></pre>"},{"location":"newoutput/service-removal-api/#batchgetapmservices","title":"BatchGetApmServices","text":"<p>Batch retrieves multiple services from the catalog. In this example, you are retrieving information on three specified services from the catalog.</p> <p>REQUEST</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.apm.services.v1.ApmServiceService/BatchGetApmServices &lt;&lt;EOF\n{\n  \"ids\": [\n    \"0e8e7662-d47a-441f-8f0c-f4d1ac952b48\",\n    \"ce970d0a-85d9-4fcc-9872-c5ae8033c0a4\",\n    \"46f9cc9c-cf83-4575-adb7-3c62345bb499\"\n  ]\n}\nEOF\n\n</code></pre> <p>RESPONSE</p> <pre><code>{\n  \"services\": {\n    \"0e8e7662-d47a-441f-8f0c-f4d1ac952b48\": {\n      \"id\": \"0e8e7662-d47a-441f-8f0c-f4d1ac952b48\",\n      \"name\": \"ws-service-catalog\",\n      \"type\": \"web\",\n      \"workloads\": [\n        \"ws-service-catalog\"\n      ],\n      \"sloStatusCount\": {},\n      \"technology\": \"mysql\"\n    },\n    \"46f9cc9c-cf83-4575-adb7-3c62345bb499\": {\n      \"id\": \"46f9cc9c-cf83-4575-adb7-3c62345bb499\",\n      \"name\": \"ws-tracing\",\n      \"type\": \"web\",\n      \"workloads\": [\n        \"ws-tracing\",\n        \"ws-tracing-graph-test\"\n      ],\n      \"sloStatusCount\": {\n        \"ok\": \"0\",\n        \"breach\": \"0\",\n        \"notAvailable\": \"1\"\n      },\n      \"technology\": \"mysql\"\n    },\n    \"ce970d0a-85d9-4fcc-9872-c5ae8033c0a4\": {\n      \"id\": \"ce970d0a-85d9-4fcc-9872-c5ae8033c0a4\",\n      \"name\": \"ws-statistics\",\n      \"type\": \"web\",\n      \"workloads\": [\n        \"ws-statistics\"\n      ],\n      \"sloStatusCount\": {},\n      \"technology\": \"mysql\"\n    }\n  }\n}\n\n</code></pre>"},{"location":"newoutput/service-removal-api/#deleteapmservice","title":"DeleteApmService","text":"<p>Deletes the specified service from the catalog. In this example, you are deleting the\u00a0<code>ws-tracing</code>\u00a0service from your subscription.</p> <p>REQUEST</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.apm.services.v1.ApmServiceService/DeleteApmService &lt;&lt;EOF\n{\n    \"id\": \"46f9cc9c-cf83-4575-adb7-3c62345bb499\"\n}\nEOF\n\n</code></pre> <p>RESPONSE</p> <pre><code>{}\n\n</code></pre>"},{"location":"newoutput/service-removal-api/#body-parameters","title":"Body Parameters","text":"Field Type Description id int32 The unique identifier for the APM service. name string The name of the specified APM service. type string The type of the APM service. e.g. \u201cweb\u201d workloads array An array of other service names associated with the service. technology string The technology associated with the APM service. e.g. \u201cmysql\u201d sloStatusCount array Object containing counts for SLO statuses. \u2514 ok string Count of services with SLO status \u201cok\u201d. \u2514 breach string Count of services with SLO status \u201cbreach\u201d. \u2514 notAvailable string Count of services with SLO status \u201cnotAvailable\u201d."},{"location":"newoutput/session-length-management/","title":"Session Length","text":"<p>As part of their user management, team administrators can define the duration of idle sessions for all team members. Enabling this option will end all current login sessions after a period of time and require users to log in again.</p>"},{"location":"newoutput/session-length-management/#access-session-length-management-settings","title":"Access Session Length Management Settings","text":"<p>STEP 1. Access your settings in the upper-right hand corner of your Coralogix dashboard.</p> <p>STEP 2. In the left-hand sidebar, in the Security section, select Sessions.</p> <p></p>"},{"location":"newoutput/session-length-management/#manage-settings","title":"Manage Settings","text":"<p>Admins have the option of activating up to two states using toggle buttons.</p> <ul> <li> <p>Force logout. Enabling this option will end all of the current login sessions and require users to login again. Regardless of activity, users will be required to log in after a period of time, which can be set in hours or days.</p> </li> <li> <p>Log users out when activity is not detected. Enabling this option will end the current login session for any user whose activity has not been detected for a period of time, which can be set in minutes or hours.</p> </li> </ul> <p></p> <p>Find out more about team and user management here.</p>"},{"location":"newoutput/session-length-management/#additional-resources","title":"Additional Resources","text":"DocumentationTeam &amp; User Management"},{"location":"newoutput/session-length-management/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/","title":"Advanced Configuration","text":"<p>Coralogix offers Kubernetes Observability using OpenTelemetry for comprehensive Kubernetes and application observability. This tutorial will guide you through advanced configuration options for Kubernetes clusters.</p> <p>For basic configuration, view our tutorial here.</p>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Kubernetes\u00a0v1.24+ installed, with the command-line tool\u00a0kubectl</p> </li> <li> <p>Helm\u00a0v3.9+ installed and configured</p> </li> </ul>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#overview","title":"Overview","text":"<p>The OpenTelemetry Integration Chart utilises the values.yaml file as its default configuration. This configuration is based on the OpenTelemetry Collector Configuration for both the OpenTelemetry Agent Collector and OpenTelemetry Cluster Collector.</p>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#default-configuration-in-3-easy-steps","title":"Default Configuration in 3 Easy Steps","text":"<p>STEP 1. Create a new YAML-formatted override file that defines certain values for the OpenTelemetry Integration Chart.</p> <p>The following global values are the minimum required configurations to getting the chart working:</p> <pre><code># values.yaml\nglobal:\n  domain: \"&lt;coralogix-endpoint&gt;\"\n  clusterName: \"&lt;k8s-cluster-name&gt;\"\n\n</code></pre> <ul> <li> <p>Input the following values:</p> <ul> <li> <p><code>domain</code>: Choose the\u00a0OpenTelemetry endpoint\u00a0for the\u00a0domain\u00a0associated with your Coralogix account.</p> </li> <li> <p><code>clusterName</code>: You are also required to specify as a cluster identifier.</p> </li> </ul> </li> <li> <p>You also may copy all or any other configurations from the repository <code>values.yaml</code> file.</p> </li> <li> <p>If you'd like to provide your own overrides for an array values such as\u00a0<code>extraEnvs</code>,\u00a0<code>extraVolumes</code>\u00a0or\u00a0<code>extraVolumeMounts</code>, be aware that Helm does not support merging arrays. Instead, arrays are nulled out.</p> </li> <li> <p>In case you'd like to provide your own values for these arrays, first copy over any existing array values\u00a0from the provided\u00a0<code>values.yaml</code>\u00a0file.</p> </li> </ul> <p>STEP 2. Save this file as <code>values.yaml</code></p> <p>STEP 3. Install with the <code>helm upgrade --install</code> command</p> <pre><code>helm upgrade --install otel-integration coralogix-charts-virtual/otel-integration -f values.yaml -n $NAMESPACE\n\n</code></pre>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#optional-configurations","title":"Optional Configurations","text":""},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#enabling-dependent-charts","title":"Enabling Dependent Charts","text":"<p>The OpenTelemetry Agent is primarily used for collecting application telemetry, while the OpenTelemetry Cluster Collector is primarily used to collect cluster-level data. Depending on your requirements, you can either use the default configuration that enables both components, or you can choose to disable either of them by modifying the <code>enabled</code> flag in the <code>values.yaml</code> file under the <code>opentelemetry-agent</code> or <code>opentelemetry-cluster-collector</code> section as shown below:</p> <pre><code>...\nopentelemetry-agent:\n  enabled: true\n  mode: daemonset\n...\nopentelemetry-cluster-collector:\n  enabled: true\n  mode: deployment\n\n</code></pre>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#installing-the-chart-on-clusters-with-mixed-operating-systems-linux-and-windows","title":"Installing the Chart on Clusters with Mixed Operating Systems (Linux and Windows)","text":"<p>Installing <code>otel-integration</code> is also possible on clusters that support running Windows workloads on Windows node alongside Linux nodes (such as EKS, AKS or GKE). The <code>kube-state-metrics</code> and collector will be installed on Linux nodes, as these components are supported only on Linux operating systems. Conversely, the agent will be installed on both Linux and Windows nodes as a daemonset, in order to collect metrics for both operating systems. In order to do so, the chart needs to be installed with few adjustments.</p> <p>Adjust the Helm command in STEP 10 of the basic configuration to use the <code>values-windows.yaml</code> file as follows:</p> <pre><code>helm upgrade --install otel-coralogix-integration coralogix/otel-integration -n $NAMESPACE -f values-windows.yaml --set global.domain=\"coralogixstg.wpengine.com\" --set global.clusterName=\"&lt;cluster name&gt;\"\n\n</code></pre>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#service-pipelines","title":"Service Pipelines","text":"<p>The OpenTelemetry Collector Configuration guides you to initialise components and then add them to the pipelines in the <code>service</code> section. It is important to ensure that the telemetry type is supported. For example, the <code>[prometheus](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver#prometheus-receiver)</code> receiver documentation in the README states that it only supports <code>metrics</code>. Therefore, the following <code>prometheus</code> receiver can only be defined under <code>receivers</code> and added to the <code>metrics</code> pipelines in the <code>service</code> block to enable it.</p> <pre><code>opentelemetry-agent:\n...\n    config:\n        receivers:\n            prometheus:\n        config:\n          scrape_configs:\n            - job_name: opentelemetry-infrastructure-collector\n              scrape_interval: 30s\n              static_configs:\n                - targets:\n                    - ${MY_POD_IP}:8888\n\n        ...\n      service:\n        pipelines:\n                logs:\n                    ...\n                metrics:\n                    receivers:\n                    - prometheus\n          traces:\n                        ...\n\n</code></pre>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#coralogix-exporter","title":"Coralogix Exporter","text":"<p>In both charts, you have the option to configure the sending of logs, metrics, and / or traces to Coralogix. This can be done by configuring the Coralogix Exporter for different pipelines. The default <code>values.yaml</code> file includes all three options, but you can customize it by removing the <code>coralogix</code> exporter from the <code>pipelines</code> configuration for either <code>logs</code>, <code>metrics</code>, or <code>traces</code>.</p> <p>The following <code>opentelemetry-agent</code> exporter configuration also applies to the <code>opentelemetry-cluster-collector</code>:</p> <pre><code>global:\n  domain: \"&lt;coralogix-domain&gt;\"\n  clusterName: \"&lt;cluster-name&gt;\"\n  defaultApplicationName: \"otel\"\n  defaultSubsystemName: \"integration\"\n...\nopentelemetry-agent:\n...\n  config:\n    ...\n    exporters:\n      coralogix:\n        timeout: \"30s\"\n        private_key: \"${CORALOGIX_PRIVATE_KEY}\"\n                ## Values set in \"global\" section\n        domain: \"{{ .Values.global.domain }}\"\n                application_name: \"{{ .Values.global.defaultApplicationName }}\"\n        subsystem_name: \"{{ .Values.global.defaultSubsystemName }}\"\n    service:\n      pipelines:\n        metrics:\n          exporters:\n            - coralogix\n                            ...\n        traces:\n          exporters:\n            - coralogix\n                            ...\n        logs:\n          exporters:\n            - coralogix\n\n</code></pre>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#opentelemetry-agent","title":"OpenTelemetry Agent","text":"<p>The OpenTelemetry Agent is enabled and deployed as a <code>daemonset</code> by default. This creates an Agent pod per node. Allowing the collection of logs, metrics, and traces from application pods to be sent to OpenTelemetry pods hosted on the same node and spreads the ingestion load across the cluster. Be aware that the OpenTelemetry Agent pods consumes resources (e.g., CPU &amp; memory) from each node on which it runs.</p> <pre><code>opentelemetry-agent:\n  enabled: true\n  mode: daemonset\n\n</code></pre> <p>Notes:</p> <ul> <li>If there are nodes without a running OpenTelemetry Agent pod, the hosted pods of applications may be missing metadata attributes (e.g. node info and host name) in the telemetry sent.</li> </ul>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#agent-presets","title":"Agent Presets","text":"<p>The multi-instanced OpenTelemetry Agent can be deployed across multiple nodes as a <code>daemonset</code>. It provides presets for collecting host metrics, Kubernetes attributes, and Kubelet metrics. When logs, metrics, and traces are generated from a pod, the collector enriches them with the metadata associated with the hosting machine. This metadata is very useful for linking infrastructure issues with performance degradation in services.</p> <p>For more information on presets, refer to the Configuration of OpenTelemetry Collector.</p> <pre><code># example\nopentelemetry-agent:\n... \n  presets:\n    logsCollection:\n      enabled: true\n    kubernetesAttributes:\n      enabled: true\n    hostMetrics:\n      enabled: true\n    kubeletMetrics:\n      enabled: true\n\n</code></pre> <p>For example, enabling the <code>kubeletMetrics</code> preset to <code>true</code> will add configuration <code>kubeletstats</code> receiver that will pull node, pod, container, and volume metrics from the API server of the host\u2019s kubelet. It will send it down metric pipeline.</p> <pre><code># example\nreceivers:\n    kubeletstats:\n        auth_type: serviceAccount\n        collection_interval: 20s\n        endpoint: ${K8S_NODE_NAME}:10250\n\n</code></pre>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#receivers","title":"Receivers","text":"<p>Once configured, you will be able to send logs, metrics, and traces to be collected in the OpenTelemetry Agent pods before exporting them to Coralogix.</p> <p>To achieve this, you need to first instrument your application with OpenTelemetry SDKs and expose the Collector to a corresponding receiver. It is recommended to use the OTLP receiver (OpenTelemetry protocol) for transmission over gRPC or HTTP endpoints.</p> <p>The <code>daemonset</code> deployment of the OpenTelemetry Agent also uses <code>hostPort</code> for the <code>otlp</code> port, allowing agent pod IPs to be reachable via node IPs, as follows:</p> <pre><code># K8s daemonset otlp port config\nports:\n- containerPort: 4317\n  hostPort: 4317\n  name: otlp\n  protocol: TCP\n\n</code></pre> <p>The following examples demonstrate how to configure an Auto-Instrumented JavaScript application to send traces to the agent pod\u2019s gRPC receiver.</p> <p>STEP 1. Set the Kubernetes environment variables of the JavaScript application\u2019s deployment/pod as in the example below. Define the <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> as the configured <code>NODE_IP</code> and <code>OTLP_PORT</code>. Configure <code>OTEL_TRACES_EXPORTER</code> to send in the <code>otlp</code> format. Choose <code>OTEL_EXPORTER_OTLP_PRO</code> as <code>grpc</code>.</p> <pre><code># kubernetes deployment manifest's env section\nspec:\n  containers:\n        ... \n    env:\n  - name: NODE_IP\n    valueFrom:\n      fieldRef:\n        fieldPath: status.hostIP\n  - name: OTLP_PORT\n    value: \"4317\"\n  - name: OTEL_EXPORTER_OTLP_ENDPOINT\n    value: \"http://$(NODE_IP):$(OTLP_PORT)\"\n  - name: OTEL_TRACES_EXPORTER\n    value: \"otlp\"\n    - name: OTEL_EXPORTER_OTLP_PROTOCOL\n    value: \"grpc\"\n\n</code></pre> <p>STEP 2. By default the agent has the otlp receiver configured as follows:</p> <pre><code># collector config\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: ${MY_POD_IP}:4317\n      http:\n        endpoint: ${MY_POD_IP}:4318\n\n</code></pre> <p>Notes:</p> <ul> <li> <p><code>${MY_POD_IP}</code> is a container environment variable that is mapped to the pod's IP address.</p> </li> <li> <p>The agent is also preconfigured to collect data from <code>jaeger</code>.</p> </li> </ul>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#processors","title":"Processors","text":"<p>Processors are generally used to process logs, metrics, and traces before the data is exported. This may include, for example, modifying or altering attributes or sampling traces.</p> <p>In the example below, a <code>k8sattributes</code> processor is used to automatically discovers k8s resources (pods), extract metadata from them and add the extracted metadata to the relevant logs, metrics and spans as resource attributes.</p> <pre><code># default in values.yaml\nprocessors:\n    k8sattributes:\n    filter:\n      node_from_env_var: KUBE_NODE_NAME\n    extract:\n      metadata:\n        - \"k8s.namespace.name\"\n        - \"k8s.deployment.name\"\n        - \"k8s.statefulset.name\"\n        - \"k8s.daemonset.name\"\n        - \"k8s.cronjob.name\"\n        - \"k8s.job.name\"\n        - \"k8s.pod.name\"\n        - \"k8s.node.name\"\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>The <code>k8sattributes</code> processor is enabled by default at the <code>preset</code> level as <code>kubernetesAttributes</code> and further extended in the default <code>values.yaml</code>.</p> </li> <li> <p>More information can be found in the Kubernetes Attributes Processor README.</p> </li> </ul>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#opentelemetry-cluster-collector","title":"OpenTelemetry Cluster Collector","text":"<p>Enable the <code>opentelemetry-cluster-collector</code> by setting <code>enabled</code> to <code>true</code>.</p> <pre><code>opentelemetry-cluster-collector:\n  enabled: true\n  mode: deployment\n\n</code></pre> <p>Notes:</p> <ul> <li>The cluster collector operates as a <code>deployment</code> workload with a minimal replica of 1 to avoid duplication of telemetry data.</li> </ul>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#cluster-collector-presets","title":"Cluster Collector Presets","text":"<p>The cluster collector is best suited to enable presets such as Kubernetes Events and Cluster Metrics. A smaller instance count of the <code>deployment</code> is sufficient to query the Kubernetes API.</p> <pre><code>    presets:\n    clusterMetrics:\n      enabled: true\n    kubernetesEvents:\n      enabled: true\n    kubernetesExtraMetrics:\n      enabled: true\n\n</code></pre> <p>For example, if you enable the <code>kubernetesEvents</code> preset, the Kubernetes objects receiver configuration will be added dynamically during the Helm installation. This configuration enables the collection of <code>events.k8s.io</code> objects from the Kubernetes API server.</p>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#kubernetes-events-reducing-the-amount-of-collected-data","title":"Kubernetes Events: Reducing the Amount of Collected Data","text":"<p>When collecting Kubernetes events using the cluster collector, it is common for the number of events to reach millions, especially in large clusters with numerous nodes and constantly scaling applications. To collect only the relevant data, you can use the following settings.</p>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#cleaning-data","title":"Cleaning Data","text":"<p>By default, a transform processor named\u00a0<code>transform/kube-events</code> is configured to remove some unneeded fields from Kubernetes events collected. You may override this or alter the fields as desired.</p> <pre><code>processors:\n    transform/kube-events:\n      log_statements:\n        - context: log\n          statements:\n            - keep_keys(body[\"object\"], [\"type\", \"eventTime\", \"reason\", \"regarding\", \"note\", \"metadata\", \"deprecatedFirstTimestamp\", \"deprecatedLastTimestamp\"])\n            - keep_keys(body[\"object\"][\"metadata\"], [\"creationTimestamp\"])\n            - keep_keys(body[\"object\"][\"regarding\"], [\"kind\", \"name\", \"namespace\"])\n\n</code></pre>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#filtering-kubernetes-events","title":"Filtering Kubernetes Events","text":"<p>In large-scale environments, where there are numerous events occurring per hour, it may not be necessary to process all of them. In such cases, you can use an additional OpenTelemetry processor to filter out the events that do not need to be sent to Coralogix.</p> <p>Below is a sample configuration for reference. This configuration filters out any event that has the field\u00a0<code>reason</code>\u00a0with one of those values\u00a0<code>BackoffLimitExceeded|FailedScheduling|Unhealthy</code>.</p> <pre><code>processors:\n  filter/kube-events:\n    logs:\n      log_record:\n        - 'IsMatch(body[\"reason\"], \"(BackoffLimitExceeded|FailedScheduling|Unhealthy)\") == true'\n\n</code></pre>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#collecting-only-warning-events","title":"Collecting Only Warning Events","text":"<p>Currently, Kubernetes has two different types of events:\u00a0<code>Normal</code>\u00a0and\u00a0<code>Warning</code>. As we have the ability to filter events according to their type, you may choose to collect only\u00a0<code>Warning</code>\u00a0events, as these events are key to troubleshooting. One example could be the use of a filter processor to drop all unwanted <code>Normal</code> typed events.</p> <pre><code>processors:\n  filter/kube-events:\n    logs:\n      log_record:\n        - 'IsMatch(body[\"object\"][\"type\"], \"Normal\")'\n\n</code></pre>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#kubernetes-infrastructure-monitoring","title":"Kubernetes Infrastructure Monitoring","text":"<p>If you already have an existing log shipper (e.g. Fluentd, Filebeat) in place and your goal is to monitor all Kubernetes elements of your cluster, follow these steps to enable only the necessary collection of metrics and Kubernetes events to be sent to Coralogix.</p> <p>STEP 1. Copy the following into a YAML-formatted override file and save as <code>values.yaml</code>.</p> <pre><code>global:\n  domain: \"&lt;coralogix-endpoint&gt;\"\n  clusterName: \"&lt;k8s-cluster-name&gt;\"\n\nopentelemetry-agent:\n  presets:\n    logsCollection:\n      enabled: false\n  config:\n    exporters:\n      logging: {}\n    receivers:\n      zipkin: null\n      jaeger: null\n\n    service:\n      pipelines:\n        traces: \n          exporters:\n            - logging\n          receivers:\n            - otlp\n        logs:\n          exporters: \n            - logging\n          receivers:\n            - otlp\n\n</code></pre> <p>STEP 2. Install with the <code>helm upgrade --install</code> command.</p> <pre><code>helm upgrade --install otel-integration coralogix-charts-virtual/otel-integration -f values.yaml -n $NAMESPACE\n\n</code></pre>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#next-steps","title":"Next Steps","text":"<p>Validation instructions can be found here.</p>"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#additional-resources","title":"Additional Resources","text":"DocumentationGitHub Repository"},{"location":"newoutput/set-up-kubernetes-observability-using-opentelemetry-advanced-configuration/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com</p>"},{"location":"newoutput/shipping-snowflake-logs-and-audit-data-to-coralogix/","title":"Shipping Snowflake Logs and Audit Data to Coralogix","text":"<p>This tutorial demonstrates how to centralize logging for Snowflake by sending your logs to Coralogix.</p>"},{"location":"newoutput/shipping-snowflake-logs-and-audit-data-to-coralogix/#overview","title":"Overview","text":"<p>Snowflake is a cloud-based data warehousing platform. Designed for storing, processing, and analyzing large volumes of data, it provides a data warehousing service that is highly scalable, allowing organizations to store and query data efficiently in the cloud.</p> <p>Snowflake does not have direct built-in integration with Coralogix to export the logs and audit related data. This step-by-step guide will teach you how to set up an AWS S3 bucket, to which you can export your Snowflake logs and audit data. Coralogix then ingests the data and presents it for optimized visualization and analysis in our platform.</p> <p></p>"},{"location":"newoutput/shipping-snowflake-logs-and-audit-data-to-coralogix/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>AWS S3 bucket created</p> </li> <li> <p>For those wishing to run all below script via commands, SnowSQL (the command line interface for Snowflake) installed</p> </li> <li> <p>ACCOUNTADMIN\u00a0role to execute all below commands</p> </li> </ul>"},{"location":"newoutput/shipping-snowflake-logs-and-audit-data-to-coralogix/#send-snowflake-data-to-your-s3-bucket","title":"Send Snowflake Data to Your S3 Bucket","text":""},{"location":"newoutput/shipping-snowflake-logs-and-audit-data-to-coralogix/#upload-data-to-the-s3-bucket","title":"Upload Data to the S3 Bucket","text":"<p>STEP 1. Create an IAM policy.</p> <p>Configure access permissions for your S3 bucket by following Step 1 here to create IAM policy now create an IAM role which will be used by Snowflake to unload data into S3 bucket.</p> <p>STEP 2. Configure an IAM role.</p> <ul> <li> <p>In your AWS account, navigate to IAM &gt; Roles &gt; Create role.</p> </li> <li> <p>Select AWS account and select This account, regardless of account ID.</p> </li> </ul> <p></p> <ul> <li>Leave the remaining settings unchanged.</li> </ul> <p>STEP 3. Create an S3 storage integration.</p> <pre><code>create or replace storage integration s3_integration\n  type = external_stage\n  storage_provider = s3\n  enabled = true\n  storage_aws_role_arn = '&lt;PUT_HERE_AWS_ROLE_ARN&gt;'\n  storage_allowed_locations = ('&lt;PUT_HERE_AWS_S3_BUCKET_PATH&gt;');\n\n</code></pre> <p>STEP 4. Create a JSON file format.</p> <pre><code>create or replace file format my_json_format\n  type = json\n  COMPRESSION = 'gzip'\n  null_if = ('NULL', 'null');\n\n</code></pre> <p>STEP 5. Create an S3 stage.</p> <pre><code>use database '&lt;PUT_HERE_DB_NAME&gt;';\nuse schema '&lt;PUT_HERE_DB_SCHEMA_NAME_TO_USE&gt;';\ncreate or replace stage my_s3_stage\n storage_integration = s3_integration\n url = '&lt;PUT_HERE_AWS_S3_BUCKET_PATH&gt;'\n file_format = my_json_format;\n\n</code></pre> <p>STEP 6. To get your snowflake Account ID, run this command:</p> <pre><code>DESC INTEGRATION s3_integration;\n\n</code></pre> <ul> <li> <p>Under STORAGE_AWS_IAM_USER_ARN, you will find your snowflake account ID.</p> </li> <li> <p>In the role created in STEP 1, under the Trust relationships tab, edit the account ID by changing it to your snowflake account ID.</p> </li> </ul> <p></p> <p>STEP 6. Execute an unload command to push data from tables to stage and in turn to AWS S3.</p> <pre><code>use database \u2018&lt;PUT_HERE_DB_NAME&gt;\u2019;\nuse WAREHOUSE \u2018&lt;PUT_HERE_WAREHOUSE_NAME&gt;\u2019;copy into@my_s3_stage/login_history from (SELECT OBJECT_CONSTRUCT(\u2018application\u2019, \u2018snowflake\u2019 ,\u2019environment\u2019, \u2018&lt;PUT_HERE_ENV_NAME&gt;\u2019, \u2018log_type\u2019, \u2018login_history\u2019, \u2018EVENT_TIMESTAMP\u2019, EVENT_TIMESTAMP, \u2018EVENT_TYPE\u2019, EVENT_TYPE, \u2018USER_NAME\u2019, USER_NAME, \u2018CLIENT_IP\u2019, CLIENT_IP, \u2018REPORTED_CLIENT_TYPE\u2019, REPORTED_CLIENT_TYPE, \u2018FIRST_AUTHENTICATION_FACTOR\u2019,FIRST_AUTHENTICATION_FACTOR, \u2018IS_SUCCESS\u2019, IS_SUCCESS, \u2018ERROR_CODE\u2019, ERROR_CODE, \u2018ERROR_MESSAGE\u2019, ERROR_MESSAGE) from snowflake.account_usage.Login_history) FILE_FORMAT = (TYPE = JSON) ;copy into@my_s3_stage/access_history from (SELECT OBJECT_CONSTRUCT(\u2018application\u2019, \u2018snowflake\u2019 ,\u2019environment\u2019, \u2018&lt;PUT_HERE_DB_NAME&gt;\u2019, \u2018log_type\u2019, \u2018access_history\u2019, \u2018QUERY_START_TIME\u2019,QUERY_START_TIME, \u2018USER_NAME\u2019, USER_NAME, \u2018DIRECT_OBJECTS_ACCESSED\u2019,DIRECT_OBJECTS_ACCESSED, \u2018BASE_OBJECTS_ACCESSED\u2019, BASE_OBJECTS_ACCESSED, \u2018OBJECTS_MODIFIED\u2019, OBJECTS_MODIFIED) from snowflake.account_usage.Access_History ) FILE_FORMAT = (TYPE = JSON);\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>The above scripts the <code>copy into</code> command to unload / extract data from various tables like\u00a0login_history, access_history,\u00a0query_history,\u00a0sessions, etc.</p> </li> <li> <p>This example uses\u00a0<code>OBJECT_CONSTRUCT</code>\u00a0to selectively get data from columns and to add that against the specified name while preparing a Json formatted file.</p> </li> <li> <p>[Recommended] To gain a better understanding and visibility into the logs, add custom data while unloading data from tables to JSON format. In the above script, application, environment and log_type tags in the JSON have been added, allowing for categorization of logs by environment.</p> </li> </ul> <p>STEP 6. [Optional] Repeat and write scripts for all below tables in which Snowflake stores logs and audit related data.</p> <p>All these tables and views are present in the\u00a0snowflake.account_usage\u00a0schema.</p> <p></p>"},{"location":"newoutput/shipping-snowflake-logs-and-audit-data-to-coralogix/#validation","title":"Validation","text":"<p>Once the above commands are run, you should see logs files created in your S3 bucket.</p> <p></p>"},{"location":"newoutput/shipping-snowflake-logs-and-audit-data-to-coralogix/#send-data-from-your-s3-bucket-to-coralogix","title":"Send Data From Your S3 Bucket to Coralogix","text":"<p>Coralogix provides multiple methods in which you can collect logs from Amazon S3. Send us your CloudTrail data from your Amazon S3 bucket using an\u00a0AWS Lambda function, with one of two event-driven design patterns:</p> <ul> <li> <p>Invoke the Lambda\u00a0directly through an S3 event</p> </li> <li> <p>Send the S3 event to a\u00a0Simple Notification Service (SNS)\u00a0queue, which in turn triggers the Lambda</p> </li> </ul> <p>Use any of our customized log collection options to allow Coralogix to ingest the logs stored in your Amazon S3 bucket and process them for further analysis and monitoring.</p>"},{"location":"newoutput/shipping-snowflake-logs-and-audit-data-to-coralogix/#validation_1","title":"Validation","text":"<p>In your Coralogix toolbar, navigate to Explore &gt; Logs. View your logs in your Coralogix dashboard.</p> <p></p>"},{"location":"newoutput/shipping-snowflake-logs-and-audit-data-to-coralogix/#additional-resources","title":"Additional Resources","text":"DocumentationGetting Started with CoralogixCoralogix Features Tour"},{"location":"newoutput/shipping-snowflake-logs-and-audit-data-to-coralogix/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/slack-data-ingestion/","title":"Slack Data Ingestion","text":"<p>Collect your Slack messages in the Coralogix platform using our automatic Contextual Data Integration Package.</p>"},{"location":"newoutput/slack-data-ingestion/#overview","title":"Overview","text":"<p>Slack is a cloud-based collaboration platform designed to enhance communication and teamwork within organizations. It offers teams and individuals the ability to communicate in real time through channels organized by topics, projects, or teams, promoting efficient information sharing and reducing the reliance on email. Slack integrates various features such as instant messaging, file sharing, and third-party app integration, making it a versatile tool for remote and distributed teams to collaborate, coordinate tasks, and streamline workflows effectively.</p> <p>Collecting your Slack messages in Coralogix offers several benefits. By collecting Slack messages in Coralogix\u2019s log management platform, you can gain a deeper understanding of communication patterns, identify trends, and gain additional context for events. This integration allows for improved collaboration tracking, performance monitoring, and compliance auditing, enabling your teams to enhance communication strategies, optimize workflows, and maintain a more secure and organized digital environment.</p> <p>For example, if you have a channel that is defined as contextual data, by looking at Slack messages in Coralogix from that channel surrounding a specific event, you can gain additional context to the investigation of that event.</p>"},{"location":"newoutput/slack-data-ingestion/#get-started","title":"Get Started","text":"<p>STEP 1.\u00a0In your navigation pane, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 2.\u00a0In the Contextual Data section, select SLACK and click\u00a0+ ADD NEW.</p> <p></p> <p>STEP 3. Enter an integration name and application name, then click CREATE NEW KEY to generate a new API key.</p> <p></p> <p>STEP 4. Click NEXT.</p> <p>STEP 5. Click INSTALL to install the Coralogix application on Slack. You\u2019ll be redirected to the Slack application to install the Coralogix app. If you already have the Coralogix application on Slack, click ALREADY INSTALLED and skip the next step.</p> <p></p> <p>STEP 6. When prompted to do so, ensure you are working in the correct workspace (via the selection in the top right corner), then click ALLOW.</p> <p></p> <p>STEP 7. Link the Coralogix application for Slack to your Coralogix account by executing the displayed command in Slack.</p> <p></p> <p>STEP 8. Once you have executed the command, click NEXT.</p> <p></p> <p>STEP 9. Invite the Coralogix app to the channels you would like to monitor in Coralogix. You can do this by executing the command shown on the screen in the message field in Slack. You must repeat this step for each channel you want to monitor.</p> <p></p> <p>Note that EU is used in this example, but depending on your Coralogix\u00a0domain, the wizard will change the snippet accordingly.</p> <p>STEP 10. Click FINISH.</p>"},{"location":"newoutput/slack-data-ingestion/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/slo-management-api/","title":"SLO Management API","text":""},{"location":"newoutput/slo-management-api/#overview","title":"Overview","text":"<p>We are introducing the SLO API to enable teams, particularly those not utilizing a UI, to efficiently manage their SLOs programmatically. This API will let you retrieve, read, create, update and delete your SLOs.</p>"},{"location":"newoutput/slo-management-api/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>API Key for Alerts, Rules &amp; Tags to successfully authenticate.</p> </li> <li> <p>Management API Endpoint that corresponds with your Coralogix subscription.</p> </li> <li> <p>Administrator permissions to manage your SLOs.</p> </li> </ul>"},{"location":"newoutput/slo-management-api/#api-endpoints","title":"API Endpoints","text":"<p>This reference document lists example requests and responses using\u00a0gRPCurl. The following calls accept arguments as JSON in the request body and return results as JSON in the response body. A complete\u00a0list of Management Endpoints\u00a0is available here.</p>"},{"location":"newoutput/slo-management-api/#authentication","title":"Authentication","text":"<p>Coralogix API uses API keys to authenticate requests. You can view and manage your API keys\u00a0from the Data Flow tab in Coralogix. You need to use an API key in the Authorization request header to successfully connect.</p> <p>Example:</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\"\n\n</code></pre> <p>Then, use one of our designated\u00a0Management Endpoints\u00a0to structure your header.</p> <pre><code>-d @ ng-api-grpc.coralogixstg.wpengine.com:443\n\n</code></pre> <p>For SLO API, the service name will be\u00a0<code>ServiceSloService</code>.</p> <pre><code>com.coralogixapis.apm.services.v1.ServiceSloService/\n\n</code></pre> <p>The complete request header should look like this:</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.apm.services.v1.ServiceSloService/\n\n</code></pre>"},{"location":"newoutput/slo-management-api/#listserviceslos","title":"ListServiceSlos","text":"<p>Lists all available SLOs from the entire service catalog. In this example, there are two available SLOs,\u00a0<code>List_bigger_than_10ms</code>\u00a0from\u00a0<code>productcatalogservice</code>\u00a0and\u00a0<code>Latency_bigger_then_1ms</code>\u00a0from the\u00a0<code>frontend</code>\u00a0service. Here you will find the\u00a0<code>id</code>\u00a0field associated with each SLO.</p> <p>REQUEST</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.apm.services.v1.ServiceSloService/ListServiceSlos &lt;&lt;EOF\n{\n}\nEOF\n\n</code></pre> <p>RESPONSE</p> <pre><code>{\n  \"slos\": [\n    {\n      \"id\": \"049b684b-4ef0-449e-8fad-33e5f90775b4\",\n      \"name\": \"List_bigger_than_10ms\",\n      \"serviceName\": \"productcatalogservice\",\n      \"description\": \"Check if the operation of list products is bigger than 10ms for 50% of the spans\",\n      \"targetPercentage\": 50,\n      \"createdAt\": \"1970-01-20T11:38:56.476Z\",\n      \"remainingErrorBudgetPercentage\": 100,\n      \"latencySli\": {\n        \"thresholdMicroseconds\": \"10000\",\n        \"thresholdSymbol\": \"THRESHOLD_SYMBOL_GREATER_OR_EQUAL\"\n      },\n      \"filters\": [\n        {\n          \"field\": \"operationname\",\n          \"compareType\": \"COMPARE_TYPE_IS\",\n          \"fieldValues\": [\n            \"hipstershop.ProductCatalogService/ListProducts\"\n          ]\n        }\n      ],\n      \"period\": \"SLO_PERIOD_7_DAYS\"\n    },\n    {\n      \"id\": \"089d1831-7c40-4a31-bab8-0f64116c18b2\",\n      \"name\": \"Latency_bigger_then_1ms\",\n      \"serviceName\": \"frontend\",\n      \"description\": \"Check if latency is bigger then 1ms \",\n      \"targetPercentage\": 1,\n      \"createdAt\": \"1970-01-20T11:40:32.323Z\",\n      \"remainingErrorBudgetPercentage\": 100,\n      \"latencySli\": {\n        \"thresholdMicroseconds\": \"1000\",\n        \"thresholdSymbol\": \"THRESHOLD_SYMBOL_GREATER_OR_EQUAL\"\n      },\n      \"period\": \"SLO_PERIOD_7_DAYS\"\n    }\n  ]\n}\n\n</code></pre>"},{"location":"newoutput/slo-management-api/#getserviceslo","title":"GetServiceSlo","text":"<p>Retrieves specific SLO information for a given service. In this example, you are retrieving all SLOs for the\u00a0<code>frontend</code>service. You need to provide the SLO\u00a0<code>id</code>.</p> <p>REQUEST</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.apm.services.v1.ServiceSloService/GetServiceSlo &lt;&lt;EOF\n{\n    \"id\": \"049b684b-4ef0-449e-8fad-33e5f90775b4\"\n}\nEOF\n\n</code></pre> <p>RESPONSE</p> <pre><code>{\n  \"slo\": {\n    \"id\": \"049b684b-4ef0-449e-8fad-33e5f90775b4\",\n    \"name\": \"List_bigger_than_10ms\",\n    \"serviceName\": \"productcatalogservice\",\n    \"description\": \"Check if the operation of list products is bigger than 10ms for 50% of the spans\",\n    \"targetPercentage\": 50,\n    \"createdAt\": \"1970-01-20T11:38:56.476Z\",\n    \"remainingErrorBudgetPercentage\": 100,\n    \"latencySli\": {\n      \"thresholdMicroseconds\": \"10000\",\n      \"thresholdSymbol\": \"THRESHOLD_SYMBOL_GREATER_OR_EQUAL\"\n    },\n    \"filters\": [\n      {\n        \"field\": \"operationname\",\n        \"compareType\": \"COMPARE_TYPE_IS\",\n        \"fieldValues\": [\n          \"hipstershop.ProductCatalogService/ListProducts\"\n        ]\n      }\n    ],\n    \"period\": \"SLO_PERIOD_7_DAYS\"\n  }\n}\n\n</code></pre>"},{"location":"newoutput/slo-management-api/#batchgetserviceslos","title":"BatchGetServiceSlos","text":"<p>Retrieves specified SLOs from the list. You need to provide the\u00a0<code>id</code>\u00a0for each required SLO. In this example, we are retrieving\u00a0<code>List_bigger_than_10ms</code>\u00a0and\u00a0<code>Latency_bigger_than_1ms</code>.</p> <p>REQUEST</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.apm.services.v1.ServiceSloService/BatchGetServiceSlos &lt;&lt;EOF\n{\n  \"ids\": [\"049b684b-4ef0-449e-8fad-33e5f90775b4\", \"089d1831-7c40-4a31-bab8-0f64116c18b2\"]\n}\nEOF\n\n</code></pre> <p>RESPONSE</p> <pre><code>{\n  \"slos\": {\n    \"049b684b-4ef0-449e-8fad-33e5f90775b4\": {\n      \"id\": \"049b684b-4ef0-449e-8fad-33e5f90775b4\",\n      \"name\": \"List_bigger_than_10ms\",\n      \"serviceName\": \"productcatalogservice\",\n      \"description\": \"Check if the operation of list products is bigger than 10ms for 50% of the spans\",\n      \"targetPercentage\": 50,\n      \"createdAt\": \"1970-01-20T11:38:56.476Z\",\n      \"remainingErrorBudgetPercentage\": 100,\n      \"latencySli\": {\n        \"thresholdMicroseconds\": \"10000\",\n        \"thresholdSymbol\": \"THRESHOLD_SYMBOL_GREATER_OR_EQUAL\"\n      },\n      \"filters\": [\n        {\n          \"field\": \"operationname\",\n          \"compareType\": \"COMPARE_TYPE_IS\",\n          \"fieldValues\": [\n            \"hipstershop.ProductCatalogService/ListProducts\"\n          ]\n        }\n      ],\n      \"period\": \"SLO_PERIOD_7_DAYS\"\n    },\n    \"089d1831-7c40-4a31-bab8-0f64116c18b2\": {\n      \"id\": \"089d1831-7c40-4a31-bab8-0f64116c18b2\",\n      \"name\": \"Latency_bigger_then_1ms\",\n      \"serviceName\": \"frontend\",\n      \"description\": \"Check if latency is bigger then 1ms \",\n      \"targetPercentage\": 1,\n      \"createdAt\": \"1970-01-20T11:40:32.323Z\",\n      \"remainingErrorBudgetPercentage\": 100,\n      \"latencySli\": {\n        \"thresholdMicroseconds\": \"1000\",\n        \"thresholdSymbol\": \"THRESHOLD_SYMBOL_GREATER_OR_EQUAL\"\n      },\n      \"period\": \"SLO_PERIOD_7_DAYS\"\n    }\n  }\n}\n\n</code></pre>"},{"location":"newoutput/slo-management-api/#createserviceslo","title":"CreateServiceSlo","text":"<p>Creates a new SLO with given configuration for a chosen service. In this example, you are creating the\u00a0<code>frontendtest</code>SLO for a\u00a0<code>frontend</code>\u00a0service.</p> <p>REQUEST</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.apm.services.v1.ServiceSloService/CreateServiceSlo &lt;&lt;EOF\n{\n  \"slo\": {\n    \"name\": \"frontendtest\",\n    \"serviceName\": \"frontend\",\n    \"description\": \"Sample description for your SLO\",\n    \"targetPercentage\": 95,\n    \"errorSli\": {\n    },\n    \"period\": \"SLO_PERIOD_7_DAYS\"\n  }\n}\nEOF\n\n</code></pre> <p>RESPONSE</p> <pre><code>{\n  \"slo\": {\n    \"id\": \"0416396c-a1ad-46d8-a10c-a05ea15dc3af\",\n    \"name\": \"frontendtest\",\n    \"serviceName\": \"frontend\",\n    \"description\": \"Sample description for your SLO\",\n    \"targetPercentage\": 95,\n    \"createdAt\": \"1970-01-20T17:43:33.485Z\",\n    \"remainingErrorBudgetPercentage\": 100,\n    \"errorSli\": {},\n    \"period\": \"SLO_PERIOD_7_DAYS\"\n  }\n}\n\n</code></pre>"},{"location":"newoutput/slo-management-api/#replaceserviceslo","title":"ReplaceServiceSlo","text":"<p>Updates an SLO with new field entries for a chosen service. In this example, you are replacing the\u00a0<code>description</code>\u00a0content for a frontend service SLO. The SLO is supposed to check for errors in the frontend.</p> <p>REQUEST</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.apm.services.v1.ServiceSloService/ReplaceServiceSlo &lt;&lt;EOF\n{\n  \"slo\": {\n    \"id\": \"0416396c-a1ad-46d8-a10c-a05ea15dc3af\",\n    \"name\": \"frontendtest\",\n    \"serviceName\": \"frontend\",\n    \"description\": \"Checks for frontend errors.\",\n    \"targetPercentage\": 95,\n    \"errorSli\": {},\n    \"period\": \"SLO_PERIOD_7_DAYS\"\n  }\n}\nEOF\n\n</code></pre> <p>RESPONSE</p> <pre><code>\"slo\": {\n    \"id\": \"0416396c-a1ad-46d8-a10c-a05ea15dc3af\",\n    \"name\": \"frontendtest\",\n    \"serviceName\": \"frontend\",\n    \"description\": \"Checks for frontend errors.\",\n    \"targetPercentage\": 95,\n    \"createdAt\": \"1970-01-20T17:43:33.485Z\",\n    \"remainingErrorBudgetPercentage\": 100,\n    \"errorSli\": {},\n    \"period\": \"SLO_PERIOD_7_DAYS\"\n  }\n}\n\n</code></pre>"},{"location":"newoutput/slo-management-api/#deleteserviceslo","title":"DeleteServiceSlo","text":"<p>Deletes a specified SLO for a given service. In this example, you are deleting the\u00a0<code>frontendtest</code>\u00a0SLO from the\u00a0<code>frontend</code>service.</p> <p>REQUEST</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogixstg.wpengine.com:443 com.coralogixapis.apm.services.v1.ServiceSloService/DeleteServiceSlo &lt;&lt;EOF\n{\n    \"id\": \"0416396c-a1ad-46d8-a10c-a05ea15dc3af\"\n}\nEOF\n\n</code></pre> <p>RESPONSE</p> <pre><code>{}\n\n</code></pre>"},{"location":"newoutput/slo-management-api/#body-parameters","title":"Body Parameters","text":"FieldTypeDefinitionidint32Unique identifier for the Service Level Objective (SLO).idsarrayList of service SLO IDs to retrieve.slosarrayList of Service Level Objectives (SLOs) with their respectivedetails.namestringName of the SLO.serviceNamestringService name associated with the SLO.descriptionstringDescription of the SLO.targetPercentageintegerTarget percentage for the SLO.createdAttimestampTimestamp indicating when the SLO was created.remainingErrorBudgetPercentageintegerRemaining error budget percentage.errorSlibooleanIndicates the presence of an error SLI.latencySliarrayDefines SLI threshold and relationship.latencySli.thresholdMicrosecondsstringThreshold in microseconds for latency.latencySli.thresholdSymbolenumSymbol indicating the threshold relationship (e.g.,GREATER_OR_EQUAL).filters.fieldstringField for the filter.filters.compareTypeenumType of comparison (e.g., IS, START_WITH).filters.fieldValuesstringList of field values for the filter.periodenumPeriod for the SLO (e.g., 7 days, 14 days). <p>CompareType</p> NameNumberDescriptionCOMPARE_TYPE_UNSPECIFIED0Filter entry is unspecifiedCOMPARE_TYPE_IS1Filters for a specific entry.COMPARE_TYPE_START_WITH2Filters for results that start with the entry.COMPARE_TYPE_ENDS_WITH3Filters for results that end with the entry.COMPARE_TYPE_INCLUDES4Filters for a result that includes the entry. <p>ThresholdSymbol</p> NameNumberDescriptionTHRESHOLD_SYMBOL_UNSPECIFIED0Threshold criterion is not given.THRESHOLD_SYMBOL_GREATER1Threshold criterion is greater than the specified number.THRESHOLD_SYMBOL_GREATER_OR_EQUAL2Threshold criterion is greater than or equal to the specified number.THRESHOLD_SYMBOL_LESS3Threshold criterion is less than the specified number.THRESHOLD_SYMBOL_LESS_OR_EQUAL4Threshold criterion is less than or equal to the specified number.THRESHOLD_SYMBOL_EQUAL5Threshold criterion is equal to the specified number.THRESHOLD_SYMBOL_NOT_EQUAL6Threshold criterion is not equal to the specified number. <p>SloPeriod</p> NameNumberDescriptionSLO_PERIOD_UNSPECIFIED0Time period for which the SLO measures results is unspecified.SLO_PERIOD_7_DAYS1Time period for which the SLO measures results is 7 days.SLO_PERIOD_14_DAYS2Time period for which the SLO measures results is 14 days.SLO_PERIOD_30_DAYS3Time period for which the SLO measures results is 30 days."},{"location":"newoutput/snyk-vulnerability-monitoring-with-coralogix/","title":"Snyk Vulnerability Monitoring with Coralogix","text":"<p>This tutorial demonstrates how to conduct Snyk vulnerability monitoring with Coralogix by exporting Snyk's security testing data using Prometheus.</p>"},{"location":"newoutput/snyk-vulnerability-monitoring-with-coralogix/#overview","title":"Overview","text":"<p>This Docker was designed by the LunarTech team and was written in Go. This exporter enables you to automatically export Snyk\u2019s security testing data into Prometheus by scraping\u00a0Snyk\u2019s API.</p>"},{"location":"newoutput/snyk-vulnerability-monitoring-with-coralogix/#prerequisites","title":"Prerequisites","text":"<ul> <li>Active Snyk account</li> </ul>"},{"location":"newoutput/snyk-vulnerability-monitoring-with-coralogix/#snyk-setup","title":"Snyk Setup","text":"<p>You are required to obtain your Organization ID and Organization API key in your Synk UI.</p> <p>To obtain your Organization ID:</p> <p>STEP 1. In your left-hand sidebar, select your Synk Group.</p> <p>STEP 2. Select your Snyk Organization.</p> <p>STEP 3. Navigate to Settings.</p> <p>STEP 4. Copy your Organization ID.</p> <p></p> <p>To access your Organization API key:</p> <p>STEP 5. Complete STEPS 1-4 above, then click Manage service accounts.</p> <p>STEP 6. Create a new service account and token for Coralogix.</p> <ul> <li> <p>Input Name &amp; Role.</p> </li> <li> <p>Click Create.</p> </li> </ul> <p></p> <p>STEP 7. Copy the Organization API key.</p>"},{"location":"newoutput/snyk-vulnerability-monitoring-with-coralogix/#configuration","title":"Configuration","text":"<p>STEP 1. Run the docker command with the following arguments:</p> <pre><code>docker run -p9532:9532 [quay.io/lunarway/snyk_exporter](&lt;http://quay.io/lunarway/snyk_exporter&gt;) --snyk.api-token 'API TOKEN' --log.level=\"info\" --snyk.organization='ORG ID\u2019\n\n</code></pre> <p>The output should appear as follows:</p> <pre><code>time=\"2023-04-25T18:45:37Z\" level=info msg=\"Starting Snyk exporter for organization '62b64ba1-aab2-4409-a1cb-6f94b3f93977'\" source=\"main.go:67\"\ntime=\"2023-04-25T18:45:37Z\" level=info msg=\"Listening on :9532\" source=\"main.go:111\"\ntime=\"2023-04-25T18:45:37Z\" level=info msg=\"Snyk API scraper starting\" source=\"main.go:140\"\ntime=\"2023-04-25T18:45:37Z\" level=info msg=\"Running Snyk API scraper for organizations: Financial Applications\" source=\"main.go:174\"\ntime=\"2023-04-25T18:45:37Z\" level=info msg=\"Collecting for organization 'Financial Applications'\" source=\"main.go:196\"\n\n</code></pre> <p>The Docker output should appear as follows:</p> <pre><code>time=\"2023-04-25T18:52:54Z\" level=info msg=\"Recorded 585 results for organization 'Financial Applications'\" source=\"main.go:205\"\ntime=\"2023-04-25T18:52:54Z\" level=info msg=\"Exposing 585 results as metrics\" source=\"main.go:216\"\n\n</code></pre> <p>STEP 2. Update Prometheus RemoteWrite by following these instructions.</p> <pre><code>remote_write:\n- url: &lt;endpoint&gt;\n  name: '&lt;customer_name&gt;'\n  remote_timeout: 120s\n  bearer_token: '&lt;Send_Your_Data_private_key&gt;'\n\n</code></pre> <ul> <li> <p><code>url</code>: Select a\u00a0Prometheus RemoteWrite endpoint URL\u00a0for the domain associated with your Coralogix account</p> </li> <li> <p><code>name</code>: Name of the timeseries</p> </li> <li> <p><code>bearerToken</code>: Your Coralogix Send-Your-Data API key</p> </li> </ul>"},{"location":"newoutput/snyk-vulnerability-monitoring-with-coralogix/#validation","title":"Validation","text":"<p>STEP 1. Access the Coralogix Grafana instance to ensure metrics are being published and scraped. Query the snyk_vulnerabilities_total metric.</p> <p>STEP 2. Install the Grafana Snyk dashboard to obtain an overview of the data.</p> <p></p>"},{"location":"newoutput/snyk-vulnerability-monitoring-with-coralogix/#additional-resources","title":"Additional Resources","text":"DocumentationPrometheus"},{"location":"newoutput/snyk-vulnerability-monitoring-with-coralogix/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/software-builds-display/","title":"Version Benchmarks","text":"<p>Version Benchmarks are the best way for you to understand your version status at a glance, integrate with your deployment pipeline, and get your latest build status. Whether it\u2019s new exceptions, higher error ratios, or new broken flows - we've got you covered.</p> <p>Version Benchmark tags can be used as a representation of any significant change or event that may impact your system. They can be received automatically from your CI/CD pipelines or inserted manually, they can be a new version, a new big customer, or a marker of the day when your dev team had sangria at lunch.\u00a0</p>"},{"location":"newoutput/software-builds-display/#dashboard","title":"Dashboard","text":"<p>In your Coralogix UI, navigate to Dashboard &gt; Version Benchmarks. Integrate into your pipeline or manually add an event tag and get a full status in the context of your event.</p>"},{"location":"newoutput/software-builds-display/#overview","title":"Overview","text":"<p>View the change\u00a0in triggered anomalies, alerts, error volume, newly introduced errors, and high severity logs ratio since the tag time. Compare this to the corresponding timeframe in the comparison tag.</p> <p></p>"},{"location":"newoutput/software-builds-display/#error-volume","title":"Error Volume","text":"<p>View the number of errors since the tag in comparison to the period prior to the tag.</p> <p></p>"},{"location":"newoutput/software-builds-display/#alert-volume","title":"Alert Volume","text":"<p>View the number of alerts since the tag, grouped by alert name and compared to the previous period.</p> <p></p>"},{"location":"newoutput/software-builds-display/#error-anomalies","title":"Error Anomalies","text":"<p>View the errors that arrived in numbers greater than their normal ratio since the tag.</p> <p></p>"},{"location":"newoutput/software-builds-display/#newly-introduced-logs","title":"Newly Introduced Logs","text":"<p>Here you'll see logs that appeared on your system for the first time after the tag/build.</p> <p></p>"},{"location":"newoutput/software-builds-display/#custom-widgets","title":"Custom Widgets","text":"<p>View pre-defined Coralogix visualizations based on your own version quality criteria. Learn more about widgets here.</p>"},{"location":"newoutput/software-builds-display/#top-error-templates","title":"Top Error Templates","text":"<p>View the top errors since the tag (maximum appearances).</p> <p></p>"},{"location":"newoutput/software-builds-display/#deployment-pipelines","title":"Deployment Pipelines","text":"<p>Integrate with any of the following deployment pipelines.</p> <ul> <li> <p>Bitbucket</p> </li> <li> <p>Azure DevOps Server</p> </li> <li> <p>GitLab</p> </li> <li> <p>Argo CD</p> </li> <li> <p>Spinnaker</p> </li> <li> <p>Heroku Pipelines</p> </li> <li> <p>Version Tags using cURL </p> </li> </ul>"},{"location":"newoutput/software-builds-display/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/spinnaker-version-tags/","title":"Spinnaker Version Tags","text":"<p>Coralogix provides seamless integration with\u00a0<code>Spinnaker</code>\u00a0deployment pipelines so you can push tags to Coralogix automatically from your pipelines.</p>"},{"location":"newoutput/spinnaker-version-tags/#prerequisites","title":"Prerequisites","text":"<ul> <li>Have\u00a0<code>Spinnaker</code>\u00a0deployed, for more information on how to deploy:\u00a0https://spinnaker.io/setup/</li> <li>API Token - should be taken from <code>Data\u00a0Flow\u00a0--&gt;\u00a0API\u00a0Keys\u00a0--&gt;\u00a0Alerts,\u00a0Rules\u00a0and\u00a0Tags\u00a0API\u00a0Key</code></li> </ul>"},{"location":"newoutput/spinnaker-version-tags/#configuration","title":"Configuration","text":"<p>To create a custom webhook stage, you\u2019ll need to add configuration for the stage in\u00a0<code>orca-local.yml</code>.</p> <p>Here is the content of orca-local.yml:</p> <pre><code>webhook:\n  preconfigured:\n    - label: Coralogix Tag\n      type: coralogixTag\n      enabled: true\n      description: Push tag to Coralogix\n      parameters:\n        - label: Coralogix API URL\n          name: url\n          type: string\n          description: Coralogix API endpoint\n          defaultValue: coralogixstg.wpengine.com\n        - label: Coralogix API Token\n          name: token\n          type: string\n          description: Coralogix API Token\n        - label: Tag\n          name: tag\n          type: string\n          description: Tag Name\n        - label: Application\n          name: application\n          type: string\n          description: Application Name\n        - label: Subsystem\n          name: subsystem\n          type: string\n          description: Subsystem Name\n      url: https://webapi.${parameterValues['url']}/api/v1/external/tags\n      method: POST\n      customHeaders:\n      Authorization:\n        - Bearer ${parameterValues['token']}\n      Content-Type:\n        - application/json\n      payload: |-\n        {\n            \"name\": \"${parameterValues['tag']}\",\n            \"application\": [\"${parameterValues['application']}\"],\n            \"subsystem\": [\"${parameterValues['subsystem']}\"],\n            \"iconUrl\": \"https://raw.githubusercontent.com/coralogix/integrations-docs/master/integrations/spinnaker/images/spinnaker.png\"\n        }\n</code></pre> <p>After you create a custom configuration, redeploy your <code>Spinnaker</code>\u00a0instance with\u00a0<code>Halyard</code>:</p> <pre><code>$ hal deploy apply\n</code></pre>"},{"location":"newoutput/spinnaker-version-tags/#usage","title":"Usage","text":"<p>Add a new stage to push the tag to Coralogix:</p> <p></p> <p>Configure stage to push tag to your account:</p> <p></p> <p>Note:\u00a0You can use Spinnaker\u00a0pipeline expressions to define parameters for the tag.</p>"},{"location":"newoutput/sta-capturing-network-traffic-into-cloud-storage/","title":"STA: Capturing Network Traffic into Cloud Storage","text":"<p>Take advantage of the Coralogix STA to store packets captured by the feature as compressed PCAP files, available for after-the-fact investigations of your data.</p>"},{"location":"newoutput/sta-capturing-network-traffic-into-cloud-storage/#overview","title":"Overview","text":"<p>Coralogix Security Traffic Analyzer (STA) can store compressed PCAP files containing captured packets. This feature proves indispensable for conducting comprehensive post-event investigations. During deployment, users can designate storage destinations, such as an S3 bucket in AWS, facilitating the seamless upload of captured packets to the specified repository.</p> <p>The significance of capturing traffic lies in its diverse applications, including the ability to monitor and track suspicious network activities. For example, users can pinpoint potential threats by scrutinizing specific database activities indicated by port numbers. Moreover, the meticulous tracking of network traffic addresses regulatory and auditory compliance requirements.</p> <p>This tutorial guides you through setting up the Coralogix STA to capture and store packets effectively.</p>"},{"location":"newoutput/sta-capturing-network-traffic-into-cloud-storage/#configuration","title":"Configuration","text":""},{"location":"newoutput/sta-capturing-network-traffic-into-cloud-storage/#upon-deployment","title":"Upon Deployment","text":"<p>The following examples specify storage for the packets in a Terraform installation:</p> <p>Azure</p> <pre><code>...\nAzure-StorageAccountPackets   = \"stapacketdemo\"\nAzure-StorageContainerPackets = \"containerpakcetdemo\"\n...\n</code></pre> <p>AWS</p> <pre><code>...\nSTA-PacketsS3BucketRequired         = false\nSTA-PacketsS3Bucket                 = \"\"\n...\n</code></pre> <p>Notes:</p> <ul> <li> <p>If the STA-PacketsS3BucketRequired variable is set to true, the provided bucket will be used to save the network packets.</p> </li> <li> <p>The stack will create one if no bucket is provided in the PacketsS3Bucket variable.</p> </li> </ul>"},{"location":"newoutput/sta-capturing-network-traffic-into-cloud-storage/#on-a-running-sta","title":"On a Running STA","text":"<p>You may configure the storage after STA installation by adding the storage name to the sta.conf file, as follows:</p> <pre><code>\"upload_pcaps_to\": \"https://stapacketdemo.blob.core.windows.net/containerpakcetdemo\"\n</code></pre>"},{"location":"newoutput/sta-capturing-network-traffic-into-cloud-storage/#filtering-the-traffic","title":"Filtering the Traffic","text":"<p>Due to the large volume of traffic usually observed by the STA, it is recommended to utilize a filter pre-capture. This is done by editing and uploading the sta.conf file.</p> <p>The STA uses the BPF filter method to sniff traffic matching the provided filter.</p> <p>To add a BPF filter, add the field named <code>capturing_bpf_filter</code> with the wanted filter. Find out more here.</p> <p>The following example presents a part of a sta.conf file, when using a storage bucket to capture only port 80:</p> <pre><code>...\n\"upload_pcaps_to\": \"https://stapacketdemo.blob.core.windows.net/containerpakcetdemo\",\n\"sync_config_from\": \"https://stastoragedemo.blob.core.windows.net/stacontainerdemo\",\n\"capturing_bpf_filter\": \"port 80\",\n...\n</code></pre> <p>Notes:</p> <ul> <li> <p>The above filter allows the STA to sniff packets with source or destination port 80 and upload them to the storage bucket specified for the packets.</p> </li> <li> <p>After editing and reuploading the sta.conf file, run the following command:</p> </li> </ul> <pre><code>sta-force-sync-configs\n</code></pre>"},{"location":"newoutput/sta-capturing-network-traffic-into-cloud-storage/#accessing-the-files","title":"Accessing the Files","text":"<p>Once the storage is configured to store the packets, they will be uploaded as zipped PCAP files.</p> <p></p> <p>After applying the filter, the traffic will be captured accordingly (using Wireshark):</p> <p></p> <p>Notes:</p> <ul> <li> <p>Ensure the file format validation of the configuration file. If the file is in JSON format, ensure it is valid before uploading it back to the STA.</p> </li> <li> <p>To stop capturing traffic, leave the field with an empty value: <code>\"upload_pcaps_to\": \"\"</code>.</p> </li> <li> <p>To stop filtering and continue capturing all traffic, leave the filter field with an empty value: <code>\"capturing_bpf_filter\": \"\"</code>.</p> </li> </ul>"},{"location":"newoutput/sta-capturing-network-traffic-into-cloud-storage/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/sta-custom-enrichment/","title":"Auto-Generated Custom Enrichments Service","text":"<p>The Coralogix Security Traffic Analyzer (STA) collects data using internal enrichment services, including AWS, Kubernetes, and / or geographical information.</p> <p>It sends those enrichments to Coralogix as CSV files per service, allowing you to enrich any of your logs with additional enriched fields, even if the source is not STA!</p> <p>Connect all types of logs sent to Coralogix\u2019s platform and improve readability, deepen your understanding of your environments, and reduce investigation time dramatically over large content of logs from different sources.</p> <p>All custom enrichment files can be found on Coralogix\u2019s platform under <code>Data Flow -&gt; Data Enrichment</code> section.</p>"},{"location":"newoutput/sta-custom-enrichment/#enrichment-types","title":"Enrichment Types","text":"<p>STA collects enrichments using several services which are running in the background.</p> <p>To see the full list of STA\u2019s enrichment modules, run the following command inside STA: <code>sta-get-status-short</code></p> <p>In the console, you\u2019ll find all running services inside STA, and their current status. See the example below:</p> <p></p> <p>All rows with the prefix: <code>coralogix.sta.enrichment</code> represent different services that handle STA's enrichments.</p> <p>Currently from the example above we can see the following services:</p> <ol> <li> <p><code>domains-tld-extract</code></p> </li> <li> <p><code>domain-stats</code></p> </li> <li> <p><code>dns-rbls</code></p> </li> <li> <p><code>unshorten-url</code></p> </li> <li> <p><code>domains-similarity</code></p> </li> <li> <p><code>freq-server</code></p> </li> <li> <p><code>geo</code></p> </li> <li> <p><code>nist-cpe</code></p> </li> <li> <p><code>aws-context</code></p> </li> <li> <p><code>k8s-context</code></p> </li> <li> <p><code>stats-info</code></p> </li> </ol> <p>Note: We are constantly improving and extending the STA, and more services might be added/modified.</p>"},{"location":"newoutput/sta-custom-enrichment/#configuration","title":"Configuration","text":"<p>In order to enable this service, an <code>Alerts API key</code> must be provided. To find the key, please head to <code>Data Flow -&gt; API Keys</code>.</p> <p></p> <p>There you'll find the relevant key under the name <code>Alerts, Rules and Tags API Key</code>.</p>   ![](images/image-22-1024x472.png)    copy alerts API key   <p>After you've copied the key, head to your <code>sta.conf</code> file that can be configured locally in STA or using Amazon S3 bucket, and search for <code>\"coralogix\"</code> JSON key (you'll see that you already provided there your <code>\"private_key\"</code>). Add the following:  </p> <pre><code>{\n    \"coralogix\": {\n        ...you will find here your private key specification and other params...,\n        \"coralogix_alerts_api_key\": \"&lt;YOUR_COPIED_ALERTS_API_KEY&gt;\",\n    }\n}\n</code></pre>"},{"location":"newoutput/sta-custom-enrichment/#structure","title":"Structure","text":"<p>The service configuration is also located in <code>sta.conf</code> with the following structure:</p> <pre><code>{\n    \"sensors\": {\n        \"custom_enrichment_producer\": {\n            \"log_level\": \"TRACE\" | \"DEBUG\" | \"INFO\" | \"WARN\" | \"ERROR\",\n            \"file_modification_minutes_threshold\": int,\n            \"max_csv_rows\": int,\n            \"enable\": bool,\n            \"customized_services\": [\n                {\n                    \"name\": str,\n                    \"headers\": [str]\n                }\n            ],\n            \"disabled_for_services\": [str],\n            \"other_services_enabled\": bool,\n        }\n    }\n}\n</code></pre>"},{"location":"newoutput/sta-custom-enrichment/#default-values","title":"Default Values","text":"<pre><code>log_level: INFO\n\nfile_modification_minutes_threshold: 10\n\nmax_csv_rows: 9000\n\nenable: true\n\ncustomized_services: []\n\ndisabled_for_services: []\n\nother_services_enabled: true\n</code></pre>"},{"location":"newoutput/sta-custom-enrichment/#variables-explanation","title":"Variables explanation","text":"NameTypeConstraintsDescription<code>log_level</code><code>Predefined string</code>Possible values:\u00a0 <code>\"TRACE\"</code>, <code>\"DEBUG\"</code>,\u00a0 <code>\"INFO\"</code>, <code>\"WARN\"</code>, <code>\"ERROR\"</code>The lowest log level to be shown in the console<code>file_modification_minutes_threshold</code><code>int</code><code>1 \u2264 x \u2264 60</code>Number of minutes threshold of idleness before sending enrichments to Coralogix. This can happen when no new enrichments are found by the STA in a given period of time.<code>max_csv_rows</code><code>int</code><code>2 \u2264 x \u2264 10000</code>Number of lines threshold per CSV file. if this threshold is reached, the file is sent to Coralogix, and a new file is created afterward.<code>enable</code><code>boolean</code>enable/disable service. By default set to true.<code>customized_services</code><code>Array of objects</code>Please see 2 rows below for object representationLeave an empty array to include all services without specific configuration.<code>customized_services.name</code><code>str</code>Predefined service names. Please see the section \u201cEnrichment Types\u201d.Names of the services to enable, if only the names are specified, only those services will be enabled. See \u201cheaders\u201d below for additional configuration.<code>customized_services.headers</code><code>Array of strings</code>Be sure that you know what headers you want as once you specify, only those will be searched, and others will be dropped.Representing the headers extracted into the service\u2019s enrichment CSV - and only those. Leave an empty array to include all possible headers.<code>disabled_for_services</code><code>Array of strings</code>Predefined service names. Please see the section \u201cEnrichment Types\u201d.Defines what services to exclude from CSV files.<code>other_services_enabled</code><code>boolean</code>Enable other services that are not specified in the variable: <code>customized_services</code>. Find additional information and use cases below."},{"location":"newoutput/sta-custom-enrichment/#use-cases","title":"Use-Cases","text":"<p>For a better understanding of how those variables interact with STA, let\u2019s see some use cases below.</p> <p>Let\u2019s assume that we want to disable <code>aws-cotext</code> enrichment service from sending CSVs to Coralogix. Our configuration should be as followed:</p> <pre><code>{\n    \"sensors\": {\n        \"custom_enrichment_producer\": {\n            \"disabled_for_services\": [\"aws-context\"],\n        }\n    }\n}\n</code></pre> <p>As mentioned, the other values will receive their default values and only the specified service will be disabled.</p> <p>Now let's assume that we want to configure <code>geo</code> with specific headers: <code>country</code>, <code>zone</code>, <code>coordinate.x</code>, <code>coordinate.y</code>. In addition, we want to enable only <code>aws-context</code> and <code>k8s-context</code> services without configuration. Our configuration should be as followed:</p> <pre><code>{\n    \"sensors\": {\n        \"custom_enrichment_producer\": {\n          \"customized_services\": [\n            {\n              \"name\":\"geo\",\n              \"headers\": [\"country\",\"zone\",\".x\", \"coordinate.y\"]\n            },\n            {\n              \"name\":\"aws-context\",\n              \"headers\": []\n            },\n            {\n              \"name\":\"k8s-context\",\n              \"headers\": []\n            }\n          ],\n        \"disabled_for_services\": [],\n        \"other_services_enabled\": false,\n        }\n    }\n}\n</code></pre> <p>Finally, let's assume again that we want to configure <code>geo</code> with specific headers: <code>country</code>, <code>zone</code>, <code>coordinate.x</code>, <code>coordinate.y</code>. In addition, we want all other services to enrich without configuration except the service <code>nist-cpe</code> which should be disabled. Our configuration should be as followed:</p> <pre><code>{\n   \"sensors\": {\n       \"custom_enrichment_producer\": {\n          \"customized_services\": [\n            {\n              \"name\":\"geo\",\n              \"headers\": [\"country\",\"zone\",\"coordinate.x\", \"coordinate.y\"]\n            }\n          ],\n          \"disabled_for_services\": [\"nist-cpe\"],\n          \"other_services_enabled\": true,\n       }\n   }\n}\n</code></pre>"},{"location":"newoutput/sta-detection-encrypted-traffic/","title":"STA Detection for Encrypted Traffic","text":"<p>Today, one of the biggest challenges a Cyber security professional have to deal with is that, according to Fortinet, for example, the total percentage of encrypted web traffic is now around 85%. More websites are encrypting their traffic by using the TLS protocol, servers are no longer managed in clear text telnet sessions but rather in secure SSH sessions, even emails are now being transmitted in SMTPS sessions. This trend is obviously good for keeping our private information private but it also allows attackers to hide their tracks and makes it harder for blue teams to detect threats in the traffic.</p> <p>You can use many different services for handling SSL termination and then send decrypted traffic to Coralogix for analysis or even configure the Coralogix STA to decrypt the data by using your Send-Your-Data API key. But even if you don\u2019t, the Coralogix STA can detect lots of things about encrypted traffic without having to decrypt it first.</p> <p>The most basic thing is, of course, the connection itself. There are servers and services that do not need any Internet access, for those, you can create a simple alert rule that will fire whenever any of them is trying to access the Internet.</p> <p>Also, since most of the encrypted communications will first query the DNS for the correct IP address, forwarding the DNS traffic to Coralogix can greatly improve its effectiveness by creating dashboards and alerts based on the DNS traffic as well.</p> <p>The following is a partial list of what Coralogix can detect in encrypted traffic.</p> Protocol Detectable from Encrypted Traffic DNS/TLS/SSL NLP based score - For every domain queried or found in a certificate, the Coralogix STA will calculate a score based on NLP to help determine if the domain was machine generated. Domain creation date - For every domain queried or found in a certificate, the Coralogix STA will attempt to retrieve its creation date. Domains that are very young are often associated with attack campaigns New public DNS server used - Normally, instances in the LAN should use a local DNS server unless they are a DNS server themselves. Based on that information it is possible to detect direct access from LAN instances to unauthorized DNS servers Non-DNS traffic on DNS ports - A Suricata signature can detect cases in which non DNS traffic was sent over DNS port under the assumption that DNS ports are open in most organizations. TLS/SSL (HTTPS, FTPS, SMTPS\u2026) Trend &amp; Rate \u2013 The amount of connections and bytes that were transferred over time. This can be used to detect anomalies TLS/SSL Version \u2013 Access to public services that were expected to be secure and are detected as using an old version of TLS can indicate that they were compromised or that an attacker is trying to cause the server and client to use a weaker type of encryption to allow him/her to eavesdrop the traffic. An attack known as SSL Stripping. Source &amp; Destination Countries \u2013 Based on IP to GeoLocation enrichment we can detect the geographic location of the source and destination servers Certificate Details \u2013 Since the certificate that is being used by the server is not encrypted, the STA will analyze it and provide details such as Common Name, Issuer Common Name, Issuer Country, Server name, TLS/SSL version, certificate validation status, Certificate chain length, Certificate cipher details Client Fingerprinting - We use JA3/JA3s algorithms to generate fingerprints that can accurately detect the software used to access or respond to SSL/TLS requests. SSH Trend &amp; Rate \u2013 The amount of connections and bytes that were transferred over time. This can be used to detect anomalies Server and client software types \u2013 In the case of outbound SSH connections from your organization to the internet, an attempt that should be considered suspicious, the client software can provide an indication of the malware and even of the attacking group. In the case of connections to your server, the client software type can indicate whether it\u2019s a legitimate client or not. Source &amp; Destination Countries \u2013 Based on IP to GeoLocation enrichment we can detect the geographic location of the source and destination servers Cipher Algorithm \u2013 The cipher algorithm used in this SSH session. This can indicate that one of your servers is using an unauthorized or non standard cipher and by that becoming vulnerable Number of observed authentication attempts \u2013 The number of authentication attempts that were observed by the server. Not all of them necessarily indicate failures but if you see a large number here that probably means that someone is trying to guess your SSH password Client Fingerprinting - We use HASSH algorithms to generate fingerprints that can accurately detect the software used to access or respond to SSH"},{"location":"newoutput/sta-whats-in-the-box/","title":"STA - What's in the box?","text":"<p>We are glad to know that you are joining our many clients who have already installed the STA. Now, you would like to know which components are installed as part of the installation of the STA and the architecture of the solution. If so, you came to the right place.</p>"},{"location":"newoutput/sta-whats-in-the-box/#structure","title":"Structure","text":"<p>As you have seen in the article \"How to install Coralogix STA\", there are multiple ways to install the STA which we will touch soon but first, let's take a closer look at the STA instance itself:</p> <p></p> <p>The STA's instance, listens for incoming traffic on its first two network interfaces (eth0 and eth1) but will never respond to those packets since no daemon is configured to listen on these NICs. The traffic from these network interfaces is then analyzed by Zeek and Suricata and optionally copied to the packets S3 bucket if specified during the installation. The data from Zeek and Suricata is then enriched automatically by several services within the STA and eventually - shipped to the Coralogix account specified during the installation.</p> <p>Some of the enrichment services require Internet access on port 80 (RDAP) and port 443 (NIST, ET Rules updates and Coralogix connection) and the ability to perform DNS requests to several servers (RDAP, DNSRBLs, NIST, ET Rules updates and Coralogix connection).</p> <p>The STA's cloud installation (either AWS CloudFormation or Terraform) will also install the following components to support the STA and facilitate its maintenance and integration:</p> <ol> <li> <p>Load balancers: An NLB will be installed and connected to the first network interface of the STA as its target and to a VPC mirror target as its source. If you chose to install Wazuh, another NLB will be created for you and will be connected to the third network interface for Wazuh ports.</p> </li> <li> <p>Elastic IP: If you chose to install an elastic IP, one will be created and attached to the third network interface.</p> </li> <li> <p>VPC Mirror Filter: A default mirror filter that mirrors all traffic will be created for you. This has no effect on your system unless you choose to use it in a mirror session.</p> </li> <li> <p>VPC Mirror Target: A mirror target pointing to the sniffing NLB will be created for you to facilitate the creation of mirror sessions later.</p> </li> <li> <p>Security Group: A security group allowing the mirrored traffic to pass through will be created and attached to the first two network interfaces.</p> </li> <li> <p>EC2 Instance: If you chose to run the STA as an on-demand instance then an EC2 instance of the type you selected will be launched with the STA image.</p> </li> <li> <p>Spot Fleet: If you chose to run the STA as a spot-fleet, such a spot fleet will be created and configured to automatically select an available instance out of a list of about 7 instance types similar to the one selected during the installation</p> </li> <li> <p>Launch Template: If you chose to run the STA as a spot instance as part of a Terraform template, a Launch Template will be created as part of the installation</p> </li> </ol>"},{"location":"newoutput/sta-whats-in-the-box/#next-steps","title":"Next Steps","text":"<p>Now that you know what each component of the STA is for and what it is doing, the next step can be to understand the default set of alerts by reading Security Traffic Analyzer (STA) Alerts or to learn how to modify a Suricata rule by reading How to Modify an STA Suricata Rule or, if you already know that, you will probably find the article about Writing Effective Suricata Rules with Examples [Best Practices] helpful.</p> <p>As always, please do not hesitate to contact us through the chat with any issue you may have.</p>"},{"location":"newoutput/statsd/","title":"StatsD","text":"<p>StatsD is an open-source standard and, by extension, a toolkit designed for sending, collecting, and aggregating custom metrics from diverse applications. Initially, StatsD denoted a daemon crafted by Etsy using Node.js.</p> <p>This tutorial demonstrates installing and running StatsD using OpenTelemetry to send your metrics to Coralogix.</p>"},{"location":"newoutput/statsd/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Otel-contrib installed</p> </li> <li> <p>Coralogix account</p> </li> </ul>"},{"location":"newoutput/statsd/#deploy-using-otel","title":"Deploy using Otel","text":"<p>STEP 1. Save this config file as <code>config.yaml</code>:</p> <pre><code>receivers:\n  statsd:\n    endpoint: \"localhost:8125\"\n\nprocessors:\n  batch:\n    send_batch_size: 1024\n    send_batch_max_size: 2048\n    timeout: \"1s\"\n\nexporters:\n  coralogix:\n    domain: \"&lt;coralogix_domain&gt;\"\n    private_key: \"&lt;private_key&gt;\"\n    application_name: \"applicationName\"\n    subsystem_name: \"subsystemName\"\n    timeout: 30s\n\nservice:\n  pipelines:\n    metrics:\n      receivers: [ statsd ]\n      processors: [ batch ] \n      exporters: [ coralogix ]\n\n</code></pre> <p>STEP 2. Replace the <code>private_key</code> with your Send-Your-Data API key, and the <code>coralogix_domain</code> with your Coralogix domain.</p> <p>STEP 3. Run the config file.</p> <pre><code>otelcol-contrib --config config.yaml\n\n</code></pre>"},{"location":"newoutput/statsd/#deploy-using-docker","title":"Deploy using Docker","text":"<p>STEP 1. To deploy StatsD using Docker, change the endpoint to <code>0.0.0.0:8125</code> in the <code>config.yaml</code> file.</p> <pre><code>receivers:\n  statsd:\n    endpoint: \"0.0.0.0:8125\"\n\n</code></pre> <p>STEP 2. Run these commands to deploy the configuration:</p> <pre><code>docker pull otel/opentelemetry-collector-contrib\ndocker run -d -v ./config.yaml:/etc/otelcol-contrib/config.yaml -p 8125:8125/udp otel/opentelemetry-collector-contrib\n</code></pre>"},{"location":"newoutput/statsd/#validation","title":"Validation","text":"<p>STEP 1. To send test metrics, run the following command to test the configuration:</p> <pre><code>echo \"test_metrics:65|c|#tag_1:value,tag_2:value_2\" | nc -u -w1 127.0.0.1 8125\necho \"test_metrics:65|g|#tag_1:value,tag_2:value_2\" | nc -u -w1 127.0.0.1 8125\n\n</code></pre> <p>View more configuration options for the StatsD receiver here.</p> <p>STEP 2. Access your Grafana account to view the metrics sent.</p> <p></p>"},{"location":"newoutput/statsd/#support","title":"Support","text":"<p>Need help?</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogix.com.</p>"},{"location":"newoutput/statuspage-logs/","title":"StatusPage Data Ingestion","text":"<p>Monitor your Statuspage RSS feed in the Coralogix platform using our automatic Contextual Data Integration Package.</p>"},{"location":"newoutput/statuspage-logs/#overview","title":"Overview","text":"<p>Statuspage is a communication platform that enables businesses to maintain transparent and real-time communication with their customers, partners, and internal teams during incidents or disruptions. It allows organizations to create a dedicated status page where they can provide updates on the status of their services, applications, and systems. With customizable incident templates, automated notifications, and integration with various monitoring tools, Statuspage helps organizations manage incidents effectively, minimize customer impact, and build trust by delivering timely and accurate information about service availability and performance.</p> <p>Sending the Statuspage feed to Coralogix facilitates centralized incident monitoring, analysis, and streamlined communication. By directing Statuspage updates into Coralogix's log management platform, organizations can consolidate incident-related information, gain insights into the impact of service disruptions, and correlate these events with other operational data. This integration empowers teams to enhance incident response, analyze patterns, and improve communication by leveraging Coralogix's log analysis and visualization tools to extract valuable insights from Statuspage feeds, ultimately leading to improved service reliability, customer satisfaction, and operational resilience.</p>"},{"location":"newoutput/statuspage-logs/#get-started","title":"Get Started","text":"<p>STEP 1. Go to the status page which you want to monitor. Example: https://metastatuspage.com/#</p> <p>STEP 2. Click on the SUBSCRIBE TO UPDATES button.</p> <p>STEP 3. Click on the RSS icon.</p> <p>STEP 4. Copy the URL of the link under the text Atom Feed.</p> <p></p> <p>STEP 5.\u00a0On your Coralogix toolbar, click\u00a0Data Flow\u00a0&gt;\u00a0Contextual Data.</p> <p>STEP 6.\u00a0In the Contextual Data section, select Statuspage and click\u00a0+ ADD.</p> <p></p> <p>STEP 7. Click ADD NEW.</p> <p>STEP 8.\u00a0Fill in the Integration Details:</p> <ul> <li> <p>Name.\u00a0Name your integration.</p> </li> <li> <p>RSS Feed URL. Enter the URL for your Statuspage RSS feed that you copied above.</p> </li> </ul> <p></p> <p>STEP 9.\u00a0Click\u00a0SAVE.</p> <p></p>"},{"location":"newoutput/statuspage-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/surf/","title":"SURF","text":"<p>Seamlessly integrate your SURF logs with Coralogix.</p>"},{"location":"newoutput/surf/#overview","title":"Overview","text":"<p>SURF empowers users with unparalleled flexibility in their work, allowing secure interaction with applications, data, and peers. Addressing a long-overlooked business asset, the browser, SURF Security transforms it into a robust security layer, ensuring complete end-user privacy with full compliance. Through an identity-first approach, SURF grants secure and seamless access to all SaaS and corporate assets via a centralized platform, enhancing security, performance, and productivity for end-users and applications.</p> <p>The integration process involves configuring SURF to transmit events, logs, and application administrative changes to Coralogix.</p>"},{"location":"newoutput/surf/#prerequisites","title":"Prerequisites","text":"<ul> <li>Admin access to your SURF account</li> </ul>"},{"location":"newoutput/surf/#surf-admin-configuration","title":"SURF Admin Configuration","text":"<p>STEP 1. Navigate to the SURF admin console.</p> <p>STEP 2. From your toolbar, navigate to Settings &gt; Integrations.</p> <p>STEP 3. Search for Coralogix and add the integration by clicking the gear icon on the right.</p> <p></p> <p>STEP 4. Update the Token with your Coralogix Send-Your-Data-API-key and the URL with your Coralogix domain.</p> <p>STEP 5. Select all the relevant checkboxes for the data to be sent. Click Submit.</p> <p></p> <p>STEP 6. Ensure that the integration is enabled and the status is \"on\".</p> <p></p> <p>STEP 7. View your logs in your Coralogix Explore screen.</p> <p></p>"},{"location":"newoutput/surf/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/suricata/","title":"Suricata","text":"<p>To integrate Suricata into Coralogix, we will use Filebeat Module.</p> <p>For this integration, we need Filebeat working (https://coralogixstg.wpengine.com/integrations/filebeat/)</p> <p>Here is an example of\u00a0filebeat.xml:</p> <pre><code>filebeat.config:\n\u00a0\u00a0modules:\n\u00a0\u00a0\u00a0\u00a0path: ${path.config}/modules.d/*.yml\n\u00a0\u00a0\u00a0\u00a0reload.enabled: false\nprocessors:\n\u00a0\u00a0- add_cloud_metadata: ~\n\u00a0\u00a0- add_docker_metadata: ~\n\u00a0\u00a0- add_fields:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0target: ''\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0fields:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0PRIVATE_KEY: \"XXXXXX-XXXXX-XXXXX-XXXXXX\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0COMPANY_ID: XXXX\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0APP_NAME: \"APP-NAME\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0SUB_SYSTEM: \"SUBSYSTEM\"\noutput.logstash:\n\u00a0\u00a0enabled: true\n\u00a0\u00a0hosts: [\"logstashserver.coralogixstg.wpengine.com:5015\"]\n  tls.certificate_authorities: [\"&lt;path to folder with certificates&gt;/ca.crt\"]\n  ssl.certificate_authorities: [\"&lt;path to folder with certificates&gt;/ca.crt\"]\n</code></pre> <p>You need to enable the Filebeat Suricata module:</p> <pre><code>\u00a0filebeat modules enable suricata\n</code></pre> <p>After that we need to edit the Suricata module configuration file, normally located in /etc/filebeat/modules.d/suricata.yml</p> <p>Here is an example:</p> <pre><code># Module: suricata\n# Docs: https://www.elastic.co/guide/en/beats/filebeat/7.13/filebeat-module-suricata.html\n\n- module: suricata\n\u00a0\u00a0# All logs\n\u00a0\u00a0eve:\n\u00a0\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0\u00a0var.paths: [\"/logs/eve.json\"]\n</code></pre> <p>Suricata is saving logs in /logs for this example.</p> <p>You will notice that you have duplicated items, as you have the original message in the key event, and then the items in JSON format.</p> <p>We can avoid this by dropping that key.</p> <pre><code>processors:\n\u00a0- drop_fields:\n\u00a0\u00a0\u00a0\u00a0\u00a0fields: [\"event\"]\n\u00a0\u00a0\u00a0\u00a0\u00a0ignore_missing: false\n</code></pre> <p>Be careful that you don't have any other key named \u201cevent\u201d because this will drop all.</p>"},{"location":"newoutput/synthetic-monitoring-coralogix-telegraf/","title":"Synthetic Monitoring With Telegraf","text":"<p>Synthetic monitoring is critical for identifying and resolving production events, as it allows companies to proactively monitor the availability and performance of their applications and websites. By simulating real user interactions with these systems, synthetic monitoring can help identify potential issues before they affect real users.</p> <p>This guide outlines the steps required to use the Coralogix Telegraf integration to monitor response codes and response times of URLs and ship the metrics to Coralogix.</p> <p>This configuration makes use of the Telegraf plugin <code>inputs.http_response</code>. This plugin allows you to provide a list of URLs to Telegraf which will then attempt to connect to each URL and provide the following metrics:</p> <ul> <li> <p>http_response_content_length</p> </li> <li> <p>http_response_http_response_code</p> </li> <li> <p>http_response_response_time</p> </li> <li> <p>http_response_result_code</p> </li> </ul>"},{"location":"newoutput/synthetic-monitoring-coralogix-telegraf/#prerequisites","title":"Prerequisites","text":"<p>1. Sign up for a Coralogix account. Set up your account on the Coralogix domain corresponding to the region within which you would like your data stored.</p> <p>2. Install and configure Telegraf.</p>"},{"location":"newoutput/synthetic-monitoring-coralogix-telegraf/#configuration","title":"Configuration","text":"<p>1. Once you have configured Telegraf, add the following code into your configuration file:</p> <pre><code>[[inputs.http_response]]\n  ## Server address (default http://localhost)\n  urls = [\"https://coralogixstg.wpengine.com\"]\n\n  ## Set response_timeout (default 5 seconds)\n  # response_timeout = \"5s\"\n\n  ## Whether to follow redirects from the server (defaults to false)\n  follow_redirects = true\n</code></pre> <p>You are required to input the following variables:</p> <ul> <li> <p><code>urls</code> (line 3): Change the values within this list to the URLs that you wish to monitor.</p> </li> <li> <p><code>response_timeout</code> (line 6): If the URLs that are being monitored usually take longer than 5 seconds to respond, we recommend to uncomment this line and increase the timeout value.</p> </li> <li> <p><code>follow_redirects</code> (line 9): This instructs Telegraf to follow redirects. Setting this to <code>false</code> will disable this feature. Any URL that returns a redirect response code (e.g. <code>301</code> or <code>302</code>) will not appear as \u2018Up\u2019.</p> </li> </ul> <p>Further information on the available values within this configuration block can be found here.</p> <p>2. Once you have input the variables above, save the file and restart Telegraf. Telegraf will now start testing connectivity to the URLs and shipping the metrics to Coralogix.</p> <p></p> <p>3. To loop over all monitored URLs and display metric data for each URL, upload the following Grafana Dashboard file to Coralogix hosted Grafana.</p> <pre><code>{\n  \"__inputs\": [\n    {\n      \"name\": \"DS_METRICS\",\n      \"label\": \"Metrics\",\n      \"description\": \"\",\n      \"type\": \"datasource\",\n      \"pluginId\": \"prometheus\",\n      \"pluginName\": \"Prometheus\"\n    }\n  ],\n  \"__requires\": [\n    {\n      \"type\": \"grafana\",\n      \"id\": \"grafana\",\n      \"name\": \"Grafana\",\n      \"version\": \"8.2.6\"\n    },\n    {\n      \"type\": \"datasource\",\n      \"id\": \"prometheus\",\n      \"name\": \"Prometheus\",\n      \"version\": \"1.0.0\"\n    },\n    {\n      \"type\": \"panel\",\n      \"id\": \"stat\",\n      \"name\": \"Stat\",\n      \"version\": \"\"\n    },\n    {\n      \"type\": \"panel\",\n      \"id\": \"timeseries\",\n      \"name\": \"Time series\",\n      \"version\": \"\"\n    }\n  ],\n  \"annotations\": {\n    \"list\": [\n      {\n        \"builtIn\": 1,\n        \"datasource\": \"-- Grafana --\",\n        \"enable\": true,\n        \"hide\": true,\n        \"iconColor\": \"rgba(0, 211, 255, 1)\",\n        \"name\": \"Annotations &amp; Alerts\",\n        \"target\": {\n          \"limit\": 100,\n          \"matchAny\": false,\n          \"tags\": [],\n          \"type\": \"dashboard\"\n        },\n        \"type\": \"dashboard\"\n      }\n    ]\n  },\n  \"editable\": true,\n  \"fiscalYearStartMonth\": 0,\n  \"gnetId\": null,\n  \"graphTooltip\": 0,\n  \"id\": null,\n  \"iteration\": 1670940529980,\n  \"links\": [],\n  \"liveNow\": false,\n  \"panels\": [\n    {\n      \"collapsed\": true,\n      \"datasource\": null,\n      \"gridPos\": {\n        \"h\": 1,\n        \"w\": 24,\n        \"x\": 0,\n        \"y\": 0\n      },\n      \"id\": 2,\n      \"panels\": [\n        {\n          \"datasource\": null,\n          \"fieldConfig\": {\n            \"defaults\": {\n              \"color\": {\n                \"mode\": \"thresholds\"\n              },\n              \"mappings\": [],\n              \"thresholds\": {\n                \"mode\": \"absolute\",\n                \"steps\": [\n                  {\n                    \"color\": \"green\",\n                    \"value\": null\n                  },\n                  {\n                    \"color\": \"#EAB839\",\n                    \"value\": 300\n                  },\n                  {\n                    \"color\": \"red\",\n                    \"value\": 400\n                  }\n                ]\n              }\n            },\n            \"overrides\": []\n          },\n          \"gridPos\": {\n            \"h\": 8,\n            \"w\": 4,\n            \"x\": 0,\n            \"y\": 1\n          },\n          \"id\": 4,\n          \"options\": {\n            \"colorMode\": \"value\",\n            \"graphMode\": \"none\",\n            \"justifyMode\": \"auto\",\n            \"orientation\": \"auto\",\n            \"reduceOptions\": {\n              \"calcs\": [\n                \"lastNotNull\"\n              ],\n              \"fields\": \"\",\n              \"values\": false\n            },\n            \"text\": {},\n            \"textMode\": \"auto\"\n          },\n          \"pluginVersion\": \"8.2.6\",\n          \"repeat\": null,\n          \"targets\": [\n            {\n              \"exemplar\": true,\n              \"expr\": \"http_response_http_response_code{}\",\n              \"format\": \"table\",\n              \"instant\": false,\n              \"interval\": \"\",\n              \"legendFormat\": \"$url\",\n              \"refId\": \"A\"\n            }\n          ],\n          \"title\": \"HTTP Response Code\",\n          \"transformations\": [],\n          \"type\": \"stat\"\n        },\n        {\n          \"datasource\": null,\n          \"fieldConfig\": {\n            \"defaults\": {\n              \"color\": {\n                \"mode\": \"thresholds\"\n              },\n              \"mappings\": [],\n              \"thresholds\": {\n                \"mode\": \"absolute\",\n                \"steps\": [\n                  {\n                    \"color\": \"green\",\n                    \"value\": null\n                  },\n                  {\n                    \"color\": \"red\",\n                    \"value\": 80\n                  }\n                ]\n              },\n              \"unit\": \"ms\"\n            },\n            \"overrides\": []\n          },\n          \"gridPos\": {\n            \"h\": 8,\n            \"w\": 4,\n            \"x\": 4,\n            \"y\": 1\n          },\n          \"id\": 25,\n          \"options\": {\n            \"colorMode\": \"value\",\n            \"graphMode\": \"area\",\n            \"justifyMode\": \"auto\",\n            \"orientation\": \"auto\",\n            \"reduceOptions\": {\n              \"calcs\": [\n                \"lastNotNull\"\n              ],\n              \"fields\": \"\",\n              \"values\": false\n            },\n            \"text\": {},\n            \"textMode\": \"auto\"\n          },\n          \"pluginVersion\": \"8.2.6\",\n          \"targets\": [\n            {\n              \"exemplar\": true,\n              \"expr\": \"avg(http_response_response_time{server=~\\\"$url\\\"})\",\n              \"interval\": \"\",\n              \"legendFormat\": \"\",\n              \"refId\": \"A\"\n            }\n          ],\n          \"title\": \"Average Response Time\",\n          \"type\": \"stat\"\n        },\n        {\n          \"datasource\": null,\n          \"fieldConfig\": {\n            \"defaults\": {\n              \"color\": {\n                \"mode\": \"fixed\"\n              },\n              \"mappings\": [],\n              \"thresholds\": {\n                \"mode\": \"absolute\",\n                \"steps\": [\n                  {\n                    \"color\": \"green\",\n                    \"value\": null\n                  }\n                ]\n              },\n              \"unit\": \"bytes\"\n            },\n            \"overrides\": []\n          },\n          \"gridPos\": {\n            \"h\": 8,\n            \"w\": 4,\n            \"x\": 8,\n            \"y\": 1\n          },\n          \"id\": 23,\n          \"options\": {\n            \"colorMode\": \"value\",\n            \"graphMode\": \"none\",\n            \"justifyMode\": \"auto\",\n            \"orientation\": \"auto\",\n            \"reduceOptions\": {\n              \"calcs\": [\n                \"lastNotNull\"\n              ],\n              \"fields\": \"\",\n              \"values\": false\n            },\n            \"text\": {},\n            \"textMode\": \"auto\"\n          },\n          \"pluginVersion\": \"8.2.6\",\n          \"targets\": [\n            {\n              \"exemplar\": true,\n              \"expr\": \"avg( http_response_content_length{server=~\\\"$url\\\"})\",\n              \"interval\": \"\",\n              \"legendFormat\": \"$url\",\n              \"refId\": \"A\"\n            }\n          ],\n          \"title\": \"Average Response Content Length\",\n          \"type\": \"stat\"\n        },\n        {\n          \"datasource\": null,\n          \"fieldConfig\": {\n            \"defaults\": {\n              \"color\": {\n                \"mode\": \"palette-classic\"\n              },\n              \"custom\": {\n                \"axisLabel\": \"\",\n                \"axisPlacement\": \"auto\",\n                \"barAlignment\": 0,\n                \"drawStyle\": \"line\",\n                \"fillOpacity\": 0,\n                \"gradientMode\": \"none\",\n                \"hideFrom\": {\n                  \"legend\": false,\n                  \"tooltip\": false,\n                  \"viz\": false\n                },\n                \"lineInterpolation\": \"linear\",\n                \"lineWidth\": 1,\n                \"pointSize\": 5,\n                \"scaleDistribution\": {\n                  \"type\": \"linear\"\n                },\n                \"showPoints\": \"auto\",\n                \"spanNulls\": false,\n                \"stacking\": {\n                  \"group\": \"A\",\n                  \"mode\": \"none\"\n                },\n                \"thresholdsStyle\": {\n                  \"mode\": \"off\"\n                }\n              },\n              \"mappings\": [],\n              \"thresholds\": {\n                \"mode\": \"absolute\",\n                \"steps\": [\n                  {\n                    \"color\": \"green\",\n                    \"value\": null\n                  },\n                  {\n                    \"color\": \"red\",\n                    \"value\": 80\n                  }\n                ]\n              },\n              \"unit\": \"ms\"\n            },\n            \"overrides\": []\n          },\n          \"gridPos\": {\n            \"h\": 8,\n            \"w\": 7,\n            \"x\": 12,\n            \"y\": 1\n          },\n          \"id\": 12,\n          \"options\": {\n            \"legend\": {\n              \"calcs\": [],\n              \"displayMode\": \"list\",\n              \"placement\": \"bottom\"\n            },\n            \"tooltip\": {\n              \"mode\": \"single\"\n            }\n          },\n          \"targets\": [\n            {\n              \"exemplar\": true,\n              \"expr\": \"http_response_response_time{server=~\\\"$url\\\"}\",\n              \"format\": \"time_series\",\n              \"interval\": \"\",\n              \"legendFormat\": \"$url\",\n              \"refId\": \"A\"\n            }\n          ],\n          \"title\": \"Response Time\",\n          \"type\": \"timeseries\"\n        }\n      ],\n      \"repeat\": \"url\",\n      \"title\": \"$url\",\n      \"type\": \"row\"\n    }\n  ],\n  \"schemaVersion\": 32,\n  \"style\": \"dark\",\n  \"tags\": [],\n  \"templating\": {\n    \"list\": [\n      {\n        \"allValue\": null,\n        \"current\": {},\n        \"datasource\": \"${DS_METRICS}\",\n        \"definition\": \"label_values(server)\",\n        \"description\": null,\n        \"error\": null,\n        \"hide\": 1,\n        \"includeAll\": true,\n        \"label\": null,\n        \"multi\": false,\n        \"name\": \"url\",\n        \"options\": [],\n        \"query\": {\n          \"query\": \"label_values(server)\",\n          \"refId\": \"StandardVariableQuery\"\n        },\n        \"refresh\": 2,\n        \"regex\": \"\",\n        \"skipUrlSync\": false,\n        \"sort\": 0,\n        \"type\": \"query\"\n      }\n    ]\n  },\n  \"time\": {\n    \"from\": \"now-1h\",\n    \"to\": \"now\"\n  },\n  \"timepicker\": {},\n  \"timezone\": \"\",\n  \"title\": \"URL Status Dashboard\",\n  \"uid\": \"49vJ1HcVz\",\n  \"version\": 5\n}\n\n\n</code></pre> <p>Your dashboard should then appear as follows:</p> <p></p>"},{"location":"newoutput/synthetic-monitoring-coralogix-telegraf/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email to\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/synthetic-monitoring-with-checkly/","title":"Synthetic Monitoring with Checkly","text":"<p>This tutorial demonstrates how to add synthetic capabilities to your Coralogix dashboard with Checkly, allowing you to view and query the results of your Checkly synthetic testing.</p>"},{"location":"newoutput/synthetic-monitoring-with-checkly/#prerequisites","title":"Prerequisites","text":"<ul> <li>Checky account</li> </ul>"},{"location":"newoutput/synthetic-monitoring-with-checkly/#setup","title":"Setup","text":"<p>In your Checkly account, create API and / or browser checks in your dashboard.</p> <p></p>"},{"location":"newoutput/synthetic-monitoring-with-checkly/#configuration","title":"Configuration","text":"<p>STEP 1. To integrate with Coralogix, define a webhook. In your Checkly navigation pane, select Alerts and define a new channel using the Coralogix Rest API <code>/singles</code> using the endpoint associated with your Coralogix domain.</p> <p></p> <p>The example above consists of the following JSON body. Note the importance of inputting application and subsystem names.</p> <pre><code>{ \n  \"applicationName\": \"synthetics\",\n  \"subsystemName\": \"{{ CHECK_NAME }}\",\n  \"computerName\": \"{{ CHECK_TYPE }}\",\n  \"severity\": 3,\n  \"text\":\"{\\\\\"message\\\\\":\\\\\"{{ ALERT_TITLE }}\\\\\",\\\\\"check_result_id\\\\\":\\\\\"{{CHECK_RESULT_ID}}\\\\\",\\\\\"check_id\\\\\":\\\\\"{{CHECK_ID}}\\\\\",\\\\\"check_name\\\\\":\\\\\"{{CHECK_NAME}}\\\\\",\\\\\"check_type\\\\\":\\\\\"{{CHECK_TYPE}}\\\\\",\\\\\"alert_type\\\\\":\\\\\"{{ALERT_TYPE}}\\\\\",\\\\\"response_time\\\\\":\\\\\"{{RESPONSE_TIME}}\\\\\",\\\\\"response_code\\\\\":\\\\\"{{API_CHECK_RESPONSE_STATUS_CODE}}\\\\\",\\\\\"response_text\\\\\":\\\\\"{{API_CHECK_RESPONSE_STATUS_TEXT}}\\\\\",\\\\\"run_location\\\\\":\\\\\"{{RUN_LOCATION}}\\\\\",\\\\\"result_link\\\\\":\\\\\"{{RESULT_LINK}}\\\\\",\\\\\"started_at\\\\\":\\\\\"{{STARTED_AT}}\\\\\",\\\\\"group_name\\\\\":\\\\\"{{GROUP_NAME}}\\\\\"}\",\n  \"category\": \"synthetic-test\",\n  \"className\": \"{{ CHECK_NAME }}\"\n}\n\n</code></pre> <p>STEP 2. Connect your webhook to your checks by adding the webhook as an alert channel for each.</p> <p></p>"},{"location":"newoutput/synthetic-monitoring-with-checkly/#terraform","title":"Terraform","text":"<p>You have the option of deploying your Coralogix-Checkly integration as a code using Terraform. Follow these steps:</p> <p>STEP 1. Set up your Terraform Checkly provider.</p> <p>STEP 2. Create the following terraform resource to deploy a Coralogix webhook within Checkly.</p> <pre><code># A Coralogix alert channel webhook for Checkly\n\nresource \"checkly_alert_channel\" \"coralogix_ac\" {\n  webhook {\n    name         = \"coralogix_alert_channel\"\n    method       = \"post\"\n    template     = &lt;&lt;EOT\n{ \n  \"applicationName\": \"synthetics\",\n  \"subsystemName\": \"{{ CHECK_NAME }}\",\n  \"computerName\": \"{{ CHECK_TYPE }}\",\n  \"severity\": 3,\n  \"text\":\"{\\\\\"message\\\\\":\\\\\"{{ ALERT_TITLE }}\\\\\",\\\\\"check_result_id\\\\\":\\\\\"{{CHECK_RESULT_ID}}\\\\\",\\\\\"check_id\\\\\":\\\\\"{{CHECK_ID}}\\\\\",\\\\\"check_name\\\\\":\\\\\"{{CHECK_NAME}}\\\\\",\\\\\"check_type\\\\\":\\\\\"{{CHECK_TYPE}}\\\\\",\\\\\"alert_type\\\\\":\\\\\"{{ALERT_TYPE}}\\\\\",\\\\\"response_time\\\\\":\\\\\"{{RESPONSE_TIME}}\\\\\",\\\\\"response_code\\\\\":\\\\\"{{API_CHECK_RESPONSE_STATUS_CODE}}\\\\\",\\\\\"response_text\\\\\":\\\\\"{{API_CHECK_RESPONSE_STATUS_TEXT}}\\\\\",\\\\\"run_location\\\\\":\\\\\"{{RUN_LOCATION}}\\\\\",\\\\\"result_link\\\\\":\\\\\"{{RESULT_LINK}}\\\\\",\\\\\"started_at\\\\\":\\\\\"{{STARTED_AT}}\\\\\",\\\\\"group_name\\\\\":\\\\\"{{GROUP_NAME}}\\\\\"}\",\n  \"category\": \"synthetic-test\",\n  \"className\": \"{{ CHECK_NAME }}\"\n},\n    EOT\n    url          = \"&lt;cx_rest_api_singles&gt;\"\n  }\n}\n\n# A Checkly check to do a test on api.spacexdata.com. And connecting this check to the above Coralogix\n# alert channel. \n\nresource \"checkly_check\" \"spacexdata_check\" {\n  name                      = \"SpacexData V2 Check\"\n  type                      = \"API\"\n  activated                 = true\n  should_fail               = false\n  frequency                 = 1\n  double_check              = true\n  use_global_alert_settings = true\n\n  locations = [\n    \"us-west-1\"\n  ]\n\n  request {\n    url              = \"&lt;https://api.spacexdata.com/v2&gt;\"\n    follow_redirects = true\n    skip_ssl         = false\n    assertion {\n      source     = \"STATUS_CODE\"\n      comparison = \"EQUALS\"\n      target     = \"200\"\n    }\n  }\n\n  alert_channel_subscription {\n    channel_id = checkly_alert_channel.coralogix_ac.id\n    activated  = true\n  }\n}\n\n</code></pre> <p>Notes:</p> <ul> <li> <p><code>cx_rest_api_singles</code>: Input the\u00a0Coralogix Logs endpoint\u00a0associated with your Coralogix\u00a0domain.</p> </li> <li> <p>The example terraform script above acreates a new Checkly check entitled \u201cSpacexData V2 Check\u201d. It then associates the Coralogix alert channel to that check as well.</p> </li> </ul> <p>STEP 3. Once applied, view the Coralogix alert channel created within your Checkly account. Apply the above to view the Coralogix alert channel created within Checkly.</p>"},{"location":"newoutput/synthetic-monitoring-with-checkly/#validation","title":"Validation","text":"<p>Depending on the frequency with which your checks are being executed and are changing states, you should be able to view the checks in your Coralogix dashboard.</p> <p>Notes:</p> <ul> <li> <p>Checkly only invokes a webhook when the state of a check changes, either from pass to fail or fail to pass.</p> </li> <li> <p>A webhook will not be invoked for every browser check, meaning that you will not receive a message within your Coralogix dashboard for every check.</p> </li> </ul>"},{"location":"newoutput/synthetic-monitoring-with-checkly/#check","title":"Check","text":"<p>A check in your Coralogix dashboard should appear as follows:</p> <pre><code>/**\n  * To learn more about Playwright Test visit:\n  * &lt;https://www.checklyhq.com/docs/browser-checks/playwright-test/&gt;\n  * &lt;https://playwright.dev/docs/writing-tests&gt;\n  */\n\nconst { expect, test } = require('@playwright/test')\n\n// Set the action timeout to 10 seconds to quickly identify failing actions.\n// By default Playwright Test has no timeout for actions (e.g. clicking an element).\n// Learn more here: &lt;https://www.checklyhq.com/docs/browser-checks/timeouts/&gt;\ntest.use({ actionTimeout: 10000 })\n\ntest('wait for an element to become visible', async ({ page }) =&gt; {\n  // Change checklyhq.com to your site's URL,\n  // or, even better, define a ENVIRONMENT_URL environment variable\n  // to reuse it across your browser checks\n  await page.goto(process.env.ENVIRONMENT_URL || \n\n  // Locate the headline and check if it's visible\n  // Learn more about all locator functions in the Playwright docs\n  // &lt;https://playwright.dev/docs/api/class-locator&gt;\n  // const mainHeadline = page.locator('title')\n  await expect(page.getByText('Full-Stack Observability')).toBeVisible();\n\n  // Take a screenshot of the current page\n  await page.screenshot({ path: 'screenshot.jpg' })\n})\n\n</code></pre>"},{"location":"newoutput/synthetic-monitoring-with-checkly/#test","title":"Test","text":"<p>A multi-step synthetic test, which logs in to a website, should appear as follows:</p> <pre><code>/**\n * To learn more about Playwright Test visit:\n * &lt;https://www.checklyhq.com/docs/browser-checks/playwright-test/&gt;\n * &lt;https://playwright.dev/docs/writing-tests&gt;\n */\nconst { expect, test, selectors } = require(\"@playwright/test\")\n\n// Set the action timeout to 10 seconds to quickly identify failing actions.\n// By default Playwright Test has no timeout for actions (e.g. clicking an element).\n// Learn more here: &lt;https://www.checklyhq.com/docs/browser-checks/timeouts/&gt;\ntest.use({ actionTimeout: 10000 })\n\ntest(\"Login into Github\", async ({ page }) =&gt; {\n  // Go to login page\n  page.goto('&lt;https://www.saucedemo.com&gt;')\n\n  // Fill in credentials\n  await page.getByPlaceholder('Username').type('standard_user')\n  await page.getByPlaceholder('Password').type('secret_sauce')\n  await page.click('input[value=\"Login\"]')\n\n  // Verify successful login\n  // await selectors.setTestIdAttribute(\"  c sccs\u00dftitle\")\n  // await expect(page.getByTestId(\"Products\"))\n  await expect(page.locator('.title')).toBeVisible();\n  // await page.click('input[value=\"add-to-cart-sauce-labs-backpack\"]')\n})\n\n</code></pre> <p>If you simulate the test to change its state, the message should appear in your Coralogix dashboard as follows:</p> <p></p>"},{"location":"newoutput/synthetic-monitoring-with-checkly/#coralogix-dashboard","title":"Coralogix Dashboard","text":""},{"location":"newoutput/synthetic-monitoring-with-checkly/#custom-dashboard","title":"Custom Dashboard","text":"<p>Below is a sample custom dashboard built with synthetic test data. Filter the view using your synthetic test.</p> <p></p>"},{"location":"newoutput/synthetic-monitoring-with-checkly/#coralogix-actions","title":"Coralogix Actions","text":"<p>Define a Coralogix action, allowing you to navigate from the synthetic test message in your Coralogix dashboard to the status of that specific test within your Checkly account.</p> <p></p> <p></p>"},{"location":"newoutput/synthetic-monitoring-with-checkly/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/syslog-coralogix/","title":"Syslog","text":"<p>Whether your system generates syslog messages in <code>rfc3164</code> or <code>rfc5424</code> format, or lets you transform them to a custom format, seamlessly send your syslogs to Coralogix.</p>"},{"location":"newoutput/syslog-coralogix/#overview","title":"Overview","text":"<p>To accommodate many different systems, this tutorial provides configuration options in <code>rfc3164</code>, <code>rfc5424</code>, and custom format.</p> <p>Regardless of format, each syslog message sent to Coralogix is labeled with a facility code, indicating the type of system generating the message, and is assigned a severity level.</p>"},{"location":"newoutput/syslog-coralogix/#parameters","title":"Parameters","text":"<p>You are required to input the following parameters in your configuration, determined by the system sending your syslogs.</p> Parameter Description Private Key Coralogix Send-Your-Data API Key Application Name Application name SubSystem Name Subsystem name SyslogEndpoint TLS/TCP syslog endpoint associated with your Coralogix domain"},{"location":"newoutput/syslog-coralogix/#format","title":"Format","text":"<p>The platform running your applications will typically support only some set of syslog formats with some customization possibilities, making it impossible send logs in one predefined, fixed format. As such, Coralogix supports various formats and associated structures to pass the needed additional data with the syslog message.</p> Format Structure rfc3164 key-value pairs rfc5424 key-value pairs or structured data JSON JSON fields"},{"location":"newoutput/syslog-coralogix/#message-structure","title":"Message Structure","text":""},{"location":"newoutput/syslog-coralogix/#rfc5424-structured-data","title":"<code>rfc5424</code> Structured Data","text":"<p>Assuming the system sending your syslog messages supports <code>rfc5424</code> format, we highly recommend using it. If the configuration allows, add structured data in the format shown in the example.</p> <p>Example</p> <pre><code>&lt;134&gt;1 2022-11-23T07:03:31.402569Z pg-454f526-1 postgres \n  285528 - [coralogix@1 application_name=\"...\" private_key=\"...\" subsystem_name=\"...\"] pid=285528,user=postgres,db=defaultdb, \n  client=[local] LOG: disconnection: session time: 0:00:00.005 \n  user=postgres database=defaultdb host=[local]\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>The first part of the message is the ID of the structured data followed by three key-value pairs. Because the structured data is named <code>coralogix@1</code>, the values inside are not prefixed with <code>cx_</code>.</p> </li> <li> <p>The <code>[coralogix@1 ...]</code> segment allows adding structured data to any message. This is important for sending syslog metadata to Coralogix.</p> </li> </ul>"},{"location":"newoutput/syslog-coralogix/#key-value-pairs","title":"Key-Value Pairs","text":"<p>Depending on your system, it may be possible to add key-value pairs to the message body. In this case, add the values prefixed with <code>cx_</code>.</p> <p>Example</p> <pre><code>cx_application_name=\"...\" cx_private_key=\"...\" cx_subsystem_name=\"...\"\n\n</code></pre>"},{"location":"newoutput/syslog-coralogix/#json","title":"JSON","text":"<p>If the system sending your syslog messages supports custom format, transform the message to JSON along with additional fields. The example below consists of the contents of the file <code>/etc/rsyslog</code>, instructing syslog how to send messages.</p> <p>Example</p> <pre><code>$template CoralogixSyslogFormat,\"{\\\\\"fields\\\\\": \n       {\\\\\"private_key\\\\\":\\\\\"...\\\\\", \n        \\\\\"application_name\\\\\":\\\\\"...\\\\\",\n        \\\\\"subsystem_name\\\\\":\\\\\"...\\\\\"},\n        \\\\\"message\\\\\": {\\\\\"message\\\\\":\\\\\"%msg:::json%\\\\\" }}\\\\n\"\n@@syslog.coralogixstg.wpengine.com:6514;CoralogixSyslogFormat\n\n</code></pre> <p>Note: This requires that rsyslog is configured to support TLS connections.</p>"},{"location":"newoutput/syslog-coralogix/#troubleshooting","title":"Troubleshooting","text":"<p>To test your connection, send a message from the terminal with the command below.</p> <pre><code>echo '&lt;14&gt;1 2023-04-03T07:48:26.086Z hostname appname - msg_id - Message cx_private_key=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" cx_application_name=\"app\" cx_subsystem_name=\"sub\" Something happened' | openssl s_client -connect syslog.coralogixstg.wpengine.com:6514\n\n</code></pre> <p>Note: Be aware that one extra space in the command will render it invalid.</p>"},{"location":"newoutput/syslog-coralogix/#monitor-your-syslog-messages","title":"Monitor Your Syslog Messages","text":"<p>View your syslog messages as rendered structured logs in your Coralogix dashboard. In your navigation bar, click on Explore &gt; logs tab.</p> <pre><code>{\n  appname:logforwarder\n  msg: Failed password for user123 from 192.168.0.1 port 12345 ssh2\n  facility:user\n  hostname:stream-logfwd20\n  msgid:panwlogs\n}\n\n</code></pre> <p>As with all the logs ingested into Coralogix, you have the ability to modify their format using parsing rules.</p>"},{"location":"newoutput/syslog-coralogix/#additional-resources","title":"Additional Resources","text":"DocumentationSyslog using OpenTelemetryBlogSyslog 101"},{"location":"newoutput/syslog-coralogix/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/syslog-using-otel/","title":"Syslog using OpenTelemetry","text":"<p>This tutorial demonstrates how to use custom syslog to send your logs to Coralogix using OpenTelemetry.</p>"},{"location":"newoutput/syslog-using-otel/#overview","title":"Overview","text":"<p>Syslog is a standard for\u00a0message logging. It allows separation of the software that generates messages, the system that stores them, and the software that reports and analyzes them. Each message is labeled with a facility code, indicating the type of system generating the message, and is assigned a severity level.</p> <p>When there is no support for custom syslog, an intermediate server is required in order to send the data to the Coralogix account.</p>"},{"location":"newoutput/syslog-using-otel/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Server to install OpenTelemetry</p> </li> <li> <p>Static public IP allocated to the server for initial configuration</p> </li> </ul>"},{"location":"newoutput/syslog-using-otel/#deployment","title":"Deployment","text":"<p>STEP 1.\u00a0Install\u00a0OpenTelemetry on your server.</p> <p>STEP 2. Create a configure file.</p> <pre><code>receivers:\n  syslog:\n    tcp:\n      listen_address: \"0.0.0.0:514\"\n    protocol: rfc5424\n    operators:\n      - type: syslog_parser\n        protocol: &lt;**message_format&gt;**\n        parse_from: body\n        parse_to: body\n            - type: remove\n        field: attributes\nexporters:\n  coralogix:\n    domain: \"&lt;coralogix_domain&gt;\"\n    private_key: \"private_key\"\n    application_name: \"applicationName\"\n    subsystem_name: \"subsystemName\"\n    timeout: 30s\nservice:\n  pipelines:\n    logs:\n      receivers: [ syslog ]\n      exporters: [ coralogix ]\n\n</code></pre> <p>Replace the following values.</p> ValueDescriptionapplicationNameApplication name\u00a0to be displayed in your Coralogix dashboardsubsystemNameSubsystem name\u00a0to be displayed in your Coralogix dashboardcoralogix_domainYour Coralogix domainprivate_keyYour Coralogix Send-Your-Data API keymessage_formatThe syslog message format ( rfc3164/rfc5424 ) <p>Notes:</p> <ul> <li> <p><code>port 514</code>\u00a0is the default port for Syslog.</p> </li> <li> <p>To change to this port, modify the OpenTelemetry configuration should be changed accordingly.</p> </li> </ul> <p>STEP 3. Save the\u00a0configure\u00a0file.</p>"},{"location":"newoutput/syslog-using-otel/#additional-resources","title":"Additional Resources","text":"DocumentationSyslog"},{"location":"newoutput/syslog-using-otel/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/tableau-plugin/","title":"Tableau Plugin","text":"<p>The Coralogix JDBC driver allows you to investigate your log data with your favorite database tool using SQL queries.</p> <p>JDBC, which stands for Java Database Connectivity, is a common standard for database drivers, and many popular querying tools support it. With the Coralogix JDBC driver, you can quickly get started on performing SQL queries against the data already stored in your Coralogix account.</p> <p>This tutorial provides instructions on using the Coralogix JDBC driver with Tableau.</p>"},{"location":"newoutput/tableau-plugin/#getting-started","title":"Getting Started","text":"<p>STEP 1. Place the .jar files in the folder for your operating system. Create a folder if it doesn't exist already.</p> OS Path Windows C:\\Program Files\\Tableau\\Drivers Mac ~/Library/Tableau/Drivers Linux /opt/tableau/tableau_driver/jdbc <p>STEP 2. Copy the coralogix.tdc\u00a0file to\u00a0<code>~/Documents/My Tableau Repository/Datasources</code>. For more details, view the Tableau documentation\u00a0 here.</p> <p>STEP 3. Create a file <code>coralogix.properties</code>\u00a0and add an entry with your\u00a0<code>apiKey</code>. In your navigation pane, select Data Flow &gt; API Keys &gt; Logs Query Key.</p> <pre><code>apiKey=&lt;Logs Query Key&gt;\n</code></pre> <ol> <li> <ol> <li>On the main screen in\u00a0<code>To a Server</code> section, select <code>Other Databases (JDBC)</code>.</li> <li>Set\u00a0<code>URL</code>\u00a0to\u00a0[table id=106 /].</li> <li>Set\u00a0<code>Dialect</code>\u00a0to\u00a0<code>MySQL</code> and leave the <code>Username</code>\u00a0and\u00a0<code>Password</code> blank. Click on <code>Browse</code>\u00a0next\u00a0<code>Properties file</code>\u00a0and choose\u00a0<code>coralogix.properties</code> you created.</li> <li>Click on\u00a0<code>Sign In</code>.</li> </ol> </li> </ol> <p>The Coralogix SQL support is based on the OpenDistro SQL interface. Refer to its documentation for further references on supported SQL features.</p>"},{"location":"newoutput/tableau-plugin/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email to\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/tail-sampling-with-coralogix-and-opentelemetry/","title":"Tail Sampling with Coralogix & OpenTelemetry","text":"<p>Are you looking for a way to improve traces observability without breaking the bank? Look no further!</p> <p>In our previous tutorial, we showed you how to set up the new OpenTelemetry Community Demo Application and send\u00a0telemetry data\u00a0to Coralogix, giving you the ability to understand the interactions between your services and visualize, alert and query them on your Coralogix dashboard.</p> <p>But what about cost? Ingesting all of your traces and spans can quickly add up and can be unnecessary in order to gain visibility into the health of your applications. That\u2019s where trace sampling comes in - in particular, tail sampling.</p> <p>By sampling your traces, you can significantly reduce the amount of data ingested into Coralogix, maintaining full visibility into your services without incurring heavy charges. Try it out with the OTel Demo App.</p>"},{"location":"newoutput/tail-sampling-with-coralogix-and-opentelemetry/#intro","title":"Intro","text":"<p>Coralogix offers a number of tutorials demonstrating how to use the OTel Collector in a load-balanced configuration with tail sampling enabled on the collector nodes using the OTel Demo App.</p> <p>The\u00a0tail sampling processor\u00a0and\u00a0probabilistic sampling processor\u00a0allow you to sample traces based on a set of rules at the collector level.</p> <p>This allows you to define more advanced rules to keep accrued visibility over error or high latency traces.</p> <p>Note: To achieve this in your environment your code should be instrumented with OpenTelemetry and emit the telemetry data to the OTel Collector.</p> <p></p>"},{"location":"newoutput/tail-sampling-with-coralogix-and-opentelemetry/#what-is-tail-sampling-why-is-it-important","title":"What is Tail Sampling? Why is it Important?","text":"<p>Tail sampling is a method of trace sampling in which sampling decisions are made at the end of the workflow, allowing for a more accurate sampling decision. This is in contrast to head-based sampling, in which the the sampling decision is made at the\u00a0beginning\u00a0of a request and usually at random. Tail sampling grants you the option of filtering your traces based on specific criteria, a plus when compared with head-based sampling.</p> <p>So why is tail sampling important, and why should you do it?</p> <ul> <li> <p>Enjoy focused observability. Tail sampling is a powerful tool for focused observability, allowing you to zero in on the traces that matter to you most. View only those traces that are of interest to you.</p> </li> <li> <p>Identify issues. Tail sampling is useful for identifying issues in your distributed system while saving on observability costs.</p> </li> <li> <p>Save on costs. By selectively exporting a predetermined subset of your traces, you can lower data ingestion and storage costs, while still being able to identify and troubleshoot issues.</p> </li> </ul>"},{"location":"newoutput/tail-sampling-with-coralogix-and-opentelemetry/#tutorials","title":"Tutorials","text":"<p>Choose one of tutorials below to use the OTel Collector in a load balanced configuration with tail sampling enabled on the collector nodes, using the OTel Demo:</p> <ul> <li> <p>Tail Sampling with Coralogix and Otel Using Docker</p> </li> <li> <p>Tail Sampling with Coralogix and Otel Using Kubernetes</p> </li> </ul>"},{"location":"newoutput/tail-sampling-with-coralogix-and-opentelemetry/#additional-resources","title":"Additional Resources","text":"DocumentationOpenTelemetryTutorialsTail Sampling with Coralogix and Otel Using DockerTail Sampling with Coralogix and Otel Using KubernetesVideosIntegrate Traces into Coralogix using Otel, Kubernetes, &amp; HelmCapture Kubernetes Logs, Transform with Logs2Metrics, and Render with DataMap"},{"location":"newoutput/tail-sampling-with-coralogix-and-opentelemetry/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-docker/","title":"Tail Sampling with OpenTelemetry using Docker Compose","text":"<p>The following tutorial demonstrates how to configure a Docker Compose environment and deploy OpenTelemetry to collect traces, as well as enable trace sampling. We will cover an example of how to enable tail sample using the small trace generating application.</p>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-docker/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Docker installed</p> </li> <li> <p>Docker Compose installed</p> </li> <li> <p>Coralogix Send-Your-Data API key</p> </li> </ul>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-docker/#how-it-works","title":"How it Works","text":"<ol> <li> <p>Traces from the application are sent to an OpenTelemetry Collector.</p> </li> <li> <p>The Collector, acting as a load balancer, forwards traces to one or more OpenTelemetry gateways. This ensures that traces with the same ID always go to the same gateway, irrespective of span or order.</p> </li> <li> <p>The gateway uses the TraceID to group traces and applies tail sampling. It then forwards the traces to the Coralogix backend.</p> </li> </ol>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-docker/#setup-demo-application","title":"Setup Demo Application","text":""},{"location":"newoutput/tail-sampling-with-opentelemetry-using-docker/#1-create-an-env-file","title":"1. Create an <code>.env</code> file","text":"<p>Create a file named <code>.env</code> and include the following content:</p> <pre><code>CORALOGIX_DOMAIN=&lt;your-coralogix-domain&gt;\nCORALOGIX_APP_NAME=otel\nCORALOGIX_SUBSYS_NAME=otel-demo\nCORALOGIX_PRIVATE_KEY=&lt;your-coralogix-private-key&gt;\nOTEL_IMAGE=otel/opentelemetry-collector-contrib:0.94.0\n</code></pre> <p>These values will configure the Coralogix backend and the OpenTelemetry Collector and Gateway.</p>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-docker/#2-create-opentelemetry-collector-configuration","title":"2. Create OpenTelemetry Collector Configuration","text":"<p>Create a file named <code>otel-collector-config.yaml</code> with the following content:</p> <pre><code>receivers:\n  otlp:\n    protocols:\n      grpc:\n\nexporters:\n  loadbalancing:\n    protocol:\n      otlp:\n        tls:\n          insecure: true\n        timeout: 1s\n    resolver:\n      static:\n        hostnames:\n        - otel-col-gateway-1:4317\n        - otel-col-gateway-2:4317\n\nprocessors:\n  batch/traces:\n    timeout: 1s\n    send_batch_size: 50\n  resourcedetection:\n    detectors: [system, env]\n    timeout: 5s\n    override: true\n\nconnectors:\n  spanmetrics:\n\nservice:\n  pipelines:\n    traces:\n      receivers: [ otlp ]\n      processors: [ batch/traces ]\n      exporters: [ loadbalancing ]\n</code></pre> <p>This configuration sets up the OpenTelemetry Collector to receive traces and forward them to the OpenTelemetry Gateway, using a load balancing exporter.</p>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-docker/#3-create-opentelemetry-gateway-configuration","title":"3. Create OpenTelemetry Gateway Configuration","text":"<p>Create a file named <code>otel-gateway-config.yaml</code> with the following content:</p> <pre><code>extensions:\n  health_check:\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4317\n\nprocessors:\n  batch/traces:\n    timeout: 1s\n    send_batch_size: 50\n  resourcedetection:\n    detectors: [system, env]\n    timeout: 5s\n    override: true\n  tail_sampling:\n    decision_wait: 10s \n    num_traces: 100\n    expected_new_traces_per_sec: 10\n    policies:\n      [          \n        {\n          name: errors-policy,\n          type: status_code,\n          status_code: {status_codes: [ERROR]}\n        },\n        {\n          name: randomized-policy,\n          type: probabilistic,\n          probabilistic: {sampling_percentage: 25}\n        },\n      ]\n  attributes/shipper:\n    actions:\n      - key: shipper\n        action: insert\n        value: '${SHIPPER_NAME}'\n\nexporters:\n  logging:\n  coralogix:    \n    domain: \"${CORALOGIX_DOMAIN}\"\n    private_key: \"${CORALOGIX_PRIVATE_KEY}\"\n    application_name: \"${CORALOGIX_APP_NAME}\"\n    subsystem_name: \"${CORALOGIX_SUBSYS_NAME}\"\n    timeout: 30s\n\nservice:\n  extensions: [health_check]\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [attributes/shipper, tail_sampling, batch/traces, resourcedetection]\n      exporters: [coralogix, logging]\n</code></pre> <p>This configuration sets up the OpenTelemetry Gateway to receive traces from the OpenTelemetry Collector, apply tail sampling, and forward the traces to the Coralogix backend.</p>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-docker/#4-create-a-docker-compose-file","title":"4. Create a Docker Compose File","text":"<pre><code>version: \"3\"\nservices:\n  go-otel-traces-demo:\n    container_name: go-otel-traces-demo\n    image: public.ecr.aws/c1s3k2h4/go-otel-traces-demo:latest\n    environment:\n      - CX_ENDPOINT=otelcol:4317\n\n  otelcol:\n    image: otel/opentelemetry-collector-contrib:0.94.0\n    container_name: otel-col\n    deploy:\n      resources:\n        limits:\n          memory: 100M\n    restart: unless-stopped\n    command: [ \"--config=/etc/otelcol-config.yml\" ]\n    volumes:\n      - ./otelcol-config.yml:/etc/otelcol-config.yml\n    ports:\n      - \"4317\"\n      - \"4318:4318\"\n\n    depends_on:\n      - otelcol_gateway_1\n      - otelcol_gateway_2\n\n  otelcol_gateway_1:\n    image: otel/opentelemetry-collector-contrib:0.94.0\n    container_name: otel-col-gateway-1\n    deploy:\n      resources:\n        limits:\n          memory: 100M\n    restart: unless-stopped\n    command: [ \"--config=/etc/otelcol-config.yml\" ]\n    volumes:\n      - ./otel-gateway.yml:/etc/otelcol-config.yml\n    ports:\n      - \"4317\"\n      - \"4318\"\n    environment:\n      - CORALOGIX_DOMAIN\n      - CORALOGIX_APP_NAME\n      - CORALOGIX_SUBSYS_NAME\n      - CORALOGIX_PRIVATE_KEY\n      - SHIPPER_NAME=gateway-1\n\n  otelcol_gateway_2:\n    image: otel/opentelemetry-collector-contrib:0.94.0\n    container_name: otel-col-gateway-2\n    deploy:\n      resources:\n        limits:\n          memory: 100M\n    restart: unless-stopped\n    command: [ \"--config=/etc/otelcol-config.yml\" ]\n    volumes:\n      - ./otel-gateway.yml:/etc/otelcol-config.yml\n    ports:\n      - \"4317\"\n      - \"4318\"\n\n    environment:\n      - CORALOGIX_DOMAIN\n      - CORALOGIX_APP_NAME\n      - CORALOGIX_SUBSYS_NAME\n      - CORALOGIX_PRIVATE_KEY\n      - SHIPPER_NAME=gateway-2\n</code></pre> <p>This Docker Compose file sets up the application, OpenTelemetry Collector, and OpenTelemetry Gateway. The application sends traces to the Collector, which forwards them to the Gateway. The Gateway applies tail sampling and forwards the traces to the Coralogix backend.</p> <p>Note: The <code>otelcol</code> service mounts <code>otel-collector-config.yaml</code>, and the <code>otelcol_gateway_1</code> and <code>otelcol_gateway_2</code> services mount <code>otel-gateway-config.yaml</code>.</p> <p>These files must be in the same directory as the Docker Compose file.</p> <p>Also, <code>CORALOGIX_DOMAIN</code>, <code>CORALOGIX_APP_NAME</code>, <code>CORALOGIX_SUBSYS_NAME</code>, and <code>CORALOGIX_PRIVATE_KEY</code> environment variables configure the Coralogix backend.</p>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-docker/#5-start-the-docker-compose-environment","title":"5. Start the Docker Compose Environment","text":"<p>Run the following command to start the Docker Compose environment:</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-docker/#validation","title":"Validation","text":"<p>Check your Coralogix dashboard for telemetry data. Traces should appear from the configured gateways.</p> <p></p>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-docker/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogix.com.</p>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-kubernetes/","title":"Tail Sampling with OpenTelemetry using Kubernetes","text":"<p>This tutorial demonstrates how to configure a Kubernetes cluster, deploy OpenTelemetry to collect logs, metrics, and traces, and enable trace sampling. We will cover an example of enabling a tail sample for the Opentelemetry Demo Application and a more precise example using the small trace-generating application.</p>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>A Kubernetes cluster</p> </li> <li> <p>Helm installed</p> </li> <li> <p>Coralogix Send-Your-Data API key</p> </li> </ul>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-kubernetes/#how-it-works","title":"How it Works","text":"<p>The Kubernetes OpenTelemetry Integration consists of the following components:</p> <ul> <li> <p>OpenTelemetry Agent. The Agent is deployed to each node within the Cluster and collects telemetry data from the applications running on that node. The agent is configured to send the telemetry data to the OpenTelemetry Gateway. The agent ensures that traces with the same ID are sent to the same gateway. This allows tail sampling to be performed on the traces correctly, even if they span multiple applications and nodes.</p> </li> <li> <p>OpenTelemetry Gateway. The Gateway is responsible for receiving telemetry data from the agents and forwarding it to the Coralogix backend. The Gateway is also responsible for load balancing the telemetry data to the Coralogix backend.</p> </li> </ul>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-kubernetes/#install-the-coralogix-opentelemetry-integration","title":"Install the Coralogix OpenTelemetry Integration","text":"<p>This integration uses the Coralogix OpenTelemetry Helm Chart. While this document focuses on tail sampling for traces, deploying this chart also deploys the infrastructure to collect logs, metrics, and traces from your Kubernetes cluster and pods.</p> <p>STEP 1. Add the Coralogix Helm repository.</p> <pre><code>helm repo add coralogix-charts-virtual https://cgx.jfrog.io/artifactory/coralogix-charts-virtual\n</code></pre> <p>STEP 2. Copy the <code>tail-sampling-values.yaml</code> file found here and update the relevant fields with your values.</p> <pre><code>global:\n  domain: \"&lt;your-coralogix-domain&gt;\"\n  clusterName: \"\"\n  defaultApplicationName: \"otel\"\n  defaultSubsystemName: \"integration\"\n  logLevel: \"warn\"\n  collectionInterval: \"30s\"\n\nopentelemetry-agent:\n  enabled: true\n  mode: daemonset\n  presets:\n    loadBalancing:\n      enabled: true\n      routingKey: \"traceID\"\n      hostname: coralogix-opentelemetry-gateway\n\n  config:\n    service:\n      pipelines:\n        traces:\n          exporters:\n            - loadbalancing\n\nopentelemetry-gateway:\n  enabled: true\n  replicaCount: 3\n\n  config:\n    processors:\n      tail_sampling:\n        decision_wait: 10s\n        num_traces: 100\n        expected_new_traces_per_sec: 10\n        policies:\n          [\n            {\n              name: errors-policy,\n              type: status_code,\n              status_code: {status_codes: [ERROR]}\n            },\n            {\n              name: randomized-policy,\n              type: probabilistic,\n              probabilistic: {sampling_percentage: 10}\n            },\n          ]\n\nopentelemetry-collector:\n  enabled: false\n\n</code></pre> <p>STEP 3. Add your Coralogix Send-Your-Data API key to the <code>tail-sampling-values.yaml</code> file.</p> <pre><code>kubectl create secret generic coralogix-keys --from-literal 'PRIVATE_KEY=&lt;your-private-key&gt;'\n</code></pre> <p>STEP 4. Install the Coralogix OpenTelemetry integration.</p> <pre><code>helm install coralogix-opentelemetry coralogix-charts-virtual/opentelemetry -f tail-sampling-values.yaml\n</code></pre> <pre><code>$ kubectl get pods\nNAME                                               READY   STATUS    RESTARTS   AGE\ncoralogix-opentelemetry-agent-86qdb                1/1     Running   0          7h59m\ncoralogix-opentelemetry-gateway-65dfbb5567-6rk4j   1/1     Running   0          7h59m\ncoralogix-opentelemetry-gateway-65dfbb5567-g7m5l   1/1     Running   0          7h59m\ncoralogix-opentelemetry-gateway-65dfbb5567-zbprd   1/1     Running   0          7h59m\n\n</code></pre> <p>You should end up with as many opentelemetry-agent pods as you have nodes in your cluster, and 3 opentelemetry-gateway pods.</p>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-kubernetes/#install-test-application-environment","title":"Install Test Application Environment","text":"<p>In the next section, we will describe the process for installing 2 application environments, the OpenTelemetry Demo Application and a Small Trace Generating. You do not need to install both these examples.</p>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-kubernetes/#install-opentelemetry-demo","title":"Install OpenTelemetry Demo","text":"<p>STEP 1. Add the Hlem chart for the OpenTelemetry Demo Application.</p> <pre><code>helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts\n</code></pre> <p>STEP 2. Create a <code>values.yaml</code> file and add the following:</p> <pre><code>default:\n  env:\n    - name: OTEL_SERVICE_NAME\n      valueFrom:\n        fieldRef:\n          apiVersion: v1\n          fieldPath: \"metadata.labels['app.kubernetes.io/component']\"\n    - name: OTEL_COLLECTOR_NAME\n      value: '{{ include \"otel-demo.name\" . }}-otelcol'\n    - name: OTEL_EXPORTER_OTLP_ENDPOINT\n      value: http://$(OTEL_COLLECTOR_NAME):4317\n    - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE\n      value: cumulative\n    - name: OTEL_RESOURCE_ATTRIBUTES\n      value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo\n\n  envOverrides:\n    - name: OTEL_COLLECTOR_NAME\n      valueFrom:\n        fieldRef:\n          apiVersion: v1\n          fieldPath: spec.nodeName\n    - name: OTEL_EXPORTER_OTLP_ENDPOINT\n      value: http://$(OTEL_COLLECTOR_NAME):4317\n\nserviceAccount:\n  create: true\n  annotations: {}\n  name: \"\"\n\nopentelemetry-collector:\n  enabled: false\n\njaeger:\n  enabled: false\n\nprometheus:\n  enabled: false\n\ngrafana:\n  enabled: false\n\n</code></pre> <p>This will configure the OpenTelemetry Demo Application to send traces to the Coralogix OpenTelemetry Agent running on the node.</p> <p>STEP 3. Install the Opentelemetry Demo Application.</p> <pre><code>$ helm install otel-demo open-telemetry/opentelemetry-demo -f values.yaml\n\nNAME: my-otel-demo\nLAST DEPLOYED: Mon Feb 19 23:29:16 2024\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\n\n</code></pre>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-kubernetes/#install-the-small-trace-generating-application","title":"Install the Small Trace-Generating Application","text":"<p>This application is a small trace-generating application. We will demonstrate how to connect it to the Coralogix OpenTelemetry Agent to enable tail sampling.</p> <p>STEP 1. Create a file <code>go-traces-demo.yaml</code> and add the following:</p> <pre><code>apiVersion: apps/v1        \nkind: Deployment\nmetadata:\n  name: go-otel-traces-demo\nspec:\n  selector:\n    matchLabels:\n      app: go-otel-traces-demo\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: go-otel-traces-demo\n    spec:\n      containers:\n        - name: go-otel-traces-demo\n          image: public.ecr.aws/c1s3k2h4/go-otel-traces-demo:latest  \n          imagePullPolicy: Always        \n          env:\n            - name: NODE_IP\n              valueFrom:\n                fieldRef:\n                  fieldPath: status.hostIP         \n            - name: CX_ENDPOINT\n              value: $(NODE_IP):4317\n\n</code></pre> <p>STEP 2. Apply the Kuberenetes deployment.</p> <pre><code>kubectl apply -f go-traces-demo.yaml\n</code></pre>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-kubernetes/#validation","title":"Validation","text":"<p>View your telemetry data in your Coralogix dashboard. Traces should arrive from the tail-sampling load balancer.</p> <p></p>"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-kubernetes/#additional-resources","title":"Additional Resources","text":"DocumentationIntroduction to Tail Sampling with Coralogix &amp; OpenTelemetry"},{"location":"newoutput/tail-sampling-with-opentelemetry-using-kubernetes/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/tco-optimizer-api/","title":"TCO Optimizer HTTP API","text":"<p>This tutorial demonstrates using our TCO Optimizer HTTP API to define, query, and manage your TCO policy overrides, which are used exclusively for logs.</p> <p>Visit this page to learn how to use our TCO Tracing gPRC API to define, query, and manage your TCO policy criteria, used both for spans and logs.</p>"},{"location":"newoutput/tco-optimizer-api/#base-url","title":"Base URL","text":"<p>Select the base API endpoint associated with your Coralogix domain.</p> Domain Base API endpoint coralogix.us (Ohio) https://api.coralogix.us/api/v1/external/tco/ cx498.coralogixstg.wpengine.com (Oregon) https://api.app.cx498.coralogixstg.wpengine.com/api/v1/external/tco/ coralogixstg.wpengine.com (Ireland) https://api.coralogixstg.wpengine.com/api/v1/external/tco/ eu2.coralogixstg.wpengine.com (Stockholm) https://api.app.eu2.coralogixstg.wpengine.com/api/v1/external/tco/ coralogix.in (Mumbai) https://api.app.coralogix.in/api/v1/external/tco/ coralogixsg.com (Singapore) https://api.app.coralogixsg.com/api/v1/external/tco/"},{"location":"newoutput/tco-optimizer-api/#header","title":"Header","text":"Key Value Content-Type application/json Authorization Bearer"},{"location":"newoutput/tco-optimizer-api/#api_key","title":"API_KEY","text":"<p>The TCO Optimizer API uses your Alerts, Rules, and Tags API Key to authenticate requests. To access this API key in your Coralogix navigation pane, click Data Flow &gt; API Keys &gt; Alerts, Rules, and Tags API Key.</p>"},{"location":"newoutput/tco-optimizer-api/#usage","title":"Usage","text":""},{"location":"newoutput/tco-optimizer-api/#severity-options","title":"Severity Options","text":"Name Value debug 1 verbose 2 info 3 warning 4 error 5 critical 6"},{"location":"newoutput/tco-optimizer-api/#priority-options","title":"Priority Options","text":"Name Value block block low low medium medium high high"},{"location":"newoutput/tco-optimizer-api/#supported-endpoints","title":"Supported Endpoints","text":""},{"location":"newoutput/tco-optimizer-api/#get-all-policy-overrides","title":"Get all policy overrides","text":"<p>GET /overrides</p> <p>Route example: https://api.coralogixstg.wpengine.com/api/v1/external/tco/overrides</p> <p>Response example:</p> <pre><code>[\n    {\n        \"id\": \"dd361b69-89c7-11ec-a5ad-0616c20b31c7\",\n        \"name\": \"default|recommendationservice|INFO\",\n        \"priority\": \"high\",\n        \"severity\": 3,\n        \"applicationName\": \"default\",\n        \"subsystemName\": \"recommendationservice\"\n    },\n    {\n        \"id\": \"61d551af-8f96-11ec-8bfb-02dd69f0920d\",\n        \"name\": \"default|checkoutservice|DEBUG\",\n        \"priority\": \"high\",\n        \"severity\": 1,\n        \"applicationName\": \"default\",\n        \"subsystemName\": \"checkoutservice\"\n    }\n]\n</code></pre>"},{"location":"newoutput/tco-optimizer-api/#get-single-policy-override-by-id","title":"Get single policy override by ID","text":"<p>GET /overrides/{id}</p> <p>Route example: https://api.coralogixstg.wpengine.com/api/v1/external/tco/overrides/*972f6b98-343c-11ee-ac29-061115d0c307*</p> <p>Response example:</p> <pre><code>{\n    \"id\": \"dd361b69-89c7-11ec-a5ad-0616c20b31c7\",\n    \"name\": \"default|recommendationservice|INFO\",\n    \"priority\": \"high\",\n    \"severity\": 3,\n    \"applicationName\": \"default\",\n    \"subsystemName\": \"recommendationservice\"\n}\n</code></pre>"},{"location":"newoutput/tco-optimizer-api/#create-single-policy-override","title":"Create single policy override","text":"<p>POST /overrides</p> <p>Route example: https://api.coralogixstg.wpengine.com/api/v1/external/tco/overrides</p> <p>Request example:</p> <pre><code>{\n    \"priority\": \"high\",\n    \"severity\": 3,\n    \"applicationName\": \"default\",\n    \"subsystemName\": \"blablabla123\"\n}\n</code></pre> <p>Response example:</p> <pre><code>{\n    \"priority\": \"high\",\n    \"severity\": 3,\n    \"applicationName\": \"default\",\n    \"subsystemName\": \"blablabla123\",\n    \"id\": \"972f6b98-343c-11ee-ac29-061115d0c307\"\n}\n</code></pre>"},{"location":"newoutput/tco-optimizer-api/#create-multiple-policy-overrides","title":"Create multiple policy overrides","text":"<p>POST /overrides/bulk</p> <p>Route example: https://api.coralogixstg.wpengine.com/api/v1/external/tco/overrides/bulk</p> <p>Request example:</p> <pre><code>[\n    {\n        \"priority\": \"high\",\n        \"severity\": 3,\n        \"applicationName\": \"default\",\n        \"subsystemName\": \"blablabla1234\"\n    },\n    {\n        \"priority\": \"high\",\n        \"severity\": 3,\n        \"applicationName\": \"default\",\n        \"subsystemName\": \"blablabla12345\"\n    }\n]\n</code></pre> <p>Response example:</p> <pre><code>[\n    {\n        \"status\": 200,\n        \"override\": {\n            \"priority\": \"high\",\n            \"severity\": 3,\n            \"applicationName\": \"default\",\n            \"subsystemName\": \"blablabla1234\",\n            \"id\": \"2c42a7aa-343d-11ee-ac29-061115d0c307\"\n        }\n    },\n    {\n        \"status\": 200,\n        \"override\": {\n            \"priority\": \"high\",\n            \"severity\": 3,\n            \"applicationName\": \"default\",\n            \"subsystemName\": \"blablabla12345\",\n            \"id\": \"2c53d05f-343d-11ee-ac29-061115d0c307\"\n        }\n    }\n]\n</code></pre>"},{"location":"newoutput/tco-optimizer-api/#update-multiple-policy-overrides","title":"Update multiple policy overrides","text":"<p>PUT /overrides/bulk</p> <p>Route example: https://api.coralogixstg.wpengine.com/api/v1/external/tco/overrides/bulk</p> <p>Request example:</p> <pre><code>[\n    {\n        \"id\": \"2c42a7aa-343d-11ee-ac29-061115d0c307\",\n        \"name\": \"default|blablabla1234|INFO\",\n        \"priority\": \"high\",\n        \"severity\": 3,\n        \"applicationName\": \"default\",\n        \"subsystemName\": \"blablabla1234\"\n    },\n    {\n        \"id\": \"2c53d05f-343d-11ee-ac29-061115d0c307\",\n        \"name\": \"default|blablabla12345|INFO\",\n        \"priority\": \"high\",\n        \"severity\": 3,\n        \"applicationName\": \"default\",\n        \"subsystemName\": \"blablabla12345\"\n    }\n]\n</code></pre> <p>Response example:</p> <pre><code>[\n    {\n        \"status\": 200,\n        \"override\": {\n            \"name\": \"default|blablabla1234|INFO\",\n            \"priority\": \"high\",\n            \"severity\": 3,\n            \"applicationName\": \"default\",\n            \"subsystemName\": \"blablabla1234\",\n            \"id\": \"2c42a7aa-343d-11ee-ac29-061115d0c307\"\n        }\n    },\n    {\n        \"status\": 200,\n        \"override\": {\n            \"name\": \"default|blablabla12345|INFO\",\n            \"priority\": \"high\",\n            \"severity\": 3,\n            \"applicationName\": \"default\",\n            \"subsystemName\": \"blablabla12345\",\n            \"id\": \"2c53d05f-343d-11ee-ac29-061115d0c307\"\n        }\n    }\n]\n</code></pre>"},{"location":"newoutput/tco-optimizer-api/#delete-single-policy-override","title":"Delete single policy override","text":"<p>DELETE /overrides/{id}</p> <p>Route example: https://api.coralogixstg.wpengine.com/api/v1/external/tco/overrides/2c53d05f-343d-11ee-ac29-061115d0c307</p> <p>Response example:</p> <pre><code>{\n    \"id\": \"2c53d05f-343d-11ee-ac29-061115d0c307\"\n}\n</code></pre>"},{"location":"newoutput/tco-optimizer-api/#delete-multiple-policy-overrides","title":"Delete multiple policy overrides","text":"<p>DELETE /overrides/bulk</p> <p>Route example: https://api.coralogixstg.wpengine.com/api/v1/external/tco/overrides/bulk</p> <p>Request example:</p> <pre><code>[\n    {\n        \"id\": \"2c42a7aa-343d-11ee-ac29-061115d0c307\"\n    }\n]\n</code></pre> <p>Response example:</p> <pre><code>[\n    {\n        \"status\": 200,\n        \"override\": {\n            \"id\": \"2c42a7aa-343d-11ee-ac29-061115d0c307\"\n        }\n    }\n]\n</code></pre>"},{"location":"newoutput/tco-optimizer-api/#additional-resources","title":"Additional Resources","text":"DocumentationTCO OptimizerTCO Tracing gPRC API"},{"location":"newoutput/tco-optimizer-api/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/tco-tracing-policy-grpc-api/","title":"TCO Tracing Policy gRPC API","text":"<p>This tutorial demonstrates how to use our TCO Tracing gPRC API to define, query, and manage your TCO policy criteria, used both for spans and logs.</p> <p>View this page to learn how to use our TCO Optimizer HTTP API to define, query, and manage your TCO policy overrides, used exclusively for logs.</p>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#overview","title":"Overview","text":"<p>Tracing policies aim to match <code>spans</code>, which are the fundamental components of a <code>trace</code>. The matching process is carried out based on the rules you define. Those rules can be defined using <code>application/subsystem/operation/service</code> names or by <code>tag</code> names and values. There are various types of matching rules, explained in detail below.</p>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#matching-process","title":"Matching Process","text":"<p>Spans that match all rules in a policy will be assigned the priority level defined in that policy.</p> <p>Every rule targets a property (e.g. <code>Application</code>) by <code>RuleTypeId</code>.</p> <p>TagRule targets a tag name and a tag value (e.g. tag name: <code>tags.http.method</code> and tag value: <code>GET</code>).</p> <p>Examples:</p> <ul> <li> <p>A rule on <code>Application</code> with the RuleTypeId <code>RULE_TYPE_ID_IS</code> and the name <code>default,app1</code> will match all spans with application name <code>default</code> or <code>app1</code>.</p> </li> <li> <p>A rule on <code>Subsystem</code> with RuleTypeId <code>RULE_TYPE_ID_START_WITH</code> and the name <code>authSer</code> will match all spans with subsystem names that start with <code>authSer</code>.</p> </li> <li> <p>A TagRule with the name <code>tags.http.target</code>, RuleTypeId <code>RULE_TYPE_ID_INCLUDES</code>, and the value <code>/api</code> will match all spans that have the tag <code>tags.http.target</code> and a value that contains <code>/api</code>.</p> </li> </ul>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#how-to-send-grpc-requests","title":"How to Send gRPC Requests","text":"<p>STEP 1. Install grpcurl.</p> <p>STEP 2. Access your Alerts, Rules and Tags API Key. From your Coralogix toolbar, navigate to Data Flow &gt; API Keys.</p> <p></p> <p>STEP 3. Run grpcurl with this template:</p> <pre><code>grpcurl -H \"Authorization: &lt;API_KEY&gt;\" -d \"&lt;DATA JSON OBJECT&gt;\" &lt;HOST_NAME&gt; &lt;GRPC_METHOD&gt;\n\n</code></pre> <p>You can find the <code>HOST_NAME</code> here, depending on your Coralogix domain and region.</p>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#supported-api-calls","title":"Supported API Calls","text":"<p>The API supports the following gRPCs:</p> <ul> <li> <p>Get Policy</p> </li> <li> <p>Create Policy</p> </li> <li> <p>Update Policy</p> </li> <li> <p>Get Company Policies</p> </li> <li> <p>Delete Policy</p> </li> <li> <p>Reorder Policies</p> </li> <li> <p>Toggle Policy</p> </li> </ul> <p>Notes:</p> <ul> <li> <p>This API supports both log and tracing policies.</p> </li> <li> <p>In some requests, the field <code>source_type_rules</code> appears, containing unique fields for each. The field <code>source_type</code> may also appear, allowing you to decide which type of policies you want to receive or affect.</p> </li> </ul> <p>The API supports the following gRPCs:</p>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#api-calls","title":"API Calls","text":""},{"location":"newoutput/tco-tracing-policy-grpc-api/#rpc-getpolicygetpolicyrequest-returns-getpolicyresponse","title":"rpc GetPolicy(GetPolicyRequest) returns (GetPolicyResponse)","text":"<p>GetPolicyRequest</p> <pre><code>message GetPolicyRequest {\n  google.protobuf.StringValue id = 1;\n}\n\n</code></pre> Field Type Description id google.protobuf.StringValue ID for the requested policy <p>GetPolicyResponse</p> <pre><code>message GetPolicyResponse {\n  Policy policy = 1;\n}\n\n</code></pre> Field Type Description policy Policy Policy returned <p>Example</p> <pre><code>grpcurl -H \"Authorization: abcd\" -d '{\n  \"id\": \"abcd\"\n}' ng-api-grpc.eu2.coralogixstg.wpengine.com:443 com.coralogix.quota.v1.PoliciesService.GetPolicy\n</code></pre>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#rpc-createpolicycreatepolicyrequest-returns-createpolicyresponse","title":"rpc CreatePolicy(CreatePolicyRequest) returns (CreatePolicyResponse)","text":"<p>CreatePolicyRequest</p> <pre><code>message CreatePolicyRequest {\n  google.protobuf.StringValue name = 1;\n  google.protobuf.StringValue description = 2;\n  Priority priority = 3;\n  optional Rule application_rule = 4;\n  optional Rule subsystem_rule = 5;\n  optional ArchiveRetention archive_retention = 6;\n  oneof source_type_rules {\n    LogRules log_rules = 7;\n    SpanRules span_rules = 8;\n  };\n}\n\n</code></pre> Field Type Description name google.protobuf.StringValue Name for the new policy description google.protobuf.StringValue Description of the new policy. priority Priority TCO pipelines (HIGH, MEDIUM, LOW) application_rule Rule [Optional] Rule that targets specific application names subsystem_rule Rule [Optional] Rule that targets specific subsystem names archive_retention ArchiveRetention [Optional] See Archive Retention Policy source_type_rules oneof Use only one of the following two fields log_rules LogRules Extra rules that can be defined for log policies span_rules SpanRules Extra rules that can be defined for span policies <p>CreatePolicyResponse</p> <pre><code>message CreatePolicyResponse {\n  Policy policy = 1;\n}\n\n</code></pre> Field Type Description policy Policy Policy created <p>Example</p> <pre><code>grpcurl -H \"Authorization: abc\" -d '{\n  \"name\": \"policy name\",\n  \"priority\": \"PRIORITY_TYPE_HIGH\",\n  \"applicationRule\": {\n    \"ruleTypeId\": \"RULE_TYPE_ID_START_WITH\",\n    \"name\": \"sdasdazxczxca\"\n  },\n  \"archiveRetention\": {\n    \"id\": \"abcdid\"\n  },\n  \"spanRules\": {\n    \"tagRules\": [],\n    \"serviceRule\": {\n      \"ruleTypeId\": \"RULE_TYPE_ID_IS\",\n      \"name\": \"asdas\"\n    }\n  }\n}' ng-api-grpc.eu2.coralogixstg.wpengine.com:443 com.coralogix.quota.v1.PoliciesService.CreatePolicy\n</code></pre>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#rpc-updatepolicyupdatepolicyrequest-returns-updatepolicyresponse","title":"rpc UpdatePolicy(UpdatePolicyRequest) returns (UpdatePolicyResponse)","text":"<p>UpdatePolicyRequest</p> <pre><code>message UpdatePolicyRequest {\n  google.protobuf.StringValue id = 1;\n  google.protobuf.StringValue name = 2;\n  google.protobuf.StringValue description = 3;\n  Priority priority = 4;\n  optional Rule application_rule = 5;\n  optional Rule subsystem_rule = 6;\n  optional ArchiveRetention archive_retention = 7;\n  oneof source_type_rules {\n    LogRules log_rules = 8;\n    SpanRules span_rules = 9;\n  };\n}\n\n</code></pre> Field Type Description id google.protobuf.StringValue ID of the policy to update name google.protobuf.StringValue Name of the updated policy description google.protobuf.StringValue Description of the updated policy priority Priority TCO pipelines (HIGH, MEDIUM, LOW) application_rule Rule [Optional] Rule that targets specific application names subsystem_rule Rule [Optional] Rule that targets specific subsystem names archive_retention ArchiveRetention [Optional] See Archive Retention Policy source_type_rules oneof Use only one of the following two fields log_rules LogRules Extra rules that can be defined for log policies span_rules SpanRules Extra rules that can be defined for span policies <p>UpdatePolicyResponse</p> <pre><code>message UpdatePolicyResponse {\n  Policy policy = 1;\n}\n\n</code></pre> Field Type Description policy Policy Policy updated <p>Example</p> <pre><code>grpcurl -H \"Authorization: abcd\" -d '{\n  \"id\": \"asdf\",\n  \"name\": \"same name\",\n  \"priority\": \"PRIORITY_TYPE_MEDIUM\",\n  \"logRules\": {\n    \"severities\": [\n      \"SEVERITY_VERBOSE\",\n      \"SEVERITY_WARNING\"\n    ]\n  }\n}' ng-api-grpc.eu2.coralogixstg.wpengine.com:443 com.coralogix.quota.v1.PoliciesService.UpdatePolicy\n</code></pre>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#rpc-getcompanypoliciesgetcompanypoliciesrequest-returns-getcompanypoliciesresponse","title":"rpc GetCompanyPolicies(GetCompanyPoliciesRequest) returns (GetCompanyPoliciesResponse)","text":"<p>GetCompanyPoliciesRequest</p> <pre><code>message GetCompanyPoliciesRequest {\n  google.protobuf.BoolValue enabled_only = 1;\n  optional SourceType source_type = 2;\n}\n\n</code></pre> Field Type Description enabled_only google.protobuf.BoolValue Whether the policies returned should include only enabled policies or also disabled policies. True = only enabled policies source_type SourceType [Optional] Set the source type for policies to be returned. Possible values include SOURCE_TYPE_LOGS and SOURCE_TYPE_SPANS. If this field remains empty, you will receive all policies. <p>GetCompanyPoliciesResponse</p> <pre><code>message GetCompanyPoliciesResponse {\n  repeated Policy policies = 1;\n}\n\n</code></pre> Field Type Description policies Policy List of the company policies <p>Example</p> <pre><code>grpcurl -H \"Authorization: abcd\" -d '{\n  \"sourceType\": \"SOURCE_TYPE_SPANS\",\n  \"enabledOnly\": true\n}' ng-api-grpc.eu2.coralogixstg.wpengine.com:443 com.coralogix.quota.v1.PoliciesService.GetCompanyPolicies\n</code></pre>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#rpc-deletepolicydeletepolicyrequest-returns-deletepolicyresponse","title":"rpc DeletePolicy(DeletePolicyRequest) returns (DeletePolicyResponse)","text":"<p>DeletePolicyRequest</p> <pre><code>message DeletePolicyRequest {\n  google.protobuf.StringValue id = 1;\n}\n\n</code></pre> Field Type Description id google.protobuf.StringValue ID of the policy to delete <p>DeletePolicyResponse</p> <pre><code>message DeletePolicyResponse {\n  google.protobuf.StringValue id = 1;\n}\n\n</code></pre> Field Type Description id google.protobuf.StringValue ID of the policy to delete <p>Example</p> <pre><code>grpcurl -H \"Authorization: abcd\" -d '{\n  \"id\": \"abcdid\"\n}' ng-api-grpc.eu2.coralogixstg.wpengine.com:443 com.coralogix.quota.v1.PoliciesService.DeletePolicy\n</code></pre>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#rpc-reorderpoliciesreorderpoliciesrequest-returns-reorderpoliciesresponse","title":"rpc ReorderPolicies(ReorderPoliciesRequest) returns (ReorderPoliciesResponse)","text":"<p>ReorderPoliciesRequest</p> <pre><code>message ReorderPoliciesRequest {\n  repeated PolicyOrder orders = 1;\n  SourceType source_type = 2;\n}\n\n</code></pre> Field Type Description orders PolicyOrder [Repeated] New order in which to put the policies source_type SourceType Source type of the policies to reorder. See the SourceType enum below for possible values. <p>ReorderPoliciesResponse</p> <pre><code>message ReorderPoliciesResponse {\n  repeated PolicyOrder orders = 1;\n}\n\n</code></pre> Field Type Description orders PolicyOrder [Repeated] New order of the policies <p>Example</p> <pre><code>grpcurl -H \"Authorization: abcd\" -d '{\n  \"orders\": [\n    {\n      \"order\": 0,\n      \"id\": \"abcd1\"\n    },\n    {\n      \"order\": 1,\n      \"id\": \"abcd2\"\n    }\n  ],\n  \"sourceType\": \"SOURCE_TYPE_LOGS\"\n}' ng-api-grpc.eu2.coralogixstg.wpengine.com:443 com.coralogix.quota.v1.PoliciesService.ReorderPolicies\n</code></pre>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#rpc-togglepolicytogglepolicyrequest-returns-togglepolicyresponse","title":"rpc TogglePolicy(TogglePolicyRequest) returns (TogglePolicyResponse)","text":"<p>TogglePolicyRequest</p> <pre><code>message TogglePolicyRequest {\n  google.protobuf.StringValue id = 1;\n  google.protobuf.BoolValue enabled = 2;\n}\n\n</code></pre> Field Type Description id google.protobuf.StringValue ID of the policy to enable or disable. enabled google.protobuf.BoolValue Whether the policy is enabled or disabled. True = policy is enabled <p>TogglePolicyResponse</p> <pre><code>message TogglePolicyResponse {\n  google.protobuf.StringValue id = 1;\n  google.protobuf.BoolValue enabled = 2;\n}\n\n</code></pre> Field Type Description id google.protobuf.StringValue ID of the enabled or disabled policy. enabled google.protobuf.BoolValue Whether the policy is enabled or disabled. True = policy is enabled <p>Example</p> <pre><code>grpcurl -H \"Authorization: abcd\" -d '{\n  \"id\": \"abcd1\",\n  \"enabled\": true\n}' ng-api-grpc.eu2.coralogixstg.wpengine.com:443 com.coralogix.quota.v1.PoliciesService.TogglePolicy\n</code></pre>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#types","title":"Types","text":""},{"location":"newoutput/tco-tracing-policy-grpc-api/#policy","title":"Policy","text":"<pre><code>message Policy {\n  google.protobuf.StringValue id = 1;\n  google.protobuf.Int32Value company_id = 2;\n  google.protobuf.StringValue name = 3;\n  google.protobuf.StringValue description = 4;\n  Priority priority = 5;\n  google.protobuf.BoolValue deleted = 6;\n  google.protobuf.BoolValue enabled = 7;\n  google.protobuf.Int32Value order = 8;\n  optional Rule application_rule = 9;\n  optional Rule subsystem_rule = 10;\n  oneof source_type_rules {\n    LogRules log_rules = 11;\n    SpanRules span_rules = 12;\n  };\n  optional google.protobuf.StringValue created_at = 13;\n  optional google.protobuf.StringValue updated_at = 14;\n  optional ArchiveRetention archive_retention = 15;\n}\n\n</code></pre> Field Type Description id google.protobuf.StringValue ID of the policy company_id google.protobuf.Int32Value Company ID of the company that owns the policy name google.protobuf.StringValue Policy name description google.protobuf.StringValue Policy description priority Priority Policy priority deleted google.protobuf.BoolValue Whether or not the policy is deleted. True = deleted enabled google.protobuf.BoolValue Whether or not the policy is enabled. True = enabled order google.protobuf.Int32Value What number the policy comes in the order of policies application_rule Rule [Optional] Rule that targets specific application names subsystem_rule Rule [Optional] Rule that targets specific subsystem names. source_type_rules oneof Use one of the following two fields log_rules LogRules [Optional] Extra rules that can be defined for log policies span_rules SpanRules [Optional] Extra rules that can be defined for span policies created_at google.protobuf.StringValue [Optional] Time and date when a policy was created updated_at google.protobuf.StringValue [Optional] Time and date when a policy was updated. archive_retention ArchiveRetention [Optional] See Archive Retention Policy"},{"location":"newoutput/tco-tracing-policy-grpc-api/#rule","title":"Rule","text":"<pre><code>message Rule {\n  RuleTypeId rule_type_id = 1;\n  google.protobuf.StringValue name = 2;\n}\n\n</code></pre> Field Type Description rule_type_id RuleTypeID Matching process is explained at the beginning of this page name google.protobuf.StringValue Rule name <p>Notes:</p> <ul> <li>For the values that need an exact match (e.g. <code>RULE_TYPE_ID_IS</code>, <code>RULE_TYPE_ID_IS_NOT</code>), the <code>name</code> property should be a list of values separated with a comma.</li> </ul> <p>Examples</p> <pre><code>const rule1: Rule = {\n  ruleTyleId: RuleTypeId.RULE_TYPE_ID_IS,\n  name: 'service1,myService2,anotherService'\n}\n\nconst rule2: Rule = {\n  ruleTyleId: RuleTypeId.RULE_TYPE_ID_START_WITH,\n  name: 'serv'\n}\n\n</code></pre>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#archiveretention","title":"ArchiveRetention","text":"<pre><code>message ArchiveRetention {\n  google.protobuf.StringValue id = 1;\n}\n\n</code></pre> Field Type Description id google.protobuf.StringValue See Archive Retention Policy"},{"location":"newoutput/tco-tracing-policy-grpc-api/#logrules","title":"LogRules","text":"<pre><code>message LogRules {\n  repeated Severity severities = 1;\n}\n\n</code></pre> Field Type Description severities Severity Every span is classified with a severity level. You can target spans with specific severity levels."},{"location":"newoutput/tco-tracing-policy-grpc-api/#spanrules","title":"SpanRules","text":"<pre><code>message SpanRules {\n  optional Rule service_rule = 1;\n  optional Rule action_rule = 2;\n  repeated TagRule tag_rules = 3;\n}\n\n</code></pre> Field Type Description service_rule Rule [Optional] Rule that targets specific service names action_rule Rule [Optional] Rule that targets specific action (operation) names tag_rules TagRule [Repeated] Rule that targets a specific tag name and value"},{"location":"newoutput/tco-tracing-policy-grpc-api/#tagrule","title":"TagRule","text":"<pre><code>message TagRule {\n  RuleTypeId rule_type_id = 1;\n  google.protobuf.StringValue tag_name = 2;\n  google.protobuf.StringValue tag_value = 3;\n}\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>Work the same way <code>Rule</code> works.</p> </li> <li> <p><code>tag_value</code> is equivalent to <code>name</code>.</p> </li> <li> <p><code>Rule</code>. <code>tag_name</code> refers to the name of the tag that the rule should impact.</p> </li> </ul>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#policyorder","title":"PolicyOrder","text":"<pre><code>message PolicyOrder {\n  google.protobuf.Int32Value order = 1;\n  google.protobuf.StringValue id = 2;\n}\n\n</code></pre> Field Type Description order google.protobuf.Int32Value Order for the policies to be in id google.protobuf.StringValue IDs of the different policies"},{"location":"newoutput/tco-tracing-policy-grpc-api/#enums","title":"Enums","text":"<pre><code>enum RuleTypeId {\n  RULE_TYPE_ID_UNSPECIFIED = 0;\n  reserved 1;\n  RULE_TYPE_ID_IS = 2;\n  RULE_TYPE_ID_IS_NOT = 3;\n  RULE_TYPE_ID_START_WITH = 4;\n  reserved 5;\n  RULE_TYPE_ID_INCLUDES = 6;\n  reserved 7;\n}\n\n</code></pre> <pre><code>enum Priority {\n  PRIORITY_TYPE_UNSPECIFIED = 0;\n  PRIORITY_TYPE_BLOCK = 1;\n  PRIORITY_TYPE_LOW = 2;\n  PRIORITY_TYPE_MEDIUM = 3;\n  PRIORITY_TYPE_HIGH = 4;\n}\n\n</code></pre> <pre><code>enum Severity {\n  SEVERITY_UNSPECIFIED = 0;\n  SEVERITY_DEBUG = 1;\n  SEVERITY_VERBOSE = 2;\n  SEVERITY_INFO = 3;\n  SEVERITY_WARNING = 4;\n  SEVERITY_ERROR = 5;\n  SEVERITY_CRITICAL = 6;\n}\n\n</code></pre> <pre><code>enum SourceType {\n  SOURCE_TYPE_UNSPECIFIED = 0;\n  SOURCE_TYPE_LOGS = 1;\n  SOURCE_TYPE_SPANS = 2;\n}\n\n</code></pre>"},{"location":"newoutput/tco-tracing-policy-grpc-api/#additional-resources","title":"Additional Resources","text":"DocumentationTCO OptimizerTCO Optimizer HTTP API"},{"location":"newoutput/tco-tracing-policy-grpc-api/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/team-management-via-cli/","title":"Team Management (via CLI)","text":"<p>The Coralogix CLI tool supports the management of teams. Actions supported on the CLI are creating teams, inviting members to teams (with role assignment). Users to be invited can be provided in one of the following formats:</p> <ul> <li> <p>Comma-separated command-line arguments</p> </li> <li> <p>New-line delimited text file\u00a0\u00a0</p> </li> </ul> <p>This capability allows team management to be automated using scripts or other provisioning tools.\u00a0</p> <p>This tutorial will guide you on managing teams using the CLI tool.</p>"},{"location":"newoutput/team-management-via-cli/#getting-started","title":"Getting Started","text":"<p>Only Organization and Platform administrators have the predefined permissions to manage teams.</p> <p>STEP 1. Install the latest version of the Coralogix CLI.</p> <p>STEP 2. Access your Teams API key by navigating to Data Flow &gt; API Keys from your Coralogix toolbar. Input this as your CORALOGIX_API_KEY environment variable.</p> <p>Note: When the environment variable is set, --api-key (-k) becomes an optional argument when using the tool.</p> <p>STEP 3. Access your Team ID. (Account -&gt; Settings -&gt; Send Your Logs)</p>"},{"location":"newoutput/team-management-via-cli/#commands","title":"Commands","text":""},{"location":"newoutput/team-management-via-cli/#create-team","title":"create-team","text":"<p>This command creates a new team.</p>"},{"location":"newoutput/team-management-via-cli/#invite","title":"invite","text":"<p>This command will invite users to join the specified team-id (optionally assign roles)</p> <p>Note: A default role of user is assigned if none is specified\u00a0</p>"},{"location":"newoutput/team-management-via-cli/#examples","title":"Examples","text":"<p>Note: The Examples below assume the api-key is provided as an environment variable.</p> <p>[table id=74 /]</p>"},{"location":"newoutput/team-management-via-cli/#options","title":"Options","text":"<p>[table id=75 /]</p>"},{"location":"newoutput/telegraf/","title":"Telegraf","text":"<p>Telegraf is a server-based agent for\u00a0collecting and sending metrics for further processing. It's a piece of software that you can install anywhere in your infrastructure and it will read metrics from specified sources \u2013 typically application logs, events, or data outputs.</p> <p>This tutorial demonstrates how to send your metrics to Coralogix via Telegraf.</p>"},{"location":"newoutput/telegraf/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Telegraf installed</p> </li> <li> <p>Active Coralogix account with metric bucket and a working Grafana dashboard for metrics</p> </li> </ul>"},{"location":"newoutput/telegraf/#configuration","title":"Configuration","text":"<p>If you are using Telegraf version <code>v1.23.4</code> or above, you can use the Coralogix dialect for a more straightforward configuration.</p> <p>In order to send your data to Coralogix, you are\u00a0required\u00a0to declare the following variables in your configuration:</p> <ul> <li> <p><code>**service_address**</code>: Coralogix OpenTelemetry endpoint associated with your Coralogix domain</p> </li> <li> <p><code>private_key</code>: Access your Coralogix Send-Your-Data API key. Your key is recorded in the override file as a secret in order to ensure that this sensitive information remains protected and unexposed.</p> </li> <li> <p><code>application</code> &amp; <code>subsystem</code>: Customize and organize your data in your Coralogix dashboard using application and subsystem names. Application name is available as a <code>__meta_applicationname</code> label for all metrics for this Telegraf instance. Subsystem name is available as a <code>__meta_subsystem</code> label for all metrics for this Telegraf instance.</p> </li> </ul> <p>The following example shows how to configure Telegraf using the Coralogix dialect:</p> <pre><code>[[outputs.opentelemetry]]\n service_address = \"&lt;coralogix-otel-endpoint&gt;\"\n compression = \"gzip\"\n [outputs.opentelemetry.coralogix]\n private_key = \"&lt;private_key&gt;\"\n application = \"&lt;application&gt;\"\n subsystem = \"&lt;subsystem&gt;\" \n</code></pre> <p>For older versions of Telegraf (<code>v1.23.3</code> or earlier), you should use the following configuration to send telemetry data using the OpenTelemetry output plugin:</p> <pre><code> [[outputs.opentelemetry]]\n   service_address = \"&lt;coralogix-otel-endpoint&gt;\"\n   insecure_skip_verify = true\n   compression = \"gzip\"\n   [outputs.opentelemetry.headers]\n     Authorization = \"Bearer &lt;private_key&gt;\"\n     ApplicationName = \"&lt;application&gt;\"\n     ApiName = \"&lt;subsystem&gt;\"\n\n</code></pre>"},{"location":"newoutput/telegraf/#monitoring-node","title":"Monitoring Node","text":"<p>In this example, we set up a Telegraf agent to monitor Node, which runs a website. It's a development environment, so we set ApplicationName to \"development\" and subsystem to \"website\". Additionally, we provide more metadata in the <code>\\\\\\\\[global\\\\\\\\_tags\\\\\\\\]</code> section, such as <code>dc=\"us-east-1\"</code> and <code>rack=\"1a\"</code>. For input plugins, we use:</p> <ul> <li> <p>cpu</p> </li> <li> <p>disk</p> </li> <li> <p>diskio</p> </li> <li> <p>kernel</p> </li> <li> <p>mem</p> </li> <li> <p>processes</p> </li> <li> <p>swap</p> </li> <li> <p>system</p> </li> </ul> <p>These plugins give us a good overview of the Node's metrics. The following example shows the complete Telegraf configuration:</p> <pre><code># Telegraf Configuration\n[global_tags]\n  # will tag all metrics with dc=us-east-1 and rack = \"1a\"\n  dc = \"us-east-1\"\n  rack = \"1a\"\n\n# Configuration for telegraf agent\n[agent]\n  interval = \"10s\"\n  round_interval = true\n  metric_batch_size = 1000\n  metric_buffer_limit = 10000\n  flush_interval = \"10s\"\n  flush_jitter = \"0s\"\n  precision = \"\"\n\n # Send OpenTelemetry metrics over gRPC\n [[outputs.opentelemetry]]\n   service_address = \"&lt;coralogix-otel-endpoint&gt;\"\n   insecure_skip_verify = true\n   compression = \"gzip\"\n   [outputs.opentelemetry.headers]\n     Authorization = \"Bearer &lt;private_key&gt;\"\n     ApplicationName = \"development\"\n     ApiName = \"website\"\n\n# Input plugins\n# Read metrics about cpu usage\n[[inputs.cpu]]\n  percpu = true\n  totalcpu = true\n  collect_cpu_time = false\n  report_active = false\n\n# Read metrics about disk usage by mount point\n[[inputs.disk]]\n  ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"]\n\n[[inputs.diskio]]\n[[inputs.kernel]]\n[[inputs.mem]]\n[[inputs.processes]]\n[[inputs.swap]]\n[[inputs.system]]\n\n\n</code></pre> <p>In Coralogix, you can query all the data using PromQL via Grafana. The following PromQL query shows us all the metrics exported by this Telegraf instance:</p> <pre><code>{__meta_applicationname=\"development\",__meta_subsystem=\"website\"}\n</code></pre> <p>Results in:</p> <pre><code>cpu_usage_guest{__meta_applicationname=\"development\", __meta_subsystem=\"website\", cpu=\"cpu-total\", cx_tenant_id=\"32680\", dc=\"us-east-1\", host=\"website-host\", hostname=\"$HOSTNAME\", rack=\"1a\"}\n....\ndiskio_io_time_total{__meta_applicationname=\"development\", __meta_subsystem=\"website\", cx_tenant_id=\"32680\", dc=\"us-east-1\", host=\"website-host, hostname=\"$HOSTNAME\", name=\"loop0\", rack=\"1a\"}\n\n\n</code></pre> <p>Note: application name, subsystem, and global tags are available as labels on the exported metrics.</p> <p>Next, you can start building dashboards and configuring different input plugins.</p>"},{"location":"newoutput/telegraf/#troubleshooting","title":"Troubleshooting","text":"<p>If you are having issues, you can enable debug logs via the Telegraf configuration:</p> <pre><code>[agent]\n  debug = true\n...\n\n\n</code></pre> <p>Telegraf will then start logging a detailed overview of what is happening. For example:</p> <pre><code>2022-10-19T11:12:34Z D! [outputs.opentelemetry] Wrote batch of 43 metrics in 87.062989ms\n2022-10-19T11:12:34Z D! [outputs.opentelemetry] Buffer fullness: 0 / 10000 metrics\n2022-10-19T11:13:20Z D! [inputs.disk] [SystemPS] -&gt; using mountpoint \"/run/user/1000/doc\"...\n2022-10-19T11:13:20Z D! [inputs.disk] [SystemPS] =&gt; dropped by disk usage (\"/run/user/1000/doc\"): operation not permitted\n\n\n</code></pre>"},{"location":"newoutput/telegraf/#additional-resources","title":"Additional Resources","text":"<p>Telegraf Docs by InfluxDB</p>"},{"location":"newoutput/telegraf/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/telegraf-operator/","title":"Telegraf Operator","text":"<p>Supported OS: linux | windows | macOS</p> <p>Telegraf Operator is an application designed to create and manage individual Telegraf instances in Kubernetes clusters. Use it to collect, process, and aggregate metrics from applications in your Kubernetes cluster and send them to Coralogix.</p> <p>Once installed, Telegraf Operator watches for pods deployed with a specific set of pod annotations. The advantage of using Telegraf Operator is that you only have to define the input plugin configuration for Telegraf when creating the pod annotations. Telegraf Operator then sets the configuration for the entire cluster, avoiding the need to configure a metrics destination when deploying applications.</p> <p>This tutorial demonstrates how to run Telegraf Operator in Kubernetes to export your telemetry data to Coralogix.</p>"},{"location":"newoutput/telegraf-operator/#prerequisites","title":"Prerequisites","text":"<p>1. Sign up for a Coralogix account. Set up your account on the Coralogix domain corresponding to the region within which you would like your data stored.</p> <p>2. Access your Coralogix Send-Your-Data API key.</p> <p>3. Install Kubernetes. This should include installation of the command-line tool kubectl, designed to operate on your Kubernetes cluster.</p> <p>4. Install and configure Helm. We suggest you use this guide to familiarize yourself with the basics of using Helm to manage packages on your Kubernetes cluster.</p>"},{"location":"newoutput/telegraf-operator/#set-up","title":"Set Up","text":""},{"location":"newoutput/telegraf-operator/#installation","title":"Installation","text":"<p>STEP 1: Add the Influxdata Helm Chart Repository</p> <p>Open a new terminal and install the Influxdata Helm chart repository:</p> <pre><code>helm repo add influxdata &lt;https://helm.influxdata.com/&gt;\n</code></pre> <pre><code>helm repo update\n</code></pre> <p>STEP 2: Install Telegraf Operator</p> <p>Update the Helm chart from the added repository:</p> <pre><code>export NAMESPACE=&lt;namespace&gt;\n</code></pre> <pre><code>helm upgrade --install telegraf-operator influxdata/telegraf-operator -n $NAMESPACE\n</code></pre>"},{"location":"newoutput/telegraf-operator/#configuration","title":"Configuration","text":"<p>STEP 1: Create YAML-Formatted Override File</p> <p>Telegraf Operator allows you to configure how Telegraf sidecars send data using classes. Each class is a subset of the Telegraf configuration and defines where Telegraf Operator should be sending its outputs. You can create or modify existing classes using Helm values file, as shown in the example below.</p> <p>In order to send your data to Coralogix, you are required to declare particular variables in your template:</p> <ul> <li> <p><code>service_address</code>: Coralogix\u00a0OpenTelemetry endpoint\u00a0associated with your Coralogix domain</p> </li> <li> <p><code>private_key</code>: Coralogix Send-Your-Data API key, recorded in the override file as a secret in order to ensure that it remains protected and unexposed</p> </li> <li> <p><code>$HOSTNAME</code> &amp; <code>$NODENAME</code> : Each Telegraf Operator class includes these global tags. <code>$HOSTNAME</code>, which will serve as your Coralogix <code>subsystem</code> name, is typically set to pod name. <code>$NODENAME</code> is typically set to node name. Find out more about defining environment variables for Kubernetes containers here.</p> </li> <li> <p><code>$NAMESPACE</code>: This variable is Telegraf Operator pod annotation <code>telegraf.influxdata.com/env-fieldref-NAMESPACE: metadata.namespace</code> that can be used to configure the Telegraf sidecar. This variable will serve as your Coralogix <code>application</code> name.</p> </li> </ul> <pre><code>image:\n sidecarImage: \"docker.io/library/telegraf:1.24.2\"\nclasses:\n  secretName: \"telegraf-operator-classes\"\n  default: \"infra\"\n  data:\n    infra: |\n      [[outputs.opentelemetry]]\n        service_address = \"&lt;coralogix_otel_endpoint&gt;\"\n        compression = \"gzip\"\n        [outputs.opentelemetry.coralogix]\n          private_key = \"&lt;private_key&gt;\"\n          application = \"$NAMESPACE\"\n          subsystem = \"$HOSTNAME\"\n      [global_tags]\n        env = \"ci\"\n        hostname = \"$HOSTNAME\"\n        nodename = \"$NODENAME\"\n        namespace = \"$NAMESPACE\"\n        type = \"infra\"\n\n</code></pre> <p>Save this file as a coralogix-values.yaml file in a directory of your choice.</p> <p>STEP 2: Apply the Configuration</p> <p>Apply the configuration by running the following command:</p> <pre><code>helm upgrade --install telegraf-operator influxdata/telegraf-operator --values &lt;path/to/coralogix-values.yaml&gt; -n &lt;namespace&gt;\n\n</code></pre>"},{"location":"newoutput/telegraf-operator/#validation","title":"Validation","text":"<p>To test your configuration, deploy a Redis instance with Telegraf Operator annotations.</p> <p>1. Create a pod configuration redis.yaml file with the following data:</p> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  serviceName: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n      annotations:\n        telegraf.influxdata.com/inputs: |+\n          [[inputs.redis]]\n            servers = [\"tcp://localhost:6379\"]\n        telegraf.influxdata.com/class: infra\n        telegraf.influxdata.com/env-fieldref-NAMESPACE: metadata.namespace\n    spec:\n      containers:\n      - name: redis\n        image: redis:alpine3.16\n\n</code></pre> <p>2. Run the pod by executing:</p> <pre><code>kubectl apply -f &lt;path/to/redis.yaml&gt; -n &lt;redis-namespace&gt;\n\n</code></pre> <p>3. The pod should contain 2 running containers: redis and telegraf. Verify this by executing:</p> <pre><code>kubectl describe pod -n &lt;redis-namespace&gt; redis-0\n\n</code></pre> <p>Here is an output example:</p> <p></p> <p>4. Verify that the pod is running:</p> <pre><code>kubectl get sts -n &lt;redis-namespace&gt; redis\n\n</code></pre> <p>Here is an output example:</p> <p></p> <p>5. Access your Coralogix - Grafana dashboard to query the monitored targets.</p> <ul> <li>On the right-hand corner of your dashboard, click on the Grafana drop-down tab.</li> </ul> <p></p> <ul> <li>Once you\u2019ve accessed your Coralogix-Grafana dashboard, click on Explore tab in the left-hand browser.</li> </ul> <p></p> <ul> <li>Click on the drop-down arrow of the Metrics browser and input <code>target_info</code> in the \u201cSelect a Metric\u201d column.</li> </ul> <p></p> <ul> <li>Click on the \u201cUse Query\u201d button in the bottom left-hand corner.</li> </ul> <p>6. The query above should return metrics similar to the following:</p> <pre><code>target_info{__meta_applicationname=\"&lt;redis-namespace&gt;\", __meta_subsystem=\"redis-0\"}\n\n</code></pre> <p>7. To discover all Redis metrics, use the following PromQL:</p> <pre><code>{__meta_applicationname=\"&lt;redis-namespace&gt;\", __meta_subsystem=\"redis-0\"}\n\n</code></pre> <p>8. This should result in:</p> <pre><code>...\nredis_total_connections_received{__meta_applicationname=\"&lt;redis-namespace&gt;, __meta_subsystem=\"redis-0\", cx_tenant_id=\"32680\", env=\"ci\", host=\"redis-0\", hostname=\"redis-0\", namespace=\"&lt;redis-namespace&gt;\", nodename=\"rock64-1\", port=\"6379\", replication_role=\"master\", server=\"localhost\", type=\"infra\"}\n\n</code></pre> <p>Here is a display of the expected result:</p> <p></p> <p>9. Once you have tested and validated your configuration with the Redis instance, delete this service by running the following command:</p> <pre><code>kubectl delete -f &lt;path/to/redis.yaml&gt; -n &lt;redis-namespace&gt;\n\n</code></pre>"},{"location":"newoutput/telegraf-operator/#additional-references","title":"Additional References","text":"GithubOfficial Telegraf-Operator DocumentationOffical Influxdata Chart Repo"},{"location":"newoutput/telegraf-operator/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/terraform-module-for-azure-queue-storage/","title":"Azure Queue Storage Terraform Module","text":"<p>Using our Terraform modules, you can easily install and manage Coralogix integrations with Azure services as modules in your infrastructure code. This tutorial demonstrates how to install our function app that connects to your storage queue and sends logs to Coralogix.</p> <p>Our modules are open-source and available on\u00a0Github\u00a0and in the\u00a0Terraform registry.</p>"},{"location":"newoutput/terraform-module-for-azure-queue-storage/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>A resource group and storage account to be used by your function app and provided as inputs in the Terraform module</p> </li> <li> <p>Preexisting storage queue</p> </li> <li> <p>Storage account associated with the storage queue configured for public access (Optional VNet support configuration available)</p> </li> </ul>"},{"location":"newoutput/terraform-module-for-azure-queue-storage/#installation","title":"Installation","text":"<p>To install our\u00a0function app, which connects to your storage queue and sends logs to Coralogix, add this declaration to your Terraform project:</p> <pre><code>terraform {\n  required_providers {\n    azurerm = {\n      source = \"hashicorp/azurerm\"\n      version = \"~&gt; 3.93\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n}\n\nmodule \"storagequeue\" {\n  source = \"coralogix/azure/coralogix//modules/storagequeue\"\n\n  CoralogixRegion = \"Europe\"\n  CustomDomain = &lt; Custom FQDN if applicable &gt;\n  CoralogixPrivateKey = &lt; Send Your Data - API Key &gt;\n  CoralogixApplication = \"Azure\"\n  CoralogixSubsystem = \"EventHub\"\n  FunctionResourceGroupName = &lt; Function ResourceGroup Name &gt;\n  FunctionStorageAccountName = &lt; Function StorageAccount Name &gt;\n  FunctionAppServicePlanType = \"Consumption\"\n  StorageQueueName = &lt; Name of the StorageQueue &gt;\n  StorageQueueStorageAccount = &lt; Name of the StorageQueue Storage Account &gt;\n  StorageQueueResourceGroupName = &lt; Name of the StorageQueue Resource Group &gt;\n}\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>Input the following variables:</p> <ul> <li> <p><code>**CoralogixRegion**</code>: The\u00a0region\u00a0associated with your Coralogix account</p> </li> <li> <p><code>CoralogixPrivateKey</code>:\u00a0Your\u00a0Coralogix Send-Your-Data API Key</p> </li> <li> <p><code>**CoralogixApplication**</code>\u00a0&amp;\u00a0<code>CoralogixSubsystem</code>:\u00a0Coralogix application and subsystem names\u00a0as they will appear in your UI</p> </li> </ul> </li> <li> <p>Descriptions for other variables can be found\u00a0here.</p> </li> </ul>"},{"location":"newoutput/terraform-module-for-azure-queue-storage/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Terraform ProviderExternal DocumentationCoralogix Function AppGithub: Azure \u2013 Coralogix Terraform ModuleTerraform Registry"},{"location":"newoutput/terraform-module-for-azure-queue-storage/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/terraform-module-for-cloudflare-logpush/","title":"Cloudflare Logpush Terraform Module","text":"<p>Terraform simplifies the way we deploy our infrastructure and allows us to maintain it as code.</p> <p>Using our Terraform Modules, you can easily install and manage Cloudflare logpush integrations to Coralogix as modules in your infrastructure code.</p> <p>Our modules are open source and available on our Github and in the Terraform Registry.</p>"},{"location":"newoutput/terraform-module-for-cloudflare-logpush/#installation","title":"Installation","text":"<p>This module will be creating a Logpush job which will send logs to your Coralogix account.</p> <p>Note: This module requires <code>Terraform</code>\u00a0 Version 1.20+\u00a0</p> <p>To use the module, first, add the provider configure block to your Terraform project:</p> <pre><code>terraform {\n  required_providers {\n    cloudflare = {\n      source  = \"cloudflare/cloudflare\"\n      version = \"~&gt; 3.0\"\n    }\n  }\n}\n\nprovider \"cloudflare\" {\n  email   = \"example@coralogixstg.wpengine.com\"\n  api_key = \"7ae12522bce3d8d988ec5f0ed8b8ef9016e09\"\n}\n</code></pre> <p>Important variables to change:</p> <p>cloudflare.email - Your email used in cloudflare.</p> <p>cloudflare.api_key - Your API key for cloudflare.</p> <p>Then add the module block:</p> <pre><code>module \"logpush-job\" {\n    source = \"coralogix/cloudflare/coralogix//modules/logpush-job\"\n\n    coralogix_region   = \"Europe\"\n    coralogix_private_key = \"79cf16dc-0dfa-430e-a651-ec76bfa96d01\"\n    cloudflare_logpush_dataset = \"http_requests\"\n    cloudflare_logpush_fields = \"RayID,ZoneName\" # can be left empty aswell for all fields\n    cloudflare_zone_id = \"ca17eeeb371963f662965e4de0ed7403\" # to be used with zone-scoped datasets\n    # cloudflare_account_id = \"bc20385621cb7dc622aeb4810ca235df\" # to be used with account-scoped datasets\n}\n</code></pre> <p>Important variables to change:</p> <ul> <li> <p>coralogix_region - Associated with your Coralogix domain, possible options are [Europe, Europe2, India, Singapore, US]</p> </li> <li> <p>coralogix_private_key - The Coralogix Send-Your-Data API key which is used to validate your authenticity</p> </li> <li> <p>cloudflare_logpush_dataset - The cloudflare logpush job data-set</p> </li> <li> <p>cloudflare_logpush_fields - The logpush dataset specific fields to log delimited with comma, leave empty to include all fields. the timestamp and its variants are included automatically.</p> </li> <li> <p>cloudflare_zone/account_id - Your zone/account id, can be retrieved from cloudflare dashboard or API.</p> </li> </ul> <p>By default, the integration will set <code>application_name</code> as <code>Cloudflare</code>, and <code>subsystem_name</code> as the data set name. To overwrite these parameters, add the following:</p> <ul> <li> <p><code>header_CX-Application-Name</code> - application name override</p> </li> <li> <p><code>header_CX-Subsystem-Name</code> - subsystem name override</p> </li> </ul> <p>We also have a Coralogix Terraform Provider to help manage your Coralogix resources such as rules and alerts. Install it here.</p> <p>If you have any questions, feel free to reach out to our team via our in-app chat!</p> <p>Values Table:</p> DatasetFieldsdns_logsColoCode, EDNSSubnet, EDNSSubnetLength, QueryName, QueryType, ResponseCached, ResponseCode, SourceIPfirewall_eventsAction, ClientASN, ClientASNDescription, ClientCountry, ClientIP, ClientIPClass, ClientRefererHost, ClientRefererPath, ClientRefererQuery, ClientRefererScheme, ClientRequestHost, ClientRequestMethod, ClientRequestPath, ClientRequestProtocol, ClientRequestQuery, ClientRequestScheme, ClientRequestUserAgent, EdgeColoCode, EdgeResponseStatus, Kind, MatchIndex, Metadata, OriginResponseStatus, OriginatorRayID, RayID, RuleID, Sourcehttp_requestsBotScoreCloudflare, BotScoreSrc, BotTags, CacheCacheStatus, CacheResponseBytes, CacheResponseStatus, CacheTieredFill, ClientASN, ClientCountry, ClientDeviceType, ClientIP, ClientIPClass, ClientMTLSAuthCertFingerprint, ClientMTLSAuthStatus, ClientRequestBytes, ClientRequestHost, ClientRequestMethod, ClientRequestPath, ClientRequestProtocol, ClientRequestReferer, ClientRequestScheme, ClientRequestSource, ClientRequestURI, ClientRequestUserAgent, ClientSSLCipher, ClientSSLProtocol, ClientSrcPort, ClientTCPRTTMs, ClientXRequestedWith, EdgeCFConnectingO2O, EdgeColoCode, EdgeColoID, EdgeEndTimestamp, EdgePathingOp, EdgePathingSrc, EdgePathingStatus, EdgeRateLimitAction, EdgeRateLimitID, EdgeRequestHost, EdgeResponseBodyBytes, EdgeResponseBytes, EdgeResponseCompressionRatio, EdgeResponseContentType, EdgeResponseStatus, EdgeServerIP, EdgeTimeToFirstByteMs, FirewallMatchesActions, FirewallMatchesRuleIDs, FirewallMatchesSources, JA3Hash, OriginDNSResponseTimeMs, OriginIP, OriginRequestHeaderSendDurationMs, OriginResponseBytes, OriginResponseDurationMs, OriginResponseHTTPExpires, OriginResponseHTTPLastModified, OriginResponseHeaderReceiveDurationMs, OriginResponseStatus, OriginResponseTime, OriginSSLProtocol, OriginTCPHandshakeDurationMs, OriginTLSHandshakeDurationMs, ParentRayID, RayID, RequestHeaders, ResponseHeaders, SecurityLevel, SmartRouteColoID, UpperTierColoID, WAFAction, WAFFlags, WAFMatchedVar, WAFProfile, WAFRuleID, WAFRuleMessage, WorkerCPUTime, WorkerStatus, WorkerSubrequest, WorkerSubrequestCount, ZoneID, ZoneNamenel_reportsClientIPASN, ClientIPASNDescription, ClientIPCountry, LastKnownGoodColoCode, Phase, Typespectrum_eventsApplication, ClientAsn, ClientBytes, ClientCountry, ClientIP, ClientMatchedIpFirewall, ClientPort, ClientProto, ClientTcpRtt, ClientTlsCipher, ClientTlsClientHelloServerName, ClientTlsProtocol, ClientTlsStatus, ColoCode, ConnectTimestamp, DisconnectTimestamp, Event, IpFirewall, OriginBytes, OriginIP, OriginPort, OriginProto, OriginTcpRtt, OriginTlsCipher, OriginTlsFingerprint, OriginTlsMode, OriginTlsProtocol, OriginTlsStatus, ProxyProtocol, Statusaudit_logsActionResult, ActionType, ActorEmail, ActorID, ActorIP, ActorType, ID, Interface, Metadata, NewValue, OldValue, OwnerID, ResourceID, ResourceTypegateway_dnsColoID, ColoName, DeviceID, DstIP, DstPort, Email, Location, MatchedCategoryIDs, Policy, PolicyID, Protocol, QueryCategoryIDs, QueryName, QueryNameReversed, QuerySize, QueryType, RData, ResolverDecision, SrcIP, SrcPort, UserIDgateway_httpAccountID, Action, BlockedFileHash, BlockedFileName, BlockedFileReason, BlockedFileSize, BlockedFileType, DestinationIP, DestinationPort, DeviceID, DownloadedFileNames, Email, HTTPHost, HTTPMethod, HTTPVersion,IsIsolated, PolicyID, Referer, RequestID, SourceIP, SourcePort, URL, UploadedFileNames, UserAgent, UserIDgateway_networkAccountID, Action, DestinationIP, DestinationPort, DeviceID, Email, OverrideIP, OverridePort , PolicyID, SNI, SessionID, SourceIP, SourcePort, Transport, UserIDnetwork_analytics_logsAttackCampaignID, AttackID, ColoCountry, ColoGeoHash, ColoID, ColoName, DestinationASN, DestinationASNDescription, DestinationCountry, DestinationGeoHash, DestinationPort, Direction, GREChecksum, GREEthertype, GREHeaderLength, GREKey, GRESequenceNumber, GREVersion, ICMPChecksum, ICMPCode, ICMPType, IPDestinationAddress, IPDestinationSubnet, IPFragmentOffset, IPHeaderLength, IPMoreFragments, IPProtocol, IPProtocolName, IPSourceAddress, IPSourceSubnet, IPTotalLength, IPTotalLengthBuckets, IPTtl, IPTtlBuckets, IPv4Checksum, IPv4DontFragment, IPv4Dscp, IPv4Ecn, IPv4Identification, IPv4Options, IPv6Dscp, IPv6Ecn, IPv6ExtensionHeaders, IPv6FlowLabel, IPv6Identification, MitigationReason, MitigationScope, MitigationSystem, ProtocolState, RuleID, RulesetID, RulesetOverrideID, SampleInterval, SourceASN, SourceASNDescription, SourceCountry, SourceGeoHash, SourcePort, TCPAcknowledgementNumber, TCPChecksum, TCPDataOffset, TCPFlags, TCPFlagsString, TCPMss, TCPOptions, TCPSackBlocks, TCPSacksPermitted, TCPSequenceNumber, TCPTimestampEcr, TCPTimestampValue, TCPUrgentPointer, TCPWindowScale, TCPWindowSize, UDPChecksum, UDPPayloadLength, Verdict <p>Common errors table:</p> ErrorDescriptioncreating a new job is not allowed: Bot Management fields are not allowed (1004)Your cloudflare account plan doesn't allow the specified fields in cloudflare_logpush_fields. contact cloudflare support to enable these fields.creating a new job is not allowed: exceeded max jobs allowed (1004)Your cloudflare account plan doesn't allow the specified dataset in cloudflare_logpush_dataset or you have reached your account maximum concurrent jobs. contact cloudflare support to ensure your account can create this logpush dataset and that you didn't exceed your maximum jobs allowed."},{"location":"newoutput/terraform-modules/","title":"AWS CloudWatch Terraform Module","text":"<p>Terraform simplifies the manner we deploy our infrastructure and allows us to maintain it as code.</p> <p>Using our Terraform Modules, you can easily install and manage Coralogix integrations with AWS services as modules in your infrastructure code.</p> <p>Our modules are open source and available on our GitHub and in the Terraform Registry.</p>"},{"location":"newoutput/terraform-modules/#installation","title":"Installation","text":"<p>This module will be installing our Cloudwatch collection lambda.</p> <p>STEP 1. Add this declaration to your Terraform project. Input the following parameters.</p> <pre><code>provider \"aws\" {\n}\nmodule \"cloudwatch_logs\" {\n  source = \"coralogix/aws/coralogix//modules/cloudwatch-logs\"\n\n  coralogix_region   = \"Europe\"\n  private_key        = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXX\"\n  application_name   = \"cloudwatch\"\n  subsystem_name     = \"logs\"\n  log_groups         = [\"test-log-group\"]\n}\n</code></pre> <p>Notes:</p> <ul> <li> <p>Input your Coralogix Send-Your-Data API key as <code>private_key</code>.</p> </li> <li> <p>Input the domain (as <code>CustomDomain</code>) and region (as <code>coralogix_region</code>) associated with your Coralogix account.</p> </li> </ul> <p>STEP 2. Execute the following:</p> <pre><code>$ terraform init\n$ terraform plan\n$ terraform apply\n</code></pre> <p>STEP 3. Run\u00a0<code>terraform destroy</code>\u00a0when you no longer need these resources.</p>"},{"location":"newoutput/terraform-modules/#aws-secrets-manager","title":"AWS Secrets Manager","text":"<p>Deploy the AWS Secrets Manager Lambda layer for any of our AWS integrations. Find out more here.</p>"},{"location":"newoutput/terraform-modules/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Terraform ProviderExternal DocumentationGitHubTerraform Registry"},{"location":"newoutput/terraform-modules/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/terraform-modules-aws-s3-logs/","title":"AWS S3 Logs Collection Terraform Module","text":"<p>Terraform simplifies the way we deploy our infrastructure and allows us to maintain it as code.</p> <p>Using our Terraform Modules, you can easily install and manage Coralogix integrations with AWS services as modules in your infrastructure code.</p> <p>Our modules are open source and available on our Github and in the Terraform Registry</p>"},{"location":"newoutput/terraform-modules-aws-s3-logs/#installation","title":"Installation","text":"<p>Install our S3 Log Collection Lambda by adding this declaration to your Terraform project.</p> <pre><code>module \"coralogix-shipper-s3\" {\n  source = \"coralogix/aws/coralogix//modules/s3\"\n\n  coralogix_region   = \"Europe\"\n  private_key        = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXX\"\n  application_name   = \"s3\"\n  subsystem_name     = \"logs\"\n  s3_bucket_name     = \"test-bucket-name\"\n  integration_type   = \"s3\"\n}\n</code></pre>"},{"location":"newoutput/terraform-modules-aws-s3-logs/#aws-secrets-manager","title":"AWS Secrets Manager","text":"<p>Deploy the AWS Secrets Manager Lambda layer for any of our AWS integrations. Find out more here.</p>"},{"location":"newoutput/terraform-modules-aws-s3-logs/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Terraform Provider"},{"location":"newoutput/terraform-modules-aws-s3-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/terraform-modules-for-amazon-web-services-aws/","title":"AWS Terraform Module","text":"<p>Terraform simplifies the way we deploy our infrastructure and allows us to maintain it as code.</p> <p>Using our Terraform Modules, you can easily install and manage Coralogix integrations with AWS services as modules in your infrastructure code.</p> <p>Our modules are open source and available on our\u00a0Github\u00a0and in the\u00a0Terraform Registry.</p>"},{"location":"newoutput/terraform-modules-for-amazon-web-services-aws/#prerequisites","title":"Prerequisites","text":"<p>STEP 1. Sign up\u00a0for a Coralogix account. Set up your account on the Coralogix\u00a0domain\u00a0corresponding to the region within which you would like your data stored.</p> <p>STEP 2. Access your Coralogix API key:</p> <ul> <li> <p>In your Coralogix navigation bar, click\u00a0Data Flow\u00a0&gt;\u00a0API Keys &gt; Alerts, Rules and Tags API Key.</p> </li> <li> <p>Copy this information.</p> </li> </ul> <p>STEP 3. Install\u00a0Terraform.</p>"},{"location":"newoutput/terraform-modules-for-amazon-web-services-aws/#installation","title":"Installation","text":"<p>Install the AWS service of your choice:</p> <ul> <li> <p>CloudTrail</p> </li> <li> <p>S3 Log Collection</p> </li> <li> <p>CloudWatch</p> </li> </ul>"},{"location":"newoutput/terraform-modules-for-amazon-web-services-aws/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/terraform-modules-for-aws-cloudtrail/","title":"AWS CloudTrail Terraform Module","text":"<p>Using Coralogix Terraform Modules, you can easily install and manage Coralogix integrations with AWS services as modules in your infrastructure code. This tutorial demonstrates how to install our CloudTrail collection Lambda.</p> <p>Our modules are open source and available on our Github and in the Terraform Registry.</p>"},{"location":"newoutput/terraform-modules-for-aws-cloudtrail/#installation","title":"Installation","text":"<p>Install our CloudTrail collection Lambda by adding this declaration to your Terraform project:</p> <pre><code>module \"coralogix-shipper-cloudtrail\" {\n  source = \"coralogix/aws/coralogix//modules/s3\"\n\n  coralogix_region   = \"Europe\"\n  private_key        = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXX\"\n  application_name   = \"cloudtrail\"\n  subsystem_name     = \"logs\"\n  s3_bucket_name     = \"test-bucket-name\"\n  integration_type   = \"cloudtrail\"\n}\n</code></pre>"},{"location":"newoutput/terraform-modules-for-aws-cloudtrail/#best-practices","title":"Best Practices","text":"<p>Customers should add environment variable\u00a0<code>CORALOGIX_BUFFER_SIZE</code>\u00a0with value\u00a0<code>268435456</code>.</p>"},{"location":"newoutput/terraform-modules-for-aws-cloudtrail/#aws-secrets-manager","title":"AWS Secrets Manager","text":"<p>Deploy the AWS Secrets Manager Lambda layer for any of our AWS integrations. Find out more here.</p>"},{"location":"newoutput/terraform-modules-for-aws-cloudtrail/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Terraform ProviderExternal DocumentationGitHubTerraform Registry"},{"location":"newoutput/terraform-modules-for-aws-cloudtrail/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/terraform-modules-for-aws-vpc-flow-logs/","title":"AWS VPC Flow Logs Terraform Module","text":"<p>Using Coralogix Terraform Modules, you can easily install and manage Coralogix integrations with AWS services as modules in your infrastructure code. This tutorial demonstrates how to install the VPC Flow Logs collection Lambda.</p> <p>Our modules are open source and available on our GitHub and in the Terraform Registry.</p>"},{"location":"newoutput/terraform-modules-for-aws-vpc-flow-logs/#installation","title":"Installation","text":"<p>Install our VPC Flow Logs collection Lambda by adding this declaration to your Terraform project:</p> <pre><code>module \"coralogix-shipper-vpc-flow-logs\" {\n  source = \"coralogix/aws/coralogix//modules/s3\"\n\n  coralogix_region   = \"Europe\"\n  private_key        = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXX\"\n  application_name   = \"vpc-flow-logs\"\n  subsystem_name     = \"logs\"\n  s3_bucket_name     = \"test-bucket-name\"\n  integration_type   = \"vpc-flow-logs\"\n}\n</code></pre>"},{"location":"newoutput/terraform-modules-for-aws-vpc-flow-logs/#aws-secrets-manager","title":"AWS Secrets Manager","text":"<p>Deploy the AWS Secrets Manager Lambda layer for any of our AWS integrations. Find out more here.</p>"},{"location":"newoutput/terraform-modules-for-aws-vpc-flow-logs/#additional-resources","title":"Additional Resources","text":"DocumentationAWS VPC Flow LogsCoralogix Terraform ProviderExternal DocumentationGitHubTerraform Registry"},{"location":"newoutput/terraform-modules-for-aws-vpc-flow-logs/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/terraform-modules-for-azure-blob-storage-via-event-grid/","title":"Azure Blob Storage via Event Grid Terraform Module","text":"<p>Using our Terraform modules, you can easily install and manage Coralogix integrations with Azure services as modules in your infrastructure code. This tutorial demonstrates how to install the Coralogix function app, allowing you to connect your Blob Storage container and send logs to Coralogix.</p> <p>Our modules are open-source and available on Github and in the Terraform registry.</p>"},{"location":"newoutput/terraform-modules-for-azure-blob-storage-via-event-grid/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>A resource group and storage account to be used by your function app</p> </li> <li> <p>Event Grid system topic configured for \"Storage Accounts (Blob &amp; GPv2)\" aligned to the storage account containing the Blob Storage container</p> <ul> <li>If it is a restricted storage account, review this optional configuration to learn about VNet support options.</li> </ul> </li> </ul>"},{"location":"newoutput/terraform-modules-for-azure-blob-storage-via-event-grid/#installation","title":"Installation","text":"<p>Install our function app that connects to your Blob Storage container and sends logs to Coralogix.</p> <p>Add this declaration to your Terraform project:</p> <pre><code>terraform {\n  required_providers {\n    azurerm = {\n      source = \"hashicorp/azurerm\"\n      version = \"~&gt; 3.93\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n}\n\nmodule \"eventhub\" {\n  source = \"coralogix/azure/coralogix//modules/eventhub\"\n\n  CoralogixRegion = \"Europe\"\n  CustomDomain = &lt; Custom FQDN if applicable &gt;\n  CoralogixPrivateKey = &lt; Send Your Data - API Key &gt;\n  CoralogixApplication = \"Azure\"\n  CoralogixSubsystem = \"EventHub\"\n  FunctionResourceGroupName = &lt; Function ResourceGroup Name &gt;\n  FunctionStorageAccountName = &lt; Function StorageAccount Name &gt;\n  FunctionAppServicePlanType = \"Consumption\"\n  EventhubInstanceName = &lt; Name of EventHub Instance &gt;\n  EventhubNamespace = &lt; Name of Eventhub Namespace &gt;\n  EventhubResourceGroupName = &lt; Name of Eventhub ResourceGroup &gt;\n}\n\n</code></pre> <p>Notes:</p> <ul> <li> <p><code>**CoralogixRegion**</code>: The region associated with your Coralogix account</p> </li> <li> <p><code>CoralogixPrivateKey</code>: Your Coralogix Send Your Data - API Key</p> </li> <li> <p><code>**CoralogixApplication**</code> &amp; <code>CoralogixSubsystem</code>: Application and subsystem names as they appear in your Coralogix UI</p> </li> <li> <p>Descriptions for other variables can be found here.</p> </li> </ul>"},{"location":"newoutput/terraform-modules-for-azure-blob-storage-via-event-grid/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Terraform ProviderExternal DocumentationGithubTerraform Registry"},{"location":"newoutput/terraform-modules-for-azure-blob-storage-via-event-grid/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/terraform-modules-for-azure-diagnostic-data/","title":"Azure Diagnostic Data Terraform Module","text":"<p>Using our Terraform modules, you can easily install and manage Coralogix integrations with Azure services as modules in your infrastructure code. This tutorial demonstrates how to install our function app, which processes logs and metrics that are forwarded through diagnostic settings to an Event Hub and are then transmitted to Coralogix.</p> <p>Our modules are open-source and available on Github and in the Terraform registry.</p>"},{"location":"newoutput/terraform-modules-for-azure-diagnostic-data/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>A resource group and storage account to be used by your function app must be provided as inputs in the Terraform module.</p> </li> <li> <p>The Event Hub namespace and instance must be preexisting.</p> </li> </ul>"},{"location":"newoutput/terraform-modules-for-azure-diagnostic-data/#installation","title":"Installation","text":"<p>This tutorial demonstrates how to install our function app, which processes logs and metrics that are forwarded through diagnostic settings to an Event Hub and are then transmitted to Coralogix.</p> <p>Add this declaration to your Terraform project:</p> <pre><code>terraform {\n  required_providers {\n    azurerm = {\n      source = \"hashicorp/azurerm\"\n      version = \"~&gt; 3.93\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n}\n\nmodule \"DiagnosticData\" {\n  source = \"coralogix/azure/coralogix//modules/DiagnosticData\"\n\n  CoralogixRegion = \"Europe\"\n  CustomDomain = &lt; Custom FQDN if applicable &gt;\n  CoralogixPrivateKey = &lt; Send Your Data - API Key &gt;\n  CoralogixApplication = \"Azure\"\n  CoralogixSubsystem = \"DiagnosticData\"\n  FunctionResourceGroupName = &lt; Function ResourceGroup Name &gt;\n  FunctionStorageAccountName = &lt; Function StorageAccount Name &gt;\n  FunctionAppServicePlanType = \"Consumption\"\n  EventhubInstanceName = &lt; Name of EventHub Instance &gt;\n  EventhubNamespace = &lt; Name of Eventhub Namespace &gt;\n  EventhubResourceGroupName = &lt; Name of Eventhub ResourceGroup &gt;\n}\n</code></pre> <p>Notes:</p> <ul> <li> <p>Input the following variables:</p> <ul> <li> <p><code>**CoralogixRegion**</code>: The region associated with your Coralogix account</p> </li> <li> <p><code>CoralogixPrivateKey</code>: Your Coralogix Send Your Data - API Key</p> </li> <li> <p><code>**CoralogixApplication**</code> &amp; <code>CoralogixSubsystem</code>: Application and subsystem names as they appear in your Coralogix UI</p> </li> <li> <p>Other variables can be found here.</p> </li> </ul> </li> <li> <p>An SAS policy will be created by the Terraform module to allow listen access to the Event Hub instance by the function app.</p> </li> </ul>"},{"location":"newoutput/terraform-modules-for-azure-diagnostic-data/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Terraform ProviderExternal DocumentationCoralogix Function AppGithub: Azure - Coralogix Terraform ModuleTerraform Registry"},{"location":"newoutput/terraform-modules-for-azure-diagnostic-data/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/terraform-modules-for-azure-eventhub/","title":"Azure Event Hub Terraform Module","text":"<p>Using our Terraform modules, you can easily install and manage Coralogix integrations with Azure services as modules in your infrastructure code. This tutorial demonstrates how to install our function app that connects to your Event Hub and sends logs to Coralogix.</p> <p>Our modules are open-source and available on\u00a0Github\u00a0and in the\u00a0Terraform registry.</p>"},{"location":"newoutput/terraform-modules-for-azure-eventhub/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>A resource group and storage account to be used by your function app must be provided as inputs in the Terraform module.</p> </li> <li> <p>The Event Hub namespace and instance must be preexisting.</p> </li> </ul>"},{"location":"newoutput/terraform-modules-for-azure-eventhub/#installation","title":"Installation","text":"<p>To install our function app, which connects to your Event Hub and sends logs to Coralogix, add this declaration to your Terraform project:</p> <pre><code>terraform {\n  required_providers {\n    azurerm = {\n      source = \"hashicorp/azurerm\"\n      version = \"~&gt; 3.93\"\n    }\n  }\n}\n\nprovider \"azurerm\" {\n  features {}\n}\n\nmodule \"eventhub\" {\n  source = \"coralogix/azure/coralogix//modules/eventhub\"\n\n  CoralogixRegion = \"Europe\"\n  CustomDomain = &lt; Custom FQDN if applicable &gt;\n  CoralogixPrivateKey = &lt; Send Your Data - API Key &gt;\n  CoralogixApplication = \"Azure\"\n  CoralogixSubsystem = \"EventHub\"\n  FunctionResourceGroupName = &lt; Function ResourceGroup Name &gt;\n  FunctionStorageAccountName = &lt; Function StorageAccount Name &gt;\n  FunctionAppServicePlanType = \"Consumption\"\n  EventhubInstanceName = &lt; Name of EventHub Instance &gt;\n  EventhubNamespace = &lt; Name of Eventhub Namespace &gt;\n  EventhubResourceGroupName = &lt; Name of Eventhub ResourceGroup &gt;\n}\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>Input the following variables:</p> <ul> <li> <p><code>**CoralogixRegion**</code>: The region associated with your Coralogix account</p> </li> <li> <p><code>CoralogixPrivateKey</code>: Your Coralogix Send Your Data - API Key</p> </li> <li> <p><code>**CoralogixApplication**</code> &amp; <code>CoralogixSubsystem</code>: Coralogix application and subsystem names as they will appear in your UI</p> </li> </ul> </li> <li> <p>Descriptions for other variables can be found here.</p> </li> <li> <p>An SAS policy will be created by the Terraform module to allow listen access to the Event Hub instance by the function app.</p> </li> </ul>"},{"location":"newoutput/terraform-modules-for-azure-eventhub/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Terraform ProviderExternal DocumentationCoralogix Function AppGithub: Azure \u2013 Coralogix Terraform ModuleTerraform Registry"},{"location":"newoutput/terraform-modules-for-azure-eventhub/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/terraform-modules-for-gcp-pub-sub/","title":"GCP Pub/Sub Terraform Module","text":"<p>Terraform simplifies the way we deploy our infrastructure and allows us to maintain it as code.</p> <p>Using our Terraform Modules, you can easily install and manage Coralogix integrations with GCP services as modules in your infrastructure code.</p> <p>Our modules are open source and available on our\u00a0GitHub\u00a0and in the\u00a0Terraform Registry.</p>"},{"location":"newoutput/terraform-modules-for-gcp-pub-sub/#installation","title":"Installation","text":"<p>This module will be installing\u00a0our function app\u00a0that gets messages from your Pub/Sub topic and sends logs to Coralogix.</p> <p>For more information about the Function itself, dynamic applicationName and subsystemName and multiline support visit our Pub/Sub guide.</p> <p>To use the module, first, add the provider configure block to your Terraform project:</p> <pre><code>terraform {\n  required_providers {\n    google = {\n      source  = \"hashicorp/google\"\n      version = \"&gt;= 4.31.0\"\n    }\n  }\n}\n\nprovider \"google\" {\n  project = \"your-project-id\"\n  region  = \"your-region\"\n}\n</code></pre> <p>Then simply add this declaration to your Terraform project:</p> <pre><code>module \"pubsub\" {\n  source = \"coralogix/google/coralogix//modules/pubsub\"\n\n  coralogix_region = \"Europe\"\n  private_key      = \"2f55c873-c0cf-4523-82d4-c3b68ee6cb46\"\n  application_name = \"Pub/Sub\"\n  subsystem_name   = \"logs\"\n  topic           = \"test-topic-name\"\n}\n</code></pre> <p>Important variables to change:</p> <ul> <li> <p>coralogix_region - The Coralogix location region, possible options are [Europe, Europe2, India, Singapore, US]</p> </li> <li> <p>private_key - The Coralogix private key which is used to validate your authenticity.</p> </li> <li> <p>application_name - The received logs applicationName.</p> </li> <li> <p>subsystem_name - The received logs subsystemName.</p> </li> <li> <p>topic - Your GCP Pub/Sub topic name.</p> </li> </ul>"},{"location":"newoutput/terraform-modules-for-gcp-pub-sub/#configuring-a-sink","title":"Configuring a sink","text":"<p>When configuring a GCP logs router sink to catch logs from different resources andthem send to a pub/sub topic be sure to EXCLUDE the function created above from the sink.</p> <p>NOTE- not following the next step will result in an infinite loop in your GCP function, RESULTING IN HIGH GCP COST; please proceed with caution.</p> <pre><code>resource \"google_logging_project_sink\" \"logs-sink\" {\n  name        = \"logs-sink\"\n  destination = \"pubsub.googleapis.com/projects/&lt;my-project-name&gt;/topics/&lt;my-pubsub-topic-name&gt;\"\n\n  exclusions {\n    name        = \"pub-sub-func\"\n    description = \"Exclude logs from the coralogix pub-sub function\"\n    filter      = \"resource.labels.function_name=\\\"${module.pubsub.function}\\\"\"\n  }\n\n  unique_writer_identity = true\n}\n</code></pre> <p>Note: replace  and  with your relevant values."},{"location":"newoutput/terraform-modules-for-gcp-pub-sub/#additional-resources","title":"Additional Resources","text":"DocumentationCoralogix Terraform Provider"},{"location":"newoutput/terraform-modules-for-gcp-pub-sub/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/time-relative-alerts/","title":"Time Relative Alerts","text":"<p>Automatically detect abnormal behavior in your system using the Time Relative Alert. Such alerts are triggered when a fixed ratio reaches a set threshold compared to a past time frame.</p>"},{"location":"newoutput/time-relative-alerts/#feature","title":"Feature","text":"<p>Use this feature to:</p> <ul> <li> <p>Receive automatic alerts regarding changes in your system\u2019s security, operations, and / or business behaviors over time</p> </li> <li> <p>Compare between each behavior across different time periods</p> </li> </ul>"},{"location":"newoutput/time-relative-alerts/#track-compare-your-system-behavior-over-time","title":"Track &amp; Compare Your System Behavior Over Time","text":"<p>Security. Receive automatic alerts comparing suspicious behavior. Compare, for instance, the amount of NX domain name responses or admin logins across days or weeks.</p> <p>Operations. Receive automatic alerts regarding error rates and page loading times in your applications. Compare, for instance, errors rates and page loading times in the past day or hour.</p> <p>Business. Receive automatic alerts when there is a shift in sales or user signups. Compare, for instance, the amount of purchases on the same day last week or the user signups over the last month.</p>"},{"location":"newoutput/time-relative-alerts/#create-an-alert","title":"Create an Alert","text":"<p>STEP 1. Create a new alert.</p> <ul> <li> <p>Click on Alerts &lt; Alert Management in the Coralogix toolbar.</p> </li> <li> <p>Click NEW ALERT on the upper right-hand corner of your dashboard.</p> </li> </ul>"},{"location":"newoutput/time-relative-alerts/#alert-details","title":"Alert Details","text":"<p>STEP 2. Define Alert Details.</p> <ul> <li> <p>Define:</p> <ul> <li> <p>Alert Name.</p> </li> <li> <p>Description.</p> </li> <li> <p>Severity. Choose from one of four options: info, warning, error, critical.</p> </li> <li> <p>Labels. Define a new label or choose from an existing one. Nest a label using <code>key:value</code>.</p> </li> </ul> </li> </ul>"},{"location":"newoutput/time-relative-alerts/#alert-type","title":"Alert Type","text":"<p>STEP 3. Select TIME RELATIVE Alert Type.</p> <p></p>"},{"location":"newoutput/time-relative-alerts/#query","title":"Query","text":"<p>STEP 4. Define a Query.</p> <ul> <li> <p>Input a new Query. Using the available RegEx cheat sheet for support.</p> </li> <li> <p>Filter by Application, Subsystem and Severities.</p> </li> </ul>"},{"location":"newoutput/time-relative-alerts/#conditions","title":"Conditions","text":"<p>STEP 5. Set the Conditions for triggering an alert.</p> <p></p>"},{"location":"newoutput/time-relative-alerts/#alert-when","title":"Alert When","text":"<p>The Alert will trigger when the query matching the alert definition will be more than/less than a number of occurrences when compared to the query results of a particular time window.</p> <p>For example, a query returns for the last hour 180 error logs. The same query but in a different timeframe (e.g. previous hour) returns 60 error logs. It means the ratio is 3. If the ratio is more than 1, then the alert will be triggered when the threshold is reached.</p>"},{"location":"newoutput/time-relative-alerts/#time-window","title":"Time Window","text":"<p>Choose a particular timeframe for comparison.</p> <p>Options include:</p> <ul> <li> <p>Previous hour - compare the timeframe \u201cnow-1hour TO now\u201d to \u201cnow-2hour TO now-1hour\u201d (1 hour)</p> </li> <li> <p>Same hour yesterday - compare the timeframe \u201cnow-1hour TO now\u201d to \u201cnow-25hours TO now-24hours\u201d (1 hour)</p> </li> <li> <p>Same hour last week - compare the timeframe \u201cnow-1hour TO now\u201d to \u201cnow-1week and 1 hour TO now-1week\u201d (1 hour)</p> </li> <li> <p>Yesterday - compare the timeframe \u201cnow-24hours TO now\u201d to \u201cnow-48hours TO now-24 hours\u201d (24 hours)</p> </li> <li> <p>Same day last week - compare the timeframe \u201cnow-24hours TO now\u201d to \u201cnow-8days TO now-7days\u201d (24 hours)</p> </li> <li> <p>Same day last month - compare the timeframe \u201cnow-24hours TO now\u201d to \u201cnow-29days TO now-28days\u201d (24 hours)</p> </li> <li> <p>A comparison between timeframes is made either every 5 minutes (for: Previous hour, Same hour yesterday, Same hour last week) or every 10 minutes (for: Yesterday, Same day last week, Same day last month).</p> </li> </ul>"},{"location":"newoutput/time-relative-alerts/#group-by","title":"Group By","text":"<p>Group your alerts using one or more aggregated values into a histogram.</p> <ul> <li> <p>An alert is triggered whenever the condition threshold is met for a specific aggregated value within the specified timeframe.</p> </li> <li> <p>If using 2 values for Group By, matching logs will first be aggregated by the parent field (ie. region), then by the child field (ie. pod_name). An alert will fire when the threshold meets the unique combination of both parent and child. Only logs that include the Group By fields will be included in the count.</p> </li> </ul>"},{"location":"newoutput/time-relative-alerts/#notifications","title":"Notifications","text":"<p>STEP 6. Define Notification settings.</p> <p>In the notification settings, you have different options, depending on whether or not you are using the Group By condition.</p>"},{"location":"newoutput/time-relative-alerts/#with-group-by","title":"With Group By","text":"<p>When using Group By conditions, you will see the following options:</p> <ul> <li> <p>Trigger a single alert when at least one combination of the group by values meets the condition. A single notification, aggregating all values matching an alert query and conditions, will be sent to your Coralogix Incidents screen.</p> </li> <li> <p>Trigger a separate alert for each combination that meets the condition. Multiple individual notifications for each Group By field value may be sent to your Coralogix Incidents screen when query conditions are met. Select one or more Keys - consisting of a subset of the fields selected in the alert conditions - in the drop-down menu. A separate notification will be sent for each Key selected.</p> </li> <li> <p>The number of Group By permutations is limited to 1000. If there are more permutations, then only the first 1000 are tracked.</p> </li> </ul>"},{"location":"newoutput/time-relative-alerts/#without-group-by","title":"Without Group By","text":"<p>When not using the Group By condition, a single alert will be triggered and sent to your Incidents Screen when the query meets the condition.</p> <p>You can define additional alert recipient(s) and notification channels in both cases by clicking + ADD WEBHOOK. Once you add a webhook, you can choose the parameters of your notification:</p> <ul> <li> <p>Notify Every. Sets the alert cadence. After an alert is triggered and a notification is sent, the alert will continue to work, but notifications will be suppressed for the duration of the suppression period.</p> </li> <li> <p>Notify when resolved. Activate to receive an automatic update once an alert has ceased.</p> </li> </ul>"},{"location":"newoutput/time-relative-alerts/#advanced-notifications","title":"Advanced Notifications","text":"<p>Once you add a webhook to the notification group, a toggle appears which enables you to move to Advanced Mode. Advanced mode lets you set the notify every &amp; notify when resolved settings for each webhook individually. Note that the toggle affects all notification groups, and when activate the toggle in one notification group, it will be turned on in all notification groups.</p>"},{"location":"newoutput/time-relative-alerts/#schedule","title":"Schedule","text":"<p>STEP 7. Set a Schedule.</p> <p>Limit triggering to specific days and times.</p> <p></p>"},{"location":"newoutput/time-relative-alerts/#notification-content","title":"Notification Content","text":"<p>STEP 8. Define Notification Content.</p> <ul> <li> <p>Choose a specific JSON key or keys to include in the alert notification.</p> </li> <li> <p>Leave blank to view the full log text.</p> </li> </ul> <p></p>"},{"location":"newoutput/time-relative-alerts/#alert-finalization","title":"Alert Finalization","text":"<p>STEP 9. Click Create Alert on the upper-right side of the screen.</p> <ul> <li>The alert will be activated after the Time Window is set in the Conditions.</li> </ul> <p></p> <ul> <li>Once triggered, the alert will display the count of logs for the current timeframe, for the compared timeframe, and the quotient of both.</li> </ul> <p></p>"},{"location":"newoutput/time-relative-alerts/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/tracing-alert/","title":"Tracing Alert","text":"<p>Tracing is one of the fundamental pillars of observability. It allows you to track interactions between all of your applications and understand where performance bottlenecks or errors are occuring.</p> <p>Coralogix allows you not only to visualize your traces, but also set alerts on them. Our tracing alert allows you to be alerted automatically of specific tags and services on the basis of a specified latency.</p>"},{"location":"newoutput/tracing-alert/#create-a-tracing-alert","title":"Create a Tracing Alert","text":"<p>STEP 1. Create a new Alert.</p> <ul> <li> <p>Click on Alerts &gt; Alert Management in the Coralogix toolbar.</p> </li> <li> <p>Click NEW ALERT on the upper right-hand corner of your dashboard.</p> </li> </ul> <p>STEP 2. Define Alert Details.</p> <ul> <li> <p>Define:</p> <ul> <li> <p>Alert Name.</p> </li> <li> <p>Alert Description.</p> </li> <li> <p>Alert Severity. Choose from one of four options: Info, Warning, Error, Critical.</p> </li> <li> <p>Labels. Define a new label or choose from an existing one. Nest a label using <code>key:value</code>.</p> </li> <li> <p>Set as Security Alert. Check this option to create an alert related to Coralogix security solutions.</p> </li> </ul> </li> </ul> <p></p> <p>STEP 3. Select TRACING Alert Type.</p> <p></p> <p>STEP 4. Define Query parameters.</p> <p></p> <ul> <li> <p>Latency is above (ms). Set this value to maximum latency - that is, the latency you are prepared to tolerate. Values greater than this will cause your spans to be counted as part of the data that will trigger your alerts.</p> </li> <li> <p>+ ADD SPAN TAG. You may choose to add span tag to filter your spans. For example, if your span has a label that indicates it is a customer-facing component, you can add this Key - Value pair to ensure the alert does not consider any of those spans. Value may be followed by any of the following: Is, Starts With, Includes, Ends With or Is.</p> </li> </ul> <p></p> <ul> <li>You may choose to add Application, Subsystem and Services filters. Maintain default options by selecting the All option. Note: Each span may have a different application and / or subsystem.</li> </ul> <p>STEP 5. Define Alert Conditions.</p> <p></p> <ul> <li> <p>Alert if. Select whether to trigger the alert immediately, or define a rule based on the number of occurrences within a specified time window.</p> </li> <li> <p>Group By. You have the option of grouping alerts by tag, with the alert monitoring each value of the specified tag.</p> <ul> <li> <p>An alert is triggered whenever the condition threshold is met for a specific aggregated value of the tag selected within the specified timeframe.</p> </li> <li> <p>If using 2 labels for Group By, matching metrics will first be aggregated by the parent label (ie. region), then by the child label (ie. pod_name). An alert will fire when the threshold meets the unique combination of both parent and child. Only metrics that include the Group By labels will be included in the count.</p> </li> </ul> </li> </ul> <p>STEP 6. Define Notification settings. Select how to be notified when an alert is triggered. By default, a single notification, aggregating all values matching an alert query and conditions, will be sent to your Coralogix Insights screen.</p> <p></p> <ul> <li> <p>+ ADD WEBHOOK. Define additional alert recipient(s) and notification channels.</p> </li> <li> <p>Notify Every. Sets the alert cadence. After an alert is triggered and a notification is sent, the alert will continue to work, but notifications will be suppressed for the duration of the suppression period.</p> </li> <li> <p>An individual notification for each of the values of the Group By fields will be sent when query conditions are met. Choose the recipient of the notification and the notification parameters.</p> </li> </ul> <p>STEP 7. Set a Schedule for trigger the alert.</p> <p></p> <p>STEP 8. Define Notification Content. Choose specific JSON keys to include in the alert notification, or leave this blank to include the full log text in the alert message.</p> <p></p>"},{"location":"newoutput/tracing-alert/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/tracing-tco-optimizer/","title":"TCO Optimizer: Traces","text":"<p>By allowing you to define the data pipeline for your traces based on the importance of that data to your business, our Tracing TCO Optimizer reduces your tracing costs by up to two-thirds and improves your real-time analysis and alerting capabilities.</p>"},{"location":"newoutput/tracing-tco-optimizer/#overview","title":"Overview","text":"<p>The Tracing TCO Optimizer allows you to assign different tracing pipelines for each application and subsystem pair and trace severity. In this way, it allows you to define the data pipeline for your traces based on the importance of that data to your business.</p> <p>The feature enables you to get all of the benefits of an ML-powered tracing solution at only a third of the cost, with an improved ability to query, monitor, and manage your data.</p>"},{"location":"newoutput/tracing-tco-optimizer/#tco-data-pipelines","title":"TCO Data Pipelines","text":"<p>The Tracing TCO Optimizer allows you to assign different roles for your data - high, medium, or low priority - and access each feature.</p>"},{"location":"newoutput/tracing-tco-optimizer/#high-frequent-search","title":"High: Frequent Search","text":"<p>Traces will be fully ingested, and all Coralogix capabilities will be available.</p> <p>Features available include:</p> <ul> <li> <p>Serverless Monitoring</p> </li> <li> <p>Rapid Query (traces)</p> </li> <li> <p>Custom Dashboards (spans)</p> </li> <li> <p>Service Catalog</p> </li> <li> <p>Service Map</p> </li> <li> <p>Alerting</p> </li> <li> <p>Events2Metrics</p> </li> <li> <p>Query Archive (traces)</p> </li> <li> <p>Viewing traces in your Explore Screen</p> </li> </ul>"},{"location":"newoutput/tracing-tco-optimizer/#medium-monitoring","title":"Medium: Monitoring","text":"<p>Traces will be processed and archived.</p> <p>Features available include:</p> <ul> <li> <p>Service Catalog</p> </li> <li> <p>Service Map</p> </li> <li> <p>Alerting</p> </li> <li> <p>Events2Metrics</p> </li> <li> <p>Query Archive (traces)</p> </li> <li> <p>Custom Dashboards</p> </li> <li> <p>Viewing traces in your Explore Screen</p> </li> </ul>"},{"location":"newoutput/tracing-tco-optimizer/#low-compliance","title":"Low: Compliance","text":"<p>Traces will be archived.</p> <p>Features available include:</p> <ul> <li> <p>Query Archive (traces)</p> </li> <li> <p>Viewing traces in your Explore Screen</p> </li> </ul>"},{"location":"newoutput/tracing-tco-optimizer/#tco-optimizer-screen","title":"TCO Optimizer Screen","text":"<p>The TCO Optimizer screen includes a Tracing tab, split into two sections:</p> <ul> <li> <p>Distribution</p> </li> <li> <p>Policy Criteria</p> </li> </ul>"},{"location":"newoutput/tracing-tco-optimizer/#distribution","title":"Distribution","text":"<p>The Distribution section presents high, medium, and low priorities.</p> <p></p>"},{"location":"newoutput/tracing-tco-optimizer/#policy-criteria","title":"Policy Criteria","text":"<p>The Policy Criteria\u00a0section includes current policies and allows for creating new ones. Policies will be applied on combinations of applications, subsystems, tags, service names, and actions as traces are ingested and will be assigned to the appropriate TCO pipeline based on the policy content. The default policy for all traces is high. Policies simplify assigning TCO pipelines and will capture any applicable future traces on ingestion. Each policy creates new default values for the traces for which the policy is applicable. If policies conflict, the first policy appearing on the screen will take precedence.</p> <p></p>"},{"location":"newoutput/tracing-tco-optimizer/#create-a-new-tracing-policy","title":"Create a New Tracing Policy","text":"<p>STEP 1. Click + ADD NEW POLICY.</p> <p></p> <p>STEP 2. Enter a policy name.</p> <p>STEP 3. Enter the policy details with the relevant applications, subsystems, and tags, adding additional criteria as needed.</p> <p>STEP 4. Set the Priority and Archive Retention for the policy. Find more information here.</p> <p>STEP 5. Click APPLY.</p>"},{"location":"newoutput/tracing-tco-optimizer/#additional-resources","title":"Additional Resources","text":"DocumentationTCO OptimizerArchive Retention PolicyAPITCO Optimizer APIArchive Setup gRPC APITCO Tracing Policy gRPC API"},{"location":"newoutput/tracing-tco-optimizer/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/troubleshooting-kubernetes-observability-using-opentelemetry/","title":"Troubleshooting","text":"<p>Troubleshoot any issues with the setup or configuration of Kubernetes Observability using OpenTelemetry.</p>"},{"location":"newoutput/troubleshooting-kubernetes-observability-using-opentelemetry/#overview","title":"Overview","text":"<p>Once you have completed the installation process for the OpenTelemetry Integration chart and instrumented applications, your telemetry data may not appear in your Coralogix account. This could indicate one of the following scenarios:</p> <ul> <li> <p>The Helm installation has failed due to an error or that your installation is unsuccessful.</p> </li> <li> <p>Your application has not been configured to send data to Coralogix via this container.</p> </li> </ul> <p>This tutorial will guide you on how to efficiently troubleshoot these issues.</p>"},{"location":"newoutput/troubleshooting-kubernetes-observability-using-opentelemetry/#opentelemetry-agent","title":"OpenTelemetry Agent","text":"<p>In order to find the source of the problem, we recommend you take a number of steps, including\u00a0troubleshooting your OpenTelemetry Agent logs\u00a0and\u00a0sending sample telemetry. If the demo works successfully, the problem lies in the configuration of your application. If the demo fails to work, the problem lies in the configuration of the collector. You will find tools to solve both of these issues below.</p>"},{"location":"newoutput/troubleshooting-kubernetes-observability-using-opentelemetry/#opentelemetry-agent-logs","title":"OpenTelemetry Agent Logs","text":"<p>STEP 1. Following installation, expect Kubernetes to run a pod with your chosen installation name. Ensure this is the case by running the following command:</p> <pre><code>kubectl get pods -o wide -n $NAMESPACE | grep opentelemetry-agent\n\n</code></pre> <p>STEP 2. Locate and copy the full name of the OpenTelemetry collector agent in your list of pods. The pod should appear exactly once with the STATUS \u201cRunning.\u201d If the STATUS is \u201cPending\u201d, rerun the command. The AGE appearing should be the time that has elapsed since your last HELM upgrade.</p> <p>STEP 3. Once you have located this specific pod, use the default logging tool command\u00a0<code>kubectl logs</code>\u00a0for retrieving its logs. Running this command with the\u00a0<code>--follow</code>\u00a0flag streams logs from the specified resource, allowing you to live tail its logs from your terminal.</p> <pre><code>kubectl logs --follow &lt;paste full name of opentelemtry collector agent pod here&gt; -n &lt;namespace&gt;\n\n</code></pre> <p>Here is an example of the expected output of STEPS 1-3:</p> <p></p> <p>STEP 4. Rerun this set of commands at any later stage as necessary.</p>"},{"location":"newoutput/troubleshooting-kubernetes-observability-using-opentelemetry/#opentelemetry-cluster-collector","title":"OpenTelemetry Cluster Collector","text":"<p>Your Kubernetes Dashboard may not be receiving data or may be giving an incomplete view (e.g. Kubernetes events missing). In order to solve the issue, take the following steps.</p> <p>STEP 1. Check that the OpenTelemetry Agent and OpenTelemetry Cluster Collector pods have a <code>Running</code> status in your cluster. If not, retrieve and troubleshoot the logs.</p> <pre><code>&gt; kubectl get pods -o wide -n $NAMESPACE\nNAME                                                  READY   STATUS    RESTARTS   AGE   IP              NODE                                               NOMINATED NODE   READINESS GATES\ncoralogix-opentelemetry-agent-jhl6q                   1/1     Running   0          43m   172.31.27.209   ip-172-31-27-209.ap-southeast-1.compute.internal   &lt;none&gt;           &lt;none&gt;\ncoralogix-opentelemetry-collector-84d684d45c-2h4rt    1/1     Running   0          43m   172.31.31.122   ip-172-31-27-209.ap-southeast-1.compute.internal   &lt;none&gt;           &lt;none&gt;\notel-integration-kube-state-metrics-bbc57cd9b-9kgrp   1/1     Running   0          43m   172.31.18.170   ip-172-31-27-209.ap-southeast-1.compute.internal   &lt;none&gt;           &lt;none&gt;\n\n</code></pre> <p>STEP 2. Ensure that the Kubernetes OpenTelemetry\u00a0extension package in your Coralogix account has been installed with the Latest Version and all relevant Applications\u00a0and\u00a0Subsystems are selected.</p> <p>STEP 3. Explore any missing data from the Kubernetes OpenTelemetry\u00a0extensions, including Grafana Dashboards:</p> <ul> <li> <p>In your Coralogix toolbar, navigate to Grafana &gt; Dashboards &gt; Manage. Under the K8sOtel folder, explore the installed dashboards.</p> </li> <li> <p>Any graph or panel showing \u201cNo data\u201d indicates missing data. This could mean one of two things:</p> <ul> <li> <p>Problem with exporting telemetry from the collectors.</p> </li> <li> <p>Misconfiguration of the components on the collector.</p> </li> </ul> </li> </ul> <p></p>"},{"location":"newoutput/troubleshooting-kubernetes-observability-using-opentelemetry/#validate-your-endpoints","title":"Validate Your Endpoints","text":"<p>Validating your Coralogix Endpoints will allow you to test the connectivity in your domain structure from the Kubernetes cluster.</p> <p>The following <code>kubectl</code> command creates a temporary <code>busybox</code> pod, which executes the <code>nslookup</code> command to the endpoint and outputs the results:</p> <pre><code>kubectl run busybox --image=busybox:1.28 --rm -it --restart=Never -- nslookup &lt;endpoint&gt;\n\n</code></pre> <p>Here is an example of the expected results for an endpoint in the EU1 region:</p> <p></p> <p>If you receive an error, this may mean that your cluster lacks connectivity to the domain server.</p>"},{"location":"newoutput/troubleshooting-kubernetes-observability-using-opentelemetry/#collector-configuration-issues","title":"Collector Configuration Issues","text":"<p>If the OpenTelemetry Integration chart exhibits unexpected behaviour, incorrect telemetry is being sent, or there is a failure when installing via Helm, this may indicate that the contents of the <code>values.yaml</code> file do not match the expected structure found in the default values file, or the Helm templating has failed to override the default values as expected for the changes made.</p> <p>These are some troubleshooting tips to help resolve the issue.</p> <p>STEP 1. Refer to and compare your configurations with the default values.yaml file of the OpenTelemetry Integration chart.</p> <p>STEP 2. View how the configurations have been overridden by extracting the deployed ConfigMaps manifests.</p> <ul> <li>Run the following to get a list of ConfigMaps:</li> </ul> <pre><code>&gt; kubectl get configmap -n $NAMESPACE\nNAME                                DATA   AGE\ncoralogix-opentelemetry-agent       1      4d\ncoralogix-opentelemetry-collector   1      4d\nkube-root-ca.crt                    1      118d\n\n</code></pre> <ul> <li>Extract either of the ConfigMap Manifests with the following:</li> </ul> <pre><code>kubectl get configmap -n otel -o yaml coralogix-opentelemetry-agent\nkubectl get configmap -n otel -o yaml coralogix-opentelemetry-collector\n\n</code></pre> <p>Read more about Helm Values Files to get additional information on the underlying Helm values templating.</p>"},{"location":"newoutput/troubleshooting-kubernetes-observability-using-opentelemetry/#next-steps","title":"Next Steps","text":"<p>Check out these Kubernetes Observability using OpenTelemetry FAQs.</p>"},{"location":"newoutput/troubleshooting-kubernetes-observability-using-opentelemetry/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com</p>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/","title":"Tutorial: Install and Configure Filebeat to Send Your Logs to Coralogix","text":"<p>This tutorial provides a step-by-step guide on to how install and configure Filebeat to send logs from a file to your Coralogix team over TLS. It does this using a deployment of Filebeat on a single Amazon Linux 2 instance.</p> <p>Note! Filebeat can be used to ship logs from a variety of sources, including Syslog, Docker, and Windows Environments.</p>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#tutorial","title":"Tutorial","text":"<p>Learn how to:</p> <ul> <li> <p>Easily install Filebeat</p> </li> <li> <p>Familiarize yourself with the Filebeat environment</p> </li> <li> <p>Create a working example of a FileBeat configuration that ships logs to Coralogix</p> </li> </ul>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#supported-versions","title":"Supported Versions","text":"<p>Coralogix supports these versions of Filebeat:</p> <ul> <li> <p>Filebeat 7.x (v7.17 as of 12.2022)</p> </li> <li> <p>Filebeat 8.x (v8.5 as of 12.2022)</p> </li> </ul> <p>Note! To avoid breaking changes between these major versions, do not upgrade directly from v7 to v8.</p>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#set-up","title":"Set Up","text":"<p>This section demonstrates how to deploy Filebeat on a single Amazon Linux 2 instance. General instructions for installing and configuring Filebeat and sending your data to Coralogix can be found here.</p>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#installation","title":"Installation","text":"<p>Install and configure Filebeat v7.17 on your Linux distribution.</p> <pre><code>curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.17.8-linux-x86_64.tar.gz\ntar xzvf filebeat-7.17.8-linux-x86_64.tar.gz\ncd filebeat-7.17.8-linux-x86_64/\n\n</code></pre>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#configuration","title":"Configuration","text":"<p>To configure Filebeat, modify the main parts of the configuration file\u00a0<code>filebeat.yml</code>: modules, inputs, fields, outputs.</p>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#modules","title":"Modules","text":"<p>Configure the modules. These are Filebeat inputs enabling the input and parser.</p> <p>The example below configures the Fortinet / Firewall module, enabling Filebeat to ingest Syslog data from FortiGate Firewall on port 9004/UDP and parse Syslog messages in JSON format.</p> <pre><code>#==========================  Modules configuration =============================\nfilebeat.modules:\n- module: fortinet\n  firewall:\n    enabled: true\n    var.input: udp\n    var.syslog_host: 0.0.0.0\n    var.syslog_port: 9004\n</code></pre> <p>Note! Modules change dramatically between different versions of Filebeat. Previous versions of Filebeat do not have all modules available.</p>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#inputs","title":"Inputs","text":"<p>To configure Filebeat manually (rather than using modules), specify a list of inputs in the <code>filebeat.inputs</code> section of the <code>filebeat.yml</code>. Inputs specify how Filebeat locates and processes input data.</p> <p>The log input in the example below enables Filebeat to ingest data from the log file. It then points Filebeat to the logs folder and uses a wildcard <code>*.log</code> to collect all files ending with <code>.log</code> .</p> <pre><code>#=========================== Filebeat inputs =============================\n\n#------------------------------ Log input --------------------------------\n- type: log\n\n  # Change to true to enable this input configuration.\n  enabled: false\n\n  # Paths that should be crawled and fetched. Glob based paths.\n  # To fetch all \".log\" files from a specific level of subdirectories\n  # /var/log/*/*.log can be used.\n  # For each file found under this path, a harvester is started.\n  # Make sure not file is defined twice as this can lead to unexpected behaviour.\n  paths:\n    - /var/log/*.log\n    #- c:\\programdata\\elasticsearch\\logs\\*\n</code></pre>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#fields","title":"Fields","text":"<p>Apply additional configuration settings (such as <code>fields</code>, <code>include_lines</code>, <code>exclude_lines</code>, <code>multiline</code>) to the lines harvested from logs. The options that you specify are applied to all of the files harvested by a single input.</p> <p>To apply different configuration settings to different files, define multiple input sections.</p> <p>Note! Ensure a file is not defined more than once across all inputs because this can lead to unexpected behavior.</p> <pre><code>filebeat.inputs:\n- type: log \n  paths:\n    - /var/log/*.log\n  fields:\n    PRIVATE_KEY: '&lt;coralogix_send-your-data-api-key&gt;'\n    COMPANY_ID: &lt;companyID&gt;\n    APP_NAME: '&lt;application_name&gt;'\n    SUB_SYSTEM: '&lt;subsystem_name&gt;'\n  fields_under_root: true\n\n</code></pre>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#outputs","title":"Outputs","text":"<p>1. Configure Filebeat to write specific outputs by setting options in the <code>output</code> section of the configuration file.</p> <p>The <code>logstash</code> output in the example below enables Filebeat to ship data to Logstash. It points Filebeat to the Coralogix logstash in the <code>coralogixstg.wpengine.com</code> domain and points Filebeat to the TLS and SSL certificates (same certificate) that are required to ship data securely to Coralogix.</p> <p>Note! Only a single output may be defined.</p> <pre><code># ================================= Logstash output =============================\n\n    output.logstash:\n    enabled: true\n    hosts: ['logstashserver.coralogixstg.wpengine.com:5015']\n    tls.certificate_authorities: ['/usr/share/Coralogix-EU.crt']\n    ssl.certificate_authorities: ['/usr/share/Coralogix-EU.crt']\n</code></pre> <p>2. Download and store the certificate in a location accessible by Filebeat.</p>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#example-configuration","title":"Example Configuration","text":"<pre><code>filebeat.yml: |-\n# ============================== Filebeat Inputs ===============================\n#-------------------------------- logs input ---------------------------------  \nfilebeat.inputs:\n\n- type: log\n      paths:\n     - \"/var/log/your_app/your_app.log\"\n        line_delimiter: \"\\n\"\n        max_message_size: 10MiB\n        timeout: 300s\n        enable_metric: true\n#------------------------ Coralogix fields configuration --------------------\n      fields:\n            PRIVATE_KEY: '&lt;coralogix_privatekey&gt;'\n            COMPANY_ID: &lt;companyID&gt;\n            APP_NAME: '&lt;application_name&gt;'\n            SUB_SYSTEM: '&lt;subsystem_name&gt;'\n      fields_under_root: true\n#==========================  Modules configuration =============================\nFilebeat.modules:\n- module: fortinet\n         Firewall:\n         enabled: true\n         var.input: udp\n         var.syslog_host: 0.0.0.0\n         var.syslog_port: 9004\n         fields:\n            PRIVATE_KEY: '&lt;coralogix_send-your-data-api-key&gt;'\n            COMPANY_ID: &lt;companyID&gt;\n            APP_NAME: '&lt;application_name&gt;'\n            SUB_SYSTEM: '&lt;subsystem_name&gt;'\n         fields_under_root: true\n\n# ================================= Logstash output =============================\n#------------------------- Coralogix Logstash output \u2014-----------------------\n\n    output.logstash:\n    enabled: true\n    hosts: ['logstashserver.coralogixstg.wpengine.com:5015']\n    tls.certificate_authorities: ['/usr/share/Coralogix-EU.crt']\n    ssl.certificate_authorities: ['/usr/share/Coralogix-EU.crt']\n</code></pre>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#urls-and-certificates","title":"URLs and Certificates","text":"EUINUSCluster Domaincoralogixstg.wpengine.comapp.coralogix.incoralogix.usSSL Certificateshttps://coralogix-public.s3-eu-west-1.amazonaws.com/certificate/Coralogix-EU.crthttps://coralogix-public.s3-eu-west-1.amazonaws.com/certificate/Coralogix-IN.pemhttps://www.amazontrust.com/repository/AmazonRootCA1.pemLogstash Server URLlogstashserver.coralogixstg.wpengine.comlogstash.app.coralogix.inlogstashserver.coralogix.us"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#validation","title":"Validation","text":"<p>Test Filebeat by running it and monitoring the logs.</p> <p>1. Modify the user credentials in <code>filebeat.yml</code> and specify a user who is authorized to publish events.</p> <pre><code>sudo chown root filebeat.yml \n\n</code></pre> <p>2. By default, Filebeat sends all of its output to Syslog. When you run Filebeat in the foreground, you can use the <code>-e</code> command line flag to redirect the output to standard error instead, as in the example below.</p> <pre><code>sudo ./filebeat -e\n\n</code></pre>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#debugging","title":"Debugging","text":"<p>To increase the verbosity of debug messages, use the <code>-d</code> command line flag to debug selectors.</p> <pre><code>./filebeat -e -d \"*\"\n\n</code></pre>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#common-messages","title":"Common Messages","text":"<p>You may encounter certain common messages as follows:</p> <ul> <li>No new logs were received by Filebeat:</li> </ul> <pre><code>INFO    [monitoring]    log/log.go:145  Non-zero metrics in the last 30s\n\n</code></pre> <ul> <li>Filebeat received logs and was able to establish a connection to Coralogix Logstash:</li> </ul> <pre><code>2022-12-19T19:35:41.758Z    INFO    [publisher_pipeline_output] pipeline/output.go:101  Connecting to backoff(async(tcp://logstashserver.coralogixstg.wpengine.com:5015))\n2022-12-19T19:35:41.886Z    INFO    [publisher_pipeline_output] pipeline/output.go:111  Connection to backoff(async(tcp://logstashserver.coralogixstg.wpengine.com:5015)) established\n</code></pre>"},{"location":"newoutput/tutorial-install-and-configure-filebeat-to-send-your-logs-to-coralogix/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/unique-count-alert/","title":"Unique Count Alert","text":"<p>As data volumes grow and the number of alerts generated by logs, metrics, and security systems exponentially increases, one of the most powerful indicators of alert importance is the number of elements affected by it. Whether it\u2019s the number of users who have encountered a 5XX error when calling an API, the number of Kafka consumer groups that returned errors, the number of CDN locations that are currently loading your site for more than 3 seconds, or the number of different passwords that a single user attempts to log in with to your cloud service console.</p> <p>The problem with most alerts is that they describe the problem, though, in order to understand the severity or broadness of the issue, users need to drill into the data or rely on dashboards.</p> <p>Unique Count alerts trigger on the number of unique values inside a selected key that matches a specific search criteria (AKA - The Cardinality of a specific key matched to a search).</p>"},{"location":"newoutput/unique-count-alert/#create-alerts","title":"Create Alerts","text":"<p>STEP 1. Create an Alert.</p> <ul> <li>In the Coralogix toolbar, click Alerts &gt; Alert Management.</li> </ul> <p></p> <ul> <li>Click NEW ALERT on the top-right area of the UI.</li> </ul> <p>STEP 2. Define Alert Details.</p> <ul> <li>Please enter:<ul> <li>Alert Name.Alert Description.Alert Severity. Choose from one of four options: Info, Warning, Error, Critical.Labels. Define a new label or choose from an existing one. Nest a label using <code>key:value</code>.Set as Security Alert. Check this option to create an alert related to Coralogix Security solutions.</li> </ul> </li> </ul> <p></p> <p>STEP 3. Select Alert Type: UNIQUE COUNT.</p> <p></p> <p>STEP 4. Define your alert search criteria.</p> <p></p> <p>STEP 5. Define Conditions.</p> <ul> <li> <p>Define the key to match to track its unique count.</p> </li> <li> <p>You can also choose to group by a specific log field to receive an alert if the unique count threshold was crossed per specific value of the group by field. (This is very useful with security use cases, i.e. send an alert if a specific user (grouped by key) logged in to my system from more than 1 country (Unique count key) at the same time).</p> </li> <li> <p>Once triggered, the alert will display the behavior of unique count per the selected key that matches the configured search criteria, and list all the unique values that were discovered within the tracked key.</p> </li> </ul> <p>Note: The total amount of permutations for the unique count by key and group by key should not exceed 10k for the alert timeframe.</p> <p></p> <p>STEP 6. Define Notification Groups.</p> <ul> <li> <p>By default, a single notification, aggregating all values matching an alert query and conditions, will be sent to your Coralogix Insights screen.</p> </li> <li> <p>+ ADD WEBHOOK. Click here to define notification recipient(s) and notification channels.</p> </li> <li> <p>Notify Every. Sets the alert cadence. After an alert is triggered and a notification is sent, the alert will continue to work, but notifications will be suppressed for the duration of the suppression period.</p> <ul> <li> <p>When an alert is triggered, it won\u2019t be triggered again until one of two things happens: either the Notify Every period passes or it is resolved. In the latter case, the Notify Every parameter is reset.</p> </li> <li> <p>Select Notify when resolved to receive an automatic update once an alert has ceased.</p> </li> </ul> </li> </ul> <p></p> <p>STEP 7. Set a Schedule. Limit triggering to specific days and times.</p> <p></p> <p>STEP 8. Define Notification Content:</p> <ul> <li> <p>Choose a specific JSON key or keys to include in the alert notification.</p> </li> <li> <p>Leave blank to view the full log text.</p> </li> </ul> <p></p> <p>STEP 9. Create your alert.</p> <ul> <li> <p>Click CREATE ALERT on the upper-right side of the screen.</p> </li> <li> <p>After saving your alert, it may take up to 15 minutes for the alert to be active in the cluster.</p> </li> </ul>"},{"location":"newoutput/unique-count-alert/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/upguard/","title":"UpGuard","text":"<p>The following tutorial demonstrates how to send your logs to Coralogix using UpGuard. Follow this five-step guide for each notification that you would like to send us.</p> <p>UpGuard uses webhooks to send notifications when an event happens in your UpGuard account. This could be when an identity breach or data leak is detected, the score of a watched vendor drops below a certain threshold, or when a user requests access to your shared profile.</p>"},{"location":"newoutput/upguard/#configuration","title":"Configuration","text":"<p>STEP 1. Create Integration.</p> <ul> <li> <p>Login to your Upguard account.</p> </li> <li> <p>Select Settings in your left-hand sidebar.</p> </li> </ul> <p></p> <ul> <li>Click on the Integrations tab.</li> </ul> <p></p> <ul> <li>Click + New Integration.</li> </ul> <p></p> <ul> <li>Select Webhook.</li> </ul> <p></p> <p>STEP 2. Select Triggers</p> <ul> <li> <p>Select from a wide-range of pre-defined triggers to use as part of this integration. Examples include:</p> <ul> <li> <p>'When my company's score drops below 600'</p> </li> <li> <p>'When a domain or IP's score drops below 600'</p> </li> <li> <p>'When a new identity breach is detected'</p> </li> <li> <p>'When a new identity breach for a VIP email is detected'</p> </li> </ul> </li> <li> <p>Enable a trigger by clicking on the associated pill, which slides to the right.</p> </li> </ul> <p></p> <ul> <li>Click Confirm and next.</li> </ul> <p>STEP 3. Name and Destination</p> <ul> <li>Provide the webhook URL corresponding to the Coralogix cluster URL associated with the domain and region where your data is stored.</li> </ul> Coralogix Cluster URLAPI Endpoint.comhttps://api.coralogixstg.wpengine.com.ushttps://api.coralogix.us.inhttps://api.app.coralogix.in.app.eu2.coralogixstg.wpengine.comhttps://api.eu2.coralogixstg.wpengine.com.app.coralogixsg.comhttps://api.coralogixsg.com SchemaEndpoint DetailsWebhook URLhttps://api.&lt;clusterURL&gt;/api/v1/logsContent-Type\u00a0application/json <p>For example, if your Coralogix data is hosted in India, your webhook URL should appear as https://api.app.coralogix.in/api/v1/logs.</p> <ul> <li>Configure the HTTP Header values by inputting Content-Type: application/json. As webhook by default uses POST method to send requests, there is no need to define the method.</li> </ul> <p></p> <p>Example:</p> <p></p> <ul> <li>Click Confirm and next.</li> </ul> <p>STEP 4. Define Payload Structure</p> <ul> <li>For each trigger, UpGuard provides a default payload template as in the example below.</li> </ul> <p></p> <ul> <li>Modify the payload template to comply with the Coralogix structure.</li> </ul> <p>POST Body</p> RequiredProperty NameProperty TypeNoteYesprivateKeyUUIDYesapplicationNamestringusually used to separate environmentsYessubsystemNamestringusually used to separate componentscomputerNamestringYeslogEntriesarray of logs <p>Log</p> RequiredProperty NameProperty TypeNotesYestimestampnumberUTC milliseconds since 1970 (supports sub millisecond via a floating point)Yesseveritynumber1 \u2013 Debug, 2 \u2013 Verbose, 3 \u2013 Info, 4 \u2013 Warn, 5 \u2013 Error, 6 \u2013 CriticalYestextstring <ul> <li>Wrap the payload template in JSON as follows. You will need to input your Coralogix Send-Your-Data API key, application and subsystem names, and computer name.</li> </ul> <pre><code>{\n\u00a0\u00a0\u00a0\"privateKey\": \"&lt;Coralogix send your data api-key&gt;\",\n\u00a0\u00a0\u00a0\"applicationName\": \"&lt;application name&gt;\",\n\u00a0\u00a0\u00a0\"subsystemName\": \"&lt;subsytem name&gt;\",\n\u00a0\u00a0\u00a0\"computerName\": \"&lt;computer name&gt;\",\n\u00a0\u00a0\u00a0\"logEntries\": [\n\u00a0\u00a0\u00a0\u00a0\u00a0{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"severity\": &lt;default severity of event 1-6&gt;,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"text\": {\n\u00a0\u00a0\u00a0\"notification\": {\n\u00a0\u00a0\u00a0}\n\u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0]\n\u00a0}\n</code></pre> <ul> <li>The following is an example of a Coralogix-compatible payload template.</li> </ul> <pre><code>{\u00a0\n\"privateKey\": \"xxxxxxx-xxxxxx-xxxxxx-xxxxxxx\",\n\u00a0\u00a0\u00a0\"applicationName\": \"upguard\",\n\u00a0\u00a0\u00a0\"subsystemName\": \"upguard\",\n\u00a0\u00a0\u00a0\"computerName\": \"upguard01\",\n\u00a0\u00a0\u00a0\"logEntries\": [\n\u00a0\u00a0\u00a0\u00a0\u00a0{\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"severity\": 4,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"text\": {\n\u00a0\u00a0\u00a0\"notification\": {\n\u00a0\u00a0\u00a0\u00a0\u00a0\"id\": {{ notification.id }},\n\u00a0\u00a0\u00a0\u00a0\u00a0\"type\": \"{{ notification.type }}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\"description\": \"{{ notification.description }}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\"occurredAt\": \"{{ notification.occurredAt }}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\"context\": \u00a0 \u00a0 {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"PrevScore\": {{ notification.context.PrevScore }},\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"PrevScoreOn\": \"{{ notification.context.PrevScoreOn }}\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Threshold\": {{ notification.context.Threshold }},\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"LatestScore\": {{ notification.context.LatestScore }},\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"LatestScoreOn\": \"{{ notification.context.LatestScoreOn }}\"\n\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0}\n\u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0]\n\u00a0}\n</code></pre> <ul> <li>Validate that your webhook is working properly by clicking Send test message. The value '200 OK' should appear as the Response.</li> </ul> <p></p> <p>Example:  </p> <p></p> <ul> <li>Validate that Coralogix has received the test notification by searching the logs in your Coralogix dashboard.</li> </ul> <p></p> <ul> <li>Click Confirm and next.</li> </ul> <p></p> <p>STEP 5. Enable the Integration</p> <ul> <li>Click the toggle to enable the integration and click Finish.</li> </ul> <p></p>"},{"location":"newoutput/upguard/#additional-resources","title":"Additional Resources","text":"UpGuardWebhook documentation for advanced modification of Webhook Payload"},{"location":"newoutput/upguard/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/user-sessions/","title":"User Sessions","text":"<p>User Sessions provides valuable insights into user experience by tracking and analyzing how users interact with your web applications. Use this tutorial to learn what user sessions are, how to use them effectively, and how they can enhance your Real User Monitoring (RUM) capabilities.</p> <p></p>"},{"location":"newoutput/user-sessions/#benefits","title":"Benefits","text":"<p>User Sessions offer the following benefits:</p> <ul> <li> <p>Performance Insights. Monitor the performance of your web applications by analyzing user sessions in real-time. Identify slow page loads, errors, and other performance bottlenecks affecting user experience.</p> </li> <li> <p>User Engagement. Track user behavior to understand how users navigate within your application. Analyze user paths, actions, and interactions.</p> </li> <li> <p>Issue Resolution. Quickly detect and diagnose issues through session data and metrics. Prioritize and resolve performance problems that impact user satisfaction.</p> </li> <li> <p>Data-Driven Decisions. Make informed decisions based on user behavior and session data. Optimize your application for improved user experience.</p> </li> </ul>"},{"location":"newoutput/user-sessions/#key-concepts","title":"Key Concepts","text":"<ul> <li>User Sessions refer to a sequence of user interactions in a web application within a defined time frame. Each view is associated with a <code>session_id</code>, allowing you to consolidate actions or other elements in one session.</li> </ul> <ul> <li> <p>Page Views are when users load a page on your website or application. Tracking page views helps you understand the most popular pages and how often they are accessed. Each view is attached to a <code>view_id</code> within the same session.</p> </li> <li> <p>User Actions encompass a wide range of specific behaviors and interactions that users perform while using the application. These actions may include navigation actions - page visits, clicking links, menu navigation; interaction with UI elements - button clicks, dropdown selection, checkbox selection; form submissions; media and content interactions - media playback or image zoom; e-commerce and user account actions; error and exception handling; or content creation. They provide valuable insights into how users engage with the system. Each action is attached to an <code>action_id</code> within the same session.</p> </li> <li> <p>Resources refer to various elements and assets that a web page loads when a user interacts with a web application. These resources can include images, scripts, stylesheets, fonts, API request, media, etc. Each is associated with a <code>resource_id</code>.</p> </li> <li> <p>Errors refer to any unexpected issues or failures that occur during a user's interaction with a web application. Monitoring errors within User Sessions is crucial for identifying and resolving issues that can negatively impact the user experience. Detecting and tracking errors can help pinpoint where problems occur and facilitate rapid troubleshooting and resolution. Each is associated with an <code>error_id</code>.</p> </li> <li> <p>Long Tasks refer to specific actions or tasks within a web application that take a relatively long time to complete. These tasks can cause delays in the user experience and lead to performance issues. Long tasks may include rendering complex graphics, heavy JavaScript execution, or large file downloads. Each is associated with a <code>LongTask_id</code>.</p> </li> </ul>"},{"location":"newoutput/user-sessions/#getting-started","title":"Getting Started","text":"<p>STEP 1. In your Coralogix toolbar, navigate to\u00a0RUM\u00a0&gt;\u00a0User Sessions.</p> <p>STEP 2. Click on the User Sessions tab.</p> <p>View the following:</p> <ul> <li>Graph. Presents a visualization of all sessions for all users.</li> </ul> <p></p> <ul> <li>Sessions Grid. Each line represents one user session. It includes start and end times, user_id, browser type, OS, number of pages visited, latency (total blocking times), and the number of long tasks.</li> </ul> <p></p> <ul> <li>Filter Panel. The left-hand panel presents you with various filtering options. You can filter and display only those sessions with errors or user recordings, among other options.</li> </ul> <p></p> <ul> <li>Geo Location. Displays your application's user sessions distributed across the globe. Hover over a particular country to view the KPIs for that specific region.</li> </ul>"},{"location":"newoutput/user-sessions/#user-session-drill-down","title":"User Session Drill Down","text":"<p>Click on a specific user session to drill down. Select from a series of tabs: ACTIONS, RESOURCES, ERRORS, and CORRELATE LOGS.</p>"},{"location":"newoutput/user-sessions/#actions","title":"Actions","text":"<p>Clicking on the ACTIONS tab will present you with the following information for a particular user session:</p> <ul> <li>Session Header. This includes information for the entire session, including session ID, duration, user ID, browser, operating system, device type, country, number of pages visited, number of errors, number of actions, number of resources, number of page loads, number of keyboard inputs, and end pages. The header includes a drop-down menu where you can choose to investigate all pages or narrow your investigation down to a specific page.</li> </ul> <p></p> <ul> <li> <p>Page Header. For each page within the user session, you will see a page header, which includes rendering and loading times, the number of errors and clicks on a page, and the number of inputs. On the right-hand side of the page header, use the page header icons to view aggregated page information: rendering time, loading time, number of errors, clicks, page load, and user inputs.</p> </li> <li> <p>Action Grid. Under each page header, you will see an Action Grid, which displays all of the events that occurred on that particular page.</p> </li> </ul> <p></p> <p>The Action Grid includes:</p> <ul> <li> <p>Page navigation information. Including load time, page size, number of resources, page referral, URL params, and UTMs.</p> </li> <li> <p>User interactions. Including action type (such as <code>click</code>, <code>input</code>, <code>scroll type</code>), element data, and timestamp.</p> </li> <li> <p>Error information. Including error type and message, stack message, timestamp, user impact, correlated actions, and failed requests.</p> </li> </ul> <p></p>"},{"location":"newoutput/user-sessions/#resources","title":"Resources","text":"<p>Clicking on the RESOURCES tab will present you with all of the resources for a particular URL. The tab includes:</p> <ul> <li> <p>Session Header. This includes information for the entire session, including session ID, duration, user ID, and application type.</p> </li> <li> <p>Filters. You have the option of viewing all resources or filtering by resource type to narrow down to a specific type.</p> </li> </ul> <p></p> <ul> <li>Resources Grid. For each resource, view the resource name, type, method, status code, size, latency, initiator, and associated waterfall. The waterfall is a visual representation that displays the loading and rendering of resources on a web page in chronological order, similar to a waterfall. Sort and filter grid results to pinpoint a resource serving as a bottleneck.</li> </ul> <p></p> <p>Clicking on the waterfall for a particular resource will produce a popup displaying queuing, stalling, and request and response times for a particular resource.</p> <p></p>"},{"location":"newoutput/user-sessions/#errors","title":"Errors","text":"<p>Clicking on the ERRORS tab will present you with all of the errors in a user session.</p> <p></p> <p>The tab includes:</p> <ul> <li> <p>Session Header. This includes information for the entire session including: session ID, duration, user ID, and application type.</p> </li> <li> <p>Errors Tab. For each error, view the application name, event and error type, error code, user ID, timestamp, event ID, application code version, device type, URL, browser, country, and user name.</p> </li> </ul> <p></p> <ul> <li> <p>Labels. Presents all associated labels for a particular error as configured in the Browser SDK. Customize labels to enrich your data in accordance with your needs.</p> </li> <li> <p>Error Message. Presents the error message in its entirety.</p> </li> </ul>"},{"location":"newoutput/user-sessions/#correlate-logs","title":"Correlate Logs","text":"<p>Clicking on the CORRELATE LOGS tab will present you with all of the logs for a user session.</p>"},{"location":"newoutput/user-sessions/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/user-team-management/","title":"Teams","text":""},{"location":"newoutput/user-team-management/#overview","title":"Overview","text":"<p>A Coralogix Team\u00a0is a platform environment with its unique URL, settings, and Send-Your-Data API key. Teams are used to group users together based on their project, department, or any other relevant criteria. Teams provide a convenient way to collectively manage permissions and settings for a group of users. You can assign teams to specific logs, alerts, or dashboards, ensuring that the right people can access the relevant information. Users may view data only of those teams of which they are members.</p> <p>Teams can consist of users from different Groups, allowing you to create flexible and dynamic access controls. For example, you can have a Development Team with users from the Engineering, Operations, and Support groups. This allows you to grant permissions and manage access granularly, aligning with your organization's structure and requirements.</p>"},{"location":"newoutput/user-team-management/#create-a-team","title":"Create a Team","text":"<p>If you haven't already done so,\u00a0sign up\u00a0for a free Coralogix account. You will be prompted to create a new team. Input a team name and click\u00a0CREATE TEAM.</p> <p>If you already have an account, click\u00a0CREATE NEW TEAM\u00a0in your login screen.</p> <p></p> <p>Alternatively, click + CREATE NEW TEAM in the upper right-hand corner of your Coralogix dashboard.</p> <p></p>"},{"location":"newoutput/user-team-management/#manage-existing-team-members","title":"Manage Existing Team Members","text":"<p>STEP 1. Access your settings in the upper-right hand corner of your Coralogix dashboard.</p> <p>STEP 2. In the left-hand sidebar, click Team Members. A list of existing team members will appear.</p> <p>STEP 3. Search existing team members. You may filter your search according to member roles.</p> <p></p> <p>STEP 4. Administrators (admins) may add team members, remove them, or change their permissions by clicking on the drop-down menu right of the user's name. Notes:</p> <ul> <li> <p>Users can be assigned to more than one team.</p> </li> <li> <p>Users may view data only of those teams of which they are members.</p> </li> <li> <p>Upon logging in, users may select the team within which they would like to work.</p> </li> <li> <p>Each team has its own unique Send-Your-Data API key.</p> </li> </ul>"},{"location":"newoutput/user-team-management/#sso-login","title":"SSO Login","text":"<p>For instructions on how to set up a single sign-on (SSO) with your IDP, follow our tutorial here.</p> <p>Notes:</p> <ul> <li> <p>If your admin configures a SAML SSO, no password is necessary for you to sign in to Coralogix.</p> </li> <li> <p>Only the admin is authorized to change the password if an SSO is enabled.</p> </li> </ul>"},{"location":"newoutput/user-team-management/#session-length-management","title":"Session Length Management","text":"<p>Team administrators can define the duration of idle sessions for all users in the team. Enabling this option will end all current login sessions and require users to log in again. Find out more here.</p>"},{"location":"newoutput/user-team-management/#role-based-access-control","title":"Role-Based Access Control","text":"<p>Role-based access control (RBAC) allows account administrators to grant some or all team members specific application and subsystem data scope permissions for logs and traces, as well as action permissions.</p> <ul> <li> <p>RBAC for logs. Find out more here.</p> </li> <li> <p>RBAC for traces. Find out more here.</p> </li> </ul>"},{"location":"newoutput/user-team-management/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/validation-kubernetes-observability-using-opentelemetry/","title":"Validation","text":"<p>Validate that you have enabled Kubernetes Observability using OpenTelemetry and are sending cluster telemetry to Coralogix.</p> <p>STEP 1. Check that the cluster collector are running in your cluster.</p> <p>Search for pods in the deployed namespace with the following:</p> <pre><code>kubectl get pods -o wide -n $NAMESPACE\n\n</code></pre> <p>OpenTelemetry Cluster Collector pods named <code>coralogix-opentelemetry-collector-xxx</code> and Kube State Metrics pods named <code>otel-integration-kube-state-metrics-xxx</code> should appear with <code>Running</code> status.</p> <p></p> <p>STEP 2. Install the Kubernetes OpenTelemetry extension packages in your Coralogix account by navigating to Data Flow &gt; Extensions in your toolbar. Use this to hit the ground running with predefined alerts, parsing rules, dashboards, saved views, and actions.</p> <ul> <li> <p>Open the Kubernetes OpenTelemetry extension with the latest version.</p> </li> <li> <p>Select\u00a0Applications\u00a0and\u00a0Subsystems\u00a0for all related Kubernetes telemetry or select All for both.</p> </li> <li> <p>Click + Deploy.</p> </li> </ul> <p></p> <p>STEP 3. The Kubernetes OpenTelemetry extension includes a set of Grafana K8s Otel dashboards when installed. It is a useful way to determine if the metrics being exported to Coralogix are satisfactory for your Kubernetes Dashboard setup.</p> <ul> <li>In the Kubernetes Dashboard, select I\u2019ve Installed OpenTelemetry. Click \u2192 GO.</li> </ul> <p></p> <ul> <li>If all the metrics and labels are present, the Kubernetes Dashboard is opened. If there are missing metrics or labels, a screen appears detailing the missing metrics and/or labels.</li> </ul> <p></p> <ul> <li> <p>Provide any missing metrics or labels.</p> </li> <li> <p>Click DONE, RELOAD MY DATA \u2192 to continue to the Kubernetes Dashboard.</p> </li> </ul> <p></p>"},{"location":"newoutput/validation-kubernetes-observability-using-opentelemetry/#next-steps","title":"Next Steps","text":"<p>Troubleshoot your configuration here.</p>"},{"location":"newoutput/validation-kubernetes-observability-using-opentelemetry/#additional-resources","title":"Additional Resources","text":"DocumentationKubernetes Dashboard"},{"location":"newoutput/validation-kubernetes-observability-using-opentelemetry/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at support@coralogixstg.wpengine.com</p>"},{"location":"newoutput/vector/","title":"Vector","text":"<p>Coralogix provides seamless integration with Vector so you can send your logs from anywhere and parse them according to your needs.</p>"},{"location":"newoutput/vector/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Vector installed</p> </li> <li> <p>For those customers with a Kubernetes environment, Helm 2.9+ Package Manager installed</p> </li> </ul>"},{"location":"newoutput/vector/#usage","title":"Usage","text":"<p>You must input the following variables when creating a Coralogix logger instance or updating your configuration file:</p> <ul> <li> <p><code>`api_key`</code>: Input your Coralogix Send-Your-Data API key.</p> </li> <li> <p><code>applicationName</code> and <code>subystemName</code>: Input application and subsystem names.</p> </li> <li> <p><code>coralogix_domain</code>: Input the Coralogix\u00a0domain associated with your account.</p> </li> </ul>"},{"location":"newoutput/vector/#configuration-for-non-kubernetes-environments","title":"Configuration for Non-Kubernetes Environments","text":"<p>STEP 1. Edit default configuration file</p> <p>Edit the default configuration file vector.toml and paste the following configuration below. Update the following:</p> <ul> <li> <p>event.log.applicationName = \"VECTOR_APP\"</p> </li> <li> <p>event.log.subsystemName = \"VECTOR_SUBSYS\"</p> </li> <li> <p>uri = REST API Singles endpoint associated with your Coralogix domain</p> </li> <li> <p>headers.authorization = \"Bearer &lt;&gt;\" <pre><code># This input is just for test or demo purposes and is supported only since Vector v0.12.0. Feel free to use any other input.\n\n[sources.my_source_id]\n  type = \"demo_logs\" # required\n  lines = [\"{\\\"msg\\\":\\\"success\\\", \\\"code\\\":200}\", \"{\\\"msg\\\":\\\"error\\\", \\\"code\\\":500}\"] # optional, no default, relevant when [`format`](#format) = `shuffle`\n  sequence = false # optional, default, relevant when [`format`](#format) = `shuffle`\n  batch_interval = 10\n  format = \"shuffle\"\n\n[transforms.prepare_for_coralogix]\n  # General\n  type = \"lua\" # required\n  inputs = [\"my_source_id\"] # required\n  version = \"2\" # required\n  # Hooks\n  hooks.process = '''\n  function (event, emit)\n    event.log = {\n      applicationName = \"VECTOR_APP\",\n      subsystemName = \"VECTOR_SUBSYS\",\n      json = event.log,\n      computerName = os.getenv(\"HOSTNAME\"),\n      timestamp = os.time() * 1000\n    }\n    emit(event)\n  end\n  '''\n\n[sinks.coralogix]\n  # General\n  type = \"http\" # required\n  inputs = [\"prepare_for_coralogix\"] # required\n  compression = \"none\" # optional, default\n  method = \"post\"\n  uri = &lt;&lt;Coralogix REST API singles endpoint&gt;&gt; # required\n  # Batch\n  batch.max_bytes = 1049000 # optional, default, bytes\n  batch.timeout_secs = 2 # optional, default, seconds\n  # Encoding\n  encoding.codec = \"json\" # required\n  # Healthcheck\n  #healthcheck.enabled = true # optional, default\n  headers.Content-Type = \"application/json\"\n  headers.authorization = \"Bearer &lt;&lt;API_key&gt;&gt;\"\n</code></pre> <p>STEP 2. Add your own sources</p> <p>That code will allow you to generate the test traffic. After testing the connection, add your own sources, as in the example below.</p> <pre><code>[sources.my_source_id]\n\u00a0 type = \"file\"\n\u00a0 include = [\"/var/log/access.log \"]\n</code></pre> <p>You can read more about this configuration here.</p> <p>STEP 3. Restart Vector</p> <p>Remember to restart Vector after every change to load updated configuration.</p>"},{"location":"newoutput/vector/#configuration-for-kubernetes-environments","title":"Configuration for Kubernetes Environments","text":"<p>STEP 1. Add Helm chart repo</p> <pre><code>helm repo add vector https://helm.vector.dev\nhelm repo update\n</code></pre> <p>STEP 2. Create an override.yaml file</p> <p>Prior to installation create an override.yaml file and paste the following configuration below. Remember to update:</p> <ul> <li> <p>event.log.applicationName: \u201cVECTOR_APP\u201d</p> </li> <li> <p>event.log.subsystemName: \u201cVECTOR_SUBSYS\u201d</p> </li> <li> <p>uri: logs endpoint associated with your Coralogix domain</p> </li> <li> <p>headers.private_key: \u201c&lt;&gt;\u201d <pre><code># vector override.yaml\n---\nrole: \"Agent\"\n\ncustomConfig:\n  sources:\n    kubernetes_logs:\n      type: kubernetes_logs\n\n  transforms:\n    prepare_for_coralogix:\n      type: lua\n      inputs: [\"kubernetes_logs\"]\n      version: \"2\"\n      #hooks\n      hooks:\n        process: |\n            function (event, emit)\n              event.log = {\n                applicationName = \"VECTOR_APP\",\n                subsystemName = \"VECTOR_SUBSYS\",\n                json = event.log,\n                computerName = os.getenv(\"HOSTNAME\"),\n                timestamp = os.time() * 1000\n              }\n              emit(event)\n            end\n\n  sinks:\n    coralogix:\n      type: http\n      inputs: [\"prepare_for_coralogix\"]\n      compression: none\n      uri: &lt;&lt;Coralogix REST API singles endpoint&gt;&gt;\n      batch:\n        maxbytes:\n          1049000\n        timeout_secs:\n          2\n      encoding:\n        codec:\n          json\n      request:\n        headers:\n          Content-Type: application/json\n          private_key: &lt;&lt;api_key&gt;&gt;\n</code></pre> <p>STEP 3. Deploy the chart</p> <pre><code>helm install vector-agent vector/vector -f override.yaml\n</code></pre> <p>STEP 4. Remove the Daemonset</p> <pre><code>helm uninstall vector-agent\n</code></pre>"},{"location":"newoutput/vector/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/version-tags-with-curl/","title":"cURL Version Tags","text":"<p>Note: This document includes cluster-dependent URLs. Please refer to the following table to select the correct Coralogix Version Tags API endpoint for your Coralogix Portal domain\u2019s extension (.com/.us/.in).  </p> <p>The following examples have been built for the .com Coralogix Portal domain extension.  </p> <p>Please use the corresponding API endpoint for your domain:</p> <p>You can add version tags per Application and Subsystem using cURL:</p> <p>URL: https://webapi.coralogixstg.wpengine.com/api/v1/external/tags</p> <p>Method: POST</p> <p>Body Schema:</p> <pre><code>{\"iconUrl\": string,\"name\": string,\"timestamp\": date type (JavaScript date string or epoch milliseconds)\"application\": string[]\"subsystem\": string[]}\n</code></pre> <p>Application: Your Application name.</p> <p>Subsystem: Your Subsystem(s) name(s). You can input more than one subsystem name, use comma delimiter \u2018,\u2019 between Subsystem names.</p> <p>Name: Your Version Tag name.</p> <p>Timestamp: Tag Timestamp. (This is OPTIONAL: If a timestamp is not supplied, the current timestamp will be applied).</p> <p>IconUrl: Tag's Picture. (This is OPTIONAL: Use an URL (URL encoded) to a valid image file uploaded to a public repository)  </p> <p>Supported Image Formats: png, jpeg, SVG.  </p> <p>Maximum File Size: 50 Kilobytes.  </p> <p>Note: If the URL to the image contains spaces, please use %20.  </p> <p>For example:</p> <p>https://www.myimagesite.com/Path%20To%20The%20Image.png.</p> <p>Example (GET):</p> <pre><code>curl --location --request GET 'http://webapi.coralogixstg.wpengine.com/api/v1/external/tags/add?key=&amp;application=Application_Name,Application_Name2&amp;subsystem=Subsystem1,Subsystem2,Subsystem3&amp;name=MyFirstTag&amp;timestamp=2020-06-23:09:00:00&amp;iconUrl=your_icon_url'\n</code></pre> <p>Example (POST):</p> <pre><code>curl --location --request POST 'https://webapi.coralogixstg.wpengine.com/api/v1/external/tags' \\\n--header 'Authorization: Bearer 12345678-abcd-efgh-ijk-123456789012' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n\"timestamp\": 1617793927675,\n\"name\": \"test tag\",\n\"application\": [\"prd\", \"dev\"],\n\"subsystem\": [\"app\", \"mobile\"],\n\"iconUrl\": \"my-avatar.png\"\n}'\n</code></pre> <p>Note: application and subsystem should always be contained between square brackets, even if it is an array of one element, for example:  </p> <p><code>\"application\": [\"prd\"]</code>, <code>\"subsystem\": [\"app\"]</code> </p> <p>The API key should be taken from:  </p> <p>Data Flow --&gt; API Keys --&gt; \"Alerts, Rules and Tags API Key\"</p> <p></p>"},{"location":"newoutput/webhooks-api/","title":"Webhooks API","text":"<p>Leverage our Webhooks API to seamlessly define, query, and manage your outbound webhooks with ease.</p>"},{"location":"newoutput/webhooks-api/#api-key","title":"API Key","text":"<p>This API uses the Alert, Rules, and Tags API key. Retrieve or generate this API key by following these instructions.</p>"},{"location":"newoutput/webhooks-api/#api-reference","title":"API Reference","text":"<p>The Webhooks API follows an HTTP-style method.</p>"},{"location":"newoutput/webhooks-api/#base-url","title":"Base URL","text":"Region URL US1 EU1 EU2 AP2 (SG) https://api./api/v1/external/integrations/ AP1 (IN) https://api.app./api/v1/external/integrations/ <p>: Input the Coralogix\u00a0domain associated with your region."},{"location":"newoutput/webhooks-api/#endpoint-details","title":"Endpoint Details","text":"HTTP Method GET / POST Content-Type application/json Header Authorization: bearer"},{"location":"newoutput/webhooks-api/#curl-command-example","title":"<code>curl</code> Command Example","text":"<p>Use this\u00a0<code>curl</code>\u00a0command with a placeholder for the token.</p> <p>For regions US1, EU1, EU2, AP2 (SG):</p> <pre><code>curl -X GET -H 'Authorization: bearer &lt;api_key&gt;' -H 'Content-Type: application/json' [https://api.&lt;cx_domain&gt;/api/v1/external/integrations/](&lt;https://api.eu2.coralogixstg.wpengine.com/api/v1/external/integrations/&gt;)\n\n</code></pre> <p>For region API (IN):</p> <pre><code>curl -X GET -H 'Authorization: bearer &lt;api_key&gt;' -H 'Content-Type: application/json' [https://api.app.&lt;cx_domain&gt;/api/v1/external/integrations/](&lt;https://api.eu2.coralogixstg.wpengine.com/api/v1/external/integrations/&gt;)\n\n</code></pre>"},{"location":"newoutput/webhooks-api/#get-request-get-webhooks","title":"GET Request: Get Webhooks","text":"<p>Get all webhooks or a single webhook using a Webhook ID.</p>"},{"location":"newoutput/webhooks-api/#get-all-webhooks","title":"Get All Webhooks","text":"<p>Request: GET https://api./api/v1/external/integrations/ <p>Result:</p> <pre><code>[\n    {\n        \"alias\": \"string\",\n        \"company_id\": number,\n        \"created_at\": \"string - ISO format\",\n        \"id\": number,\n        \"url\": \"string\",\n        \"integration_type_fields\": \"escaped json\" or null,\n        \"integration_type_id\": number,\n        \"integrationTypeId\": number,\n        \"integration_type\": {\n            \"label\": \"string\",\n            \"icon\": \"string\",\n            \"id\": number\n        },\n        \"updated_at\": \"string - ISO format\"\n    }, // array of webhooks\n]\n\n# RLRL - Example of a schema in the result with comments\n[\n  {\n    # The name of the webhook\n    \"alias\": string,\n    # The company id\n    \"company_id\": number\n    # The webhook creation time in ISO8601 Format (e.g. 2021-01-01T12:32:23)\n    \"created_at\": string\n    ...\n  },\n  ...\n]\n\n</code></pre>"},{"location":"newoutput/webhooks-api/#get-a-specific-webhook","title":"Get a Specific Webhook","text":"<p>Request: GET https://api./api/v1/external/integrations/ <p>Result:</p> <pre><code>{\n        \"alias\": \"string\",\n        \"company_id\": number,\n        \"created_at\": \"string - ISO format\",\n        \"id\": number,\n        \"url\": \"string\",\n        \"integration_type_fields\": \"escaped json\" or null,\n        \"integration_type_id\": number,\n        \"integrationTypeId\": number,\n        \"integration_type\": {\n            \"label\": \"string\",\n            \"icon\": \"string\",\n            \"id\": number\n        },\n        \"updated_at\": \"string - ISO format\"\n    }\n\n</code></pre>"},{"location":"newoutput/webhooks-api/#post-request-create-a-new-webhook","title":"POST Request: Create a New Webhook","text":"<p>Create or update a single webhook.</p>"},{"location":"newoutput/webhooks-api/#body-parameters","title":"Body Parameters","text":"Parameter Description Type Notes alias webhook name string integration_type webhook type object described below, must be a complete block from values as shown in Table A integration_type.label webhook type name string integration_type.icon webhook icon string integration_type.id webhook type id number integration_type_id webhook type id number must be from values shown in Table A integration_type_fields webhook additional fields string escaped json, an array of objects in the form of name + value. must comply with the structure shown in Table B url webhook url string <p>Table A: <code>integration_type</code> Object</p> Type JSON Object slack {\u201cid\u201d: 0, \u201cname\u201d: \u201cSlack\u201d, \u201cicon\u201d: \u201c/assets/settings/slack-48.png\u201d} webhook {\u201cid\u201d: 1, \u201cname\u201d: \u201cWebHook\u201d, \u201cicon\u201d: \u201c/assets/webhook.png\u201d} pager_duty {\u201cid\u201d: 2, \u201cname\u201d: \u201cPagerDuty\u201d, \u201cicon\u201d: \u201c/assets/settings/pagerDuty.png\u201d} sendlog {\u201cid\u201d: 3, \u201cname\u201d: \u201cSendLog\u201d, \u201cicon\u201d: \u201c/assets/invite.png\u201d} email_group {\u201cid\u201d: 4, \u201cname\u201d: \u201cEmail Group\u201d, \u201cicon\u201d: \u201c/assets/email-group.png\u201d} microsoft_teams {\u201cid\u201d: 5, \u201cname\u201d: \u201cMicrosoft Teams\u201d, \u201cicon\u201d: \u201c/assets/settings/teams.png\u201d} jira {\u201cid\u201d: 6, \u201cname\u201d: \u201cJira\u201d, \u201cicon\u201d: \u201c/assets/settings/jira.png\u201d} opsgenie {\u201cid\u201d: 7, \u201cname\u201d: \u201cOpsgenie\u201d, \u201cicon\u201d: \u201c/assets/settings/opsgenie.png\u201d} demisto {\u201cid\u201d: 8, \u201cname\u201d: \u201cDemisto\u201d, \u201cicon\u201d: \u201c/assets/settings/demisto.png\u201d} <p>Table B: <code>Integration_type_fields</code> String</p> Type Field value_type Notes pager_duty serviceKey string jira apiToken string jira email string jira projectKey string email_group payload array of strings webhook,demisto,sendlog uuid string in UUID format webhook,demisto,sendlog method string must be one of [\u201cget\u201d,\u201dpost\u201d,\u201dput\u201d] webhook,demisto,sendlog headers object a json object of headers webhook,demisto,sendlog payload object a json object of the webhook body"},{"location":"newoutput/webhooks-api/#create-a-slack-webhook","title":"Create a Slack Webhook","text":"<p>Request: POST https://api./api/v1/external/integrations/ <p>Result:</p> <pre><code>{\n        \"alias\": \"slack-webhook\",\n        \"url\": \"&lt;slack-webhook-url&gt;\",\n        \"integration_type_fields\": \"[]\",\n        \"integration_type_id\": 0,\n        \"integration_type\": {\n            \"label\": \"Slack\",\n            \"icon\": \"/assets/settings/slack-48.png\",\n            \"id\": 0\n        }\n    }\n\n</code></pre>"},{"location":"newoutput/webhooks-api/#create-a-sendlog-webhook","title":"Create a \u2018sendlog\u2019 Webhook","text":"<p>Request: POST https://api./api/v1/external/integrations/ <pre><code>{\n    \"alias\": \"sendlog-webhook\",\n    \"integration_type\": {\n        \"icon\": \"/assets/invite.png\",\n        \"id\": 3,\n        \"name\": \"Send Log\"\n    },\n    \"integration_type_id\": 3,\n    \"integration_type_fields\": \"[{\\\\\"name\\\\\":\\\\\"uuid\\\\\",\\\\\"value\\\\\":\\\\\"&lt;uuid&gt;\\\\\"},{\\\\\"name\\\\\":\\\\\"method\\\\\",\\\\\"value\\\\\":\\\\\"post\\\\\"},{\\\\\"name\\\\\":\\\\\"headers\\\\\",\\\\\"value\\\\\":{\\\\\"Content-Type\\\\\":\\\\\"application/json\\\\\"}},{\\\\\"name\\\\\":\\\\\"payload\\\\\",\\\\\"value\\\\\":{\\\\\"privateKey\\\\\":\\\\\"&lt;send-your-data-api-key&gt;\\\\\",\\\\\"applicationName\\\\\":\\\\\"$APPLICATION_NAME\\\\\",\\\\\"subsystemName\\\\\":\\\\\"$SUBSYSTEM_NAME\\\\\",\\\\\"computerName\\\\\":\\\\\"$COMPUTER_NAME\\\\\",\\\\\"logEntries\\\\\":[{\\\\\"severity\\\\\":3,\\\\\"timestamp\\\\\":\\\\\"$EVENT_TIMESTAMP_MS\\\\\",\\\\\"text\\\\\":{\\\\\"integration_text\\\\\":\\\\\"&lt;Insert your desired integration description&gt;\\\\\",\\\\\"alert_severity\\\\\":\\\\\"$EVENT_SEVERITY\\\\\",\\\\\"alert_id\\\\\":\\\\\"$ALERT_ID\\\\\",\\\\\"alert_name\\\\\":\\\\\"$ALERT_NAME\\\\\",\\\\\"alert_url\\\\\":\\\\\"$ALERT_URL\\\\\",\\\\\"hit_count\\\\\":\\\\\"$HIT_COUNT\\\\\"}}],\\\\\"uuid\\\\\":\\\\\"&lt;same-uuid&gt;\\\\\"}}]\",\n    \"url\": \"&lt;https://api.coralogix.us/api/v1/logs&gt;\"\n}\n\n</code></pre> <p>Inside <code>integration_type_fields</code> modify the following:</p> <ul> <li> <p> and  \u2013 with a newly generated uuid <li> <p> \u2013 with your\u00a0Send-Your-Data API key <li> <p> <p>Result:</p> <pre><code>{\n    \"id\": 1051,\n    \"alias\": \"sendlog-webhook\",\n    \"url\": \"&lt;https://api.coralogix.us/api/v1/logs&gt;\",\n    \"integration_type_fields\": \"[{\\\\\"name\\\\\":\\\\\"uuid\\\\\",\\\\\"value\\\\\":\\\\\"17a3b9e3-b0bc-4bc0-9e06-98d2a4c54ecb\\\\\"},{\\\\\"name\\\\\":\\\\\"method\\\\\",\\\\\"value\\\\\":\\\\\"post\\\\\"},{\\\\\"name\\\\\":\\\\\"headers\\\\\",\\\\\"value\\\\\":{\\\\\"Content-Type\\\\\":\\\\\"application/json\\\\\"}},{\\\\\"name\\\\\":\\\\\"payload\\\\\",\\\\\"value\\\\\":{\\\\\"privateKey\\\\\":\\\\\"5ef4a0d1-7e1f-47b2-ac0a-1282002aa2a1\\\\\",\\\\\"applicationName\\\\\":\\\\\"$APPLICATION_NAME\\\\\",\\\\\"subsystemName\\\\\":\\\\\"$SUBSYSTEM_NAME\\\\\",\\\\\"computerName\\\\\":\\\\\"$COMPUTER_NAME\\\\\",\\\\\"logEntries\\\\\":[{\\\\\"severity\\\\\":3,\\\\\"timestamp\\\\\":\\\\\"$EVENT_TIMESTAMP_MS\\\\\",\\\\\"text\\\\\":{\\\\\"integration_text\\\\\":\\\\\"Insert your desired integration description\\\\\",\\\\\"alert_severity\\\\\":\\\\\"$EVENT_SEVERITY\\\\\",\\\\\"alert_id\\\\\":\\\\\"$ALERT_ID\\\\\",\\\\\"alert_name\\\\\":\\\\\"$ALERT_NAME\\\\\",\\\\\"alert_url\\\\\":\\\\\"$ALERT_URL\\\\\",\\\\\"hit_count\\\\\":\\\\\"$HIT_COUNT\\\\\"}}],\\\\\"uuid\\\\\":\\\\\"17a3b9e3-b0bc-4bc0-9e06-98d2a4c54ecb\\\\\"}}]\",\n    \"integration_type_id\": 3,\n    \"integrationTypeId\": 3,\n    \"company_id\": 12345,\n    \"updated_at\": \"2022-08-22T07:32:09.348Z\",\n    \"created_at\": \"2022-08-22T07:32:09.348Z\",\n    \"companyId\": 12345\n}\n\n</code></pre>"},{"location":"newoutput/webhooks-api/#post-request-create-new-bulk-webhooks","title":"POST Request: Create New bulk Webhooks","text":"<p>Create webhooks in bulk.</p> <p>To create several webhooks in one request, please send an\u00a0array of objects, where each object is a different webhook.</p>"},{"location":"newoutput/webhooks-api/#create-2-webhooks-in-the-same-request","title":"Create 2 Webhooks in the Same Request","text":"<p>Request: POST https://api.coralogixstg.wpengine.com/api/v1/external/integrations-bulk</p> <pre><code>[\n       {\n        \"alias\": \"slack-webhook\",\n        \"url\": \"&lt;slack-webhook-url&gt;\",\n        \"integration_type_fields\": \"[]\",\n        \"integration_type_id\": 0,\n        \"integration_type\": {\n            \"label\": \"Slack\",\n            \"icon\": \"/assets/settings/slack-48.png\",\n            \"id\": 0\n        }\n    },\n {\n    \"alias\": \"sendlog-webhook\",\n    \"integration_type\": {\n        \"icon\": \"/assets/invite.png\",\n        \"id\": 3,\n        \"name\": \"Send Log\"\n    },\n    \"integration_type_id\": 3,\n    \"integration_type_fields\": \"[{\\\\\"name\\\\\":\\\\\"uuid\\\\\",\\\\\"value\\\\\":\\\\\"&lt;uuid&gt;\\\\\"},{\\\\\"name\\\\\":\\\\\"method\\\\\",\\\\\"value\\\\\":\\\\\"post\\\\\"},{\\\\\"name\\\\\":\\\\\"headers\\\\\",\\\\\"value\\\\\":{\\\\\"Content-Type\\\\\":\\\\\"application/json\\\\\"}},{\\\\\"name\\\\\":\\\\\"payload\\\\\",\\\\\"value\\\\\":{\\\\\"privateKey\\\\\":\\\\\"&lt;send-your-data-api-key&gt;\\\\\",\\\\\"applicationName\\\\\":\\\\\"$APPLICATION_NAME\\\\\",\\\\\"subsystemName\\\\\":\\\\\"$SUBSYSTEM_NAME\\\\\",\\\\\"computerName\\\\\":\\\\\"$COMPUTER_NAME\\\\\",\\\\\"logEntries\\\\\":[{\\\\\"severity\\\\\":3,\\\\\"timestamp\\\\\":\\\\\"$EVENT_TIMESTAMP_MS\\\\\",\\\\\"text\\\\\":{\\\\\"integration_text\\\\\":\\\\\"&lt;Insert your desired integration description&gt;\\\\\",\\\\\"alert_severity\\\\\":\\\\\"$EVENT_SEVERITY\\\\\",\\\\\"alert_id\\\\\":\\\\\"$ALERT_ID\\\\\",\\\\\"alert_name\\\\\":\\\\\"$ALERT_NAME\\\\\",\\\\\"alert_url\\\\\":\\\\\"$ALERT_URL\\\\\",\\\\\"hit_count\\\\\":\\\\\"$HIT_COUNT\\\\\"}}],\\\\\"uuid\\\\\":\\\\\"&lt;same-uuid&gt;\\\\\"}}]\",\n    \"url\": \"&lt;https://api.coralogix.us/api/v1/logs&gt;\"\n }\n]\n\n</code></pre> <p>Result:</p> <pre><code>[\n {\n    \"id\": 1050,\n    \"alias\": \"slack-webhook\",\n    \"url\": \"&lt;slack-webhook-url&gt;\",\n    \"integration_type_fields\": \"[]\",\n    \"integration_type_id\": 0,\n    \"integrationTypeId\": 0,\n    \"company_id\": 12345,\n    \"updated_at\": \"2022-08-22T07:32:09.348Z\",\n    \"created_at\": \"2022-08-22T07:32:09.348Z\",\n    \"companyId\": 12345\n},\n{\n    \"id\": 1051,\n    \"alias\": \"sendlog-webhook\",\n    \"url\": \"&lt;https://api.coralogix.us/api/v1/logs&gt;\",\n    \"integration_type_fields\": \"[{\\\\\"name\\\\\":\\\\\"uuid\\\\\",\\\\\"value\\\\\":\\\\\"17a3b9e3-b0bc-4bc0-9e06-98d2a4c54ecb\\\\\"},{\\\\\"name\\\\\":\\\\\"method\\\\\",\\\\\"value\\\\\":\\\\\"post\\\\\"},{\\\\\"name\\\\\":\\\\\"headers\\\\\",\\\\\"value\\\\\":{\\\\\"Content-Type\\\\\":\\\\\"application/json\\\\\"}},{\\\\\"name\\\\\":\\\\\"payload\\\\\",\\\\\"value\\\\\":{\\\\\"privateKey\\\\\":\\\\\"5ef4a0d1-7e1f-47b2-ac0a-1282002aa2a1\\\\\",\\\\\"applicationName\\\\\":\\\\\"$APPLICATION_NAME\\\\\",\\\\\"subsystemName\\\\\":\\\\\"$SUBSYSTEM_NAME\\\\\",\\\\\"computerName\\\\\":\\\\\"$COMPUTER_NAME\\\\\",\\\\\"logEntries\\\\\":[{\\\\\"severity\\\\\":3,\\\\\"timestamp\\\\\":\\\\\"$EVENT_TIMESTAMP_MS\\\\\",\\\\\"text\\\\\":{\\\\\"integration_text\\\\\":\\\\\"Insert your desired integration description\\\\\",\\\\\"alert_severity\\\\\":\\\\\"$EVENT_SEVERITY\\\\\",\\\\\"alert_id\\\\\":\\\\\"$ALERT_ID\\\\\",\\\\\"alert_name\\\\\":\\\\\"$ALERT_NAME\\\\\",\\\\\"alert_url\\\\\":\\\\\"$ALERT_URL\\\\\",\\\\\"hit_count\\\\\":\\\\\"$HIT_COUNT\\\\\"}}],\\\\\"uuid\\\\\":\\\\\"17a3b9e3-b0bc-4bc0-9e06-98d2a4c54ecb\\\\\"}}]\",\n    \"integration_type_id\": 3,\n    \"integrationTypeId\": 3,\n    \"company_id\": 12345,\n    \"updated_at\": \"2022-08-22T07:32:09.348Z\",\n    \"created_at\": \"2022-08-22T07:32:09.348Z\",\n    \"companyId\": 12345\n }\n]\n\n</code></pre>"},{"location":"newoutput/webhooks-api/#post-request-update-a-single-webhook-or-bulk-webhooks","title":"POST Request: Update a Single Webhook or Bulk Webhooks","text":"<p>To update an existing webhook, send a POST request with all of the usual values. Add an ID field with the webhook ID as the value.</p> <p>To update a group of webhooks, make sure your POST request consists of an array of objects and that your URL ends with <code>/integrations-bulk</code>.</p>"},{"location":"newoutput/webhooks-api/#copying-webhooks-between-accounts","title":"Copying Webhooks Between Accounts","text":"<p>To copy webhooks from one account to another:</p> <ul> <li> <p>Create a GET request with the API key of the origin account and GET all webhooks.</p> </li> <li> <p>Copy the full response and remove the ID of the webhook. Do not remove the ID under\u00a0<code>integration_type</code>.</p> </li> <li> <p>Copy the edited array to a new POST request:</p> <ul> <li> <p>Make sure to modify the API key to the API key of the destination account.</p> </li> <li> <p>Make sure your URL ends with <code>/integrations-bulk</code>.</p> </li> </ul> </li> </ul>"},{"location":"newoutput/webhooks-api/#delete-request-delete-a-webhook","title":"DELETE Request \u2013 Delete a Webhook","text":""},{"location":"newoutput/webhooks-api/#delete-a-webhook","title":"Delete a Webhook","text":"<p>Request: DELETE https://api./api/v1/external/integrations/"},{"location":"newoutput/webhooks-api/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/whats-new/","title":"What's New in Coralogix","text":"<p>Get up to speed on everything new and improved in the Coralogix platform!</p>"},{"location":"newoutput/whats-new/#january-2024","title":"January 2024","text":""},{"location":"newoutput/whats-new/#take-control-with-our-new-role-based-access","title":"Take Control with Our New Role Based Access","text":"<p>Whether you\u2019re a growing business or enterprise, you need to manage your users\u2019 permissions clearly and effectively. With Coralogix\u2019s new RBAC, you can easily control user access. Create a brand new user role by choosing from over 300 different permissions, or select one of our seven predefined system roles\u2014all customizable.</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#suppress-your-alerts-when-needed","title":"Suppress Your Alerts When Needed","text":"<p>Use our new Alert Suppression Rules to mute alerts you don\u2019t need during scheduled maintenance, testing, auto-scaling events or outside working hours. Simply select your specific parameters to suppress what, when and which alerts go off in your system.\u00a0\u00a0</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#more-rum-anyone-core-web-vitals-now-available-to-optimize-user-experience","title":"More RUM Anyone? Core Web Vitals Now Available To Optimize User Experience","text":"<p>In addition to our updated RUM Browser SDK and error tracking alerts, we\u2019ve added Core Web Vitals, so you can easily keep an eye on your web performance and overall user experience.\u00a0  </p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#easily-import-and-export-custom-dashboard","title":"Easily Import and Export Custom Dashboard","text":"<p>Effortlessly share your custom dashboards across teams within your organization by importing and exporting them, eliminating the need to recreate dashboards and minimizing overhead.</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#view-metrics-and-what-they-monitor","title":"View Metrics And What They Monitor","text":"<p>Adding metrics to your Custom Dashboard? You can now use free text to search for a metric of your choice and see what that metric monitors. Simply hover over any metric to view its system-generated metadata labels and values.</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#_1","title":"What's New in Coralogix","text":"<p>December 2023</p>"},{"location":"newoutput/whats-new/#rum-user-sessions","title":"RUM User Sessions","text":"<p>Track every step of your users\u2019 interaction with your website or application. Analyze and correlate real-time data for rapid response to critical issues. Gain insight into user flow for improved conversions and high user satisfaction.</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#genai-query-assistant","title":"GenAI Query Assistant","text":"<p>Our new Query Assistant is an AI-powered search feature that lets you use natural language for querying data. Just type in what you are looking for and the Query Assistant will translate your request into Coralogix\u2019s DataPrime query language. You can modify your query, either in natural language or by changing the generated DataPrime query itself.</p> <p>Learn More&gt;</p>"},{"location":"newoutput/whats-new/#incidents-screen","title":"Incidents Screen","text":"<p>Gain more insights into your triggered alerts with our improved Incidents feature. With a secondary set of notifications, you\u2019ll get more contextual information to drill down around any issues that come your way. Group by tags by region, environment, and more.</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#metric-alert-less-than-usual-condition","title":"Metric Alert - Less Than Usual Condition","text":"<p>Our AI-leveraged alerts give you full visibility of what is happening in your services, setting the most accurate threshold for your data. The alert system allows for prediction modeling methods and variability to enhance your system\u2019s overall performance.</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#apdex-score","title":"Apdex Score","text":"<p>Check out our latest widget added to our Service Catalog, the Apdex Score, or Application Performance Index. Measure and quantify user requests with a configured response time threshold that\u2019s true to customer satisfaction, from tolerated to frustrated categorization.</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#opentelemetry-lambda-auto-instrumentation","title":"OpenTelemetry Lambda Auto Instrumentation","text":"<p>Coralogix now supports the latest version of OpenTelemetry Lambda auto instrumentation wrappers, together with our AWS Lambda Telemetry Exporter. Continue to get complete telemetry and generate traces for Lambda. \u00a0</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#october-2023","title":"October 2023","text":""},{"location":"newoutput/whats-new/#error-template-view","title":"Error Template View","text":"<p>Simplify error management with advanced templating of similar errors into definable issues. Effectively reduce unnecessary noise with drill-down based on customizable filters such as error type, username, URL, session ID, data source and more.\u00a0</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#gcp-traces","title":"GCP Traces","text":"<p>Expand your APM coverage by sending your Google Cloud traces seamlessly to Coralogix, being able to search, analyze and visualize your applications\u2019 performance and health.</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#aws-kinesis-data-firehose-metrics","title":"AWS Kinesis Data Firehose Metrics","text":"<p>Monitor all your AWS services with real-time streaming data delivered from AWS Kinesis Data Firehose directly to Coralogix. With our simple integration, you can get started in just a few steps.</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#azure-resource-manager-arm-integration","title":"Azure Resource Manager (ARM) Integration","text":"<p>We\u2019ve added more to our ARM integration so you can create custom Coralogix templates to send events from not only Event Hub but also Blob storage, Queue storage and Diagnostic storage.</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#september-2023","title":"September 2023","text":""},{"location":"newoutput/whats-new/#new-quota-manager","title":"New Quota Manager","text":"<p>For extensive org-level data management, the Quota Manager now presents data consumption as both Coralogix Units and GB sent; with group by options: Pillar (Logs, Metrics, Traces) and Priority (Blocked, Low, Medium, High).</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#extensions-improvements","title":"Extensions Improvements","text":"<p>Our evolving extensions page simplifies data transfer to Coralogix. We've added new integration flows for GCP Spans, Azure ARM, Azure Metrics, and Slack. Plus, you can now preview flow alerts in Coralogix extensions.</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#dashboard-improvements","title":"Dashboard Improvements","text":"<p>Exciting dashboard updates include interlinked filters, stacked bar charts, and a new colour scheme for graphs. Plus, enjoy design flexibility with the new Markdown Widget and query archived data within custom dashboards.</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#rum-onboarding","title":"RUM Onboarding","text":"<p>Enable Real User Monitoring quickly with the RUM Integration Package. It automates RUM Browser SDK setup and source map uploads, capturing and sending network requests and errors to Coralogix once configured.</p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#robust-cspm","title":"Robust CSPM","text":"<p>We've improved our CSPM (Cloud Security Posture Management) offering by extending GCP support, offering enhanced clarity and guidance on issue resolution across AWS and GCP in around 600 scenarios.</p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#august-2023","title":"August 2023","text":""},{"location":"newoutput/whats-new/#unified-threat-intelligence","title":"Unified Threat Intelligence","text":"<p>Coralogix\u2019s \u2018Unified Threat Intelligence\u2019 relies on our Streama technology to provide you with built-in seamless integration with some of the world\u2019s leading threat intelligence feeds. These feeds show hundreds and thousands of threat entities curated by our security experts, allowing you to discover malicious network activities.</p> <p>API integration, special syntax, or format change is not required. You can automatically enrich your log data with malicious indicators in real time, letting you query, visualize, and set alerts on potential threats.</p> <p></p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#kubernetes-collector","title":"Kubernetes Collector","text":"<p>You can now leverage our powerful Kubernetes dashboard with greater ease, thanks to the newly added \u2018Kubernetes Collector\u2019 extension in Coralogix Apps. With the new collector, you can collect the exact data (metrics and K8s events) needed for the installation and install via OpenTelemetry, without the need to manually install each different component.</p> <p>A preset of the OpenTelemetry collector, this collector is designed to enhance your open telemetry data and seamlessly push it to Coralogix and view it in the Kubernetes dashboard. The Kubernetes Collector is the first step towards simplifying the open telemetry configuration.</p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#dashboard-improvements_1","title":"Dashboard Improvements","text":"<p>Here\u2019s what\u2019s new in this month\u2019s edition of \u2018Custom Dashboard\u2019 improvements:\u00a0</p> <ul> <li>Custom dashboards will now support a longer time range, to query data from as past as 90 days. This ability will be helpful to query metrics especially.</li> </ul> <p></p> <ul> <li>Line chart widgets now come with an option to create a new alert. You can just set the alert thresholds and customize the alert conditions, group-by etc. to get started.</li> </ul> <p></p> <ul> <li>Money has been added as a new set to the \u2018Unit\u2019 field, with currencies like cents, euros and US dollars.</li> </ul> <p></p> <ul> <li>Lastly, you can now edit multiple queries even further, with quick options to hide, rename and duplicate the queries.</li> </ul> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#dataprime-log-enrichment","title":"DataPrime Log Enrichment","text":"<p>You can now use Coralogix\u2019s DataPrime query language to enrich and filter your logs using additional context from a lookup table. With this feature, you can enrich old logs already ingested into Coralogix. Moreover, the on-demand log enrichment doesn\u2019t increase the size of the stored logs.</p> <p>This becomes immensely useful when you need to get more context into your existing logs. For example, assume you would like to explore all activity logs of all users in your Finance department, while your logs only include the user names. Enrichment of your existing user activity logs with the user's department allows you to run such a query.</p> <p></p> <p>View Documentation&gt; (skip to \"enrich\" section)</p>"},{"location":"newoutput/whats-new/#july-2023","title":"July 2023","text":""},{"location":"newoutput/whats-new/#multiple-queries-within-a-single-widget","title":"Multiple Queries within a Single Widget","text":"<p>Custom dashboards now support multiple queries, allowing you to compare logs, metrics and spans on the same line chart. Each query gets a dedicated query panel at the bottom and a collapsible side panel on the right, with options to customize each query.</p> <p>The side panel includes options to change the data source, add multiple group-by values, define data aggregation type, switch between linear and logarithmic scales, select time or data units and view only errors if required.</p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#tracing-visualization","title":"Tracing Visualization","text":"<p>While adding group-by values for tracing graphs, you can now choose to display what aggregate value is shown, the options being \u2018max, min and avg\u2019. You can also share a tracing dependency graph as a shareable URL, that can be opened without changing the saved view.</p> <p>A \u201cServices\u201d tab displays the execution time of the services in the selected trace. Clicking on the listed services takes you to the respective service catalog. An option to recenter the traces dependency view is also added as a part of the tracing enhancements.</p> <p></p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#coralogix-apps","title":"Coralogix Apps","text":"<p>The integrations on \u201cCoralogix Apps\u201d have been extended to include CloudTrail, S3 Log Collection, CloudWatch and VPC Flow Logs from AWS. When deploying, you must add all the required details to automatically collect data from your AWS account. These deployments execute a CloudFormation that adds your relevant resources. To remove an integration, you just need to delete the CloudFormation from your AWS console.</p> <p>Apart from the AWS resources, Okta can also be integrated in a much easier way. All you have to do is add your Okta domain and the API key to kickstart the data ingestion. The extensions that can be applied to any integration are mentioned right below the integration details.</p> <p></p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#bar-chart-additions","title":"Bar Chart Additions","text":"<p>Bar charts in custom dashboards now have the option to group charts either by time buckets or multiple group-by values. The time buckets can either be auto-set on pre-defined time intervals or manually changed by specifying the required time interval.</p> <p>You can also customize the bar chart visuals by defining the number of bars, slices per bar, stacking options, selecting time or data units, and switching between linear and logarithmic scales. When stacking, you can also define a custom stack name using free text and labels.</p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#recording-rules","title":"Recording Rules","text":"<p>You can now see/update existing recording rules and create new ones using the new UI for Recording Rules. It allows you to easily define a recording rule and categorize it under rule sets and rule groups.</p> <p>Rule group, with its evaluation interval and series limit, is applied to all the recording rules under it. Rule sets are a way for you to logically sort all of your recording rules.\u00a0</p> <p>You can also see a preview of the recording rules for the last 3 hours (limited to 100 permutations). Lastly, recording rules are also supported in extensions as read-only.</p> <p></p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#dashboard-improvements_2","title":"Dashboard Improvements","text":"<p>Here are some new additions and improvements made to custom dashboards:</p> <ul> <li> <p>Going forward, when a dashboard is saved, the selected timeframe will be saved too. Also, an error message will be displayed when the dashboard fails to save.  </p> </li> <li> <p>Multiple group-by fields can be selected for Spans in pie charts, bar charts and line charts. Also, Spans in the data table are now clickable, directing you to the relevant trace.  </p> </li> <li> <p>You can choose from multiple time, data and currency units to customize the data series graphs. The data units can either be base 2 or base 10.</p> </li> <li> <p>A new tooltip tab is added to dashboard widgets, to show a single time series or all time series, with and without labels.</p> </li> <li> <p>In Pie Charts, you can define which group-by keys should be grouped together in the \u201cother\u201d category by setting a display threshold.</p> </li> <li> <p>A context menu with follow-up actions like \u2018include, exclude and explore\u2019 is added to the legend area of Line Charts.  </p> </li> <li> <p>When multiple y-axes are used for the same panel, hovering a graph of one series now also highlights the relevant y-axis.</p> </li> </ul> <p></p> <p></p> <p></p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#traces-flame-view","title":"Traces Flame View","text":"<p>A new \u2018Flame View\u2019 is added to the existing Traces Dependency view and Gantt view. The flame view is particularly useful for large traces. Like the dependency view, the flame view also supports group-by with aggregation.\u00a0</p> <p>This view has the unique feature of zooming onto a specific span for more details and a reset button to return to the original chart view. The view also comes with a context menu that has follow-up actions to view related logs, pod, host, service map and service catalog.</p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#more-than-usual-alerts","title":"More Than Usual Alerts","text":"<p>You can now set the 'More Than Usual' condition for metric alerts, to detect anomalies in the values of any metric received (including E2M).</p> <p>The new mechanism utilizes an improved machine learning-based prediction algorithm, which creates a forecast for the metric's values for the coming 24 hours, based on the last 7 days of data.\u00a0</p> <p>It creates a prediction for every group-by permutation of the selected metric, selecting the evaluation time window and setting a minimal threshold for triggering.</p> <p></p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#june-2023","title":"June 2023","text":""},{"location":"newoutput/whats-new/#traces-tco-optimizer","title":"Traces TCO Optimizer","text":"<p>Prioritize important traces and reduce up to two-thirds of your tracing costs. Assign high, medium and low priority levels and access a different range of features for each. Allocate data pipelines for your traces based on the importance of that data to your business. Priority levels will simplify assigning TCO pipelines and will capture any applicable future traces on ingestion.</p> <p>\u2018High\u2019 priority, frequent search traces will be fully ingested and all Coralogix capabilities will be available for the same. Monitoring traces will be processed and archived when a \u2018Medium\u2019 priority level is assigned. Compliance traces will be archived and assigned a \u2018Low\u2019 priority.</p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#data-usage-metrics","title":"Data Usage Metrics","text":"<p>Admins can now enable the collection of data usage metrics as native Prometheus metrics, to get granular details on their data consumption. This feature requires the team to have a valid S3 metrics archive bucket configured. With these new metrics, you will be able to create custom dashboards, insights, alerts, and an overview of your data.</p> <p>The Data Usage Metrics feature creates two new metrics \u2013 GB Sent and Counted Units \u2013 which count toward your team\u2019s daily metrics quota. Metric names will appear as: \u2018cx_data_usage_units\u2019 and \u2018cx_data_usage_bytes_total\u2019 on dashboards.</p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#incoming-webhooks","title":"Incoming Webhooks","text":"<p>It\u2019s now easier to connect your applications to Coralogix with our new simplified process that lets you generate webhook URLs. This simplifies the process of generating a webhook URL to connect applications and systems with Coralogix, expanding your selection of connections.</p> <p>Each webhook is associated with an API key and you can define whether the incoming data is plain text or JSON, application &amp; subsystem names, and lastly the timestamp located in the log. This new integration flow will be available on CloudTrail to start with.</p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#dashboard-improvements_3","title":"Dashboard Improvements","text":"<p>Custom Dashboards are now more powerful. Pie charts and bar charts have been added as new additions to widgets, along with pre-existing line charts, gauge and data tables. All 5 widget types will support all 3 data types (logs, metrics and spans) going forward.</p> <p>In addition, for advanced filtering and customization, both pie charts and bar charts come with options like group-by, stack-by, linear and logarithmic scales, advanced visualization filters and more. Data tables have been upgraded too, with a \u2018values over time\u2019 graph which generates a small plot to understand the behavior over the dashboard\u2019s query time.</p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#explore-screen-improvements","title":"Explore Screen Improvements","text":"<p>Here are some of the new additions to the \u2018Explore\u2019 screen:</p> <ul> <li>A \u2018Group-by\u2019 option has been added to the logs archive histogram to cluster the graph by any log field. You can access this option either by the graph menu at the top or by the key context menu. This is enabled only for customers who have moved to the datafusion query engine.</li> </ul> <p></p> <p></p> <ul> <li>The traces dependency graph now supports a few 1,000s of spans instead of only 500. A context menu is added to each span which serves as a shortcut to the window containing details on related logs, events, pod and host. Lastly, a group-by option is added to focus on only relevant span tags at a time, producing a simpler traces graph. The two default group-by options are operationName and serviceName.</li> </ul> <p></p> <p></p> <ul> <li>Lastly, a \u2018Manage Keys\u2019 pop-up is added to columns displaying objects in the Explore screen. This will allow you to pin, sort and exclude log keys of your choice; bring important log information to the top and stay focussed on the current context.</li> </ul> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#service-catalog-filters","title":"Service Catalog Filters","text":"<p>The service catalog now includes a filter panel, similar to the logs Explore screen. This allows you to filter services based on specific characteristics in the span tags. The selected filters are also passed on to any service you dive into.</p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#permutations-graph","title":"Permutations Graph","text":"<p>Events2Metrics now includes a graph that displays the amount of permutations actually used and permutations left from the logs archive and the high tier for spans, for a 7-day period. You can choose up to 10 labels for the visualization and once the changes are made, hit refresh.</p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#metric-recording-rules","title":"Metric Recording Rules","text":"<p>With the latest UI improvements in Recording Rules, it is now much easier to create new recorded metrics, from the existing ones in Coralogix. The new UI allows you to create new metrics either by importing a YAML file or creating a rule group.</p> <p>This will simplify complex and resource-intensive PromQL queries into leaner and more quickly queried metrics, which can be used in various dashboard visualizations and high-performance analytics.</p> <p></p> <p>View Documentation&gt;</p>"},{"location":"newoutput/whats-new/#may-2023","title":"May 2023","text":""},{"location":"newoutput/whats-new/#service-catalog","title":"Service Catalog","text":"<p>Service Catalog is a new feature added under Coralogix\u2019s APM offering. It includes a list of all the services in your system and you can view details such as service type, number of requests sent by the service, the error rate, and the P95 latency by specifying a time range.</p> <p>You can drill down on each service to look at a service map in the left pane and additional details in the right pane. The service map shows all the services connected to a root service and the right pane provides tabs for a detailed overview, actions taken by the services, health status of resources and logs of each service.</p> <p></p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#rolling-window","title":"Rolling Window","text":"<p>Under standard user-defined alerts, if the alert conditions are set for \u2018More than\u2019, you can now choose a new type of evaluation window to define the queried time window. You can choose between the newly added Rolling Window and the existing Dynamic Creation.\u00a0</p> <p>Rolling Window is a fixed timeframe and doesn\u2019t change with alert triggers. Dynamic Duration changes the queried time period when an alert is triggered. Rolling Window is now the default choice of evaluation window and is recommended to be used when using \u2018Group-by\u2019 with an alert.</p> <p></p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#dashboard-improvements_4","title":"Dashboard Improvements","text":"<p>Pie Chart is added as a new visualization widget under custom dashboards. It comes with abilities to group-by, aggregate and stack-by data sources and other advanced controls to customize the visuals. Pie Chart is the 4th visualization widget added to custom dashboards.</p> <p></p> <p>In addition, \u2018Metrics\u2019 is added as a new filter type to make dashboards customizable by the source of data. A \u2018Save As\u2019 button is added to clone existing dashboards easily, and a \u2018Show Only Errors\u2019 button is added to highlight only errors in span-based widgets.</p> <p></p> <p></p> <p>Go to Platform &gt;</p>"},{"location":"newoutput/whats-new/#extensions-improvements_1","title":"Extensions Improvements","text":"<p>With the new feature and design improvements, you can now do more with Extensions on Coralogix. Prometheus recording rules can now be deployed as part of an extension. Security and AWS enrichments can be deployed in addition to Geo and Custom Enrichments. Also, selective deployment of enrichments is now possible, allowing you to select only specific enrichment rules when deploying an extension.</p> <p>A notification will be displayed if deploying the extension will take you over the maximum enrichment quota. Also, hover backgrounds are now shown on dashboard screenshots. And lastly, design and UI improvements have been made to Extensions.</p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#flow-alert-group-by","title":"Flow Alert Group-by","text":"<p>The flow of alerts can now be grouped by the common group-by keys used in the defined alerts. This allows you to examine all alert stages in the context of a single field value. The group-by options available for each alert can be viewed in the Flow Builder by hovering over the alerts.\u00a0</p> <p>The group-by fields that can be selected are automatically pulled from the list of fields (from Log-based alerts) and tags (from Span-based alerts), and only ones that show up throughout all alerts will be pulled out automatically for selection.</p> <p></p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#otel-metrics","title":"OTEL Metrics","text":"<p>The K8S dashboard in Coralogix now supports OTEL metrics if you are using an OpenTelemetry collector with a Kubernetes orchestration together with Prometheus to send your data to Coralogix. You can use this feature along with our other application performance monitoring (APM) features, to get a full picture of your system performance.</p> <p>The K8S dashboard gives you a comprehensive view of your system's clusters, nodes and pods. In addition, the dashboard also shows the health status of your resources and displays all Kubernetes events. Collect Kubernetes events using OpenTelemetry.</p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#march-2023","title":"March 2023","text":""},{"location":"newoutput/whats-new/#spans-in-custom-dashboards","title":"Spans in Custom Dashboards","text":"<p>Custom Dashboards are now more flexible. Use spans as a new data source, along with logs and metrics. The data source is available across all existing visualizations including Line Chart, Data Table and Gauge visualizations.</p> <p>Spans can also be added as a data source for filters on any Custom Dashboard, allowing you to apply filters on parameters such as Span Service. Filters in Custom Dashboards also support Span Tags, which can be added as fields in the filter. Spans filters will only affect spans widgets on a dashboard, and will not affect any widgets based on logs or metrics.</p> <p></p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#notification-groups","title":"Notification Groups","text":"<p>Alert notifications can now be grouped into multiple \u2018Notification Groups\u2019 for different keys. You can add multiple webhooks to split the alert triggers and customize the notification frequency for each webhook.\u00a0</p> <p>You can further activate the \u2018Notify when Resolved\u2019 trigger to stay on top of alert notifications.</p> <p>In addition, an individual notification is sent to each value of the Group By key when the unique query conditions are met within the specified timeframe.\u00a0</p> <p>For example, if you have used \u201cregion\u201d as a Group By key and let\u2019s say, it has 2 values i.e. Region A and Region B; individual notifications will be sent to both A and B respectively.</p> <p></p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#content-column","title":"Content Column","text":"<p>A new Content Column is added to the \u2018Explore\u2019 screen grid, to improve visibility for content fields and distinguish them from the labels fields. Most logs will have content fields and it is now easy to extract these fields from logs, according to a predefined order list (message, log, k8s.log\u00a0</p> <p>etc).</p> <p>You can edit the content list and truncate long keys and values in the label fields and show them flattened instead of JSON. With this new feature, you can define important/favourite labels that will be sorted first, have a default favourites list and allow an extension to update it.</p> <p></p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#multiple-api-keys","title":"Multiple API Keys","text":"<p>Admins can now configure multiple private API keys to send data to their Coralogix teams. You can further customize these API keys and activate/deactivate them as required. After creating the new key, it is visible in the Send Your Data section.</p> <p>Customizations are made available to the Send-Your-Data API key management to create, modify, activate, deactivate, reactivate and delete the private API keys created.</p> <p>To enable this feature please contact your TAM or Coralogix\u2019s support.</p> <p></p> <p>Go To Platform &gt;</p>"},{"location":"newoutput/whats-new/#extensions","title":"Extensions","text":"<p>Managing your extensions is now easier as you will be notified about the updates available for your extensions through a notification bar found at the top of the Extensions screen. Further, an \u2018Update\u2019 button is added, that can be used to update an already deployed extension.\u00a0</p> <p>For some older extensions, you may have to remove and redeploy them by selecting the required applications and sub-systems. This way you can switch between versions and stay up to date with the extensions you are actively using.</p> <p></p> <p>Go To Platform &gt;</p>"},{"location":"newoutput/whats-new/#data-usage-report","title":"Data Usage Report","text":"<p>Under the Data Usage section, you can now download a detailed data usage report. The report\u00a0</p> <p>is exported as a CSV file and the generation of these reports is available by API.</p> <p>The detailed usage report will contain multiple fields including the application, sub-systems, severity, TCO tier, type of data source, amount of GBs sent and amount of units sent.</p> <p></p> <p>Go To Platform &gt;</p>"},{"location":"newoutput/whats-new/#metric-usage-trend","title":"Metric Usage Trend","text":"<p>Along with the Metrics Cardinality feature, the \u2018Metrics Usage\u2019 screen can offer you a visual representation of your metrics-related data. Expanding on this capability, we have now added a new visualization i.e. \u2018Metrics Usage Trend\u2019. This allows you to visualise 7 days of history for each metric value within the Metrics Usage screen.</p> <p>The time history can help you identify the metrics that are using up your quota very frequently. Identifying such metrics can help you in cost optimization and get better insights into noisy or expensive applications.</p> <p></p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#february-2023","title":"February 2023","text":""},{"location":"newoutput/whats-new/#new-custom-dashboards","title":"New! Custom Dashboards","text":"<p>Create unlimited, personalized custom dashboards. Use three new visualizations \u2013 Data Table, Line Chart, and Gauge \u2013 to define and create a dashboard catered to your specific observability needs. Then query across your widgets using our new Filter and Variable capabilities.</p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#logs2metrics-is-now-events2metrics","title":"Logs2Metrics is now Events2Metrics!","text":"<p>You can now generate metrics from spans in addition to log data! Get started in the platform under Data Flow &gt; Events2Metrics.</p> <p>Go to Platform &gt;</p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#new-service-map","title":"New! Service Map","text":"<p>In the Dashboard menu dropdown, find the new Service Map screen which shows a system overview on all services based on the traces being sent.</p> <p>Service Map works out-of-the-box if you\u2019ve already got tracing data in Coralogix!</p> <p>Go to Platform &gt;</p>"},{"location":"newoutput/whats-new/#dataprime-available-on-frequent-search","title":"DataPrime Available on Frequent Search","text":"<p>DataPrime is now available across fully indexed, frequent search logs AND data that sits in your own archive.</p> <p>Use DataPrime to explore your data, perform schema on read transformations, group and aggregate fields, extract data, and much more!</p> <p>Go to Platform &gt;</p>"},{"location":"newoutput/whats-new/#set-multiple-retention-periods-with-archive-tags","title":"Set Multiple Retention Periods with Archive Tags","text":"<p>Control the length of archive retention for different groups of logs. Define different lifecycle policies in the Setup Archive page and configure permissions in your bucket.</p> <p>In the TCO Optimizer, you can configure which logs are tagged with each retention. Only logs that are received AFTER the rule has been created will be tagged.</p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#content-column_1","title":"Content Column","text":"<p>New column in \u2018Explore\u2019 screen grid automatically extracts the key with the meaningful textual content in the log (i.e. \u2019message\u2019 / \u2018msg\u2019 / \u2018info\u2019\u2026)</p> <p>Go to Platform &gt;</p>"},{"location":"newoutput/whats-new/#january-2023","title":"January 2023","text":""},{"location":"newoutput/whats-new/#uiux-improvements","title":"UI/UX Improvements","text":"<p>We\u2019ve added some fine touches to the logs view to help you find what you\u2019re looking for faster.</p> <ul> <li> <p>Default \u2018flat\u2019 mode makes the display a lot cleaner for the 1-line, 2-line, and condensed presentation modes.</p> </li> <li> <p>New \u2018List\u2019 presentation mode presents log data in an easy-to-read list of key-value pairs.</p> </li> <li> <p>Object columns can now be filtered to display a subset of the fields contained within the object.</p> </li> </ul> <p>View in Platform &gt;</p>"},{"location":"newoutput/whats-new/#lucene-syntax-highlighting","title":"Lucene Syntax Highlighting","text":"<p>Lucene syntax highlighting makes it easier to read and write queries.</p> <p></p> <p>Highlighting is available everywhere in the platform where you can write Lucene queries. In the Explore Screen, terms and fields that match your query are also highlighted within the resulting logs.</p> <p>View in Platform &gt;</p>"},{"location":"newoutput/whats-new/#incident-management-in-insights-screen","title":"Incident Management in Insights Screen","text":"<p>You can now acknowledge, assign, and resolve alerts directly from the Insights UI. Use Coralogix as a task allocation and process control UI to extend and consolidate your monitoring and alerting workflow.</p> <p>View in Platform &gt;</p>"},{"location":"newoutput/whats-new/#alerts-map","title":"Alerts Map","text":"<p>Visualize logical groupings of your alerts and their statuses for a powerful \u201cat a glance\u201d view of what\u2019s going on in your systems.</p> <p>For example, if you group your alerts by cluster and node, the Alerts Map will highlight which cluster-node pairings have one or more triggered alerts.</p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#tracing-updates","title":"Tracing Updates","text":"<p>Query your tracing data with Lucene syntax in the tracing UI. You can now build complex searches for specific fields and phrases contained within traces and spans.</p> <p></p> <p>The dependency view is now the default tracing view with the metadata about the trace now shown on the right-hand side of the screen. Hovering over a given node in the graph will reveal a tooltip with information about the trace and associated span.</p> <p>View in Platform &gt;</p>"},{"location":"newoutput/whats-new/#create-new-teams-in-org-admin-screen","title":"Create New Teams in Org Admin Screen","text":"<p>Organization admins can now create new teams from the \u201cMy Teams\u201d tab in the Settings page. To create a new team, fill in the 3-step wizard with your new team name, unit allocation, and team admins.</p> <p>If you need help getting started, reach out to our support team using the in-app chat!</p>"},{"location":"newoutput/whats-new/#extended-integration-support","title":"Extended Integration Support","text":"<p>Prometheus Recording Rules Prometheus Recording Rules can now be pulled from Coralogix using cURL commands. Use recording rules to pre-compute new timeseries based on existing ones. Once defined, the corresponding recording rules are automatically created as additional metrics in Coralogix. View Documentation &gt;</p> <p>AWS Lambda Metrics Our Lambda extension now supports the collection of AWS Lambda metrics for essential invocation measurements. Additionally, CX_Metadata is included alongside incoming documents on the Coralogix platform to provide context around cloud provider, account ID, cloud region and more. View Documentation &gt;</p> <p>APM for Amazon EC2 Using Amazon Kinesis Data Firehose, you can now send Amazon EC2 metrics to Coralogix and view them on your Coralogix dashboard correlated with relevant traces and spans. View Documentation &gt;</p> <p>JumpCloud SCIM Identity Management New SAML integration with JumpCloud allows you to manage groups and users from in JumpCloud. Once integrated, those changes will be automatically reflected in the Coralogix UI. View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#november-2022","title":"November 2022","text":"<p>Application Performance Monitoring (APM)</p> <p>Coralogix now offers key features of\u00a0Application Performance Monitoring\u00a0(APM) for modern, cloud-native environments!</p> <p></p> <p>With expanded visibility into service performance, you can more effectively monitor latency and pinpoint components responsible for issues like performance degradation or an increase in errors.</p> <p>Read more about our unique approach to APM and how we can help you turn 30-minute investigations into 30-second discoveries.</p> <p>Get Started &gt;</p> <p>OpenTelemetry Support for Logs, Metrics &amp; Traces</p> <p>You can now use OpenTelemetry to send logs, metrics, and tracing data to Coralogix!</p> <p>Leverage the popular vendor-neutral, open-source framework for instrumentation and collection of your telemetry data for analysis in the Coralogix platform.</p> <p>Get Started &gt;</p> <p>AWS Resource Enrichment</p> <p>New AWS Resource Enrichment allows you to enrich your logs with tags from Amazon Web Services (AWS) EC2 instances. Use this feature to connect your business and operation metadata from AWS and gain greater insight into your data.</p> <p></p> <p>To get started, visit our documentation for setup and configuration.</p> <p>View Documentation &gt;</p> <p>Lucene Query Auto-Complete</p> <p>Auto-complete for Lucene queries is now available across the platform. Suggestions are provided for operators (including field range hints) and data type hints!</p> <p>Try It Now &gt;</p> <p>Extensions Improvements</p> <p>The Coralogix Extensions lobby has been improved with search, label, and status filters.</p> <p>In addition, individual extensions can now be deployed to specific applications and subsystems as part of their configuration.</p> <p>Note that selecting \u201call applications\u201d or \u201call subsystems\u201d will apply the extension to all existing and future applications and subsystems created in the Coralogix platform.</p> <p>View Documentation &gt;</p> <p>Integrations</p>"},{"location":"newoutput/whats-new/#aws-lambda-telemetry-exporter","title":"AWS Lambda Telemetry Exporter","text":"<p>Our new\u00a0Coralogix AWS Lambda Telemetry Exporter is now available in the\u00a0AWS Serverless Application Repository\u00a0as an open beta. We encourage you to try it out and welcome any feedback.</p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#telegraf-operator","title":"Telegraf Operator","text":"<p>Use our new integration with Telegraf Operator to simplify metric collection in Kubernetes. With Telegraf Operator, you only need to define the input plugin configuration for Telegraf when creating the pod annotations. Telegraf Operator then sets the configuration for the entire cluster, avoiding the need to configure a metrics destination when deploying applications.</p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#google-cloud-pubsub","title":"Google Cloud Pub/Sub","text":"<p>We\u2019ve updated our integration with Google Cloud Pub/Sub to use a push subscription to send logs to Coralogix.</p> <p>We recommend to use the updated integration with push subscription as it avoids running any additional software (i.e. functions) in your GCP account which can contribute to operational overhead and costs.</p> <p>View Documentation &gt;</p>"},{"location":"newoutput/whats-new/#october-2022","title":"October 2022","text":"<p>Dashboard Updates</p> <p>Logs Graph Grouped by Severity</p> <p>Previously, each column would display a single combined amount for Warning/Error/Critical logs. Now, each severity is stacked and a tooltip displays the breakdown of the severity count.</p> <p>Clicking a section will navigate to the Explore tab with the relevant severity and timeframe filters applied.</p> <p></p> <p>View Dashboard &gt;</p> <p>Help Tooltips on Dashboard Widgets</p> <p>New tooltips have been added to the dashboard widgets with an explanation of the information being displayed.</p> <p>Top 3 Abnormal Errors - Above Normal / Newly Introduced</p> <p>The Top 3 Abnormal Errors widget now differentiates between \u2018Above Normal\u2019 and \u2018Newly Introduced\u2019 errors. \u2018Above Normal\u2019 shows the amount above normal that is being seen (e.g. 3.2X normal levels), and \u2018Newly Introduced\u2019 shows the number of occurrences of the new errors.</p> <p>View Dashboard &gt;</p> <p>Explore Updates</p> <p>TCO Priority Added as Metadata</p> <p>A Metadata field is added to each field indicating TCO Level. This can be seen in tooltips within the dashboard.</p> <p>Explore Now &gt;</p> <p>Tracking Logical Data Types</p> <p>Logical data types (i.e. IP Addresses, URLs, Emails) are now being detected in addition to raw data types (i.e. String, Number, etc).</p> <p>This data is also displayed in the column tooltip. If more than one data type has been detected in a field (e.g. IP and String), they will all be displayed in the tooltip.</p> <p></p> <p>Content Type Icons added to Column Headers</p> <p>New icons indicating content type have been added to column headers.</p> <p>Explore Now &gt;</p> <p>Archive Query Improvements</p> <p>DataFusion Query Engine Enhancements</p> <p>The most recent round of improvements to the DataFusion query engine are supporting a conservative 5X increase in query speeds over AWS Athena.</p> <p>With DataFusion there is no longer a hard limit to the number of partitions that can be scanned, it is restricted only by the resources assigned to it.</p> <p>Try It Now &gt;</p> <p>22 New Scalar Functions in DataPrime</p> <p>22 new Scalar functions have been added to DataPrime, including:</p> <ul> <li> <p>New String functions</p> </li> <li> <p>IP functions</p> </li> <li> <p>UUID functions</p> </li> <li> <p>URL functions</p> </li> <li> <p>Number functions</p> </li> </ul> <p>Lucene Query Improvements</p> <p>Autocomplete for Lucene queries has been added within the DataPrime archive query UI, and will soon be added to the standard log query fields across the platform.</p> <p>Try It Now &gt;</p>"},{"location":"newoutput/whats-new/#september-2022","title":"September 2022","text":"<p>Tracing Alert</p> <p>Create Tracing alerts for high latency on specified Tags and Services. Tracing alerts can be grouped by different Tags and specified for a specific threshold of Latency and Spans.</p> <p>Learn More &gt;</p> <p>'Group By' for 'Less Than' Alerts</p> <p>Enhance your \u2018Less Than\u2018 alerts using the \u2018Group By\u2019 option. Values under the \u2018Group By\u2019 fields are aggregated into a histogram, and an alert will trigger whenever the condition threshold is met for an aggregated value within the specified timeframe.</p> <p>Learn More &gt;</p> <p>Enhancements to Logs Screen UI</p>"},{"location":"newoutput/whats-new/#mapping-exception-indicator","title":"Mapping Exception indicator","text":"<p>A new warning icon in the Logs Screen indicates if there are any mapping exceptions in the selected time frame.</p>"},{"location":"newoutput/whats-new/#new-row-formats-in-logs-screen","title":"New Row Formats in Logs Screen","text":"<p>To streamline analysis and reduce the screen space each log takes, we added new Row Formats so you can choose how your logs are displayed.</p> <p>Choose from the following options:</p> <ul> <li> <p>1-Line \u2013 logs are condensed into one line</p> </li> <li> <p>2-Line \u2013 logs are condensed into two lines</p> </li> <li> <p>Condensed (default) \u2013 the whole log is visible but without breaking into lines</p> </li> <li> <p>JSON \u2013 the default view where JSON objects are parsed</p> </li> </ul> <p>Learn More &gt;</p> <p>Organization and Admin Console</p> <p>Some companies prefer separate teams to isolate data based on the environment it originates from like Dev, QA, and Production. While others prefer to isolate the data based on organizational units like Infrastructure, Security, and Application.</p> <p>Coralogix supports multi-tenancy, allowing a single organization to contain multiple teams. The Org Admin can then manage quota, settings, users, and more at the organization level from a single interface.</p> <p>Organizations can be created upon request. Please contact us through our in-app chat or via email at support@coralogixstg.wpengine.com.</p> <p>New Integrations</p> <ul> <li>Zabbix</li> </ul>"},{"location":"newoutput/whats-new/#august-2022","title":"August 2022","text":"<p>New &amp; Improved Dashboard!</p> <p>Our dashboard just got a refresh!\u00a0</p> <p></p> <p>Get an overview of your system health \u2013 informed by all of your observability data \u2013 with more in-depth widgets showing a summary of your anomalies, alerts, and more.</p> <p>Plus, use the new sidebar filters to seamlessly drill down and investigate further with a single click to the Explore UI.</p> <p>Open Dashboard &gt;</p> <p>DataPrime Archive Query Syntax</p> <p>DataPrime is now officially GA!</p> <p>Use DataPrime to parse and query unstructured data fields on the fly. Plus, generate synthetic fields and run calculations from your archived data. From the query UI, view your query history and access a complete cheat sheet with documentation of the query format and operators.</p> <p>Learn More &gt;</p> <p>Explore UI Improvements</p> <p>Log Screen</p> <p>Improvements to the Top Graph in the Logs screen enable you to view and investigate your data more efficiently:</p> <ul> <li> <p>Sort tooltips by ascending or descending order</p> </li> <li> <p>Compact large numbers for easier visualization</p> </li> <li> <p>Legend container is now resizable and customizable\u00a0</p> </li> </ul> <p>The state of columns in the Log Screen are persistent in the URL and custom views can be easily shared with additional team members.</p> <p>Avg, Max, Min, and Sum aggregations can now be used to visualize log fields that contain numeric values!</p> <p>Tracing Screen</p> <p>Hovering over graphs in tracing will now show a crosshair for faster analysis.</p> <p>DataMap Filtering &amp; Actions</p> <p>You can now filter hexagons in the DataMap using multiple Field Operators and Regex.</p> <p>Create and access Actions from DataMap using metric labels ($l.) to seamlessly connect DataMaps to external resources using metric variables and labels. <p>Learn More &gt;</p> <p>General UI Improvements</p> <p>Insights UI</p> <p>New 'Go To Explore' functionality allows you to jump from the Insights UI directly to the relevant query in the Explore tab.</p> <p>Alerts Menu</p> <p>You can now Clone and Delete alerts directly from the main alerts menu!</p> <p>Webhooks</p> <p>Added fields that can be shipped in the body of the payload.</p> <p>Session Timeout &amp; Force Logout</p> <p>Admins can now configure a session length on the Setting page after which users will be forced to log back in with no dependency on activity.</p> <p>New Integrations</p> <ul> <li> <p>StatsD Metrics</p> <ul> <li>Send StatsD metrics directly to Coralogix using our plugin with the StatsD daemon.</li> </ul> </li> <li> <p>Cloudflare LogPush</p> <ul> <li>Deploy LogPush with Terraform, and push logs of any log type directly from Cloudflare.</li> </ul> </li> <li> <p>Open Commerce API</p> </li> <li> <p>Amazon EKS (Fargate) Logs</p> </li> <li> <p>Telegraf</p> </li> <li> <p>Amazon EventBridge</p> </li> <li> <p>Nagios</p> </li> </ul>"},{"location":"newoutput/whats-new/#july-2022","title":"July 2022","text":"<p>Flow Alert</p> <p>Define a sequence of alerts in our Drag &amp; Drop Flow Builder UI that\u00a0combines Logs, Metrics, Tracing, and Security information to create a single alert flow\u00a0that will trigger based on multiple conditions within defined timeframes.</p> <p>Use Flow Alerts to generate actionable insights from your observability data:</p> <ul> <li> <p>Combine multiple alerts into a single, comprehensive flow\u00a0to cover your security, infra, and business events with reduced noise and false positives.</p> </li> <li> <p>Create alerts with the Root Cause built-in\u00a0(e.g. Error elevation due to CPU, causing SLO breach) to track the entire chain of events leading to an error.</p> </li> <li> <p>Identify potential security incidents and proactively remediate them\u00a0with alert occurrences over time.</p> </li> </ul> <p>Learn More &gt;</p> <p>New Integrations</p> <ul> <li> <p>Amazon Kinesis Data Firehose - Metrics</p> </li> <li> <p>Custom Metrics</p> </li> <li> <p>Terraform Module for Cloudflare Logpush</p> </li> </ul>"},{"location":"newoutput/whats-new/#june-2022","title":"June 2022","text":"<p>New Parsing Rules</p> <p>This month we are introducing 2 new parsing rules to bring more value to customers who have many fields and nested fields in their log data.</p> <p>The new Stringify JSON Field and Parse JSON Field rules enable you to parse escaped JSON values within a field to a valid JSON object and vice versa \u2013 stringify a JSON object to an escaped string.</p> <p>Learn More &gt;</p> <p>DataMap Updates</p> <p>The DataMap allows you to build custom mappings of your infrastructure using metric data for monitoring system health and quickly identifying issues.</p> <p>In the Group Editor, you\u2019ll find new options to:</p> <ul> <li> <p>Sort the display by attributes (e.g. sort by severity for defined thresholds)</p> </li> <li> <p>Scale threshold values to make metric graphs more readable</p> </li> <li> <p>Limit the number of hexagons shown per group</p> </li> </ul> <p>In the DataMap display, use new 'Compare to others' functionality to compare an element with 10 others in the same group. Plus, expand and collapse specific groups to minimize the number of displayed elements.</p> <p>Learn More &gt;</p> <p>Tracing Updates</p> <p>New dynamic graphs and saved views in the Tracing UI enable it to serve as SLA dashboards for any application or service.</p> <p>In addition to the original default graph for Max duration by Action, there are now two additional default graphs for Count by Service and Error Count by Service.</p> <p>All three graphs can be customized, and aggregation operators have been added for 99, 95, and 50th percentiles to help deepen your ability to monitor business SLOs.</p> <p>When investigating traces in the explore section, you can now save your current view and load saved views just like you do in the Logs UI.</p> <p>Learn More &gt;</p> <p>*Note that the aggregation operators, as well as the Duration filter in the sidebar, are run over the Spans.</p> <p>Archive Query Updates</p> <p>Improvements to the archive query now allow timeframes up to 3 days for added accessibility to data in your remote bucket.</p> <p>Additional updates to the Archive Query in Explore Screen include:</p> <ul> <li> <p>New Execute Archive Query function allows you to review active filters before clicking 'Run Query'. To prevent unexpected wait times, queries will no longer run automatically when switching from Logs to Archive. </p> </li> <li> <p>Non-optimal archive queries (e.g. \u201chello\u201d) will trigger a warning pop up recommending to improve the query conditions.</p> </li> </ul> <p>Learn More &gt;</p> <p>New Integrations</p> <ul> <li> <p>Amazon Kinesis Data Firehose - Logs</p> </li> <li> <p>Terraform Modules for:</p> <ul> <li> <p>Azure EventHub</p> </li> <li> <p>AWS Cloudtrail</p> </li> <li> <p>AWS Cloudwatch</p> </li> <li> <p>AWS S3 Logs Collection</p> </li> </ul> </li> <li> <p>Google Cloud Pub/Sub</p> </li> <li> <p>GitHub Version Tags</p> </li> <li> <p>Coralogix RabbitMQ Agent</p> </li> <li> <p>Salesforce Cloud Commerce</p> </li> </ul>"},{"location":"newoutput/whats-new/#may-2022","title":"May 2022","text":"<p>DataMap</p> <p>Build custom mappings of your infrastructure, log-based, and business metrics to visualize and monitor your system health.</p> <ul> <li> <p>Choose your base metric (ex. load_time)</p> </li> <li> <p>Create a hierarchy of metric labels (ex. region&gt;instance&gt;pod)</p> </li> <li> <p>Set thresholds and preview visualization</p> </li> <li> <p>Save and load views for each unique use case</p> </li> </ul> <p>Pro-tip! Use tooltips to view additional information about the area of your system that you are looking at in the mapping visualization.</p> <p>Learn More &gt;</p> <p>Tracing UI</p> <p>Use our powerful Tracing UI to explore your data and streamline investigations and troubleshooting.</p> <ul> <li> <p>Collect tracing data using the Coralogix Exporter in OpenTelemetry</p> </li> <li> <p>Pinpoint issues with familiar filtering and aggregation capabilities</p> </li> <li> <p>Drill down into spans and visualize data flows with dependency view</p> </li> <li> <p>View related logs and jump directly to the Logs Tab for streamlined investigation</p> </li> <li> <p>Define user permissions with RBAC Control for tracing data</p> </li> <li> <p>Alerting for Tracing (coming soon!)</p> </li> </ul> <p>Learn more &gt;</p> <p>CX-DATA Archive Format</p> <p>We\u2019ve launched a new archive format based on Parquet that improves archive query performance by 5X!</p> <p>In addition to the CSV format, supported today, this new CX-DATA format can be configured in your S3 Buckets Settings and selected in the Archive Query screen in the platform.</p> <p>Learn More &gt;</p> <p>Snooze Alerts in Alerts Tab</p> <p>The option to snooze an alert, which was previously only available from the Insights screen, will now be available from within the main Alerts screen. This allows for centralized management of alert statuses across your team.</p> <p>Learn More &gt;</p> <p>Session Timeout Management</p> <p>A new option on the Settings page gives users the ability to choose how much idle time will trigger a force logout from the system. In case of inactivity, a pop-up will appear to alert the user, prior to logout.</p> <p>Learn More &gt;</p> <p>New Integrations</p> <ul> <li> <p>Terraform Modules for:</p> <ul> <li> <p>AWS CloudWatch</p> </li> <li> <p>AWS S3 Logs Collection</p> </li> </ul> </li> <li> <p>Google Cloud Pub/Sub</p> </li> </ul>"},{"location":"newoutput/windows-event-logs-opentelemetry/","title":"Windows Event Logs & OpenTelemetry","text":"<p>Utilizing OpenTelemetry in conjunction with the Windows Event Log receiver is an excellent method for collecting Windows Event Logs. To implement this solution, it is essential to deploy an Opentelemetry Collector directly onto the Windows Server and configure it as a service to enable seamless integration with the Windows Event Log receiver.</p>"},{"location":"newoutput/windows-event-logs-opentelemetry/#supported-os","title":"Supported OS","text":"<p>Windows</p>"},{"location":"newoutput/windows-event-logs-opentelemetry/#overview","title":"Overview","text":"<p>Leveraging the Windows Event Log receiver facilitates monitoring specific Event Log sources known as Channels. These channel receivers are specified and configured in the Opentelemetry Collector configuration file. Typical channels to be monitored include:</p> <ul> <li> <p>Application</p> </li> <li> <p>Security</p> </li> <li> <p>System</p> </li> </ul>"},{"location":"newoutput/windows-event-logs-opentelemetry/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Microsoft User account with permissions to access EventLog</p> </li> <li> <p>Microsoft User account with permissions to create Services</p> </li> </ul>"},{"location":"newoutput/windows-event-logs-opentelemetry/#setup","title":"Setup","text":""},{"location":"newoutput/windows-event-logs-opentelemetry/#installation","title":"Installation","text":"<p>OpenTelemetry is released as a binary by the OpenTelemetry group here.</p> <p>STEP 1. Download the latest <code>otelcol-contrib</code> binary, as the standard otelcol binary does not include the WindowsEventLog receiver or our Coralogix exporter. After downloading, extract the binary to C:\\cx-otel for documentation purposes and establish it as a service using the sc framework.</p> <p>STEP 2. Create the service definition using the following command:</p> <pre><code>sc.exe create cx-otelcol displayname=cx-otelcol start=delayed-auto binPath=\"C:\\\\cx-otel\\\\otelcol-contrib.exe --config C:\\\\cx-otel\\\\config.yaml\"\n\n</code></pre> <p>Note: After creating the configuration file, you must restart the parameters and start the service.</p>"},{"location":"newoutput/windows-event-logs-opentelemetry/#configuration","title":"Configuration","text":"<p>Configuring the Opentelemetry Collector is done through the use of a standard config.yaml file. The example file below demonstrates the configuration of two separate Channels, with the appropriate pipeline configurations. This file must be saved as config.yaml in the same folder as the OpenTelemetry Collector binary.</p> <pre><code>receivers:\n    windowseventlog/application:\n        channel: application\n    windowseventlog/system:\n        channel: system\n\nprocessors:\n  resourcedetection:\n    detectors: [system]\n    system:\n      hostname_sources: [\"os\"]\n  batch:\n    send_batch_size: 1024\n    send_batch_max_size: 2048\n    timeout: \"1s\"\n\nexporters:\n  coralogix:\n    domain: \"coralogixstg.wpengine.com\"\n    private_key: \"cxtp_&lt;redacted&gt;\"\n    application_name: \"Windows\"\n    subsystem_name: \"EventLogs\"\n    timeout: 30s\n\nservice:\n  pipelines:\n    logs:\n      receivers: [ windowseventlog/application, windowseventlog/system ]\n      processors: [ resourcedetection, batch ]\n      exporters: [ coralogix ]\n\n</code></pre> <p>Notes:</p> <ul> <li> <p>Replace <code>domain</code> with your Coralogix domain, and <code>private_key</code> with your Coralogix Send-Your-Data API key. The <code>application_name</code> and <code>subsystem_name</code> may be configured as desired.</p> </li> <li> <p>For <code>batch</code>, Coralogix recommends the default otel-integration chart settings for\u00a0batch processors\u00a0in all collectors. Find out more here.</p> </li> </ul>"},{"location":"newoutput/windows-event-logs-opentelemetry/#finalization","title":"Finalization","text":"<p>Once you\u2019ve created your config.yaml file, configure the restart settings and start the service. To do so, use the following commands:</p> <pre><code>sc.exe failure cx-otelcol reset= 86400 actions= restart/5000/restart/5000/restart/5000\nsc.exe start cx-otelcol\n\n</code></pre>"},{"location":"newoutput/windows-event-logs-opentelemetry/#validation","title":"Validation","text":"<p>Once you\u2019ve configured the <code>cx-otelcol</code> service, you should start seeing messages for your configured <code>application_name</code> and <code>subsystem_name</code> arriving in the Coralogix UI. If your Windows server is very inactive, you can restart the cx-otelcol service from the service console to trigger a System channel event. This restart event should be captured in your Coralogix UI.</p>"},{"location":"newoutput/windows-event-logs-opentelemetry/#troubleshooting","title":"Troubleshooting","text":"<p>If you don\u2019t see messages arriving in your Coralogix UI, you can troubleshoot this integration by adding a \u201cdebug\u201d exporter and viewing the output in the console after executing the collector manually.</p> <p>STEP 1. Stop your defined service and make a copy of your config.yaml file.</p> <p>STEP 2. In this new copy, edit the config to add a \u201cdebug:\u201d exporter and add \u201cdebug\u201d to the logs pipeline exporters array, as follows:</p> <pre><code>exporters:\n  debug:\n    verbosity: detailed\n  coralogix: ..\n\nservice:\n  pipelines:\n    logs:\n      receivers: ..\n      processors: ..\n      exporters: [ coralogix, debug ]\n\n</code></pre> <p>STEP 3. Execute the Opentelemetry Collector from the command line like so:</p> <pre><code>C:\\\\cx-otel\\\\otelcol-contrib.exe --config C:\\\\cx-otel\\\\config_debug.yaml\n\n</code></pre> <p>You should then see the events in your console and confirm they are processing correctly.</p> <p>STEP 4. Once you\u2019ve verified that they are being processed correctly, begin troubleshooting the network, starting with your local firewall and then your egress path to the internet.</p> <p>If you\u2019re using AWS PrivateLink, validate that the PrivateLink FQDN resolves from the host.</p>"},{"location":"newoutput/windows-event-logs-opentelemetry/#additional-resources","title":"Additional Resources","text":"GitHub DocumentationWindowsEventLogReceiver"},{"location":"newoutput/windows-event-logs-opentelemetry/#support","title":"Support","text":"<p>Need help?</p> <p>Contact us\u00a0via our in-app chat\u00a0or by emailing\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/zabbix/","title":"Zabbix","text":"<p>Zabbix is\u00a0an open-source monitoring software tool for diverse IT components, including networks, servers, virtual machines (VMs), and cloud services. Zabbix provides monitoring metrics, such as network utilization, CPU load, and disk space consumption.</p> <p>This tutorial demonstrates how to send your metrics to Coralogix via Zabbix.</p>"},{"location":"newoutput/zabbix/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Active Coralogix\u00a0account\u00a0with\u00a0metric bucket and a working Grafana dashboard for metrics</p> </li> <li> <p>Zabbix installed</p> </li> <li> <p>Fluent Bit installed</p> </li> </ul>"},{"location":"newoutput/zabbix/#configuration","title":"Configuration","text":"<p>To tail the logs with Fluentbit, we must first enable the Zabbix export directory. The steps differ if you're using Docker or VM, follow the one that suits your use case.</p>"},{"location":"newoutput/zabbix/#vm","title":"VM","text":"<ul> <li> <p>The relevant config is located in: /etc/zabbix/zabbix_server.conf</p> </li> <li> <p>Edit the config</p> </li> <li> <p>Uncomment and update the lines mentioned as follows (use a custom path to store the data from Zabbix, the path will later be used in Fluentbit):</p> </li> </ul> <pre><code>ExportDir=&lt;Path_To_Export_TheData&gt;\nExportFileSize=1M\n</code></pre> <p>Note: If the service doesn't run, check the permissions on the path used for the export.</p>"},{"location":"newoutput/zabbix/#docker","title":"Docker","text":"<ul> <li> <p>Enter Zabbix dir</p> </li> <li> <p>Paste the following commands:     This will export the data from Zabbix to this path: /var/lib/zabbix/export</p> </li> </ul> <pre><code>echo \"ZBX_EXPORTFILESIZE=1M\" &gt;&gt; ./env_vars/.env_srv\necho \"ZBX_EXPORTDIR=/var/lib/zabbix/export\" &gt;&gt; ./env_vars/.env_srv\n</code></pre>"},{"location":"newoutput/zabbix/#fluent-bit-configuration","title":"Fluent Bit Configuration","text":"<p>Below will be the config needed for Fluentbit to tail the data from Zabbix and forward it to Coralogix. Paste the config to the fluent-bit.conf and fill in the mandatory fields:</p> FieldDescriptionHostingress.coralogix.inPort443URI/zabbix/api/v1/real-time-exportPath_To_Data:Location of the exported data from ZabbixAppNameApplication name added to your metric attributesSubsystemNameSubsystem name added to your metric attributesPrivateKeyCoralogix Send-Your-Data API key <pre><code>[INPUT]\n    Name    tail\n    Path    &lt;Path_To_Data&gt;/history-history-syncer-*.ndjson\n    Tag     Item\n[OUTPUT]\n    Name                  http\n    Match                 Item\n    Host                  &lt;Endpoint&gt;\n    Port                  443\n    URI                   /zabbix/api/v1/real-time-export\n    Format                json\n    TLS                   On\n    Json_date_key         false\n    Header                Authorization Bearer &lt;PrivateKey&gt;\n    Header                data-type Item\n    Header                destination Metrics\n    Header                application-name &lt;AppName&gt;\n    Header                subsystem &lt;SubsystemName&gt;\n    compress              gzip\n    Retry_Limit           10\n\n[INPUT]\n    Name    tail\n    Path    &lt;Path_To_DATA&gt;/trends-history-syncer-*.ndjson\n    Tag     Trend\n[OUTPUT]\n    Name                  http\n    Match                 Trend\n    Host                  &lt;Endpoint&gt;\n    Port                  443\n    URI                   /zabbix/api/v1/real-time-export\n    Format                json\n    TLS                   On\n    Json_date_key         false\n    Header                Authorization Bearer &lt;PrivateKey&gt;\n    Header                data-type Trend\n    Header                destination Metrics\n    Header                application-name &lt;AppName&gt;\n    Header                subsystem &lt;SubsystemName&gt;\n    compress              gzip\n    Retry_Limit           10\n\n\n[INPUT]\n    Name    tail\n    Path    &lt;Path_To_DATA&gt;/problems-history-syncer-*.ndjson\n    Tag     Trigger\n[OUTPUT]\n    Name                  http\n    Match                 Trigger\n    Host                  &lt;Endpoint&gt;\n    Port                  443\n    URI                   /zabbix/api/v1/real-time-export\n    Format                json\n    TLS                   On\n    Json_date_key         false\n    Header                Authorization Bearer &lt;PrivateKey&gt;\n    Header                data-type Trigger\n    Header                destination Metrics\n    Header                application-name &lt;AppName&gt;\n    Header                subsystem &lt;SubsystemName&gt;\n    compress              gzip\n    Retry_Limit           10\n</code></pre> <p>Notes:</p> <ul> <li> <p>The header \"destination Metrics\" will send Coralogix metrics only.</p> </li> <li> <p>To access data for both logs and metrics, delete the header. The default is \"LogsAndMetrics\"</p> </li> </ul>"},{"location":"newoutput/zabbix/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/zeek/","title":"Zeek","text":"<p>In order to ship Zeek logs to Coralogix, we need to first install Filebeat.</p> <p>If you haven't already, you can follow our documentation here: https://coralogixstg.wpengine.com/integrations/filebeat/</p>"},{"location":"newoutput/zeek/#installation","title":"Installation","text":"<p>First, enable the Filebeat module for Zeek:</p> <pre><code>filebeat modules enable zeek\n</code></pre>"},{"location":"newoutput/zeek/#configuration","title":"Configuration","text":"<p>You need to configure the Zeek module file zeek.yml. Usually this file is located in /etc/filebeat/modules.d/</p> <p>In this configuration, you need to add the base directory where Zeek saves the logs usually, in this example replacing\u00a0/opt/zeek/logs/current with the path of your Zeek scan results.</p> <p>Here is an example of\u00a0zeek.yml:</p> <pre><code># Module: zeek\n# Docs: /guide/en/beats/filebeat/7.6/filebeat-module-zeek.html\n- module: zeek\n\u00a0capture_loss:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/capture_loss.log\"]\n\u00a0connection:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/conn.log\"]\n\u00a0dce_rpc:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/dce_rpc.log\"]\n\u00a0dhcp:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/dhcp.log\"]\n\u00a0dnp3:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/dnp3.log\"]\n\u00a0dns:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/dns.log\"]\n\u00a0dpd:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/dpd.log\"]\n\u00a0files:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/files.log\"]\n\u00a0ftp:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/ftp.log\"]\n\u00a0http:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/http.log\"]\n\u00a0intel:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/intel.log\"]\n\u00a0irc:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/irc.log\"]\n\u00a0kerberos:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/kerberos.log\"]\n\u00a0modbus:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/modbus.log\"]\n\u00a0mysql:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/mysql.log\"]\n\u00a0notice:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/notice.log\"]\n\u00a0ntlm:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/ntlm.log\"]\n\u00a0ocsp:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/ocsp.log\"]\n\u00a0pe:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/pe.log\"]\n\u00a0radius:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/radius.log\"]\n\u00a0rdp:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/rdp.log\"]\n\u00a0rfb:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/rfb.log\"]\n\u00a0#\u00a0 signatures:\n\u00a0#\u00a0 \u00a0 enabled: true\n\u00a0\u00a0\u00a0#\u00a0 \u00a0 var.paths: [\"/opt/zeek/logs/current/signatures.log\"]\n\u00a0sip:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/sip.log\"]\n\u00a0smb_cmd:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/smb_cmd.log\"]\n\u00a0smb_files:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/smb_files.log\"]\n\u00a0smb_mapping:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/smb_mapping.log\"]\n\u00a0smtp:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/smtp.log\"]\n\u00a0snmp:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/snmp.log\"]\n\u00a0socks:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/socks.log\"]\n\u00a0ssh:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/ssh.log\"]\n\u00a0ssl:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/ssl.log\"]\n\u00a0stats:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/stats.log\"]\n\u00a0syslog:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/syslog.log\"]\n\u00a0traceroute:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/traceroute.log\"]\n\u00a0tunnel:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/tunnel.log\"]\n\u00a0weird:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/weird.log\"]\n\u00a0x509:\n\u00a0\u00a0\u00a0enabled: true\n\u00a0\u00a0\u00a0var.paths: [\"/opt/zeek/logs/current/x509.log\"]\n</code></pre> <p>This way every time a Zeek scan is executed, Filebeat will ship logs to Coralogix.</p>"},{"location":"newoutput/zenduty/","title":"Zenduty","text":"<p>Zenduty is an incident management platform that helps businesses manage and resolve incidents quickly and efficiently. It provides a range of features such as alert management, on-call scheduling, incident response automation, and post-mortem analysis. Coralogix now supports seamless integration with Zenduty.</p>"},{"location":"newoutput/zenduty/#configuration","title":"Configuration","text":"<p>STEP 1. Navigate to\u00a0\u2018Teams\u2019\u00a0on your Zenduty dashboard and click on the team to which the integration will be added.</p> <p>STEP 2. Select\u00a0\u2018Services\u2019\u00a0and click on the relevant Service.</p> <p>STEP 3. Under\u00a0\u2018Integrations\u2019, click\u00a0\u2018Add New Integration\u2019. Give it a name and select the application\u00a0\u2018Coralogix\u2019\u00a0from the dropdown menu.  </p> <p>STEP 4. Provide the details 'Name', 'Summary', 'Create incidents for ', 'set default Urgency' and click on \u2018Add Integration\u2019 </p> <p></p> <p>STEP 5. Click\u00a0\u2018Configure\u2019\u00a0and copy the generated Webhook URL.  </p> <p></p>"},{"location":"newoutput/zenduty/#coralogix-setup","title":"Coralogix Setup","text":"<p>STEP 1. Navigate to\u00a0Data Flow &gt; Webhooks in your Coralogix account.\u00a0</p> <p>STEP 2. Select the\u00a0Webhooks\u00a0tab to create a new webhook.\u00a0  </p> <p>STEP 3. For the new webhook, give it a suitable name and paste the copied URL from Zenduty Configure.\u00a0  </p> <p></p> <p>STEP 4. Save the webhook.</p> <p>STEP 5. Validate your configuration by clicking the\u00a0\u2018Test Configuration\u2019\u00a0button. The test alert should appear on your Zenduty Alert log.</p> <p>STEP 6. Click Alerts\u00a0in your navigation bar to set the destination in the new alert.\u00a0  </p> <p></p> <p>STEP 7. Define the\u00a0\u2018Alert Name\u2019\u00a0and\u00a0\u2018Description\u2019. Severity is to be set to\u00a0\u2018Warning\u2019\u00a0or\u00a0\u2018Critical\u2019\u00a0for a new incident to be created. </p> <p>STEP 8. Define the conditions for which the alert is to be triggered.</p> <p>STEP 9. Under\u00a0\u2018Recipients\u2019, search and add the previously defined Zenduty webhook.\u00a0</p> <p>Note: If \u2018Notify when Resolved\u2019 option is enabled, then when the conditions go back to normal, the incident will be auto-resolved.\u00a0 In Zenduty, Incident will be auto resolved.</p> <p></p> <p>STEP 10. Save the Alert.</p>"},{"location":"newoutput/zenduty/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"newoutput/zscaler-secure-private-access-zpa/","title":"Zscaler Secure Private Access (ZPA)","text":"<p>Configure Zscaler Secure Private Access (ZPA) to seamlessly send logs to Coralogix.</p>"},{"location":"newoutput/zscaler-secure-private-access-zpa/#configuration","title":"Configuration","text":"<p>Configure Zscaler Secure Private Access (ZPA) to send logs in JSON format to a local server hosted on-premises or on-cloud on a custom port (for example, 5514).</p> <p>STEP 1. Edit the Log Receiver. Click Next.</p> <p></p> <p>STEP 2. Configure the Log Stream. Select JSON Log Template. Click Next.</p> <p></p> <p>STEP 3. Install Fluentd on a local server.</p> <p>STEP 4. Use the configuration below to listen to Zscaler on the configured port, parse it to JSON format, and send it to Coralogix.</p> <pre><code> &lt;source&gt;\n @type tcp #tcp input filter, in case you are sending it via UDP use udp input plugin and corresponding configuration\n @log_level debug\n tag zscaler-tcp # required\n &lt;parse&gt;\n   @type json\n &lt;/parse&gt;\n port 5514   #port on which you are sending zscaler logs\n bind 0.0.0.0\n delimiter \"\\n\"\n&lt;/source&gt;\n\n&lt;filter **&gt;\n@type record_transformer\n&lt;record&gt;\ntag ${tag_parts[0]}\n&lt;/record&gt;\n&lt;/filter&gt;\n&lt;filter **&gt;\n@type record_transformer\n@log_level warn\nenable_ruby true\nauto_typecast true\nrenew_record true\n&lt;record&gt;\napplicationName ${record['tag']} #this sets the application name same as tag specified, you can also set a static value\nsubsystemName ${record['tag']} #this sets the application name same as tag specified, you can also set a static value\ncomputerName ${hostname}\ntext ${record.to_json}\n&lt;/record&gt;\n&lt;/filter&gt;\n\n&lt;match **&gt;\n@type http\n@id out_http_coralogix\nendpoint \"https://api.app.coralogix.in/logs/rest/singles\" #for india region, update this as per your Coralogix region\nheaders {\"private_key\":\"&lt;Coralogix send your data api-key&gt;\"}\nerror_response_as_unrecoverable false\n&lt;buffer tag&gt;\n@type memory\n   compress gzip\n   flush_thread_count 4\n   chunk_limit_size 6MB\n   flush_interval 1s\n   overflow_action throw_exception\n   retry_max_times 10\n   retry_type periodic\n   retry_wait 8\n   total_limi_size 512MB\n&lt;/buffer&gt;\n&lt;/match&gt;\n\n&lt;label @FLUENT_LOG&gt;\n&lt;match fluent.*&gt;\n@type stdout\n&lt;/match&gt;\n&lt;/label&gt;\n</code></pre>"},{"location":"newoutput/zscaler-secure-private-access-zpa/#parameters-descriptions","title":"Parameters &amp; Descriptions","text":"<p>Input the API endpoint associated with your Coralogix domain in the configuration above.</p> DomainAPI Endpoint.coralogixstg.wpengine.comhttps://api.coralogixstg.wpengine.com/logs/rest/singles.app.coralogix.ushttps://api.coralogix.us/logs/rest/singles.app.coralogix.inhttps://api.app.coralogix.in/logs/rest/singles.app.eu2.coralogixstg.wpengine.comhttps://api.eu2.coralogixstg.wpengine.com/logs/rest/singles.app.coralogixsg.comhttps://api.coralogixsg.com/logs/rest/singles"},{"location":"newoutput/zscaler-secure-private-access-zpa/#additional-resources","title":"Additional Resources","text":"<p>Secure Private Access (ZPA) Help: Configuring a Log Receiver</p>"},{"location":"newoutput/zscaler-secure-private-access-zpa/#support","title":"Support","text":"<p>Need help?</p> <p>Our world-class customer success team is available 24/7 to walk you through your setup and answer any questions that may come up.</p> <p>Feel free to reach out to us\u00a0via our in-app chat\u00a0or by sending us an email at\u00a0support@coralogixstg.wpengine.com.</p>"},{"location":"reference/","title":"Reference Documentation","text":"<p>Optimize Coralogix\u2019s observability monitoring and unlock its most powerful features by using our wide range of APIs. Use them to send data to Coralogix, build visualizations, manage your data, and query it. </p>"},{"location":"reference/#quick-links","title":"Quick Links","text":"Data Ingestion Data Query Data Management Send us your data. Analyze data inside the platform. Access features to control data use."},{"location":"reference/#data-ingestion","title":"Data Ingestion","text":"<p>Data is ingested seamlessly and reliably into the Coralogix platform using our Data Ingestion APIs.</p> API Repository Link Coralogix REST API /singles Send us your logs using either our /singles endpoint. GitHub Coralogix REST API /bulk Send us your logs using either our /bulk endpoint. GitHub OpenTelemetry Custom Traces Send your custom traces to Coralogix using our OpenTelemetry-compatible endpoint. GitHub OpenTelemetry Custom Metrics Employ our OpenTelemetry-compatible custom metric endpoint, including serverless computing and quick cURL-like calls, to send counters, gauges, and histograms to Coralogix. GitHub OpenTelemetry Custom Logs Send your custom logs to Coralogix using our OpenTelemetry-compatible endpoint. GitHub"},{"location":"reference/#data-query","title":"Data Query","text":"<p>Access and query your data using our Data Query APIs.</p> API Description Repository Link Direct Archive Query HTTP API Direct Query API. Run DataPrime or Lucene queries of your indexed and archived logs without the need to access your Coralogix UI. GitHub"},{"location":"reference/#data-management","title":"Data Management","text":"<p>The Data Management APIs enable you to configure the Coralogix platform, customize your user interface, and optimize it for your observability requirements.</p> API Description Repository Link SLO Management API GitHub Service Removal API GitHub TCO Tracing Policy API Define, query, and manage your TCO tracing policy criteria. GitHub Archive Setup API Many of our customers send us their telemetry data via their Amazon S3 bucket. Our S3 Archive Setup gRPC API allows you to view bucket definitions, set a target bucket and to define archive retentions. GitHub Data Usage API Coralogix provides a Data Usage Service API in support of our Detailed Data Usage Report, which presents you with the data you\u2019ve sent to Coralogix, per policy, for either the current month or retroactively 30 or 90 days. The API allows you to query your data consumption in a given time period. GitHub Send-Your-Data Management API Coralogix offers its customers the option of creating multiple Send-Your-Data API keys with advanced security settings, allowing you to minimize security vulnerabilities and utilize different keys across different systems, deployment methods, and teams. GitHub Recording Rules API Coralogix recording rules allow you to pre-process and derive new time series from existing ones. The Recording Rules API allows you to manage these rules, which are executed in the background at regular intervals. GitHub Hosted Grafana API Visualize your logs, metrics, and traces using our Grafana-hosted view without the need for any plugins. We provide a secure Grafana API to manage your Grafana-hosted dashboard, allowing you to create, edit, export, import, and query your data in that platform. GitHub Webhooks API Define, query, and manage your webhooks. GitHub TCO Optimizer HTTP API Define, query, and manage your TCO log policy overrides. GitHub Custom Data Enrichment API Easily enrich your log data with business, operations, or security information using our Enrichment API. Automatically add fields to your JSON logs based on specific matches in your log data, using a predefined custom data source of your choice. GitHub Insights API Manage our Insights Detection service, automatically detecting possible threats and security related anomalies in your traffic. GitHub Parsing Rules API Use our log parsing rules to process, parse, and restructure log data for monitoring and analysis in the Coralogix platform. Create, read, update, or delete these rules and rule groups for your data. GitHub Alerts API Coralogix gives you the ability to create monitors that actively check system performance and notify you when there are changes to your data. Our Alerts API allows you to define, query, and manage your alerts. GitHub"},{"location":"reference/api/api-grpc-incidents/","title":"Incidents API","text":""},{"location":"reference/api/api-grpc-incidents/#overview","title":"Overview","text":"<p>This document outlines the Incidents Management API. It includes various methods for managing incidents, such as retrieving incident details, listing incidents, aggregating incidents, assigning and unassigning incidents, acknowledging and resolving incidents, and more. The service is designed to handle all operations related to incident management within Coralogix.</p>"},{"location":"reference/api/api-grpc-incidents/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, please make sure you have the following:</p> <ul> <li>API Key for Alerts, Rules &amp; Tags to successfully authenticate.</li> <li>Management API Endpoint that corresponds with your Coralogix subscription.</li> <li>Administrator permissions to manage your services.</li> </ul>"},{"location":"reference/api/api-grpc-incidents/#authentication","title":"Authentication","text":"<p>Coralogix API uses API keys to authenticate requests. You can view and manage your API keys from the Data Flow tab in Coralogix. You need to use this API key in the Authorization request header to successfully connect.</p> <p>Example:</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\"\n</code></pre> <p>Then, use one of our designated Management Endpoints to structure your header. </p> <pre><code>-d @ ng-api-grpc.coralogix.com:443\n</code></pre> <p>For the Incidents Service API, the service name will be <code>IncidentsService</code>.</p> <pre><code>com.coralogixapis.incidents.v1.IncidentsService/\n</code></pre> <p>The complete request header should look like this:</p> <pre><code>grpcurl -H \"Authorization: Bearer API_KEY_HERE\" -d @ ng-api-grpc.coralogix.com:443 com.coralogixapis.incidents.v1.IncidentsService/\n</code></pre>"},{"location":"reference/api/api-grpc-incidents/#sample-request","title":"Sample Request","text":"<p>Lists all available incidents based on specified filters and order. In this case, incidents are shown per <code>assignee</code>. The list is ordered in an unspecified direction and sorted by time created.</p> <pre><code>grpcurl -H \"Authorization: Bearer APY_KEY_HERE\" -d @ ng-api-grpc.coralogix.com:443 com.coralogixapis.incidents.v1.IncidentsService/ListIncidents &lt;&lt;EOF \n{\n    \"filter\": {\n        \"assignee\": [\n            {\n                \"value\": \"assignee@coralogix.com\"\n            }\n        ]},\n    \"order_bys\": [\n        {\n            \"direction\": \"ORDER_BY_DIRECTION_UNSPECIFIED\",\n            \"incident_field\": \"INCIDENTS_FIELDS_CREATED_TIME\"\n        }\n    ]\n}\nEOF\n</code></pre>"},{"location":"reference/api/api-grpc-incidents/#sample-response","title":"Sample Response","text":"<pre><code>{\n    \"incidents\": [\n        {\n            \"assignments\": [\n                {\n                    \"assigned_to\": {\n                        \"user_id\": {\n                            \"value\": \"assignee@coralogix.com\"\n                        }\n                    },\n                    \"assigned_by\": {\n                        \"user_id\": {\n                            \"value\": \"assignee@coralogix.com\"\n                        }\n                    }\n                }\n            ],\n            \"events\": [],\n            \"contextual_labels\": [\n                {\n                    \"key\": \"alert_id\",\n                    \"value\": \"e2e1e00f-552f-4dfc-9d24-ab9d21d4979c\"\n                },\n                {\n                    \"key\": \"alert_name\",\n                    \"value\": \"inalert\"\n                },\n                {\n                    \"key\": \"alert_type\",\n                    \"value\": \"Standard\"\n                },\n                {\n                    \"key\": \"alert_severity\",\n                    \"value\": \"Info\"\n                },\n                {\n                    \"key\": \"alert_group_by_fields\",\n                    \"value\": \"coralogix.metadata.applicationName , coralogix.metadata.subsystemName\"\n                },\n                {\n                    \"key\": \"alert_notification_group_id\",\n                    \"value\": \"ab8dee0e-063b-43c2-89a3-bdbb068ff851\"\n                },\n                {\n                    \"key\": \"alert_notification_group_grouping_fields\",\n                    \"value\": \"coralogix.metadata.applicationName , coralogix.metadata.subsystemName\"\n                },\n                {\n                    \"key\": \"alert_notification_group_integrations_ids\",\n                    \"value\": \"\"\n                }\n            ],\n            \"display_labels\": [\n                {\n                    \"key\": \"coralogix.metadata.subsystemName\",\n                    \"value\": \"coralogix-operator\"\n                },\n                {\n                    \"key\": \"coralogix.metadata.applicationName\",\n                    \"value\": \"staging\"\n                }\n            ],\n            \"id\": {\n                \"value\": \"cdfaf78b-27ee-401f-8d13-ebd2daa08232\"\n            },\n            \"name\": null,\n            \"state\": \"INCIDENT_STATE_TRIGGERED\",\n            \"status\": \"INCIDENT_STATUS_TRIGGERED\",\n            \"description\": null,\n            \"severity\": \"INCIDENT_SEVERITY_INFO\",\n            \"created_at\": {\n                \"seconds\": \"1703677320\",\n                \"nanos\": 0\n            },\n            \"closed_at\": null,\n            \"last_state_update_time\": {\n                \"seconds\": \"1706088981\",\n                \"nanos\": 286000000\n            },\n            \"last_state_update_key\": {\n                \"value\": \"8cde7807-dedc-418b-b542-62d78fead629\"\n            },\n            \"is_muted\": {\n                \"value\": false\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"reference/api/api-grpc-incidents/#api-endpoints","title":"API Endpoints","text":"<p>incidents_service.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidentsservice","title":"IncidentsService","text":"<p>The IncidentsService is designed to provide a set of functionalities and operations to facilitate the effective management, monitoring, and resolution of incidents. Here are some key methods within the IncidentsService:</p> Method Name Description ListIncidents Lists incidents based on filters. This method is used to retrieve a collection of incidents that match certain criteria. ListIncidentAggregations Lists incident aggregations. This method is  used to retrieve aggregated information about incidents, grouped by specific parameters. GetIncident Retrieves detailed information about a specific incident. This method is used to get comprehensive details regarding a single incident. GetIncidentEvents Retrieves events associated with a particular incident. This method is  used to obtain a chronological sequence of events related to an incident. BatchGetIncident Retrieves details for multiple incidents. This method is designed to efficiently retrieve information for a batch of incidents in a single request. AssignIncidents Assigns incidents to specific users or teams. This method is used for managing the assignment of incidents to responsible parties. UnassignIncidents Unassigns incidents from users or teams. This method is  used to remove assignment associations for incidents. AcknowledgeIncidents Acknowledges incidents. PaginationRequest Retrieves pagination information for incidents. CloseIncidents Closes incidents. DeleteIncidents Deletes incidents. ResolveIncidents Resolves incidents."},{"location":"reference/api/api-grpc-incidents/#listincidents","title":"ListIncidents","text":""},{"location":"reference/api/api-grpc-incidents/#listincidentsrequest","title":"ListIncidentsRequest","text":"Field Type Label Description filter IncidentQueryFilter Filter for listing incidents. pagination PaginationRequest Pagination details. order_bys OrderBy repeated Ordering options for listing incidents."},{"location":"reference/api/api-grpc-incidents/#listincidentsresponse","title":"ListIncidentsResponse","text":"Field Type Label Description incidents Incident repeated List of incidents. pagination PaginationResponse Pagination details."},{"location":"reference/api/api-grpc-incidents/#listincidentaggregations","title":"ListIncidentAggregations","text":""},{"location":"reference/api/api-grpc-incidents/#listincidentaggregationsrequest","title":"ListIncidentAggregationsRequest","text":"Field Type Label Description filter IncidentQueryFilter Filter for aggregations. group_bys GroupBy repeated Grouping options for aggregations. pagination PaginationRequest Pagination details."},{"location":"reference/api/api-grpc-incidents/#listincidentaggregationsresponse","title":"ListIncidentAggregationsResponse","text":"Field Type Label Description incident_aggs IncidentAggregation repeated Aggregated incident data. pagination PaginationResponse Pagination details."},{"location":"reference/api/api-grpc-incidents/#getincident","title":"GetIncident","text":""},{"location":"reference/api/api-grpc-incidents/#getincidentrequest","title":"GetIncidentRequest","text":"Field Type Label Description id google.protobuf.StringValue ID of the incident to retrieve."},{"location":"reference/api/api-grpc-incidents/#getincidentresponse","title":"GetIncidentResponse","text":"Field Type Label Description incident Incident Retrieved incident."},{"location":"reference/api/api-grpc-incidents/#getincidentevents","title":"GetIncidentEvents","text":""},{"location":"reference/api/api-grpc-incidents/#getincidenteventsrequest","title":"GetIncidentEventsRequest","text":"Field Type Label Description incident_id google.protobuf.StringValue ID of the incident to get events for."},{"location":"reference/api/api-grpc-incidents/#getincidenteventsresponse","title":"GetIncidentEventsResponse","text":"Field Type Label Description incident_events IncidentEvent repeated Events associated with the incident."},{"location":"reference/api/api-grpc-incidents/#batchgetincident","title":"BatchGetIncident","text":""},{"location":"reference/api/api-grpc-incidents/#batchgetincidentrequest","title":"BatchGetIncidentRequest","text":"Field Type Label Description ids google.protobuf.StringValue repeated IDs of incidents to be retrieved in batch."},{"location":"reference/api/api-grpc-incidents/#batchgetincidentresponse","title":"BatchGetIncidentResponse","text":"Field Type Label Description incidents BatchGetIncidentResponse.IncidentsEntry repeated Retrieved incidents in batch. not_found_ids google.protobuf.StringValue repeated IDs of incidents not found."},{"location":"reference/api/api-grpc-incidents/#batchgetincidentresponseincidentsentry","title":"BatchGetIncidentResponse.IncidentsEntry","text":"Field Type Label Description key string Incident key. value Incident Retrieved incident."},{"location":"reference/api/api-grpc-incidents/#assignincidents","title":"AssignIncidents","text":""},{"location":"reference/api/api-grpc-incidents/#assignincidentsrequest","title":"AssignIncidentsRequest","text":"Field Type Label Description incident_ids google.protobuf.StringValue repeated IDs of incidents to be assigned. assigned_to UserDetails Details of the user to whom incidents are to be assigned."},{"location":"reference/api/api-grpc-incidents/#assignincidentsresponse","title":"AssignIncidentsResponse","text":"Field Type Label Description incidents Incident repeated Assigned incidents."},{"location":"reference/api/api-grpc-incidents/#unassignincidents","title":"UnassignIncidents","text":""},{"location":"reference/api/api-grpc-incidents/#unassignincidentsrequest","title":"UnassignIncidentsRequest","text":"Field Type Label Description incident_ids google.protobuf.StringValue repeated IDs of incidents to be unassigned."},{"location":"reference/api/api-grpc-incidents/#unassignincidentsresponse","title":"UnassignIncidentsResponse","text":"Field Type Label Description incidents Incident repeated Unassigned incidents."},{"location":"reference/api/api-grpc-incidents/#acknowledgeincidents","title":"AcknowledgeIncidents","text":""},{"location":"reference/api/api-grpc-incidents/#acknowledgeincidentsrequest","title":"AcknowledgeIncidentsRequest","text":"Field Type Label Description incident_ids google.protobuf.StringValue repeated IDs of incidents to be acknowledged."},{"location":"reference/api/api-grpc-incidents/#acknowledgeincidentsresponse","title":"AcknowledgeIncidentsResponse","text":"Field Type Label Description incidents Incident repeated Acknowledged incidents."},{"location":"reference/api/api-grpc-incidents/#closeincidents","title":"CloseIncidents","text":""},{"location":"reference/api/api-grpc-incidents/#closeincidentsrequest","title":"CloseIncidentsRequest","text":"Field Type Label Description incident_ids google.protobuf.StringValue repeated IDs of incidents to be closed."},{"location":"reference/api/api-grpc-incidents/#closeincidentsresponse","title":"CloseIncidentsResponse","text":"Field Type Label Description incidents Incident repeated Closed incidents."},{"location":"reference/api/api-grpc-incidents/#deleteincidents","title":"DeleteIncidents","text":""},{"location":"reference/api/api-grpc-incidents/#deleteincidentrequest","title":"DeleteIncidentRequest","text":"Field Type Label Description id google.protobuf.StringValue ID of the incident to be deleted."},{"location":"reference/api/api-grpc-incidents/#resolveincidents","title":"ResolveIncidents","text":""},{"location":"reference/api/api-grpc-incidents/#resolveincidentsrequest","title":"ResolveIncidentsRequest","text":"Field Type Label Description incident_ids google.protobuf.StringValue repeated IDs of incidents to be resolved."},{"location":"reference/api/api-grpc-incidents/#resolveincidentsresponse","title":"ResolveIncidentsResponse","text":"Field Type Label Description incidents Incident repeated Resolved incidents."},{"location":"reference/api/api-grpc-incidents/#paginationrequest","title":"PaginationRequest","text":""},{"location":"reference/api/api-grpc-incidents/#paginationrequest_1","title":"PaginationRequest","text":"Field Type Label Description page_size google.protobuf.UInt32Value Number of items per page. page_token google.protobuf.StringValue Token for the next page."},{"location":"reference/api/api-grpc-incidents/#paginationresponse","title":"PaginationResponse","text":"Field Type Label Description total_size google.protobuf.UInt32Value Total number of items. next_page_token google.protobuf.StringValue Token for the next page."},{"location":"reference/api/api-grpc-incidents/#auditlogdescription","title":"AuditLogDescription","text":"Field Type Label Description description string optional Description for audit logging."},{"location":"reference/api/api-grpc-incidents/#contextuallabels","title":"ContextualLabels","text":"Field Type Label Description field_name google.protobuf.StringValue Name of the contextual label field. field_value google.protobuf.StringValue Value of the contextual label."},{"location":"reference/api/api-grpc-incidents/#groupbyvalues","title":"GroupByValues","text":"Field Type Label Description incident_field IncidentFieldOneOf Field to group incidents by. contextual_labels ContextualLabels Contextual labels for grouping incidents."},{"location":"reference/api/api-grpc-incidents/#incident","title":"Incident","text":"<p>The incident represents an unexpected or disruptive event within your system. The definition below outlines the essential attributes associated with managing and documenting such incidents in Coralogix. Here's a detailed description of the key components:</p> Field Type Label Description id google.protobuf.StringValue Unique identifier for the incident. name google.protobuf.StringValue Name of the incident. state IncidentState Current state of the incident. status IncidentStatus Current status of the incident. assignments Assignment repeated List of assignments for the incident. description google.protobuf.StringValue Description of the incident. severity IncidentSeverity Severity level of the incident. contextual_labels Incident.ContextualLabelsEntry repeated Contextual labels associated with the incident. display_labels Incident.DisplayLabelsEntry repeated Display labels associated with the incident. events IncidentEvent repeated List of events related to the incident. created_at google.protobuf.Timestamp Timestamp when the incident was created. closed_at google.protobuf.Timestamp Timestamp when the incident was closed. last_state_update_time google.protobuf.Timestamp Timestamp of the last state update for the incident. last_state_update_key google.protobuf.StringValue Key associated with the last event that caused a state change in the incident. is_muted google.protobuf.BoolValue Indicates whether the incident is muted or suppressed."},{"location":"reference/api/api-grpc-incidents/#incidentcontextuallabelsentry","title":"Incident.ContextualLabelsEntry","text":"Field Type Label Description key string Key of the contextual label. value string Value of the contextual label."},{"location":"reference/api/api-grpc-incidents/#incidentdisplaylabelsentry","title":"Incident.DisplayLabelsEntry","text":"Field Type Label Description key string Key of the display label. value string Value of the display label."},{"location":"reference/api/api-grpc-incidents/#incidentaggregation","title":"IncidentAggregation","text":"<p>Incident Aggregation is specifically used for grouping and summarizing incidents based on certain criteria. This aggregation seems to provide a consolidated view of multiple incidents, offering insights into various aspects of their states, statuses, severities, assignments, and other relevant details.</p> Field Type Label Description group_bys_value GroupByValues repeated Group by fields and values for the aggregation. agg_state_count IncidentStateCount repeated Count of incidents for each state in the aggregation. agg_status_count IncidentStatusCount repeated Count of incidents for each status in the aggregation. agg_severity_count IncidentSeverityCount repeated Count of incidents for each severity in the aggregation. agg_assignments_count IncidentAssignmentCount repeated Count of incidents for each assignment in the aggregation. first_created_at google.protobuf.Timestamp Timestamp of the first incident created in the aggregation. last_closed_at google.protobuf.Timestamp Timestamp of the last incident closed in the aggregation. all_values_count google.protobuf.UInt32Value Total count of incidents in the aggregation. list_incidents_id google.protobuf.StringValue repeated List of incident IDs in the aggregation. last_state_update_time google.protobuf.Timestamp Timestamp of the last state update in the aggregation."},{"location":"reference/api/api-grpc-incidents/#incidentassignmentcount","title":"IncidentAssignmentCount","text":"Field Type Label Description assigned_to UserDetails Details of the user to whom incidents are assigned. count google.protobuf.UInt32Value Count of incidents assigned to the user."},{"location":"reference/api/api-grpc-incidents/#incidentfieldoneof","title":"IncidentFieldOneOf","text":"Field Type Label Description id google.protobuf.StringValue Unique identifier for the incident field. severity IncidentSeverity Severity level of the incident field. name google.protobuf.StringValue Name of the incident field. created_at google.protobuf.Timestamp Timestamp when the incident field was created. closed_at google.protobuf.Timestamp Timestamp when the incident field was closed. state IncidentState State of the incident field. status IncidentStatus Status of the incident field. last_state_update_time google.protobuf.Timestamp Timestamp of the last state update for the incident field. application_name google.protobuf.StringValue Application name associated with the incident field. subsystem_name google.protobuf.StringValue Subsystem name associated with the incident field."},{"location":"reference/api/api-grpc-incidents/#incidentseveritycount","title":"IncidentSeverityCount","text":"Field Type Label Description severity IncidentSeverity Severity level. count google.protobuf.UInt32Value Count of incidents for the severity level."},{"location":"reference/api/api-grpc-incidents/#incidentstatecount","title":"IncidentStateCount","text":"Field Type Label Description state IncidentState State of the incident. count google.protobuf.UInt32Value Count of incidents for the state."},{"location":"reference/api/api-grpc-incidents/#incidentstatuscount","title":"IncidentStatusCount","text":"Field Type Label Description status IncidentStatus Status of the incident. count google.protobuf.UInt32Value Count of incidents for the status."},{"location":"reference/api/api-grpc-incidents/#incidentfields","title":"IncidentFields","text":"Name Number Description INCIDENTS_FIELDS_UNSPECIFIED 0 Unspecified incident field. INCIDENTS_FIELDS_ID 1 Incident ID. INCIDENTS_FIELDS_SEVERITY 2 Incident severity. INCIDENTS_FIELDS_NAME 3 Incident name. INCIDENTS_FIELDS_CREATED_TIME 4 Timestamp when the incident was created. INCIDENTS_FIELDS_CLOSED_TIME 5 Timestamp when the incident was closed. INCIDENTS_FIELDS_STATE 6 Current state of the incident. INCIDENTS_FIELDS_STATUS 7 Current status of the incident. INCIDENTS_FIELDS_LAST_STATE_UPDATE_TIME 8 Timestamp of the last state update for the incident. INCIDENTS_FIELDS_APPLICATION_NAME 9 Application name associated with the incident. INCIDENTS_FIELDS_SUBSYSTEM_NAME 10 Subsystem name associated with the incident. <p>incident_severity.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidentseverity","title":"IncidentSeverity","text":"Name Number Description INCIDENT_SEVERITY_UNSPECIFIED 0 Unspecified incident severity. INCIDENT_SEVERITY_INFO 1 Informational incident severity. INCIDENT_SEVERITY_WARNING 2 Warning incident severity. INCIDENT_SEVERITY_ERROR 3 Error incident severity. INCIDENT_SEVERITY_CRITICAL 4 Critical incident severity. <p>incident_status.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidentstatus","title":"IncidentStatus","text":"Name Number Description INCIDENT_STATUS_UNSPECIFIED 0 Unspecified incident status. INCIDENT_STATUS_TRIGGERED 1 Incident is triggered. INCIDENT_STATUS_ACKNOWLEDGED 2 Incident is acknowledged. INCIDENT_STATUS_RESOLVED 3 Incident is resolved. <p>assignee.proto</p>"},{"location":"reference/api/api-grpc-incidents/#assignment","title":"Assignment","text":"Field Type Label Description assigned_to UserDetails Details of the user to whom the incident is assigned. assigned_by UserDetails Details of the user who assigned the incident."},{"location":"reference/api/api-grpc-incidents/#userdetails","title":"UserDetails","text":"Field Type Label Description user_id google.protobuf.StringValue ID of the user. <p>incident_event/incident_event_unassign.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidenteventunassign","title":"IncidentEventUnassign","text":"<p>This represents an event where an incident is unassigned.</p> <p>incident_event/incident_event_acknowledge.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidenteventacknowledge","title":"IncidentEventAcknowledge","text":"Field Type Label Description acknowledged_by UserDetails Details of the user who acknowledged the incident. <p>incident_event/incident_event_type.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidenteventtype","title":"IncidentEventType","text":"Name Number Description INCIDENT_EVENT_TYPE_UNSPECIFIED 0 Unspecified incident event type. INCIDENT_EVENT_TYPE_UPSERT_STATE 2 Incident event for upserting state. INCIDENT_EVENT_TYPE_OPEN 4 Incident event for opening an incident. INCIDENT_EVENT_TYPE_CLOSE 5 Incident event for closing an incident. INCIDENT_EVENT_TYPE_SNOOZE_INDICATOR 6 Incident event for snooze indicator. INCIDENT_EVENT_TYPE_ASSIGN 7 Incident event for assigning an incident. INCIDENT_EVENT_TYPE_UNASSIGN 9 Incident event for unassigning an incident. INCIDENT_EVENT_TYPE_ACKNOWLEDGE 8 Incident event for acknowledging an incident. <p>incident_event/incident_event_assign.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidenteventassign","title":"IncidentEventAssign","text":"Field Type Label Description assignment Assignment Details of the assignment event. <p>incident_event/incident_event_originator_operational.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidenteventoriginatoroperational","title":"IncidentEventOriginatorOperational","text":"Field Type Label Description system_name google.protobuf.StringValue Name of the operational system originating the event. <p>incident_event/incident_event_originator_administrative.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidenteventoriginatoradministrative","title":"IncidentEventOriginatorAdministrative","text":"Field Type Label Description user_id google.protobuf.StringValue ID of the administrative user originating the event. <p>incident_event/incident_event_originator_type.proto</p>"},{"location":"reference/api/api-grpc-incidents/#originatortype","title":"OriginatorType","text":"Name Number Description ORIGINATOR_TYPE_UNSPECIFIED 0 Unspecified originator type. ORIGINATOR_TYPE_OPERATIONAL 1 Operational originator type. ORIGINATOR_TYPE_ADMINISTRATIVE 2 Administrative originator type. <p>incident_event/incident_event_snooze_indicator.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidenteventsnoozeindicator","title":"IncidentEventSnoozeIndicator","text":"Field Type Label Description start_time google.protobuf.Timestamp Start time of the snooze period. duration_minutes google.protobuf.Int32Value Duration of the snooze period in minutes. user_id google.protobuf.StringValue ID of the user who initiated the snooze. <p>incident_event/incident_event.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidentevent","title":"IncidentEvent","text":"<p>An incident event typically refers to a specific occurrence or action related to the management and lifecycle of an incident within your system. This structured definition includes various fields to capture details about the event.</p> Field Type Label Description id google.protobuf.StringValue Unique identifier for the incident event. incident_event_type IncidentEventType Type of the incident event. snooze_indicator IncidentEventSnoozeIndicator Information related to snooze indicator event. assignment IncidentEventAssign Information related to assignment event. unassign IncidentEventUnassign Information related to unassignment event. upsert_state IncidentEventUpsertState Information related to upsert state event. acknowledge IncidentEventAcknowledge Information related to acknowledgment event. close IncidentEventClose Information related to closure event. originator_type OriginatorType Type of the originator (administrative or operational) for the incident event. administrative_event IncidentEventOriginatorAdministrative Details of the administrative user who triggered the event. operational_event IncidentEventOriginatorOperational Details of the operational system that triggered the event. <p>incident_event/incident_event_upsert_state.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidenteventupsertstate","title":"IncidentEventUpsertState","text":"Field Type Label Description state_type UpsertIncidentStateType Type of upsert state event. payload UpsertIncidentStatePayload Payload associated with the upsert state event. is_muted google.protobuf.BoolValue Indicates whether the incident is muted during the upsert state event."},{"location":"reference/api/api-grpc-incidents/#upsertincidentstatepayload","title":"UpsertIncidentStatePayload","text":"Field Type Label Description cx_event_key google.protobuf.StringValue Coralogix event key associated with the incident."},{"location":"reference/api/api-grpc-incidents/#upsertincidentstatetype","title":"UpsertIncidentStateType","text":"Name Number Description UPSERT_INCIDENT_STATE_TYPE_UNSPECIFIED 0 Unspecified upsert incident state type. UPSERT_INCIDENT_STATE_TYPE_TRIGGERED 1 Upsert state for triggered incidents. UPSERT_INCIDENT_STATE_TYPE_RESOLVED 2 Upsert state for resolved incidents. <p>incident_event/incident_event_close.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidenteventclose","title":"IncidentEventClose","text":"Field Type Label Description closed_by UserDetails Details of the user who closed the incident. <p>incident_query_filter.proto</p>"},{"location":"reference/api/api-grpc-incidents/#contextuallabelvalues","title":"ContextualLabelValues","text":"Field Type Label Description contextual_label_values google.protobuf.StringValue repeated List of contextual label values."},{"location":"reference/api/api-grpc-incidents/#incidentqueryfilter","title":"IncidentQueryFilter","text":"<p>Incident Query Filter is used for specifying criteria when querying or filtering incidents within a your system. This definition includes various fields that can be used to filter incidents based on different attributes. </p> Field Type Label Description assignee google.protobuf.StringValue repeated List of assignee user IDs. status IncidentStatus repeated List of incident statuses. state IncidentState repeated List of incident states. severity IncidentSeverity repeated List of incident severities. contextual_labels IncidentQueryFilter.ContextualLabelsEntry repeated List of contextual labels and their values. search_query IncidentSearchQuery Search query for incidents. application_name google.protobuf.StringValue repeated List of application names. subsystem_name google.protobuf.StringValue repeated List of subsystem names. is_muted google.protobuf.BoolValue Indicates whether incidents are muted. created_at_range TimeRange Filters all incidents created at the given time range incident_duration_range TimeRange Filters all incidents open (alive) at the given time range"},{"location":"reference/api/api-grpc-incidents/#incidentqueryfiltertimerange","title":"IncidentQueryFilter.TimeRange","text":"Field Type Label Description start_time google.protobuf.Timestamp Start time for filtering incidents. end_time google.protobuf.Timestamp End time for filtering incidents."},{"location":"reference/api/api-grpc-incidents/#incidentqueryfiltercontextuallabelsentry","title":"IncidentQueryFilter.ContextualLabelsEntry","text":"Field Type Label Description key string Key of the contextual label. value ContextualLabelValues List of values for the contextual label. <p>incident_state.proto</p>"},{"location":"reference/api/api-grpc-incidents/#incidentstate","title":"IncidentState","text":"Name Number Description INCIDENT_STATE_UNSPECIFIED 0 Unspecified incident state. INCIDENT_STATE_TRIGGERED 1 Incident is triggered. INCIDENT_STATE_RESOLVED 2 Incident is resolved. <p>incident_query.proto</p>"},{"location":"reference/api/api-grpc-incidents/#groupby","title":"GroupBy","text":"Field Type Label Description incident_field IncidentFields Field to group incidents by. contextual_label google.protobuf.StringValue Contextual label to group incidents by. order_by_direction OrderByDirection Direction for ordering the grouped incidents."},{"location":"reference/api/api-grpc-incidents/#incidentsearchquery","title":"IncidentSearchQuery","text":"Field Type Label Description query google.protobuf.StringValue Search query string. incident_field IncidentFields Field to search incidents by. contextual_label google.protobuf.StringValue Contextual label to search incidents by."},{"location":"reference/api/api-grpc-incidents/#orderby","title":"OrderBy","text":"Field Type Label Description incident_field IncidentFields Field for ordering incidents. contextual_label google.protobuf.StringValue Contextual label for ordering incidents. direction OrderByDirection Direction for ordering incidents."},{"location":"reference/api/api-grpc-incidents/#orderbydirection","title":"OrderByDirection","text":"Name Number Description ORDER_BY_DIRECTION_UNSPECIFIED 0 Unspecified order direction. ORDER_BY_DIRECTION_ASC 1 Ascending order. ORDER_BY_DIRECTION_DESC 2 Descending order."},{"location":"reference/api/api-grpc-incidents/#orderbyfields","title":"OrderByFields","text":"Name Number Description ORDER_BY_FIELDS_UNSPECIFIED 0 Unspecified order field. ORDER_BY_FIELDS_ID 1 Order by incident ID. ORDER_BY_FIELDS_SEVERITY 2 Order by incident severity. ORDER_BY_FIELDS_NAME 3 Order by incident name. ORDER_BY_FIELDS_CREATED_TIME 4 Order by incident creation time. ORDER_BY_FIELDS_CLOSED_TIME 5 Order by incident closure time."},{"location":"reference/api/api-grpc-incidents/#scopedetails","title":"ScopeDetails","text":"Field Type Label Description subsystem_name google.protobuf.StringValue Name of the subsystem. application_name google.protobuf.StringValue Name of the application."},{"location":"reference/api/api-rest-bulk/","title":"REST API /bulk","text":""},{"location":"reference/api/api-rest-singles/","title":"REST API /singles","text":""},{"location":"reference/api/api-rest-singles/#example-request","title":"Example Request","text":"<pre><code>\ncurl --location --request POST 'https://ingress.&lt;domain&gt;/logs/v1/singles' \\\n  --header 'Content-Type: application/json' \\\n  --header 'Authorization: Bearer &lt;Send-Your-Data API key&gt;' \\\n  --data-raw '[\n    {\n      \"applicationName\": \"*insert desired application name*\",\n      \"subsystemName\": \"*insert desired subsystem name*\",\n      \"computerName\": \"*insert computer name*\",\n      \"severity\": 3,\n      \"text\": \"this is a normal text message\",\n      \"category\": \"cat-1\",\n      \"className\": \"class-1\",\n      \"methodName\": \"method-1\",\n      \"threadId\": \"thread-1\",\n      \"timestamp\": 1675148539123.342\n    },\n    {\n      \"applicationName\": \"*insert desired application name*\",\n      \"subsystemName\": \"*insert desired subsystem name*\",\n      \"computerName\": \"*insert computer name*\",\n      \"hiResTimestamp\": \"1675148539789123123\",\n      \"severity\": 5,\n      \"text\": \"{\\\"key1\\\":\\\"val1\\\",\\\"key2\\\":\\\"val2\\\",\\\"key3\\\":\\\"val3\\\",\\\"key4\\\":\\\"val4\\\"}\",\n      \"category\": \"DAL\",\n      \"className\": \"UserManager\",\n      \"methodName\": \"RegisterUser\",\n      \"threadId\": \"a-352\"\n    }\n  ]'\n\n</code></pre>"},{"location":"reference/sdk/python/","title":"Coralogix Python SDK","text":""},{"location":"reference/sdk/python/#installation","title":"Installation","text":"<p>Install the Coralogix python logger with <code>pip</code>:</p> <pre><code>pip install coralogix_logger\n</code></pre> <p>or directly from the sources:</p> <pre><code>git clone https://github.com/coralogix/python-coralogix-sdk.git\ncd sdk-python\npython setup.py install\n</code></pre>"},{"location":"reference/sdk/python/#configuration","title":"Configuration","text":"<p>Using Coralogix Python SDK requires these mandatory parameters:</p> <p>PRIVATE KEY - A unique ID which represents your company. This ID will be sent to your mail once you register to <code>Coralogix</code>.</p> <p>APPLICATION NAME - The name of your main application. For example, a company named \u201cSuperData\u201d could insert the \u201cSuperData Production\u201d string parameter; or if they want to debug their test environment, they might insert the \u201cSuperData \u2013 Test\u201d.</p> <p>SUBSYSTEM NAME - Your application probably has multiple subsystems. For example: Backend servers, Middleware, Frontend servers etc. In order to help you examine and filter the data you need, inserting the subsystem parameter is vital.</p>"},{"location":"reference/sdk/python/#implementation","title":"Implementation","text":"<p>Adding <code>Coralogix</code> logging handler in your logging system:</p> <pre><code>import logging\n# For version 1.*\nfrom coralogix.coralogix_logger import CoralogixLogger\n# For version 2.*\nfrom coralogix.handlers import CoralogixLogger\n\nPRIVATE_KEY = \"[YOUR_PRIVATE_KEY_HERE]\"\nAPP_NAME = \"[YOUR_APPLICATION_NAME]\"\nSUB_SYSTEM = \"[YOUR_SUBSYSTEM_NAME]\"\n\n# Get an instance of Python standard logger.\nlogger = logging.getLogger(\"Python Logger\")\nlogger.setLevel(logging.DEBUG)\n\n# Get a new instance of Coralogix logger.\ncoralogix_handler = CoralogixLogger(PRIVATE_KEY, APP_NAME, SUB_SYSTEM)\n\n# Add coralogix logger as a handler to the standard Python logger.\nlogger.addHandler(coralogix_handler)\n\n# Send message\nlogger.info(\"Hello World!\")\n</code></pre> <p>Also, you can configure the SDK with <code>dictConfig</code> for <code>Python</code> <code>logging</code> library:</p> <pre><code>import logging\n\nPRIVATE_KEY = '[YOUR_PRIVATE_KEY_HERE]'\nAPP_NAME = '[YOUR_APPLICATION_NAME]'\nSUB_SYSTEM = '[YOUR_SUBSYSTEM_NAME]'\n\nlogging.config.dictConfig({\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'default': {\n            'format': '[%(asctime)s]: %(levelname)s: %(message)s',\n        }\n    },\n    'handlers': {\n        'coralogix': {\n            'class': 'coralogix.handlers.CoralogixLogger',\n            'level': 'DEBUG',\n            'formatter': 'default',\n            'private_key': PRIVATE_KEY,\n            'app_name': APP_NAME,\n            'subsystem': SUB_SYSTEM,\n        }\n    },\n    'root': {\n        'level': 'DEBUG',\n        'handlers': [\n            'coralogix',\n        ]\n    },\n    'loggers': {\n        'backend': {\n            'level': 'DEBUG',\n            'handlers': [\n                'coralogix',\n            ]\n        }\n    }\n})\n</code></pre>"},{"location":"reference/sdk/python/#configuration-under-uwsgi","title":"Configuration under uWSGI","text":"<p>By default <code>uWSGI</code> does not enable threading support within the Python interpreter core. This means it is not possible to create background threads from <code>Python</code> code. As the Coralogix logger relies on being able to create background threads (for sending logs), this option is required.</p> <p>You can enable threading either by passing --enable-threads to uWSGI command line:</p> <p>.. code-block:: bash</p> <pre><code>$ uwsgi wsgi.ini --enable-threads\n</code></pre> <p>Another option is to enable threads in your wsgi.ini file:</p> <p>wsgi.ini:</p> <p>.. code-block:: python</p> <pre><code>...\nenable-threads = true\n...\n</code></pre>"}]}